{"posts":[{"title":"计算机网络","text":"*OSI七层和作用 **访问网页全过程 首先浏览器对 URL 进行解析，生成HTTP请求报文，包括方法、数据源路径、版本、消息头、消息体。 根据 URL 的域名通过 DNS 协议获取对应 IP地址（DNS 查找过程：浏览器缓存、路由器缓存、DNS 缓存） 根据 IP 地址+端口号，发送 TCP 连接请求建立连接。生成 TCP 报文（若HTTP 报文超出 MSS，需进行分割） 生成IP报文封装源 IP、目标 IP。 根据目标 IP 查找路由表（路由选择协议）得到下一跳 IP，再利用ARP获取对应 MAC 地址，生成 MAC 报文。 网卡驱动程序加上报头和帧校验序列FCS将数字信号转换为电信号传输 路由器对包接收，进行帧校验，检查 MAC 头部中的接收方 MAC 地址是否是自己，若是则查找路由表寻找下一跳路由器IP及其MAC 地址，封装包并转发，重复直至将数据包传输到服务器。 服务器依次拆解报文，得到 HTTP 请求报文，返回封装网页的HTTP 响应报文，重复刚刚的流程发送给浏览器。 浏览器根据 HTTP 响应报文，解析响应体中的 HTML 代码，渲染网页的结构和样式，根据 HTML 中的其他资源的 URL，再次发起 HTTP 请求，获取这些资源 浏览器在不需要和服务器通信时，可以主动关闭 TCP 连接，或者等待服务器的关闭请求 应用层 DNS（Domain Name System）： 解决域名和 IP 地址的映射 域名解析的工作流程 HTTP：根据 DNS获取的目标主机的 IP 发送 HTTP 报文 传输层 HTTP 基于 TCP，数据要经过这俩个协议的封装 网络层 应用层、传输层是端到端协议；网络层是中间件协议，主机与中间系统进行交互。 网络层的核心功能：路由选择和分组转发 路由选择：确定分组从源到目的最优路径的过程 分组转发：将分组从路由器的输入端口转移到合适的输出端口 往哪里传输？或者说，要把数据包发到哪个路由器上？（怎么路由） 根据报文的目标 IP 地址跟路由表每个表项的掩码字段做“与”操作，判断是否匹配该表项的目标 IP 地址。 匹配完所有的表项选择掩码最长的匹配项，根据该表项的出接口和下一跳路由 IP将报文转发；若没有匹配到，则查找是否有缺省路由；若都没有，则丢弃报文 路由表项怎么来的？ direct：链路层发现，优点：自动发现，开销小。缺点：只发现接口所属网段 static：静态路由，需要人为调整 缺省路由 动态路由：RIP（Routing Information Protocol使用跳数作为路径距离）、OSPF（Open Shortest Path First链路开销值判断路径长短）和 BGP（Border Gateway Protocol边界网关协议，在路由选择域之间交换网络层可达性信息） HTTP *HTTP 常见状态码「200 OK」请求已正常处理。 「204 No Content」请求处理成功，但响应头没有 body 数据。 「206 Partial Content」表示响应返回的 body 数据并不是资源的全部，而是其中的一部分 「301 Moved Permanently」 永久性重定向 「302 Found」临时性重定向 「304 Not Modified」不具有跳转的含义，表示资源未修改 「400 Bad Request」表示客户端请求的报文有错误，但只是个笼统的错误 「403 Forbidden」服务器禁止访问资源。权限，未授权IP等 「404 Not Found」服务器上没有请求的资源。路径错误 「500 Internal Server Error」服务器发生错误，是个笼统通用的错误码 「501 Not Implemented」表示客户端请求的功能还不支持 「502 Bad Gateway」网关错误 后端服务器故障（ping、telnet、curl） nginx配置问题（nginx.conf、查看日志） 高负载或者资源耗尽（top） nginx与后端服务器通信问题（查看防火墙） 「503 Service Unavailable」表示服务器当前很忙，暂时无法响应客户端 「504 Gateway timeout」 网关超时 一般指nginx做反向代理服务器时，所连接的服务器tomcat无响应导致的。 为了完成您的 HTTP 请求，该服务器访问一个上游服务器，但没得到及时的响应 nginx超过了自己设置的超时时间 **HTTP与HTTPS区别 HTTP 是超文本传输协议，信息是明文传输，存在安全隐患。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 默认端口号不一样，HTTP：80 HTTPS：443 HTTP 连接只需要 TCP 三次握手；HTTPS 连接还需要 SSL/TLS 四次握手，进入加密报文传输，传输内容对称加密，但对称加密的密钥是用服务器方的证书进行非对称加密 url 前缀：http:// https// SEO（搜索引擎优化）：搜索引擎通常会更青睐使用 HTTPS 协议的网站 SSL/TLS解决了 HTTP 数据透明的问题 工作原理流程 非对称加密 SSL/TLS 的核心要素是非对称加密。非对称加密采用两个密钥——一个公钥，一个私钥。公钥加密的内容，使用私钥可以解开；而私钥加密的内容，公钥可以解开。目前使用最为广泛的非对称密钥为RSA算法。 对称加密 使用 SSL/TLS 进行通信的双方需要使用非对称加密方案来通信，但是非对称加密设计了较为复杂的数学算法，在实际通信过程中，计算的代价较高，效率太低，因此，SSL/TLS 实际对消息的加密使用的是对称加密。 hash算法加密 它是一种不可逆的加密方式，对一组数据使用哈希算法加密，加密后不能解密 保证公钥传输的信赖性（*数字签名&amp;数字证书） CA 发放证书 服务器把证书内容给 CA，利用 hash算法 +私钥生成签名。 数字证书=证书内容+证书签名 服务器发送数字证书给客户端 客户端验证证书 hash算法加密证书内容，CA 公钥解密证书签名，判断是否相同 HTTP 为什么基于 TCP 协议HTTP协议基于TCP协议是出于对数据传输可靠性和完整性的需求，TCP协议提供了数据传输的可靠性和可控性，而HTTP协议定义了数据的格式和意义，两者协同工作来实现Web应用的各种功能。 HTTP 和 RPC 有什么区别 RPC，因为它定制化程度更高，可以采用体积更小的 Protobuf 或其他序列化协议去保存结构体数据，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的。因此性能也会更好一些，这也是在公司内部微服务中抛弃 HTTP，选择使用 RPC 的最主要原因。 HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。 **HTTP/1.1 相比 HTTP/1.0 提高了什么性能？ 连接方式 : HTTP/1.0 为短连接，HTTP/1.1 支持长连接。 状态响应码 : HTTP/1.1 中新加入了大量的状态码，光是错误响应状态码就新增了 24 种。 缓存处理 : 在 HTTP1.0 中主要使用 header 里的 If-Modified-Since,Expires 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，如If-Unmodified-Since 带宽优化及网络连接的使用：HTTP1.0 中，存在一些浪费带宽的现象，HTTP1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是 206（Partial Content） Host 头处理 : HTTP/1.1 在请求头中加入了Host字段（域名系统（DNS）允许多个主机名绑定到同一个 IP 地址上，但是 HTTP/1.0 并没有考虑这个问题） HTTP/2 做了什么优化？多路复用（Multiplexing）：串行方式（每个请求和响应都需要独立的连接）变为同一连接上可以同时传输多个请求和响应。使 HTTP2更加高效。 二进制帧（Binary Frames）：文本格式的报文变为二进制帧。二进制帧更加紧凑和高效，减少了传输的数据量和带宽消耗。 头部压缩（Header Compression）：HTTP/1.1 支持Body压缩，Header不支持压缩。HTTP/2.0 支持对Header压缩，使用了专门为Header压缩而设计的 HPACK 算法，减少了网络开销。 服务器推送（Server Push）：HTTP/2.0 支持服务器推送，可以在客户端请求一个资源时，将其他相关资源一并推送给客户端，从而减少了客户端的请求次数和延迟。而 HTTP/1.1 需要客户端自己发送请求来获取相关资源。 HTTP/3 做了哪些优化？传输协议：HTTP/2.0 是基于 TCP 协议实现的，HTTP/3.0 新增了 QUIC（Quick UDP Internet Connections） 协议来实现可靠的传输，提供与 TLS/SSL 相当的安全性，具有较低的连接和传输延迟。你可以将 QUIC 看作是 UDP 的升级版本，在其基础上新增了很多功能比如加密、重传等等。 连接建立：HTTP/2.0 需要经过经典的 TCP 三次握手过程（由于安全的 HTTPS 连接建立还需要 TLS 握手，共需要大约 3 个 RTT）。由于 QUIC 协议的特性（TLS 1.3，TLS 1.3 除了支持 1 个 RTT 的握手，还支持 0 个 RTT 的握手）连接建立仅需 0-RTT 或者 1-RTT。这意味着 QUIC 在最佳情况下不需要任何的额外往返时间就可以建立新连接。 队头阻塞：HTTP/2.0 多请求复用一个 TCP 连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。由于 QUIC 协议的特性，HTTP/3.0 在一定程度上解决了队头阻塞（Head-of-Line blocking, 简写：HOL blocking）问题，一个连接建立多个不同的数据流，这些数据流之间独立互不影响，某个数据流发生丢包了，其数据流不受影响（本质上是多路复用+轮询）。 错误恢复：HTTP/3.0 具有更好的错误恢复机制，当出现丢包、延迟等网络问题时，可以更快地进行恢复和重传。而 HTTP/2.0 则需要依赖于 TCP 的错误恢复机制。 安全性：HTTP/2.0 和 HTTP/3.0 在安全性上都有较高的要求，支持加密通信，但在实现上有所不同。HTTP/2.0 使用 TLS 协议进行加密，而 HTTP/3.0 基于 QUIC 协议，包含了内置的加密和身份验证机制，可以提供更强的安全性。 TCPTCP 基础TCP 头部格式 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。 控制位： ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。 RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。 SYN：该位为 1 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。 FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位为 1 的 TCP 段。 为什么需要 TCP 协议？ TCP 工作在哪一层？网络层不可靠，如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。 什么是 TCP 连接？用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接 建立一个 TCP 连接是需要客户端与服务端达成上述三个信息的共识。 Socket：由 IP 地址和端口号组成 序列号：用来解决乱序问题等 窗口大小：用来做流量控制 如何唯一确定一个 TCP 连接呢？TCP 四元组可以唯一的确定一个连接，四元组包括如下： 源地址 源端口 目的地址 目的端口 源地址和目的地址的字段（32 位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。 源端口和目的端口的字段（16 位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。 UDP 包大小 原理上，UDP 包长度 16 位，UDP 包的大小为 2^16-1，即 65535 字节 以太网(Ethernet)数据帧的长度必须在==46-1500==字节之间,这是由以太网的物理特性决定的.这个1500字节被称为链路层的MTU(最大传输单元). 但这并不是指链路层的长度被限制在1500字节,其实这这个MTU指的是链路层的数据区.并不包括链路层的首部和尾部的18个字节.又因为UDP数据报的首部8字节,所以UDP数据报的数据区最大长度为1472字节（1500-20-8）. 但在网络编程中，Internet中的路由器可能有设置成不同的值(小于默认值)，鉴于Internet上的标准MTU值为==576==字节,所以我建议在进行Internet的UDP编程时. 最好将UDP的数据长度控件在548字节(576-20-8)以内. 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？ MTU：一个网络包的最大长度，以太网中一般为 1500 字节； MSS：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度； 因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传。因此，可以得知由 IP 层进行分片传输，是非常没有效率的。所以，为了达到最佳的传输效能 TCP 协议在建立连接的时候通常要协商双方的 MSS 值，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。经过 TCP 层分片后，如果一个 TCP 分片丢失后，进行重发时也是以 MSS 为单位，而不用重传所有的分片，大大增加了重传的效率。 **UDP 和 TCP 有什么区别呢？分别的应用场景是？TCP 和 UDP 区别： 1. 面向连接 2. 服务对象（一对一；一对一，一对多，多对多） 3. 可靠性 可靠交付数据，数据可以无差错、不丢失、不重复、按序到达； 尽最大努力交付，不保证可靠交付数据 4. 拥塞控制、流量控制 5. 首部开销（&gt;=20B，是否使用「选项」字段；8B） 6. 传输方式（流式传输，没有边界；一个包一个包的发送，是有边界的） 7. 分片不同 TCP 的数据大小如果大于 MSS （Maximum Segment Size）大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。 UDP 的数据大小如果大于 MTU （Maximum Transmit Unit）大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。 TCP 和 UDP 应用场景： 由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于： FTP 文件传输； HTTP / HTTPS； 远程登录 由于 UDP 面向无连接，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS 等； 语音、电话、视频； TCP 和 UDP 可以使用同一个端口吗？答案：可以的。传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。因此，TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。 TCP 和 UDP 可以同时绑定相同的端口吗？ 可以的。 TCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。 当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。 因此， TCP/UDP 各自的端口号也相互独立，互不影响。 多个 TCP 服务进程可以同时绑定同一个端口吗？ 如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。 如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行 bind() 不会出错。 如何解决服务端重启时，报错“Address already in use”的问题？ 当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。 当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 要解决这个问题，我们可以对 socket 设置 ==SO_REUSEADDR== 属性。 这样即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。 客户端的端口可以重复使用吗？ 在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。 TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。 所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？ 要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？ 打开 net.ipv4.tcp_tw_reuse 这个内核参数。 因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态。 如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。 **TCP连接/断开TCP 三次握手过程是怎样的？ ==第三次握手是可以携带数据的，前两次握手是不可以携带数据的== 如何在 Linux 系统中查看 TCP 状态？TCP 的连接状态查看，在 Linux 可以通过 netstat -napt(p—pid，t—tcp，u—udp) 命令查看。 为什么要三次握手？不是两次或者四次SYN 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement）消息响应 三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 TCP 连接：用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接。 Socket是由IP地址和端口结合的，提供向应用层进程传送数据包的机制。 不使用「两次握手」和「四次握手」的原因： 「两次握手」：无法防止历史连接的建立（主要原因），会造成双方资源的浪费，也无法可靠的同步双方序列号； 「四次握手」：三次握手就已经理论建立可靠连接，所以不需要使用更多的通信次数。 什么是 SYN 攻击？如何避免 SYN 攻击？假设攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会占满服务端的半连接队列，使得服务端不能为正常用户服务。 避免 SYN 攻击方式，可以有以下方法： 增大 TCP 半连接队列 减少 SYN+ACK 重传次数 TCP 四次挥手过程是怎样的？ 为什么挥手需要四次？再来回顾下四次挥手双方发 FIN 包的过程，就能理解为什么需要四次了。 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务端收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，因此是需要四次挥手。 为什么第四次挥手客户端需要等待 2*MSL（报文段最长寿命） 确保服务端收到客户端的 ack 确保上一次的 tcp 报文信息不会影响下一次的 tcp 连接 TIME_WAIT 过多有什么危害？ 占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等； 占用端口资源，端口资源也是有限的 服务器出现大量 TIME_WAIT 状态的原因有哪些？ 第一个场景：HTTP 没有使用长连接 第二个场景：HTTP 长连接超时 第三个场景：HTTP 长连接的请求数量达到上限 服务器出现大量 CLOSE_WAIT 状态的原因有哪些？当服务端出现大量 CLOSE_WAIT 状态的连接的时候，通常都是代码的问题，这时候我们需要针对具体的代码一步一步的进行排查和定位，主要分析的方向就是服务端为什么没有调用 close。 **TCP 如何保证传输的可靠性？ 序列号（对失序数据包重排以及去重） 校验和 重传机制（超时重传；快速重传） 流量控制 拥塞控制 滑动窗口假如发送一个数据包，要等待 ack 才发送下一个，效率太低。 引入窗口，指定窗口大小，指无需等待确认应答，而可以继续发送数据的最大值。 TCP 如何实现流量控制？TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 TCP 为全双工(Full-Duplex, FDX)通信，双方可以进行双向通信，客户端和服务端既可能是发送端又可能是服务端。因此，两端各有一个发送缓冲区与接收缓冲区，两端都各自维护一个发送窗口和一个接收窗口。 TCP 发送窗口可以划分成四个部分： 已经发送并且确认的 TCP 段（已经发送并确认）； 已经发送但是没有确认的 TCP 段（已经发送未确认）； 未发送但是接收方准备接收的 TCP 段（可以发送）； 未发送并且接收方也并未准备接受的 TCP 段（不可发送） TCP 接收窗口可以划分成三个部分： 已经接收并且已经确认的 TCP 段（已经接收并确认）； 等待接收且允许发送方发送 TCP 段（可以接收未确认）； 不可接收且不允许发送方发送 TCP 段（不可接收）。 窗口关闭如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。 只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 TCP 的拥塞控制是怎么实现的？拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。 为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。 拥塞控制主要是四个算法: 慢启动：当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。指数性的增长 拥塞避免：每当收到一个 ACK 时，cwnd 增加 1/cwnd。线性增长 快速重传：当接收方发现丢了一个中间包的时候，重复3 次发送前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。这种情况下网络阻塞不严重，将ssthresh慢开始门限设为cwnd/2，cwnd 重置为ssthresh，进入快速恢复算法。 快速恢复：由于发送方现在认为网络很可能没有发生拥塞（如果网络发生了严重拥塞，就不会一连有好几个报文段连续到达接收方，也就不会导致接收方连续发送重复确认）。将 cwnd 变为 ssthresh，开始拥塞避免 如何理解是 TCP 面向字节流协议？如何理解字节流？ 先来说说为什么 UDP 是面向报文的协议？ 当用户消息通过 UDP 协议传输时，操作系统不会对消息进行拆分，也就是每个 UDP 报文就是一个用户消息的边界 再来说说为什么 TCP 是面向字节流的协议？ 当用户消息通过 TCP 协议传输时，消息可能会被操作系统分组成多个的 TCP 报文，也就是一个完整的用户消息被拆分成多个 TCP 报文进行传输。我们不能认为一个用户消息对应一个 TCP 报文，正因为这样，所以 TCP 是面向字节流的协议。 粘包/拆包因为TCP是面向流，没有边界，而操作系统在发送TCP数据时，会通过缓冲区来进行优化，例如缓冲区为1024个字节大小。 如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送，这就形成了粘包问题。 如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包。 解决： 粘包：固定包的长度，不足用 0 填充 拆包：特殊字符作为边界（FTP 协议） TCP 协议有什么缺陷？ 升级 TCP 的工作很困难； TCP 协议是在内核中实现的，应用程序只能使用不能修改，如果要想升级 TCP 协议，那么只能升级内核。内核升级困难。 TCP 建立连接的延迟； 现在大多数网站都是使用 HTTPS 的，这意味着在 TCP 三次握手之后，还需要经过 TLS 四次握手后，才能进行 HTTP 数据的传输，这在一定程序上增加了数据传输的延迟。 TCP 存在队头阻塞问题； TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据。 网络迁移需要重新建立 TCP 连接； 基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接。 如何基于 UDP 协议实现可靠传输？市面上已经有基于 UDP 协议实现的可靠传输协议的成熟方案了，那就是 QUIC 协议，已经应用在了 HTTP/3。 TCP 四次挥手，可以变成三次吗？当被动关闭方（上图的服务端）在 TCP 挥手过程中，「没有数据要发送」并且「开启了 TCP 延迟确认机制」（默认开启），那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。 什么是 TCP 延迟确认机制？ 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK TCP 序列号和确认号是如何变化的？ 公式一：序列号 = 上一次发送的序列号 + len（数据长度）。特殊情况，如果上一次发送的报文是 SYN 报文或者 FIN 报文，则改为 上一次发送的序列号 + 1。 公式二：确认号 = 上一次收到的报文中的序列号 + len（数据长度）。特殊情况，如果收到的是 SYN 报文或者 FIN 报文，则改为上一次收到的报文中的序列号 + 1。 TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？HTTP 的 Keep-Alive 也叫 HTTP 长连接，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。 TCP 的 Keepalive 也叫 TCP 保活机制，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接。 ARP 协议详解(网络层)ARP 协议工作时有一个大前提，那就是 ARP 表。 在一个局域网内，每个网络设备都自己维护了一个 ARP 表，ARP 表记录了某些其他网络设备的 IP 地址-MAC 地址映射关系，该映射关系以 &lt;IP, MAC, TTL&gt; 三元组的形式存储。 ARP 的工作原理将分两种场景讨论： 同一局域网内的 MAC 寻址； 从一个局域网到另一个局域网中的网络设备的寻址。 工作原理：ARP 表、广播问询、单播响应","link":"/2024/03/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"面经","text":"3/13 芒星面经 策略模式跟状态模式的区别 黑盒测试、白盒测试 一个项目的研发过程 进程有哪几个状态 如何用 ArrayList 写一个阻塞队列 策略模式封装的是行为，而状态模式封装的是变化。策略是外界给的，策略怎么变，是调用者考虑的事情，系统只是根据所给的策略做事情。状态是系统自身的固有的，由系统本身控制，调用者不能直接指定或改变系统的状态转移。 黑盒测试也称：数据驱动测试，包括功能测试和性能测试。白盒测试也称：逻辑驱动测试，包括语句覆盖、判定覆盖、条件覆盖、路径覆盖等。判定某种方法是否为黑盒测试方法，关键还是看是否针对被测对象内部结构还是针对被测对象的整体进行测试。 策略（Strategy）模式的定义：该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于行为模式。 3/20 腾讯 pcg 一面 os 内核级别 shell 脚本 if-else 如何实现 mysql 如何查看死锁 array 跟 arraylist 的区别 innodb 跟 myisam 的读写性能区别 Linux一般会有7个运行级别（可由init N来切换，init0为关机，init 6为重启系统）0 - 停机1 - 单用户模式2 - 多用户，但是没有NFS ，不能使用网络3 - 完全多用户模式4 - 打酱油的，没有用到5 - X11 图形化登录的多用户模式6 - 重新启动 （如果将默认启动模式设置为6，Linux将会不断重启） 是否死锁 1234567891011121314151617181920use ***db;show ENGINE INNODB STATUS;其他方法：1、查询是否锁表show OPEN TABLES where In_use &gt; 0;2、查询进程show processlist (查询到相对应的进程===然后 kill id )3、查看正在锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 4、查看等待锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; Linux查看内存 12free -hcat /proc/meminfo 查看 cpu 信息 12lscpucat /proc/cpuinfo linux查看tcp的状态命令： netstat -nat 查看TCP各个状态的数量 lsof -i:port 可以检测到打开套接字的状况 sar -n SOCK 查看tcp创建的连接数 tcpdump -iany tcp port 9000 对tcp端口为9000的进行抓包 Kafka为什么 redis Pub/Sub 比 kafka 更快一些？二者如何选取Redis是一个内存数据库，其Pub/Sub功能将消息保存在内存中。由于内存访问速度通常远快于磁盘访问速度，因此Redis在处理实时性较高的消息推送时具有优势；Redis的Pub/Sub模型相对简单，使得它在处理发布和订阅操作时的开销较小。 Kafka是一个完整的系统，提供了高吞吐量、分布式的提交日志。它旨在处理大规模数据流，具有强大的持久化能力和容错性。Kafka的分布式架构和分区机制使得它能够在多个消费者之间实现负载均衡，从而提高整体处理能力。 Redis PUB/SUB使用场景： 消息持久性需求不高 吞吐量要求不高 可以忍受数据丢失 数据量不大 Kafka使用场景：(上面以外的其他场景) 高可靠性 高吞吐量 持久性高 多样化的消费处理模型","link":"/2024/03/11/%E9%9D%A2%E7%BB%8F/"},{"title":"Java基础","text":"基础概念JVM vs JDK vs JREJava 虚拟机（JVM）是运行 Java 字节码的虚拟机。 JDK（Java Development Kit），它是功能齐全的 Java SDK，是提供给开发者使用，能够创建和编译 Java 程序的开发套件。 JRE（Java Runtime Environment） 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，主要包括 Java 虚拟机（JVM）、Java 基础类库（Class Library）。 什么是字节码?采用字节码的好处是什么?在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。 Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。 为什么说 Java 语言“编译与解释并存”？编译型语言：执行效率高、开发效率低 解释型语言：开发效率高、执行效率低 Java语言需要经过编译(源码经过 javac 编译变成字节码)、解释（字节码经过 解释器和 JIT(即时编译器)变成机器码） AOT 有什么优点？为什么不全部使用 AOT 呢？AOT(ahead of time Compilation)。和 JIT 不同的是，这种编译模式会在程序被执行前就将其编译成机器码，属于静态编译（C、 C++，Rust，Go 等语言就是静态编译） 优点：启动速度、内存占用、打包体积。 缺点：极限处理能力不如 JIT AOT 不能使用反射、动态代理、动态加载，常用框架和库（spring，CGLIB）都需要这些特性质。 基本数据类型基本类型和包装类型的区别？ 基本类型 vs 包装类型 用途： 基本类型：定义常量和局部变量 包装类型：方法参数、对象属性 包装类型可用于泛型，而基本类型不可以。 存储方式： 基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中（类的实例），被static 修饰的存放在 Java 虚拟机的堆中（Class对象）。 包装类型属于对象类型，几乎所有对象实例都存在于堆中。 占用空间：相比于包装类型（对象类型）， 基本数据类型占用的空间往往非常小。 默认值：成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null。 比较方式：对于基本数据类型来说，== 比较的是值。对于包装数据类型来说，== 比较的是对象的内存地址。所有整型包装类对象之间值的比较，全部使用 equals() 方法。 为什么说是几乎所有对象实例都存在于堆中呢？这是因为 HotSpot 虚拟机引入了 JIT 优化之后，会对对象进行逃逸分析，如果发现某一个对象并没有逃逸到方法外部，那么就可能通过标量替换来实现栈上分配，而避免堆上分配内存不充部分 基本数据类型是否都存放在栈中？基本数据类型的存储位置取决于它们的作用域和声明方式。如果它们是局部变量，那么它们会存放在栈中；如果它们是成员变量，那么它们会存放在堆中(是否被 static 修饰都是在堆中) 包装类型的缓存机制了解么？作用：提升性能 Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。 Character 缓存源码: 1234567891011121314public static Character valueOf(char c) { if (c &lt;= 127) { // must cache return CharacterCache.cache[(int)c]; } return new Character(c);}private static class CharacterCache { private CharacterCache(){} static final Character cache[] = new Character[127 + 1]; static { for (int i = 0; i &lt; cache.length; i++) cache[i] = new Character((char)i); }} 如果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。 两种浮点数类型的包装类 Float,Double 并没有实现缓存机制。 自动装箱与拆箱了解吗？原理是什么？什么是自动拆装箱？ 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 装箱其实就是调用了 包装类的valueOf()方法，拆箱其实就是调用了 xxxValue()方法。 因此， Integer i = 10 等价于 Integer i = Integer.valueOf(10) int n = i 等价于 int n = i.intValue(); 注意：如果频繁拆装箱的话，也会严重影响系统的性能。我们应该尽量避免不必要的拆装箱操作。（常量、局部变量用基本数据类型） 为什么浮点数运算的时候会有精度丢失的风险？与计算机保存浮点数机制有关。表示一个数字时，宽度有限，无线循环的小数存储在计算机时只能被截断，所以导致精度丢失。 浮点型从二进制的视角是怎么存储的？符号位+指数位+尾号位 这32个二进制位的内存编号从高到低 (从31到0), 共包含如下几个部分: sign: 符号位, 即图中蓝色的方块 biased exponent: 偏移后的指数位（偏移量 127 保证指数非负数）, 即图中绿色的方块 fraction: 尾数位, 即图中红色的方块 （IEEE 754）小数位如何计算出来的？如果我们现在想用浮点数表示 0.2，它的结果会是多少呢？ 0.2 转换为二进制数的过程为，不断乘以 2，直到不存在小数为止，在这个计算过程中，得到的整数部分从上到下排列就是二进制的结果。 1234560.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 10.6 * 2 = 1.2 -&gt; 10.2 * 2 = 0.4 -&gt; 0（发生循环）... 所以 0.2(D) = 0.00110…(B)。 如何解决浮点数运算的精度丢失问题？BigDecimal 可以实现对浮点数的运算，不会造成精度丢失。通常情况下，大部分需要浮点数精确运算结果的业务场景（比如涉及到钱的场景）都是通过 BigDecimal 来做的。 超过 long 整型的数据应该如何表示？基本数值类型都有一个表达范围，如果超过这个范围就会有数值溢出的风险。 在 Java 中，64 位 long 整型是最大的整数类型。 123long l = Long.MAX_VALUE;System.out.println(l + 1); // -9223372036854775808System.out.println(l + 1 == Long.MIN_VALUE); // true BigInteger 内部使用 int[] 数组来存储任意大小的整形数据。 相对于常规整数类型的运算来说，BigInteger 运算的效率会相对较低。 变量成员变量与局部变量的区别？ 语法形式：成员变量属于类，局部变量属于代码块、方法定义变量和方法的参数；成员变量可以被访问控制修饰符以及static 修饰、局部变量不可以。 存储方式：成员变量存在于堆内存，局部变量则存在于栈内存。 生存时间：从变量在内存中的生存时间上看，成员变量——对象同步，而局部变量——方法同步。 默认值：从变量是否有默认值来看，成员变量如果没有被赋初始值，则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。 为什么成员变量有默认值？ 不考虑变量类型。若没有默认值，变量存储的是内存地址对应的任意随机值，即遗留值，可以会出现意外或信息泄露问题 对于编译器 javac 来说，局部变量没赋值很好判断，可以直接报错。而成员变量可能是运行时赋值，无法判断，误报“没默认值”影响用户体验，所以采用自动赋默认值 静态变量有什么作用？所有类的实例共用一份静态变量，节省内存 （若被 private 修饰就不能用类名.变量名访问） 通常情况下，静态变量会被 final 关键字修饰成为常量。 字符型常量和字符串常量的区别?形式 : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。 含义 : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)。 占内存大小：字符常量只占 2 个字节; 字符串常量占若干个字节。 ⚠️ 注意 char 在 Java 中占两个字节。 方法静态方法为什么不能调用非静态成员?在类的非静态成员不存在的时候静态方法就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。 静态方法和实例方法有何不同？ 调用方式： ​ 在外部调用静态方法时，可以使用 类名.方法名 的方式，也可以使用 对象.方法名 的方式，而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象 。 ​ 为了避免混淆建议静态方法使用前一种 访问类成员是否存在限制： ​ 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），不允许访问实例成员（即实例成员变量和实例方法），而实例方法不存在这个限制。 重载和重写有什么区别？重载 发生在同一个类中（或者父类和子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。 重写 重写发生在运行期，是子类对父类的允许访问的方法的实现过程进行重新编写。 方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等，抛出的异常范围小于等于父类（更精细），访问修饰符范围大于等于父类。 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明。 构造方法无法被重写 总结 综上：重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变。 方法的重写要遵循“两同两小一大”（以下内容摘录自《疯狂 Java 讲义》，issue#892open in new window ）： “两同”即方法名相同、形参列表相同； “两小”指的是子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等； “一大”指的是子类方法的访问权限应比父类方法的访问权限更大或相等。 ⭐️ 关于 重写的返回值类型 这里需要额外多说明一下，上面的表述不太清晰准确：如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的。 面相对象基础面向对象和面向过程的区别两者的主要区别在于解决问题的方式不同： 面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。 面向对象会先抽象出对象，然后用对象执行方法的方式解决问题。 另外，面向对象开发的程序一般更易维护、易复用、易扩展。 对象的相等和引用相等的区别 对象的相等一般比较的是内存中存放的内容是否相等。 引用相等一般比较的是他们指向的内存地址是否相等。 如果一个类没有声明构造方法，该程序能正确执行吗?构造方法是一种特殊的方法，主要作用是完成对象的初始化工作。 如果一个类没有声明构造方法，也可以执行！因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。如果我们自己添加了类的构造方法（无论是否有参），Java 就不会添加默认的无参数的构造方法了。 我们一直在不知不觉地使用构造方法，这也是为什么我们在创建对象的时候后面要加一个括号（因为要调用无参的构造方法）。如果我们重载了有参的构造方法，记得都要把无参的构造方法也写出来（无论是否用到），因为这可以帮助我们在创建对象的时候少踩坑。 构造方法有哪些特点？是否可被 override?构造方法特点如下： 名字与类名相同。 没有返回值，但不能用 void 声明构造函数。 生成类的对象时自动执行，无需调用。 构造方法不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。 面向对象三大特征封装 封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。但是可以提供一些可以被外界访问的方法来操作属性。 继承 继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承，可以快速地创建新的类，可以提高代码的重用，程序的可维护性，节省大量创建新类的时间 ，提高我们的开发效率。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。 多态 多态，顾名思义，表示一个对象具有多种的状态，具体表现为父类的引用指向子类的实例。 多态的特点: 对象类型和引用类型之间具有继承（类）/实现（接口）的关系； 引用类型变量发出的方法调用的到底是哪个类中的方法，必须在程序运行期间才能确定； 多态不能调用“只在子类存在但在父类不存在”的方法； 如果子类重写了父类的方法，真正执行的是子类覆盖的方法，如果子类没有覆盖父类的方法，执行的是父类的方法。 *接口和抽象类有什么共同点和区别？共同点： 都不能被实例化。 都可以包含抽象方法。 都可以有默认实现的方法（Java 8 可以用 default 关键字在接口中定义默认方法）。 区别： 接口主要用于对类的行为进行约束，你实现了某个接口就具有了对应的行为。抽象类主要用于代码复用，强调的是所属关系。 一个类只能继承一个类，但是可以实现多个接口。 接口中的成员变量只能是 public static final 类型的，不能被修改且必须有初始值，而抽象类的成员变量默认 default，可在子类中被重新定义，也可被重新赋值。 深拷贝和浅拷贝区别了解吗？什么是引用拷贝？关于深拷贝和浅拷贝区别，我这里先给结论： 浅拷贝：浅拷贝会在堆上创建一个新的对象（区别于引用拷贝的一点），不过，如果原对象内部的属性是引用类型的话，浅拷贝会直接复制内部对象的引用地址，也就是说拷贝对象和原对象共用同一个内部对象。 深拷贝：深拷贝会完全复制整个对象，包括这个对象所包含的内部对象。 ObjectObject 类的常见方法有哪些？1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * native 方法，用于返回当前运行时对象的 Class 对象，使用了 final 关键字修饰，故不允许子类重写。 */public final native Class&lt;?&gt; getClass()/** * native 方法，用于返回对象的哈希码，主要使用在哈希表中，比如 JDK 中的HashMap。 */public native int hashCode()/** * 用于比较 2 个对象的内存地址是否相等，String 类对该方法进行了重写以用于比较字符串的值是否相等。 */public boolean equals(Object obj)/** * native 方法，用于创建并返回当前对象的一份拷贝。 */protected native Object clone() throws CloneNotSupportedException/** * 返回类的名字实例的哈希码的 16 进制的字符串。建议 Object 所有的子类都重写这个方法。 */public String toString()/** * native 方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。 */public final native void notify()/** * native 方法，并且不能重写。跟 notify 一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 */public final native void notifyAll()/** * native方法，并且不能重写。暂停线程的执行。注意：sleep 方法没有释放锁，而 wait 方法释放了锁 ，timeout 是等待时间。 */public final native void wait(long timeout) throws InterruptedException/** * 多了 nanos 参数，这个参数表示额外时间（以纳秒为单位，范围是 0-999999）。 所以超时的时间还需要加上 nanos 纳秒。。 */public final void wait(long timeout, int nanos) throws InterruptedException/** * 跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念 */public final void wait() throws InterruptedException/** * 实例被垃圾回收器回收的时候触发的操作 */protected void finalize() throws Throwable { } Java值传递和引用传递你是怎么理解的？因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。 例子 类A有个方法f，传递参数为Node node，在方法内node = new Node();，这里会影响到外面的node吗？如果在方法内修改node的参数，他会影响到外面的node吗 == 和 equals() 的区别== 对于基本类型和引用类型的作用效果是不同的： 对于基本数据类型来说，== 比较的是值。 对于引用数据类型来说，== 比较的是对象的内存地址。 equals() 不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。equals()方法存在于Object类中，而Object类是所有类的直接或间接父类，因此所有的类都有equals()方法。 Object 类 equals() 方法： 123public boolean equals(Object obj) { return (this == obj);} equals() 方法存在两种使用情况： 类没有重写 equals()方法：通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象，使用的默认是 Object类equals()方法。 类重写了 equals()方法：一般我们都重写 equals()方法来比较两个对象中的属性是否相等；若它们的属性相等，则返回 true(即，认为这两个对象相等)。 hashCode() 有什么用？hashCode() 的作用是获取哈希码（int 整数），也称为散列码。这个哈希码的作用是确定该对象在哈希表中的索引位置。 hashCode() 定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是：Object 的 hashCode() 方法是本地方法，也就是用 C 语言或 C++ 实现的。 ⚠️ 注意：该方法在 Oracle OpenJDK8 中默认是 “使用线程局部状态来实现 Marsaglia’s xor-shift 随机数生成”, 并不是 “地址” 或者 “地址转换而来”, 不同 JDK/VM 可能不同在 Oracle OpenJDK8 中有六种生成方式 (其中第五种是返回地址), 通过添加 VM 参数: -XX:hashCode=4 启用第五种。 为什么要有 hashCode？在 HashMap 和 HashSet 中都需要用到 hashCode，以 HashSet 为例： ​ 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashCode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashCode 值作比较，如果没有相符的 hashCode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashCode 值的对象，这时会调用 equals() 方法来检查 hashCode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 其实， hashCode() 和 equals()都是用于比较两个对象是否相等。 那为什么 JDK 还要同时提供这两个方法呢？ 这是因为在一些容器（比如 HashMap、HashSet）中，有了 hashCode() 之后，判断元素是否在对应容器中的效率会更高 那为什么不只提供 hashCode() 方法呢？ 这是因为两个对象的hashCode 值相等并不代表两个对象就相等。 那为什么两个对象有相同的 hashCode 值，它们也不一定是相等的？ 因为 hashCode() 所使用的哈希算法也许刚好会让多个对象传回相同的哈希值。越糟糕的哈希算法越容易碰撞，但这也与数据值域分布的特性有关（所谓哈希碰撞也就是指的是不同的对象得到相同的 hashCode )。 总结下来就是： 如果两个对象的hashCode 值相等，那这两个对象不一定相等（哈希碰撞）。 如果两个对象的hashCode 值相等并且equals()方法也返回 true，我们才认为这两个对象相等。 如果两个对象的hashCode 值不相等，我们就可以直接认为这两个对象不相等。 为什么重写 equals() 时必须重写 hashCode() 方法？因为两个相等的对象的 hashCode 值必须是相等。也就是说如果 equals 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。 如果重写 equals() 时没有重写 hashCode() 方法的话就可能会导致 equals 方法判断是相等的两个对象，hashCode 值却不相等。 思考：重写 equals() 时没有重写 hashCode() 方法的话，使用 HashMap 可能会出现什么问题。 两个相同的对象加到 HashMap 中，对应的 hashCode 不同，但是 HashMap 是先判断 hashCode 是否相同来判断是否有重复 key，最终会导致 HashMap 存在两个相同的对象同时作为 Key，这与 HashMap 的 key 不可以重复相悖。 总结： equals 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。 两个对象有相同的 hashCode 值，他们也不一定是相等的（哈希碰撞）。 概念对象 Object：表示的是某一类事物的抽象的名词和概念，是对一类事物的抽象表示 类 Class：对象在计算机中的表示，如定义一个“人”的类 实例 Instance：根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 Oop：ordinary object point 对象的创建过程 申请空间，给成员变量赋默认值 调用 init 构造函数，给成员变量赋值 建立引用和对象的连接 单例模式饿汉式12345678910/** * 饿汉式单例模式 */public class singletonPattern01 { private static final singletonPattern01 SINGLE = new singletonPattern01(); private singletonPattern01(){}; public static singletonPattern01 getSingle(){ return SINGLE; }} 优点：这种写法比较简单，就是在类加载的时候就完成实例化。避免了线程同步问题。 缺点：在类加载的时候就完成实例化，没有达到Lazy Loading的效果。如果从未使用过这个实例，则会造成内存的浪费。 懒汉式起到了Lazy Loading的效果，但是只能在单线程下使用。 如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以 在多线程环境下不可使用这种方式。 结论：在实际开发中，不要使用这种方式. 1234567891011121314 * 懒汉式单例模式 */public class singletonPattern02 { private static singletonPattern02 SINGLE; private singletonPattern02(){ }; public static singletonPattern02 getInstance(){ if(SINGLE == null){ SINGLE = new singletonPattern02(); } return SINGLE; }} 双重检查锁（DCL double check lock）双重检查概念是多线程开发中常使用到的，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。 这样，实例化代码只会执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象，也避免的反复进行方法同步. 线程安全；延迟加载；效率较高 结论：在实际开发中，推荐使用这种单例设计模式 12345678910111213141516171819202122public class singletonPattern02 { private static volatile singletonPattern02 SINGLE; private singletonPattern02(){ }; public static singletonPattern02 getInstance(){ if(SINGLE == null){ synchronized (singletonPattern02.class){ if (SINGLE == null){ try{ Thread.sleep(1L); }catch (InterruptedException e) { e.printStackTrace(); } SINGLE = new singletonPattern02(); } } } return SINGLE; }} DCL要不要加 volatile 要。 假如 new singletonPattern02()时候发生指令重排序，先建立了连接，那么 SINGLE！=null ，多线程时候另一个线程就会直接 返回半初始化的对象。 所以说，这段代码要不要加volatile？必须加！加了volatile的这块内存，对于它的读写访问不可以重排序！ https://blog.csdn.net/zhaoyajie1011/article/details/106812327 静态内部类形式这种方式采用了类加载的机制来保证初始化实例时只有一个线程。 静态内部类方式在Singleton类被加载时并不会立即实例化，而是在需要实例化时，调用getSingleTon方法，才会加载Inner类，从而完成Singleton的实例化。 类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。 优点：避免了线程不安全，利用静态内部类特点实现延迟加载，效率高 结论：推荐使用. 1234567891011public class singletonPattern03 { private singletonPattern03(){}; private static class singletonHolder{ private static singletonPattern03 SINGLE = new singletonPattern03(); } public static singletonPattern03 getInstance(){ return singletonHolder.SINGLE; }} java中的引用类型的对象存放在哪里根据上下文来确定。 123456789void func(){ Object obj = new Object();//这个obj在函数的栈里。}class Test{ private Object obj = new Object();//这个obj随对应的Test对象分配在堆里} 对于方法中的局部变量的引用时存放在java运行时数据区的栈中 对于实例变量则是存放在java运行时数据区的堆中。 Class 实例究竟在 method area 还是在 heaphotspot使用了 OOP-KLASS 模型来表示 java 对象 main方法中：Object o = new Object(); jvm在加载class时，创建instanceKlass，表示其元数据，包括常量池、字段、方法等，存放在方法区；instanceKlass是jvm中的数据结构；（vm加载的字节码，也就是.class文件，被加载到方法区里面，叫Klass，是一个C++对象，含有类的信息、虚方法表等。） 在new一个对象时，jvm创建instanceOopDesc，来表示这个对象，存放在堆区，其引用，存放在栈区；它用来表示对象的实例信息，看起来像个指针实际上是藏在指针里的对象；instanceOopDesc对应java中的对象实例； HotSpot并不把instanceKlass暴露给Java，而会另外创建对应的instanceOopDesc来表示java.lang.Class对象，并将后者称为前者的“Java镜像”，klass持有指向oop引用(_java_mirror便是该instanceKlass对Class对象的引用)； 要注意，new操作返回的instanceOopDesc类型指针指向instanceKlass，而instanceKlass指向了对应的类型的Class实例的instanceOopDesc；有点绕，简单说，就是Person实例——&gt;Person的instanceKlass——&gt;Person的Class。 对象在内存中的存储布局instanceOopDesc，只包含数据信息，它包含三部分： Mark Word，主要存储对象运行时记录信息，如hashcode, GC分代年龄，锁状态标志，线程ID，时间戳等; （64 位 os 就是 64 位即 8 字节） 元数据指针，即指向方法区的instanceKlass实例（压缩钱 8 字节，压缩后 4 字节） 实例数据; （成员变量） 另外，如果是数组对象，还多了一个数组长度 对象如何定位直接使用直接指针访问，Java堆中对象的内存布局就必须考虑如何防止访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。 间接（句柄访问）使用句柄访问，Java堆中将可能会划分出一块内存用来作为句柄池，reference中寸的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息。 两种访问方式的优势句柄访问： 最大的好处是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要被修改。指针访问：最大的好处时速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本。 对象怎么分配 判断分配到栈上。 逃逸分析。没有发生逃逸的对象优先尝试在栈上分配 是否大。 jvm 调优 TLAB（Thread local allocation buffer）。线程本地分配缓存 多线程时，给各线程分配特定的空间 为什么 hotspot 不实用 c++对象来代表 java 对象c++对象有虚函数表，java 对象的虚函数表在 Class 对象中 StringString、StringBuffer、StringBuilder 的区别？可变性 String 是不可变的（后面会详细分析原因）。 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串，不过没有使用 final 和 private 关键字修饰，最关键的是这个 AbstractStringBuilder 类还提供了很多修改字符串的方法比如 append 方法。 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer String 为什么是不可变的?被 final 关键字修饰的类不能被继承，修饰的方法不能被重写，修饰的变量是基本数据类型则值不能改变，修饰的变量是引用类型则不能再指向其他对象。因此，final 关键字修饰的数组保存字符串并不是 String不可变的根本原因，因为这个数组保存的字符串是可变的（final 修饰引用类型变量的情况）。 String 真正不可变有下面几点原因： 保存字符串的数组被 final 修饰且为私有的，并且String 类没有提供/暴露修改这个字符串的方法。 String 类被 final 修饰导致其不能被继承，进而避免了子类破坏 String 不可变。 不可变的好处 这个最简单地原因，就是为了安全。 再看下面这个HashSet用StringBuilder做元素的场景，问题就更严重了，而且更隐蔽。 123456789101112131415class Test{ public static void main(String[] args){ HashSet&lt;StringBuilder&gt; hs=new HashSet&lt;StringBuilder&gt;(); StringBuilder sb1=new StringBuilder(\"aaa\"); StringBuilder sb2=new StringBuilder(\"aaabbb\"); hs.add(sb1); hs.add(sb2); //这时候HashSet里是{\"aaa\",\"aaabbb\"} StringBuilder sb3=sb1; sb3.append(\"bbb\"); //这时候HashSet里是{\"aaabbb\",\"aaabbb\"} System.out.println(hs); }}//Output://[aaabbb, aaabbb] StringBuilder型变量sb1和sb2分别指向了堆内的字面量“aaa”和”aaabbb”。把他们都插入一个HashSet。到这一步没问题。但如果后面我把变量sb3也指向sb1的地址，再改变sb3的值，因为StringBuilder没有不可变性的保护，sb3直接在原先”aaa”的地址上改。导致sb1的值也变了。这时候，HashSet上就出现了两个相等的键值”aaabbb”。破坏了HashSet键值的唯一性。所以千万不要用可变类型做HashMap和HashSet键值。 字符串拼接用“+” 还是 StringBuilder?Java 语言本身并不支持运算符重载，“+”和“+=”是专门为 String 类重载过的运算符，也是 Java 中仅有的两个重载过的运算符。 字符串对象通过“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 不过，在循环内使用“+”进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用，会导致创建过多的 StringBuilder 对象。 不过，使用 “+” 进行字符串拼接会产生大量的临时对象的问题在 JDK9 中得到了解决。在 JDK9 当中，字符串相加 “+” 改为了用动态方法 makeConcatWithConstants() 来实现，而不是大量的 StringBuilder 了。 String s1 = new String(“abc”);这句话创建了几个字符串对象？会创建 1 或 2 个字符串对象。 1、如果字符串常量池中不存在字符串对象“abc”的引用，那么它会在堆上创建两个字符串对象，其中一个字符串对象的引用会被保存在字符串常量池中。 示例代码（JDK 1.8）： 1String s1 = new String(\"abc\"); 对应的字节码： ldc 命令用于判断字符串常量池中是否保存了对应的字符串对象的引用，如果保存了的话直接返回，如果没有保存的话，会在堆中创建对应的字符串对象并将该字符串对象的引用保存到字符串常量池中。 2、如果字符串常量池中已存在字符串对象“abc”的引用，则只会在堆中创建 1 个字符串对象“abc”。 示例代码（JDK 1.8）： 1234// 字符串常量池中已存在字符串对象“abc”的引用String s1 = \"abc\";// 下面这段代码只会在堆中创建 1 个字符串对象“abc”String s2 = new String(\"abc\"); 对应的字节码： 这里就不对上面的字节码进行详细注释了，7 这个位置的 ldc 命令不会在堆中创建新的字符串对象“abc”，这是因为 0 这个位置已经执行了一次 ldc 命令，已经在堆中创建过一次字符串对象“abc”了。7 这个位置执行 ldc 命令会直接返回字符串常量池中字符串对象“abc”对应的引用。 *String#intern 方法有什么作用?String.intern() 是一个 native（本地）方法，其作用是将指定的字符串对象的引用保存在字符串常量池中，可以简单分为两种情况： 如果字符串常量池中保存了对应的字符串对象的引用，就直接返回该引用。 如果字符串常量池中没有保存了对应的字符串对象的引用，那就在常量池中创建一个指向该字符串对象的引用并返回。 应用场景使用方法 1234567public class Person{String name; public void setName(String paramString) { String str = paramString.intern(); }} 这里是一个能展现出inern()实际作用的场景,首先假设我从数据库里读了一个人的信息出来,然后把这个人的名字赋值给这个Person对象.那么,从数据库读数据,毫无疑问得创建一个字符串对象出来,假定读了10个人的数据,其中三个都叫小明,那么在不使用intern()的情况下,对字符串对象的引用情况如图所示 在使用intern的情况下,对字符串对象的引用情况如图所示 很显然,剩下的那两个小明字符串对象,就都可以回收了,大大节省空间. String 类型的变量和常量做“+”运算时发生了什么？ 先来看字符串不加 final 关键字拼接的情况（JDK1.8）： 12345678String str1 = \"str\";String str2 = \"ing\";String str3 = \"str\" + \"ing\";String str4 = str1 + str2;String str5 = \"string\";System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 在编译过程中，Javac 编译器（下文中统称为编译器）会进行一个叫做 常量折叠(Constant Folding) 的代码优化。 常量折叠会把常量表达式的值求出来作为常量嵌在最终生成的代码中，这是 Javac 编译器会对源代码做的极少量优化措施之一(代码优化几乎都在即时编译器中进行)。 对于 String str3 = \"str\" + \"ing\"; 编译器会给你优化成 String str3 = \"string\"; 。 并不是所有的常量都会进行折叠，只有编译器在程序编译期就可以确定值的常量才可以： 基本数据类型( byte、boolean、short、char、int、float、long、double)以及字符串常量。 final 修饰的基本数据类型和字符串变量 字符串通过 “+”拼接得到的字符串、基本数据类型之间算数运算（加减乘除）、基本数据类型的位运算（&lt;&lt;、&gt;&gt;、&gt;&gt;&gt; ） 引用的值在程序编译期是无法确定的，编译器无法对其进行优化。 对象引用和“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 我们在平时写代码的时候，尽量避免多个字符串对象拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 不过，字符串使用 final 关键字声明之后，可以让编译器当做常量来处理。 示例代码： 123456final String str1 = \"str\";final String str2 = \"ing\";// 下面两个表达式其实是等价的String c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 常量池中的对象System.out.println(c == d);// true 被 final 关键字修饰之后的 String 会被编译器当做常量来处理，编译器在程序编译期就可以确定它的值，其效果就相当于访问常量。 如果 ，编译器在运行时才能知道其确切值的话，就无法对其优化。 示例代码（str2 在运行时才能确定其值）： 12345678final String str1 = \"str\";final String str2 = getStr();String c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 在堆上创建的新的对象System.out.println(c == d);// falsepublic static String getStr() { return \"ing\";} 异常Exception 和 Error 有什么区别？在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类: Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理)。 Error：Error 属于程序无法处理的错误 ，我们不建议通过catch捕获 。例如 Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止 Checked Exception 和 Unchecked Exception 有什么区别？Checked Exception 即 受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 catch或者throws关键字处理的话，就没办法通过编译。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有：IO 相关的异常、ClassNotFoundException、SQLException…。 Unchecked Exception 即 不受检查异常 ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。 RuntimeException 及其子类都统称为非受检查异常，常见的有（建议记下来，日常开发中会经常用到）： NullPointerException(空指针错误) IllegalArgumentException(参数错误比如方法入参类型错误) NumberFormatException（字符串转换为数字格式错误，IllegalArgumentException的子类） ArrayIndexOutOfBoundsException（数组越界错误） ClassCastException（类型转换错误） ArithmeticException（算术错误） SecurityException （安全错误比如权限不够） UnsupportedOperationException(不支持的操作错误比如重复创建同一用户) Throwable 类常用方法有哪些？ String getMessage(): 返回异常发生时的简要描述 String toString(): 返回异常发生时的详细信息 String getLocalizedMessage(): 返回异常对象的本地化信息。使用 Throwable 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 getMessage()返回的结果相同 void printStackTrace(): 在控制台上打印 Throwable 对象封装的异常信息 try-catch-finally 如何使用？ try块：用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块：用于处理 try 捕获到的异常。 finally 块：无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。 注意：不要在 finally 语句块中使用 return! 当 try 语句和 finally 语句中都有 return 语句时，try 语句块中的 return 语句会被忽略。这是因为 try 语句中的 return 返回值会先被暂存在一个本地变量中，当执行到 finally 语句中的 return 之后，这个本地变量的值就变为了 finally 语句中的 return 返回值。 123456789101112131415public static void main(String[] args) { System.out.println(f(2));}public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } }}输出0 finally 中的代码一定会执行吗？不一定的！在某些情况下，finally 中的代码不会被执行。 finally 之前虚拟机被终止运行的话，finally 中的代码就不会被执行。 程序所在的线程死亡。 关闭 CPU。 如何使用 try-with-resources 代替try-catch-finally？适用范围（资源的定义）： 任何实现 java.lang.AutoCloseable或者 java.io.Closeable 的对象 关闭资源和 finally 块的执行顺序： 在 try-with-resources 语句中，任何 catch 或 finally 块在声明的资源关闭后运行 Java 中类似于InputStream、OutputStream、Scanner、PrintWriter等的资源都需要我们调用close()方法来手动关闭，一般情况下我们都是通过try-catch-finally语句来实现这个需求，如下： 1234567891011121314//读取文本文件的内容Scanner scanner = null;try { scanner = new Scanner(new File(\"D://read.txt\")); while (scanner.hasNext()) { System.out.println(scanner.nextLine()); }} catch (FileNotFoundException e) { e.printStackTrace();} finally { if (scanner != null) { scanner.close(); }} 使用 Java 7 之后的 try-with-resources 语句改造上面的代码: 1234567try (Scanner scanner = new Scanner(new File(\"test.txt\"))) { while (scanner.hasNext()) { System.out.println(scanner.nextLine()); }} catch (FileNotFoundException fnfe) { fnfe.printStackTrace();}a 当然多个资源需要关闭的时候，使用 try-with-resources 实现起来也非常简单，如果你还是用try-catch-finally可能会带来很多问题。 通过使用分号分隔，可以在try-with-resources块中声明多个资源。 12345678910try (BufferedInputStream bin = new BufferedInputStream(new FileInputStream(new File(\"test.txt\"))); BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(new File(\"out.txt\")))) { int b; while ((b = bin.read()) != -1) { bout.write(b); }}catch (IOException e) { e.printStackTrace();} 异常使用有哪些需要注意的地方？ 不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。 抛出的异常信息一定要有意义。 建议抛出更加具体的异常比如字符串转换为数字格式错误的时候应该抛出NumberFormatException而不是其父类IllegalArgumentException。 使用日志打印异常之后就不要再抛出异常了（两者不要同时存在一段代码逻辑中）。 …… 泛型什么是泛型？泛型：是一种把明确类型的工作推迟到创建对象或者调用方法的时候才去明确的特殊的类型。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，而这种参数类型可以用在类、方法和接口中，分别被称为泛型类、泛型方法、泛型接口。注意:一般在创建对象时，将未知的类型确定具体的类型。当没有指定泛型时，默认类型为Object类型。 有什么作用？ 避免了类型强转的麻烦。 它提供了编译期的类型安全，确保在泛型类型（通常为泛型集合）上只能使用正确类型的对象，避免了在运行时出现ClassCastException。 泛型的使用方式有哪几种？ 泛型类： 泛型类型用于类的定义中，被称为泛型类。最典型的就是各种集合框架容器类，如：List、Set、Map。 123456789101112131415161718192021222324泛型类的定义格式：修饰符 class 类名&lt;代表泛型的变量&gt; { }怕你不清楚怎么使用，这里我还是做了一个简单的泛型类：/** * @param &lt;T&gt; 这里解释下&lt;T&gt;中的T: * 此处的T可以随便写为任意标识，常见的有T、E等形式的参数表示泛型 * 泛型在定义的时候不具体，使用的时候才变得具体。 * 在使用的时候确定泛型的具体数据类型。即在创建对象的时候确定泛型。 */public class Generic&lt;T&gt;{ private T key; public Generic(T key) { this.key = key; } public T getKey(){ return key; }}泛型在定义的时候不具体，使用的时候才变得具体。在使用的时候确定泛型的具体数据类型。即：在创建对象的时候确定泛型。 泛型接口： 12345定义格式修饰符 interface接口名&lt;代表泛型的变量&gt; { }public interface Generator&lt;T&gt; { public T method();} 实现泛型接口，不指定类型： 123456class GeneratorImpl&lt;T&gt; implements Generator&lt;T&gt;{ @Override public T method() { return null; }} 实现泛型接口，指定类型： 123456class GeneratorImpl&lt;T&gt; implements Generator&lt;String&gt;{ @Override public String method() { return \"hello\"; }} 泛型方法： 123456789101112131415161718192021222324定义格式：修饰符 &lt;代表泛型的变量&gt; 返回值类型 方法名(参数){ }例如：/** * * @param t 传入泛型的参数 * @param &lt;T&gt; 泛型的类型 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间&lt;T&gt;非常重要，可以理解为声明此方法为泛型方法。 * 2）只有声明了&lt;T&gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 * 3）&lt;T&gt;表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。 * 4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E等形式的参数常用于表示泛型。 */ public &lt;T&gt; T genercMethod(T t){ System.out.println(t.getClass()); System.out.println(t); return t;}fanxing fanxing = new fanxing();String string = fanxing.genercMethod(\"string\");Integer integer = fanxing.genercMethod(123); 泛型通配符 当使用泛型类或者接口时，传递的数据中，泛型类型不确定，可以通过通配符&lt;?&gt;表示。但是一旦使用泛型的通配符后，只能使用Object类中的共性方法，集合中元素自身方法无法使用。 12345678static void tongpeifu(List&lt;?&gt; list){ //只能用 Object 修饰，所以也只能用 Object 自带的方法 Object o = list.get(0);}public static void main(String[] args) { tongpeifu(Arrays.asList(\"1111\"));} 通配符基本使用 泛型的通配符:不知道使用什么类型来接收的时候,此时可以使用?,?表示未知通配符。 此时只能接受数据,不能往该集合中存储数据。 12345678// ？代表可以接收任意类型// 泛型不存在继承、多态关系,泛型左右两边要一样。jdk1.7后右边的泛型可以省略//ArrayList&lt;Object&gt; list = new ArrayList&lt;String&gt;();这种是错误的//泛型通配符?:左边写&lt;?&gt; 右边的泛型可以是任意类型ArrayList&lt;?&gt; list = new ArrayList&lt;String&gt;();//编译错误 不可以存储数据list.add(\"1111\"); 泛型通配符?主要应用在参数传递方面 12345678public static void main(String[] args) { ArrayList&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); test(list1); ArrayList&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); test(list2);}public static void test(ArrayList&lt;?&gt; coll){} 通配符高级使用 之前设置泛型的时候，实际上是可以任意设置的，只要是类就可以设置。但是在JAVA的泛型中可以指定一个泛型的上限和下限。 泛型的上限： 格式： 类型名称 &lt;? extends 类 &gt; 对象名称 意义： 只能接收该类型及其子类 泛型的下限： 格式： 类型名称 &lt;? super 类 &gt; 对象名称 意义： 只能接收该类型及其父类型 123456789101112131415161718192021public static void main(String[] args) { Collection&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); Collection&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); Collection&lt;Number&gt; list3 = new ArrayList&lt;Number&gt;(); Collection&lt;Object&gt; list4 = new ArrayList&lt;Object&gt;(); getElement1(list1); getElement1(list2);//报错 getElement1(list3); getElement1(list4);//报错 getElement2(list1);//报错 getElement2(list2);//报错 getElement2(list3); getElement2(list4); }// 泛型的上限：此时的泛型?，必须是Number类型或者Number类型的子类public static void getElement1(Collection&lt;? extends Number&gt; coll){}// 泛型的下限：此时的泛型?，必须是Number类型或者Number类型的父类public static void getElement2(Collection&lt;? super Number&gt; coll){} 反射何为反射反射之所以被称为框架的灵魂，主要是因为它赋予了我们在运行时分析类以及执行类中方法的能力。 通过反射你可以获取任意一个类的所有属性和方法，并调用这些方法和属性。 反射的应用场景了解么？Spring/Spring Boot、MyBatis 等等框架中都大量使用了反射机制。这些框架中也大量使用了动态代理，而动态代理的实现也依赖反射。 像 Java 中的一大利器 注解 的实现也用到了反射。基于反射分析类，然后获取到类/属性/方法/方法的参数上的注解。你获取到注解之后，就可以做进一步的处理。 反射机制的优缺点优点：可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利 缺点：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点（编译器无法优化，无法使用 JIT），不过，对于框架来说实际是影响不大的。 反射原理​ 要想通过反射获取一个类的信息，首先要获取该类对应的Class类实例（Class 对象），Class类的实例代表了正在运行中的Java应用的类和接口。Class类没有公共的构造方法，Class类对象是在二进制字节流（一般是.class文件，也可通过网络或zip包等路径获取）被JVM加载时，通过调用类加载器的defineClass()方法来构建的。（译自Class类的JDK源码） ​ JVM在加载阶段要完成的3件事情中正好有Class对象的生成： 通过一个类的全限定名来获取定义此类的二进制字节流（.class字节码文件）。 将这个字节流所代表的的静态存储结构转化为方法区的运行时数据结构（hotspot vm 中即InstanceKlass，即元数据）。 在内存（堆）中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 获取 Class 对象的四种方式12345678910111213//1. 知道具体类的情况下可以使用：Class alunbarClass = TargetObject.class;//2. 通过 Class.forName()传入类的全路径获取：Class alunbarClass1 = Class.forName(\"cn.javaguide.TargetObject\");//3. 通过对象实例instance.getClass()获取：TargetObject o = new TargetObject();Class alunbarClass2 = o.getClass();//4. 通过类加载器xxxClassLoader.loadClass()传入类路径获取:ClassLoader.getSystemClassLoader().loadClass(\"cn.javaguide.TargetObject\");//通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一系列步骤，静态代码块和静态对象不会得到执行 反射的一些基本操作 创建一个我们要使用反射操作的类 TargetObject。 123456789101112131415public class TargetObject { private String value; public TargetObject() { value = \"JavaGuide\"; } public void publicMethod(String s) { System.out.println(\"I love \" + s); } private void privateMethod() { System.out.println(\"value is \" + value); }} 使用反射操作这个类的方法以及参数 1234567891011121314151617181920212223242526272829import java.lang.reflect.Field;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class test_targetObject { public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchMethodException, InvocationTargetException, NoSuchFieldException { Class&lt;?&gt; targetObject = Class.forName(\"TargetObject\"); Object o = targetObject.newInstance(); for (Method declaredMethod : targetObject.getDeclaredMethods()) { System.out.println(declaredMethod); } Method publicMethod = targetObject.getDeclaredMethod(\"publicMethod\", String.class); publicMethod.invoke(o, \"fml\"); Field value = targetObject.getDeclaredField(\"value\"); //为了调用private方法我们取消安全检查 value.setAccessible(true); value.set(o, \"fmlzuibang\"); /** * 调用 private 方法 */ Method privateMethod = targetObject.getDeclaredMethod(\"privateMethod\"); privateMethod.setAccessible(true); privateMethod.invoke(o); }} 注解何谓注解Annotation （注解） 是 Java5 开始引入的新特性，可以看作是一种特殊的注释，主要用于修饰类、方法或者变量，提供某些信息供程序在编译或者运行时使用。 注解本质是一个继承了Annotation 的特殊接口： 123456789@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override {}public interface Override extends Annotation{} JDK 提供了很多内置的注解（比如 @Override、@Deprecated），同时，我们还可以自定义注解。 注解的解析方法有哪几种？注解只有被解析之后才会生效，常见的解析方法有两种： 编译期直接扫描：编译器在编译 Java 代码的时候扫描对应的注解并处理，比如某个方法使用@Override 注解，编译器在编译的时候就会检测当前的方法是否重写了父类对应的方法。 运行期通过反射处理：像框架中自带的注解(比如 Spring 框架的 @Value、@Component)都是通过反射来进行处理的。 SPI何谓 SPI?SPI 即 Service Provider Interface ，字面意思就是：“服务提供者的接口”。一种服务发现机制，允许在运行时动态地加载实现特定接口的类，而不需要在代码中显式地指定该类，从而实现解耦和灵活性。 SPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。 SPI 和 API 有什么区别？说到 SPI 就不得不说一下 API 了，从广义上来说它们都属于接口，而且很容易混淆。下面先用一张图说明一下： 一般模块之间都是通过接口进行通讯，那我们在服务调用方和服务实现方（也称服务提供者）之间引入一个“接口”。 当实现方提供了接口和实现，我们可以通过调用实现方的接口从而拥有实现方给我们提供的能力，这就是 API ，这种接口和实现都是放在实现方的。 当接口存在于调用方这边时，就是 SPI ，由接口调用方确定接口规则，然后由不同的厂商去根据这个规则对这个接口进行实现，从而提供服务。 Java SPI 的优缺点？优点： 松耦合度：在运行时动态加载实现类，而无需在编译时将实现类硬编码到代码中 扩展性：可以为同一个接口定义多个实现类。这使得应用程序更容易扩展和适应变化。 缺点： 安全性不足：SPI提供者必须将其实现类名称写入到配置文件中，因此如果未正确配置，则可能存在安全风险。 性能损失：每次查找服务提供者都需要重新读取配置文件，这可能会增加启动时间和内存开销。 上面对Java SPI的缺点说了一下，我们来说一下：Spring的SPI机制相对于Java原生的SPI机制进行了改造和扩展，主要体现在以下几个方面： 支持多个实现类：Spring的SPI机制允许为同一个接口定义多个实现类，而Java原生的SPI机制只支持单个实现类。这使得在应用程序中使用Spring的SPI机制更加灵活和可扩展。 支持自动装配：Spring的SPI机制支持自动装配，可以通过将实现类标记为Spring组件（例如@Component），从而实现自动装配和依赖注入。这在一定程度上简化了应用程序中服务提供者的配置和管理。 支持动态替换：Spring的SPI机制支持动态替换服务提供者，可以通过修改配置文件或者其他方式来切换服务提供者。而Java原生的SPI机制只能在启动时加载一次服务提供者，并且无法在运行时动态替换。 提供了更多扩展点：Spring的SPI机制提供了很多扩展点，例如BeanPostProcessor、BeanFactoryPostProcessor等，可以在服务提供者初始化和创建过程中进行自定义操作。 应用场景Java SPI机制是一种服务提供者发现的机制，适用于需要在多个实现中选择一个进行使用的场景。 常见的应用场景包括： 应用名称 具体应用场景 数据库驱动程序加载 JDBC为了实现可插拔的数据库驱动，在Java.sql.Driver接口中定义了一组标准的API规范，而具体的数据库厂商则需要实现这个接口，以提供自己的数据库驱动程序。在Java中，JDBC驱动程序的加载就是通过SPI机制实现的。 日志框架的实现 流行的开源日志框架，如Log4j、SLF4J和Logback等，都采用了SPI机制。用户可以根据自己的需求选择合适的日志实现，而不需要修改代码。 Spring框架 Spring框架中的Bean加载机制就使用了SPI思想，通过读取classpath下的META-INF/spring.factories文件来加载各种自定义的Bean。 Dubbo框架 Dubbo框架也使用了SPI思想，通过接口注解@SPI声明扩展点接口，并在classpath下的META-INF/dubbo目录中提供实现类的配置文件，来实现扩展点的动态加载。 参考：https://blog.csdn.net/qq_52423918/article/details/130968307 序列化和反序列化什么是序列化?什么是反序列化? 序列化：将数据结构或对象转换成二进制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过 序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中。 序列化协议对应于 TCP/IP 4 层模型的哪一层？ 应用层（表示层） 如果有些字段不想进行序列化怎么办？使用 transient 关键字修饰 关于 transient 还有几点注意： transient 只能修饰变量，不能修饰类和方法。 transient 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 int 类型，那么反序列后结果就是 0。 static 变量因为不属于任何对象(Object)，所以无论有没有 transient 关键字修饰，均不会被序列化。 JDK 自带的序列化方式JDK 自带的序列化，只需实现 java.io.Serializable接口即可。 serialVersionUID 有什么作用？序列化号 serialVersionUID 属于版本控制的作用。反序列化时，会检查 serialVersionUID 是否和当前类的 serialVersionUID 一致。如果 serialVersionUID 不一致则会抛出 InvalidClassException 异常。强烈推荐每个序列化类都手动指定其 serialVersionUID，如果不手动指定，那么编译器会动态生成默认的 serialVersionUID serialVersionUID 不是被 static 变量修饰了吗？为什么还会被“序列化”？static 修饰的变量是静态变量，位于方法区，本身是不会被序列化的。但是，serialVersionUID 的序列化做了特殊处理，在序列化时，会将 serialVersionUID 序列化到二进制字节流中；在反序列化时，也会解析它并做一致性判断。 为什么不推荐使用 JDK 自带的序列化？ 性能差 存在安全问题 不支持跨语言调用 I/OJava IO 流了解吗？IO 即 Input/Output，输入和输出。数据输入到计算机内存的过程即输入，反之输出到外部存储（比如数据库，文件，远程主机）的过程即输出。数据传输过程类似于水流，因此称为 IO 流。IO 流在 Java 中分为输入流和输出流，而根据数据的处理方式又分为字节流和字符流。 Java IO 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 I/O 流为什么要分为字节流和字符流呢?问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？ 字符流是由 Java 虚拟机将字节转换得到的，这个过程还算是比较耗时（字节流优势）； 如果我们不知道编码类型的话，使用字节流的过程中很容易出现乱码问题（字符流优势）。 Java 中 3 种常见 IO 模型BIO (Blocking I/O)同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。 NIO (Non-blocking/New I/O) 存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 I/O 多路复用模型 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间 -&gt; 用户空间）还是阻塞的。 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持。 select 调用：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。 epoll 调用：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。 Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。 AIO (Asynchronous I/O) 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。 代理模式一种设计模式：使用代理对象来代替对真实对象(real object)的访问，这样就可以在不修改原目标对象的前提下，扩展目标对象的功能。（比如说在目标对象的某个方法执行前后你可以增加一些自定义的操作。） 静态代理 实现和应用角度： ​ 对目标对象的每个方法的增强都是手动完成的，非常不灵活（比如接口一旦新增加方法，目标对象和代理对象都要进行修改）且麻烦(需要对每个目标类都单独写一个代理类） JVM 层面： ​ 静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。 静态代理实现步骤: 定义一个接口及其实现类； 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。 动态代理JDK 动态代理机制在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。 使用步骤： 定义一个接口及其实现类； 定义一个实现 InvocationHandler 并重写invoke方法的类，在 invoke 方法中会调用原生方法（被代理类的方法）并自定义一些处理逻辑； 12345678public interface InvocationHandler { /** * 当你使用代理对象调用方法的时候实际会调用到这个方法 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;} 通过 Proxy.newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) 方法创建代理对象； 12345678910/**loader :类加载器，用于加载代理对象。* interfaces : 被代理类实现的一些接口；* h : 实现了 InvocationHandler 接口的对象；**/public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException{ ......} CGLIB 动态代理机制在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer类是核心。 使用步骤 定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 123456789101112131415161718192021222324import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * 自定义MethodInterceptor */public class DebugMethodInterceptor implements MethodInterceptor { /** * @param o 被代理的对象（需要增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { //调用方法之前，我们可以添加自己的操作 System.out.println(\"before method \" + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\"after method \" + method.getName()); return object; }} 通过 Enhancer 类的 create()创建代理类； 1234567891011121314151617import net.sf.cglib.proxy.Enhancer;public class CglibProxyFactory { public static Object getProxy(Class&lt;?&gt; clazz) { // 创建动态代理增强类 Enhancer enhancer = new Enhancer(); // 设置类加载器 enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类 enhancer.setSuperclass(clazz); // 设置方法拦截器 enhancer.setCallback(new DebugMethodInterceptor()); // 创建代理类 return enhancer.create(); }} JDK 动态代理和 CGLIB 动态代理对比 JDK 动态代理只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。 CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。 就二者的效率来说，大部分情况都是 JDK 动态代理更优秀 静态代理和动态代理的对比 灵活性：动态代理更加灵活，不需要必须实现接口（静态代理、jdk 动态代理），可以直接代理实现类(CGLIB)，并且可以不需要针对每个目标类都创建一个代理类。另外，静态代理中，接口一旦新增加方法，目标对象和代理对象都要进行修改，这是非常麻烦的！ JVM 层面：静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。而动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。","link":"/2024/03/12/Java%E5%9F%BA%E7%A1%80/"},{"title":"集合","text":"集合框架底层数据结构总结List ArrayList：Object[] 数组。详细可以查看：ArrayList 源码分析。 Vector：Object[] 数组。 LinkedList：双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环)。详细可以查看：LinkedList 源码分析。 Set HashSet(无序，唯一): 基于 HashMap 实现的，底层采用 HashMap 来保存元素。 LinkedHashSet: LinkedHashSet 是 HashSet 的子类，并且其内部是通过 LinkedHashMap 来实现的。 TreeSet(有序，唯一): 红黑树(自平衡的排序二叉树)。 Queue PriorityQueue: Object[] 数组来实现小顶堆。详细可以查看：PriorityQueue 源码分析。 DelayQueue:PriorityQueue。详细可以查看：DelayQueue 源码分析。 ArrayDeque: 可扩容动态双向数组。 再来看看 Map 接口下面的集合。 Map HashMap：JDK1.8 之前 HashMap 由数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。详细可以查看：HashMap 源码分析。 LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：LinkedHashMap 源码分析 Hashtable：数组+链表组成的，数组是 Hashtable 的主体，链表则是主要为了解决哈希冲突而存在的。 TreeMap：红黑树（自平衡的排序二叉树）。 List**ArrayList 和 Array（数组）的区别？ArrayList 内部基于动态数组实现，比 Array（静态数组） 使用起来更加灵活： 动态地扩容或缩容 使用泛型来确保类型安全 只能存储对象（Array 都可以） 不需要指定大小 ArrayList 插入和删除元素的时间复杂度？尾部插入：当 ArrayList 的容量未达到极限时，往列表末尾插入元素的时间复杂度是 O(1)，因为它只需要在数组末尾添加一个元素即可；当容量已达到极限并且需要扩容时，则需要执行一次 O(n) 的操作将原数组复制到新的更大的数组中，然后再执行 O(1) 的操作添加元素。 LinkedList 插入和删除元素的时间复杂度？ 指定位置插入/删除：需要先移动到指定位置，再修改指定节点的指针完成插入/删除，因此需要移动平均 n/2 个元素，时间复杂度为 O(n)。 LinkedList 为什么不能实现 RandomAccess 接口？RandomAccess 是一个标记接口，用来表明实现该接口的类支持随机访问（即可以通过索引快速访问元素）。由于 LinkedList 底层数据结构是链表，内存地址不连续，只能通过指针来定位，不支持随机快速访问，所以不能实现 RandomAccess 接口。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！是为了能够更好地判断集合是否ArrayList或者LinkedList，从而能够更好选择更优的遍历方式，提高性能！（Collections类中的binarySearch（）） ArrayList 与 LinkedList 区别?底层数据结构 插入和删除是否受元素位置的影响 是否支持快速随机访问 内存空间占用 **ArrayList 的扩容机制 先从 ArrayList 的构造函数说起 有三种构造方法 1234567891011121314/** * 带初始容量参数的构造函数。（用户自己指定容量） */public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) {//初始容量大于0 //创建initialCapacity（10）大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) {//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else {//初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \" + initialCapacity); }} 以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。 add 方法 这里以无参构造函数创建的 ArrayList 为例分析，即一开始为空的数组。 当我们要 add 进第 1 个元素到 ArrayList 时，elementData.length 为 0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为 10。此时，minCapacity - elementData.length &gt; 0成立，所以会进入 grow(minCapacity) 方法。 当 add 第 2 个元素时，minCapacity 为 2，此时 elementData.length(容量)在添加第一个元素后扩容成 10了。此时，minCapacity - elementData.length &gt; 0 不成立，所以不会进入 （执行）grow(minCapacity)方法。 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。 直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。 grow 方法 1private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数. 需要保证newCapacity&gt;=minCapacity 假如newCapacity &gt; MAX_ARRAY_SIZE需要进一步进入hugeCapacity() 方法判断minCapacity &gt; MAX_ARRAY_SIZE 无序性和不可重复性的含义是什么 无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。 不可重复性是指添加的元素按照 equals() 判断时 ，返回 false，需要同时重写 equals() 方法和 hashCode() 方法。 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同同： HashSet、LinkedHashSet 和 TreeSet 都是 Set 接口的实现类，都能保证元素唯一，并且都不是线程安全的。 异： HashSet、LinkedHashSet 和 TreeSet 的主要区别在于底层数据结构不同。HashSet 的底层数据结构是哈希表（基于 HashMap 实现）。LinkedHashSet 的底层数据结构是链表和哈希表，元素的插入和取出顺序满足 FIFO。TreeSet 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序。 底层数据结构不同又导致这三者的应用场景不同。HashSet 用于不需要保证元素插入和取出顺序的场景，LinkedHashSet 用于保证元素的插入和取出顺序满足 FIFO 的场景，TreeSet 用于支持对元素自定义排序规则的场景。 QueueQueue 与 Deque 的区别Queue 是单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循 先进先出（FIFO） 规则。 Deque 是双端队列，在队列的两端均可以插入或删除元素。 Deque 还提供有 push() 和 pop() 等其他方法，可用于模拟栈。 1Deque&lt;Integer&gt; a = new LinkedList&lt;&gt;(); ArrayDeque 与 LinkedList 的区别ArrayDeque 和 LinkedList 都实现了 Deque 接口，两者都具有队列的功能，但两者有什么区别呢？ ArrayDeque 是基于可变长的数组和双指针来实现，而 LinkedList 则通过链表来实现。 ArrayDeque 不支持存储 NULL 数据，但 LinkedList 支持。 ArrayDeque 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 LinkedList 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。 从性能的角度上，选用 ArrayDeque 来实现队列要比 LinkedList 更好。此外，ArrayDeque 也可以用于实现栈。 说一说 PriorityQueue PriorityQueue 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据 PriorityQueue 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。 PriorityQueue 是非线程安全的，且不支持存储 NULL 和 non-comparable 的对象。 PriorityQueue 默认是小顶堆，但可以接收一个 Comparator 作为构造参数，从而来自定义元素优先级的先后。 PriorityQueue 在面试中可能更多的会出现在手撕算法的时候，典型例题包括堆排序、求第 K 大的数、带权图的遍历等，所以需要会熟练使用才行。 Map（重要）HashMap 和 Hashtable 的区别 线程是否安全 效率 对 Null key 和 Null value 的支持 初始容量大小和每次扩充容量大小的不同: ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小 底层数据结构:JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间 HashMap 和 HashSet 区别如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 HashMap 和 TreeMap 区别相比于HashMap来说 TreeMap 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。 HashSet 如何检查重复?当你把对象加入HashSet时，HashSet 会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让加入操作成功。 HashMap 的底层实现JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashcode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法影响性能 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 1234567 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。 TreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 HashMap 的长度为什么是 2 的幂次方Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。散列值是不能直接拿来用的。 用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) &amp; hash”。（n 代表数组长度）。一开始想到的是 % 取余 但是效率比 按位与 低。这也就解释了 HashMap 的长度为什么是 2 的幂次方。 解释：（只有 n 为 2 的幂次方，n-1 用二进制表示时低位才可能都是 1，与 hash 做按位&amp; 才能得到全部低位数据） HashMap 多线程操作导致死循环问题JDK1.7 及之前版本的 HashMap 在多线程环境下扩容操作可能存在死循环问题，这是由于当一个桶位中有多个元素需要进行扩容时，多个线程同时对链表进行操作，头插法可能会导致链表中的节点指向错误的位置，从而形成一个环形链表，进而使得查询元素的操作陷入死循环无法结束。 为了解决这个问题，JDK1.8 版本的 HashMap 采用了尾插法而不是头插法来避免链表倒置，使得插入的节点永远都是放在链表的末尾，避免了链表中的环形结构。但是还是不建议在多线程下使用 HashMap，因为多线程下使用 HashMap 还是会存在数据覆盖的问题。并发环境下，推荐使用 ConcurrentHashMap 。 *HashMap 为什么线程不安全？JDK1.7 及之前版本，在多线程环境下，HashMap 扩容时会造成死循环和数据丢失的问题。 数据丢失这个在 JDK1.7 和 JDK 1.8 中都存在 情况 1：哈希冲突，导致两个线程同时进行 hash 判断都可以插入，a 线程判断可以插入后挂起，b 线程执行插入，完成后另a 线程继续执行插入导致数据被覆盖 情况 2：两个线程同时 put 操作导致 size 的值只增加了 1，实际上只有一个元素被添加到了 HashMap 中，进而导致数据覆盖的问题 ConcurrentHashMap 和 Hashtable 的区别底层数据结构 JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样，数组+链表/红黑二叉树。 Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要） 在 JDK1.7 的时候，ConcurrentHashMap 对整个桶数组进行了分割分段(Segment，分段锁)，每一把锁只锁容器其中一部分数据（下面有示意图） 到了 JDK1.8 的时候，ConcurrentHashMap 已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并使用 synchronized 、CAS 、node来保证并发安全。 （未发生 hash 冲突时，采用 CAS加入新 node 到数组中；若发生 hash冲突时，采用 synchronized 来添加该节点到链表 or 红黑树） Hashtable(同一把锁) :使用 synchronized 来保证线程安全，锁住整个数组，效率底下。 Hashtable : JDK1.7 的 ConcurrentHashMap： Segment 数组中的每个元素包含一个 HashEntry 数组，每个 HashEntry 数组属于链表结构。 JDK1.8 的 ConcurrentHashMap： JDK1.8 的 ConcurrentHashMap 不再是 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。不过，Node 只能用于链表的情况，红黑树的情况需要使用 **TreeNode**。当冲突链表达到一定长度时，链表会转换成红黑树。 CAS&amp;synchronizedCAS：在判断数组中当前位置为null的时候，使用CAS来把这个新的Node写入数组中对应的位置 synchronized ：当数组中的指定位置不为空时，通过加锁来添加这个节点进入数组(链表&lt;8)或者是红黑树（链表&gt;=8） 所以这部分是进行了cas： else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } ConcurrentHashMap 为什么 key 和 value 不能为 null？避免二义性： 值没有在集合中 ； 值本身就是 null。 单线程下可以容忍歧义，而多线程下无法容忍","link":"/2024/03/12/%E9%9B%86%E5%90%88/"},{"title":"JVM","text":"**Java 内存区域JDK 1.8： 线程私有的： 程序计数器：记录程序执行到的位置 虚拟机栈 局部变量表：存放了编译期可知的各种数据类型 操作数栈：存放方法执行过程中产生的中间计算结果 动态链接：将符号引用转换为调用方法的直接引用。多态 方法返回地址 本地方法栈：执行本地方法 线程共享的： 堆：存放类实例和数组 JDK 8 版本之后 PermGen(永久代) 已被 Metaspace(元空间) 取代，元空间使用的是本地内存 方法区：（方法区逻辑上在堆中）存储已被虚拟机加载的 类信息、字段信息、方法信息、常量、静态变量、JIT 代码缓存等数据 运行时常量池：字面量（Literal 源代码中的固定值的表示法，字面量包括整数、浮点数和字符串字面量）和符号引用（Symbolic Reference，包括类符号引用、字段符号引用、方法符号引用、接口方法符号） 字符串常量池。主要目的是为了避免字符串的重复创建。 java 内存模型\\happen-before 原则 有什么存在作用？ java 内存模型跟 cpu 缓存模型类型，是基于 cpu 缓存模型来建立的，Java内存模型定义了共享内存系统中多线程程序读写操作行为的规范，是为了解决并发编程问题而存在的。 JMM对内存的划分？ 划分为主内存和工作内存两种 规定所有的变量都存储在主内存中，每条线程还有自己的工作内存，线程的工作内存中保存了该线程中用到的变量的主内存的副本拷贝，线程对变量的所有操作都必须在工作内存中进行，而不能直接读写主内存。 主内存和工作内存的交互操作 首先是从lock加锁开始，把主内存中的变量标志为一条线程独占的状态；read读取，将一个变量的值从主内存传输到工作内存中；load加载，把read得到的值加载到工作内存的变量副本中；use使用，把工作内存中变量的值传递给执行引擎；assign赋值，把从执行引擎接收到的值赋值给工作内存的变量；store存储，把工作内存中变量的值传送回主内存中；write写入，把store得到的值放入主内存的变量中；最后是unlock解锁，把主内存中处于锁定状态的变量释放出来， 内存交互基本操作的三个特性的理解？ 原子性（synchroinzed）、可见性（volatile）、以及有序性（happen-before）。 happen-before happen-before原则是Java内存模型中定义的两项操作之间的偏序关系。 Happens-Before关系只是描述结果的可见性，并不表示指令执行的先后顺序，也就是说只要不对结果产生影响，仍然允许指令重排序。 垃圾回收内存分配和回收原则大对象直接进入老年代大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 大对象直接进入老年代的行为是由虚拟机动态决定的，它与具体使用的垃圾回收器和相关参数有关。大对象直接进入老年代是一种优化策略，旨在避免将大对象放入新生代，从而减少新生代的垃圾回收频率和成本。 主要进行 gc 的区域针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种： 部分收集 (Partial GC)： 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集； 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集； 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。 整堆收集 (Full GC)：收集整个 Java 堆和方法区。 空间分配担保空间分配担保是为了确保在 Minor GC 之前老年代本身还有容纳新生代所有对象的剩余空间。 JVM是如何判断一个对象是可回收的？ 引用计数法 给对象中添加一个引用计数器： 每当有一个地方引用它，计数器就加 1； 当引用失效，计数器就减 1； 任何时候计数器为 0 的对象就是不可能再被使用的。 **这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间循环引用的问题。 可达性分析算法 基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。 哪些对象可以作为 GC Roots 呢？ 虚拟机栈(栈帧中的局部变量表)中引用的对象 本地方法栈(Native 方法)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 所有被同步锁持有的对象 垃圾收集算法标记-清除算法标记-清除（Mark-and-Sweep）算法分为“标记（Mark）”和“清除（Sweep）”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。 这种垃圾收集算法会带来两个明显的问题： 效率问题：标记和清除两个过程效率都不高。 空间问题：标记清除后会产生大量不连续的内存碎片。 复制算法为了解决标记-清除算法的效率和内存碎片问题，复制（Copying）收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 虽然改进了标记-清除算法，但依然存在下面这些问题： 可用内存变小：可用内存缩小为原来的一半。 不适合老年代：如果存活对象数量比较大，复制性能会变得很差。 标记-整理算法标记-整理（Mark-and-Compact）算法是根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 由于多了整理这一步，因此效率也不高，适合老年代这种垃圾回收频率不是很高的场景。 分代收集算法比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 **垃圾收集器Serial 收集器新生代采用标记-复制算法，老年代采用标记-整理算法。 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 不良用户体验但简单而高效（与其他收集器的单线程相比） ParNew 收集器ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 parnew+cms Parallel Scavenge 收集器Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 新生代采用标记-复制算法，老年代采用标记-整理算法。 **CMS 收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。并发收集器。 初始标记：记录下直接与 root 相连的对象，速度很快 并发标记： 同时开启 GC 和用户线程，去记录可达对象 重新标记：修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录 并发清除：开启用户线程，同时 GC 线程开始对未标记的区域做清扫 主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 对 CPU 资源敏感； 无法处理浮动垃圾； 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。 **G1 收集器以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 特点： 并行与并发：使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间 分代收集：虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。 空间整合：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”算法实现的。 可预测的停顿：这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内，消耗在垃圾收集上的时间不得超过 N 毫秒。 几个步骤： 初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) 三色标记算法 白色：没有检查（或者检查过了，确实没有引用指向它了） 灰色：自身被检查了，成员没被检查完（可以认为访问到了，但是正在被检查，就是图的遍历里那些在队列中的节点） 黑色：自身和成员都被检查完了 第一种问题： 错标标记过不是垃圾的，变成了垃圾（也叫浮动垃圾） 第二种问题：漏标，或者叫错杀G1：写屏障+ SATB （Snapshot At The Beginning） 尝试保留开始时的对象图，即原始快照（Snapshot At The Beginning，SATB），当某个时刻 的GC Roots确定后，当时的对象图就已经确定了。如果期间发生变化，则可以记录起来，保证标记依然按照原本的视图来。 cms：写屏障+ 增量更新 新增如果新增黑色到白色的引用，那么jvm会通过写屏障，来把黑色置为灰色 删除如果删除引用，jvm什么都不会做，这个导致了浮动垃圾 当有新引用插入进来时，记录下新的引用对象 讲一下JVM调优过程？https://zhuanlan.zhihu.com/p/488615913 分析和定位当前系统的瓶颈 1）CPU指标 123456// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pid 2）JVM 内存指标 123456// 查看 Java 进程的配置信息，包括系统属性和JVM命令行标志jinfo pid// 输出 Java 进程当前的 gc 情况jstat -gc pid// 输出 Java 堆详细信息jmap -heap pid 制订优化方案 代码bug：升级修复bug。典型的有：死循环、使用无界队列。 不合理的JVM参数配置：优化 JVM 参数配置。典型的有：年轻代内存配置过小、堆内存配置过小、元空间配置过小。 对比优化前后的指标，统计优化效果 GC日志查看可以通过在java命令种加入参数来指定对应的gc类型，打印gc日志信息并输出至文件等策略。 123456-XX:+PrintGC 输出GC日志-XX:+PrintGCDetails 输出GC的详细日志-XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式）-XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800）-XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息-Xloggc:../logs/gc.log 日志文件的输出路径","link":"/2024/03/12/JVM/"},{"title":"操作系统","text":"进程和线程进程有哪几种状态创建、就绪、执行、阻塞、结束 java 线程有哪几种状态new、runnable、blocked、waiting、time_waiting、terminated 虚拟内存操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。 分段问题： 外部内存碎片 内存交换的效率低（段太大，导致内存交换代价高） 分页：分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小 页表：虚拟地址与物理地址之间通过页表来映射 多级页表 页表一定要覆盖全部虚拟地址空间 如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表 大小页面的优缺点 页面小可以减少内部碎片的浪费，但页表项相对较多；页面大可以减少页表项，增加TLB命中率，但是内部碎片浪费较严重。具体系统选择应该根据业务需求来决定。","link":"/2024/03/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"title":"MySQL","text":"综合题慢查询的原因。怎么定位慢查询，怎么优化慢查询，思路是什么。原因 索引失效 连接查询没有用到索引，执行了笛卡尔查询 大数据量排序与分组 复杂的子查询 表结构设计问题 事务锁竞争 定位 慢查询日志。MySQL的慢查询日志会记录执行时间超过long_query_time 的SQL语句 show processlist。实时展示当前MySQL正在执行的线程,可以通过processlist来发现一些状态显示为Lock等的慢查询。 优化 加索引 SQL 语句优化。去除无效查询条件,优化Join查询,避免全表扫描等。 优化表结构。去除冗余字段,拆分过大的表,适当垂直拆分或者水平拆分表。 使用视图。将复杂查询封装为视图,以简化查询过程。 加缓存优化数据统计类的慢查询 部署读写分离架构,主库负责写,从库负责读,分散数据库压力。 基础执行一条 SQL 查询语句，期间发生了什么？ 连接器：建立连接、管理链接、校验个人身份 查询缓存：key-value形式——查询语句是否命中 解析器：词法解析、语法解析，建立语法树 执行 SQL： 预处理阶段：判断是否存在表面、字段名；将*替换成全部列 优化阶段：基于查询成本，选择最佳的执行计划 执行阶段：根据执行计划执行 SQL查询语句，从存储引擎读取记录，返回给客户端 MySQL 一行记录是怎么存储的？ MySQL 的 NULL 值是怎么存放的？ MySQL 的 Compact 行格式中会用「NULL值列表」来标记值为 NULL 的列，NULL 值并不会存储在行格式中的真实数据部分。 NULL值列表会占用 1 字节空间，当表中所有字段都定义成 NOT NULL，行格式中就不会有 NULL值列表，这样可节省 1 字节的空间。 MySQL 怎么知道 varchar(n) 实际占用数据的大小？ MySQL 的 Compact 行格式中会用「变长字段长度列表」存储变长字段实际占用的数据大小。 varchar(n) 中 n 最大取值为多少？ 一行记录最大能存储 65535 字节的数据，但是这个是包含「变长字段字节数列表所占用的字节数」和「NULL值列表所占用的字节数」。所以， 我们在算 varchar(n) 中 n 最大值时，需要减去这两个列表所占用的字节数。 如果一张表只有一个 varchar(n) 字段，且允许为 NULL，字符集为 ascii。varchar(n) 中 n 最大取值为 65532。 计算公式：65535 - 变长字段字节数列表所占用的字节数 - NULL值列表所占用的字节数 = 65535 - 2 - 1 = 65532。 如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 &lt;= 65535。 行溢出后，MySQL 是怎么处理的？ 如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。 Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。 Compressed 和 Dynamic 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。 为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？1、B+Tree vs B Tree B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。 B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。 B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化； 2、B+Tree vs 二叉树 对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。 在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 34 层左右，也就是说一次数据查询操作只需要做 34 次的磁盘 I/O 操作就能查询到目标数据。根节点可以包含的关键字数量范围是 [2, m-1]。 非根节点至少包含m/2个关键字，至多包含m-1个关键字。 而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。 3、B+Tree vs Hash Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。 但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。 多表查询（各种join连接详解）A）内连接：join=inner join B）外连接：left join=left outer join，right join=right outer join，union C）交叉连接：cross join 2.1 内连接（只有一种场景） 123select a.*, b.* from tablea ainner join tableb bon a.id = b.id 这种场景下得到的是满足某一条件的A，B内部的数据；正因为得到的是内部共有数据，所以连接方式称为内连接。 2.2 外连接（六种场景） 2.2.1 left join 或者left outer join(等同于left join) 123select a.*, b.* from tablea aleft outer join tableb bon a.id = b.id 这种场景下得到的是A的所有数据，和满足某一条件的B的数据; 2.2.2 [ left join 或者left outer join(等同于left join) ] + [ where B.column is null ] 1234select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idWhere b.id is null left join表a的数据全部显示，匹配表b的数据也显示，而b.id再次过滤掉 表b的id为空的。 2.2.3 right join 或者right outer join(等同于right join) 123select a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.id 2.2.5 full join （mysql不支持，但是可以用 left join union right join代替） 1234567select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idunionselect a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.id 这种场景下得到的是满足某一条件的公共记录，和独有的记录 2.2.6 full join + is null（mysql不支持，但是可以用 （left join + is null） union （right join+is null代替） 123456789select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idwhere b.id is nullunionselect a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.idwhere a.id is null 2.3 交叉连接 （cross join） 2.3.1 实际应用中还有这样一种情形，想得到A，B记录的排列组合，即笛卡儿积，这个就不好用集合和元素来表示了。需要用到cross join： 12select a.id aid,a.age,b.id bid,b.name from tablea across join tableb b 2.3.2 还可以为cross join指定条件 （where）： 123select a.id aid,a.age,b.id bid,b.name from tablea across join tableb bwhere a.id = b.id 这种情况下实际上实现了内连接的效果 MyISAM 和 InnoDB 有什么区别？ 事务支持：MyISAM 不提供事务支持。InnoDB 提供事务支持，实现了 SQL 标准定义了四个隔离级别。 表锁差异：InnoDB 支持行级别的锁粒度，MyISAM 不支持，只支持表级别的锁粒度 读写过程：MyISAM在读写过程中相互阻塞；InnoDB读写阻塞与事务隔离级别相关 读写性能：MyISAM读取性能优越，但是写入性能差（如果执行大量的select，MyISAM是更好的选择）；InnoDB写入性能较强（如果执行大量的insert或者update，InnoDB是更好的选择） 外键支持：MyISAM 不支持外键，而 InnoDB 支持。 MyISAM 不支持数据库异常崩溃后的安全恢复，而 InnoDB 支持。 索引联合索引范围查询联合索引的最左匹配原则，在遇到范围查询（如 &gt;、&lt;）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。注意，对于 &gt;=、&lt;=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配 索引下推 在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个回表，到「主键索引」上找出数据行，再对比 b 字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 什么时候需要 / 不需要创建索引？需要： 字段唯一；经常用于where查询、 groupby条件和 orderby条件 的字段 不需要：区分度小的；数据少的；经常更新的 设置索引的时候会考虑哪些方面设计原则 针对你的SQL语句里的where条件、order by条件以及group by条件去设计索引 where条件里是要根据哪些字段来筛选数据？order by要根据哪些字段来排序？group by要根据哪些字段来分组聚合 考虑方向： （1）字段基数，尽量选择值多的，发挥 B+树快速二分查找的优势。判断方式——select count(distinct(column_name))/count(*) from table_name; 超过了0.5就适合做索引，越接近1越适合 （2）选字段类型小的列，占用磁盘空间小，查询性能好 （3）频繁更新的字段不适合做主键。主键一定是自增的而且业务无关，防止聚簇索引频繁页分裂 （4）查询语句不加函数，不然回走全表扫描 根据这些字段设计一个或者两个联合索引 哪些字段要放到联合索引中去？在联合索引里，字段的顺序要怎么排列呢？ （1）尽量让每一个联合索引都尽量去包含上你的where、order by、group by里的字段。 （2）字段顺序要符合最左匹配原则：仔细审查每个SQL语句，是不是每个where、order by、group by后面跟的字段顺序，都是某个联合索引的最左侧字段开始的部分字段。 （3）先让联合索引最左侧开始的多个字段使用等值匹配，最最后一个字段使用范围匹配 有什么优化索引的方法？ 前缀索引优化； 提高索引的查询速度；减小索引项的大小 覆盖索引优化； 不需要查询出包含整行记录的所有信息，也就减少了大量的 I/O 操作。 主键索引最好是自增的； 避免页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。 防止索引失效； 使用左或者左右模糊匹配的时候，也就是 like %xx 或者 like %xx%这两种方式 在查询条件中对索引列做了计算（+ - * /）、函数、类型转换操作 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。 在 WHERE 子句中，or 的前后字段都要上索引 列和列对比，这种情况会被认为还不如走全表扫描。 存在NULL值条件(is null/ is not null)，索引中无法存储NULL值，所以where条件判断如果对字段进行了NULL值判断（is NULL/ is not null），则数据库放弃索引而进行全表查询 聚簇索引&amp;二级索引如果叶子节点存储的是实际数据的就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。 在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。 MySQL 使用 like “%x“，索引一定会失效吗？从这个思考题我们知道了，使用左模糊匹配（like “%xx”）并不一定会走全表扫描，关键还是看数据表中的字段。 如果数据库表中的字段只有主键+二级索引，那么即使使用了左模糊匹配，也不会走全表扫描（type=all），而是走全扫描二级索引树(type=index)。 count(*) 和 count(1) 有什么区别？哪个性能最好？count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。 所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。 再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。 事务事务有哪些特性？原子性（Atomicity） 一个事务中的所有操作，要么全部完成，要么全部不完成 一致性（Consistency） 数据满足完整性约束，数据库保持一致性状态 隔离性（Isolation） 隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致 持久性（Durability） 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？原子性是通过 undo log（回滚日志）; 一致性则是通过持久性+原子性+隔离性来保证； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 持久性是通过 redo log （重做日志）来保证的； 并行事务会引发什么问题？脏读如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。 不可重复读在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。 幻读在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。 事务的隔离级别有哪些？各自的应用场景 读未提交(Read Uncommitted) 特点：事务中的修改操作(INSERT、UPDATE、DELETE)立即生效，无需等待事务提交；事务读取数据时可以读取其他事务未提交的数据。 应用场景：对于一些对数据一致性要求不高的场景，比如读取系统的实时监控数据。 读已提交(Read Committed) 特点：事务中的修改操作需要等待事务提交后才生效；事务读取数据时只能读取其他事务已提交的数据。 应用场景：适用于大部分常规业务场景，能够保证读取的数据具有较高的一致性。 可重复读(Repeatable Read)： 特点：事务中的修改操作需要等待事务提交后才生效；事务读取数据时只能读取事务开始时的快照数据，其他事务对数据的修改不可见。 应用场景：适用于需要保证读取数据一致性的应用，例如订单交易等。 串行化(Serializable)： 特点：事务中的修改操作需要等待事务提交后才生效；事务读取数据时只能读取事务开始时的快照数据，并且其他事务对数据进行了读取和修改的过程中，该数据将被锁定，其他事务无法访问。 应用场景：适用于对数据一致性要求极高的场景，例如金融领域的转账操作。 Read View Read View 中四个字段作用； 聚簇索引记录中两个跟事务有关的隐藏列； trx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里； roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。 MVCC通过「事务的 Read View 里的字段」和「记录中的两个隐藏列」的比对,来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同： 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View 「可重复读」隔离级别是启动事务时生成一个 Read View 对于幻读现象，不建议将隔离级别升级为串行化。MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读 针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读 MySQL 可重复读隔离级别，完全解决幻读了吗？场景一： 对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。 场景二： T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id &gt; 100 得到了 3 条记录。 T2 时刻：事务 B 往插入一个 id= 200 的记录并提交； T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id &gt; 100 for update 就会得到 4 条记录，此时也发生了幻读现象。 总结： 所以，MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select … for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。 锁全局锁要使用全局锁，则要执行这条命令： 1flush tables with read lock 执行后，整个数据库就处于只读状态了，这时其他线程执行以下操作，都会被阻塞： 对数据的增删改操作，比如 insert、delete、update等语句； 对表结构的更改操作，比如 alter table、drop table 等语句。 如果要释放全局锁，则要执行这条命令： 1unlock tables 当然，当会话断开了，全局锁会被自动释放。 全局锁应用场景是什么？ 全局锁主要应用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。 加全局锁又会带来什么缺点呢？ 加上全局锁，意味着整个数据库都是只读状态。 那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而不能更新数据，这样会造成业务停滞。 既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？ 有的，如果数据库的引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作，不会影响备份的数据。 备份数据库的工具是 mysqldump，在使用 mysqldump 时加上 –single-transaction 参数的时候，就会在备份数据库之前先开启事务。这种方法只适用于支持「可重复读隔离级别的事务」的存储引擎。 InnoDB 存储引擎默认的事务隔离级别正是可重复读，因此可以采用这种方式来备份数据库。 但是，对于 MyISAM 这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法。 表级锁 MySQL 表级锁有哪些？具体怎么用的。 MySQL 里面表级别的锁有这几种： 表锁； 表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。 也就是说如果本线程对学生表加了「共享表锁」，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放。 元数据锁（MDL）; 我们不需要显示的使用 MDL，因为当我们对数据库表进行操作时，会自动给这个表加上 MDL： 对一张表进行 CRUD 操作时，加的是 MDL 读锁； 对一张表做结构变更操作的时候，加的是 MDL 写锁； MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。 意向锁； 意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables … read）和独占表锁（lock tables … write）发生冲突。表锁和行锁是满足读读共享、读写互斥、写写互斥的。意向锁的目的是为了快速判断表里是否有记录被加锁，从而判断是否能加表锁。 AUTO-INC 锁； 在插入数据时，会加一个表级别的 AUTO-INC 锁，然后为被 AUTO_INCREMENT 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。 什么 SQL 语句会加行级锁？InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁，所以后面的内容都是基于 InnoDB 引擎 的。 普通的 select 语句是不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过 MVCC（多版本并发控制）实现的。 如果要在查询时对记录加行级锁，可以使用下面这两个方式，这两种查询会加锁的语句称为锁定读。 12345//对读取的记录加共享锁(S型锁)select ... lock in share mode;//对读取的记录加独占锁(X型锁)select ... for update; 上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin 或者 start transaction 开启事务的语句。 **除了上面这两条锁定读语句会加行级锁之外，update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)**。 12345//对操作的记录加独占锁(X型锁)update table .... where id = 1;//对操作的记录加独占锁(X型锁)delete from table where id = 1; 共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。 行级锁有哪些种类？不同隔离级别下，行级锁的种类是不同的。 在读已提交隔离级别下，行级锁的种类只有记录锁（解决脏读），也就是仅仅把一条记录锁上。 在可重复读隔离级别下，行级锁的种类除了有记录锁，还有间隙锁（避免幻读），所以行级锁的种类主要有三类： Record Lock，记录锁，也就是仅仅把一条记录锁上； Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的。 Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。 MySQL 行级锁的加锁规则唯一索引等值查询： 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「记录锁」。 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成「间隙锁」。 非唯一索引等值查询： 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。 当查询的记录「不存在」时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。 非唯一索引和主键索引的范围查询的加锁规则不同之处在于： 唯一索引在满足一些条件的时候，索引的 next-key lock 退化为间隙锁或者记录锁。 非唯一索引范围查询，索引的 next-key lock 不会退化为间隙锁和记录锁。 其实理解 MySQL 为什么要这样加锁，主要要以避免幻读角度去分析，这样就很容易理解这些加锁的规则了。 update 没加索引会锁全表？还有一件很重要的事情，在线上在执行 update、delete、select … for update 等具有加锁性质的语句，一定要检查语句是否走了索引，**==如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了==**，这是挺严重的问题。 解决方案： 打开 MySQL sql_safe_updates 参数 使用 force index([index_name]) 可以告诉优化器使用哪个索引 MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读吗？在 MySQL 的可重复读隔离级别下，针对当前读的语句会对索引加记录锁+间隙锁，这样可以避免其他事务执行增、删、改时导致幻读的问题。 MySQL 死锁了，怎么办？ 避免死锁： 设置事务等待锁的超时时间 开启主动死锁检测 两个事务即使生成的间隙锁的范围是一样的，也不会发生冲突，因为间隙锁目的是为了防止其他事务插入数据，因此间隙锁与间隙锁之间是相互兼容的。 在执行插入语句时，如果插入的记录在其他事务持有间隙锁范围内，插入语句就会被阻塞，因为插入语句在碰到间隙锁时，会生成一个插入意向锁，然后插入意向锁和间隙锁之间是互斥的关系。 如果两个事务分别向对方持有的间隙锁范围内插入一条记录，而插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：互斥、占有且等待、不可强占用、循环等待，因此发生了死锁。 日志三种日志： undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。 redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复； binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制； 为什么需要 undo log？执行执行一条“增删改”语句的时候，虽然没有输入 begin 开启事务和 commit 提交事务，但是 MySQL 会隐式开启事务来执行“增删改”语句的，执行完就自动提交事务的，这样就保证了执行完“增删改”语句后，我们可以及时在数据库表看到“增删改”的结果了。 每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，比如： 在插入一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了； 在删除一条记录时，要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了； 在更新一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列更新为旧值就好了。 在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。 undo log 两大作用： 实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。 实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。 为什么需要 Buffer Pool？Innodb 存储引擎设计了一个缓冲池（Buffer Pool），来提高数据库的读写性能。 有了 Buffer Poo 后： 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。 为什么需要 redo log ？为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。 后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。 WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。 什么是 redo log？ redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。 在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。 当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。 被修改 Undo 页面，需要记录对应 redo log 吗？ 需要的。 开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。 不过，在内存修改该 Undo 页面后，需要记录对应的 redo log。 redo log 和 undo log 区别在哪？ 这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于： redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值； undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值； 事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务。 所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）。可以看出来， redo log 保证了事务四大特性中的持久性。 redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？ 可以说这是 WAL 技术的另外一个优点：MySQL 的写操作从磁盘的「随机写」变成了「顺序写」，提升语句的执行性能。 针对为什么需要 redo log 这个问题我们有两个答案： 实现事务的持久性，让 MySQL 有 crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失； 将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。 产生的 redo log 是直接写入磁盘的吗？ 不是的。 实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。 所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘如下图： redo log 什么时候刷盘？主要有下面几个时机： MySQL 正常关闭时； 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘； InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘 为什么需要 binlog ？binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。 为什么有了 binlog， 还要有 redo log？ 1、适用对象不同： binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用； redo log 是 Innodb 存储引擎实现的日志； 2、文件格式不同： binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致； ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已； MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新； 3、写入方式不同： binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。 redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。 4、用途不同： binlog 用于备份恢复、主从复制； redo log 用于掉电等故障恢复。 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？ 不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。 因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。 binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。 什么时候 binlog cache 会写到 binlog 文件？ MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog =N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 MySQL日志具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下: 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录： 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新； 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样： 如果一样的话就不进行后续更新流程； 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作； 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。 InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。 至此，一条记录更新完了。 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）： prepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘； commit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）； 至此，一条更新语句执行完成。 内存Buffer PoolInnodb 存储引擎设计了一个缓冲池（*Buffer Pool*），来提高数据库的读写性能。 Buffer Pool 以页为单位缓冲数据，可以通过 innodb_buffer_pool_size 参数调整缓冲池的大小，默认是 128 M。 Innodb 通过三种链表来管理缓页： Free List （空闲页链表），管理空闲页； Flush List （脏页链表），管理脏页； LRU List，管理脏页+干净页，将最近且经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。； InnoDB 对 LRU 做了一些优化，我们熟悉的 LRU 算法通常是将最近查询的数据放到 LRU 链表的头部，而 InnoDB 做 2 点优化： 将 LRU 链表 分为young 和 old 两个区域，加入缓冲池的页，优先插入 old 区域；页被访问时，才进入 young 区域，目的是为了解决预读失效的问题。 当「页被访问」且「 old 区域停留时间超过 innodb_old_blocks_time 阈值（默认为1秒）」时，才会将页插入到 young 区域，否则还是插入到 old 区域，目的是为了解决批量数据访问，大量热数据淘汰的问题。 可以通过调整 innodb_old_blocks_pct 参数，设置 young 区域和 old 区域比例。 在开启了慢 SQL 监控后，如果你发现「偶尔」会出现一些用时稍长的 SQL，这可因为脏页在刷新到磁盘时导致数据库性能抖动。如果在很短的时间出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。","link":"/2024/03/12/MySQL/"},{"title":"并发","text":"并发Java 线程和操作系统的线程有啥区别？一句话概括 Java 线程和操作系统线程的关系：现在的 Java 线程的本质其实就是操作系统的线程。 线程持有读锁还能获取写锁吗？【1】在线程持有读锁的情况下，该线程不能取得写锁(因为获取写锁的时候，如果发现当前的读锁被占用，就马上获取失败，不管读锁是不是被当前线程持有)。【2】在线程持有写锁的情况下，该线程可以继续获取读锁(获取读锁时如果发现写锁被占用，只有写锁没有被当前线程占用的情况才会获取失败)。 原因：当线程获取读锁的时候，可能有其他线程也在持有读锁，因此不能把获取读锁的线程“升级”为写锁；而对于获得写锁的线程，它一定独占了读写锁，因此可以继续让他获取读锁，当它同时获取了写锁和读锁后，还可以先释放写锁继续持有读锁，这样一个写锁就“降级”为了读锁。 线程池什么是线程池、为什么要用、底层实现线程池是一种池化技术，主要思想是资源复用。 优点：减少创建销毁的开销；任务响应快；限制线程数量，防止资源利用率过高 底层逻辑是使用线程+阻塞队列实现，队列中有任务线程就会消费，假如没有线程就会被阻塞，等待队列中新的任务的到来，假如队列满了，则会出发设定的饱和策略（包括抛出异常、原线程执行、丢弃任务、丢弃最早未处理任务） 如何创建一般使用 TreadPoolExecutor 建立线程池，有三个重要参数（核心线程数、最大线程数、阻塞队列）。不使用 Executors 去创建的原因：fixedThreadPool 阻塞队列的长度是 Integer.MAX_VALUE，cachedThreadPool 的线程数量最大是Integer.MAX_VALUE。都会导致 OOM 线程池处理任务的流程了解吗？ submit() or execute()一个任务的流程 当前线程数量&lt;核心线程数，新建线程处理任务 阻塞队列未满，加到阻塞队列等待执行 阻塞队列满&amp;当前线程数&lt;最大线程数，新建线程处理 达到最大线程数，使用饱和策略","link":"/2024/03/12/%E5%B9%B6%E5%8F%91/"},{"title":"Redis","text":"Redis 基础什么是 Redis基于 c 开发 NoSQL 数据库 内存数据库，支持持久化 KY键值对数据 为什么要用 Redis/为什么要用缓存？主要是因为 Redis 具备「高性能」和「高并发」两种特性。 1、高性能：直接操作内存 2、高并发：单台设备的 Redis 的 QPS（Query Per Second，每秒钟处理完请求的次数） 是 MySQL 的 10 倍 Redis 单线程模式是怎样的？Redis 数据结构Redis 数据类型以及使用场景分别是什么？常见的有五种数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）、Zset（有序集合）。 Redis 五种数据类型的应用场景： String 类型的应用场景：缓存对象、常规计数、分布式锁、共享 session 信息等。 List 类型的应用场景：消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。 Hash 类型：缓存对象、购物车等。 Set 类型：聚合计算（并集、交集、差集）场景，比如点赞、共同关注、抽奖活动等。 Zset 类型：排序场景，比如排行榜、电话和姓名排序等。 特殊数据类型 BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等； HyperLogLog（2.8 版新增）：海量数据基数统计的场景，比如百万级网页 UV 计数等； GEO（3.2 版新增）：存储地理位置信息的场景，比如滴滴叫车； Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。 五种常见的 Redis 数据类型是怎么实现？ String 类型内部实现 String 类型的底层的数据结构实现主要是 SDS（简单动态字符串）。 SDS 不仅可以保存文本数据，还可以保存二进制数据。因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。 **SDS 获取字符串长度的时间复杂度是 O(1)**。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)；而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1)。 Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。 List 类型内部实现 List 类型的底层数据结构是由双向链表或压缩列表实现的： 如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用压缩列表作为 List 类型的底层数据结构； 如果列表的元素不满足上面的条件，Redis 会使用双向链表作为 List 类型的底层数据结构； 但是在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表。 Hash 类型内部实现 Hash 类型的底层数据结构是由压缩列表或哈希表实现的： 如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用压缩列表作为 Hash 类型的底层数据结构； 如果哈希类型元素不满足上面条件，Redis 会使用哈希表作为 Hash 类型的底层数据结构。 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 Set 类型内部实现 Set 类型的底层数据结构是由哈希表或整数集合实现的： 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Redis 会使用整数集合作为 Set 类型的底层数据结构； 如果集合中的元素不满足上面条件，则 Redis 使用哈希表作为 Set 类型的底层数据结构。 ZSet 类型内部实现 Zset 类型的底层数据结构是由压缩列表或跳表实现的： 如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构； 如果有序集合的元素不满足上面的条件，Redis 会使用跳表作为 Zset 类型的底层数据结构； 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 Redis 持久化Redis 共有三种数据持久化的方式： AOF 日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里； RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘； 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点； AOF 日志是如何实现的？Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。 「$3 set」表示这部分有 3 个字节，也就是「set」命令这个字符串的长度。 为什么先执行命令，再把数据写入日志呢？ 好处： 避免额外的检查开销（语法检查） 不会阻塞当前写操作命令的执行 风险： 数据可能会丢失 可能阻塞后续操作 AOF 写回策略有几种？ AOF 日志过大，会触发什么机制？ Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。 重写 AOF 日志的过程是怎样的？ redis 的重写 AOF 过程是由后台子进程 *bgrewriteaof* 来完成的 好处： 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 但是重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？ 为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作： 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 RDB 快照是如何实现的呢？AOF 缺点：因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。 RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。 因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。 RDB 做快照时会阻塞线程吗？ Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行： 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程； 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞； Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。 RDB 在执行快照的时候，数据能修改吗？ 可以的，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）。 执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。 如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。 为什么会有混合持久化？RDB 优点是数据恢复速度快，但是快照的频率不好把握。 AOF 优点是丢失数据少，但是数据恢复不快。 混合持久化工作在 AOF 日志重写过程，既保证了 Redis 重启速度，又降低数据丢失风险。 AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。 这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。 加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。 混合持久化优点： 混合持久化结合了 RDB 和 AOF 持久化的优点，开头为 RDB 的格式，使得 Redis 可以更快的启动，同时结合 AOF 的优点，有减低了大量数据丢失的风险。 混合持久化缺点： AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差； 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。 Redis 集群/高可用Redis 如何实现服务高可用？要想设计一个高可用的 Redis 服务，一定要从 Redis 的多服务节点来考虑，比如 Redis 的主从复制、哨兵模式、切片集群。 主从复制 主从复制是 Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是「读写分离」的方式。 主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。 也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。 注意，主从服务器之间的命令复制是异步进行的。 具体来说，在主从服务器命令传播阶段，主服务器收到新的写命令后，会发送给从服务器。但是，主服务器并不会等到从服务器实际执行完命令后，再把结果返回给客户端，而是主服务器自己在本地执行完命令后，就会向客户端返回结果了。如果从服务器还没有执行主服务器同步过来的命令，主从服务器间的数据就不一致了。 所以，无法实现强一致性保证（主从数据时时刻刻保持一致），数据不一致是难以避免的。 哨兵模式 在使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复。 为了解决这个问题，Redis 增加了哨兵模式（Redis Sentinel），因为哨兵模式做到了可以监控主从服务器，并且提供主从节点故障转移的功能。 切片集群模式 当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 Redis 切片集群（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。 Redis Cluster 方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步： 根据键值对的 key，按照 CRC16 算法 (opens new window)计算一个 16 bit 的值。 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。 接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案： 平均分配： 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。 手动分配： 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。 为了方便你的理解，我通过一张图来解释数据、哈希槽，以及节点三者的映射分布关系。 上图中的切片集群一共有 2 个节点，假设有 4 个哈希槽（Slot 0～Slot 3）时，我们就可以通过命令手动分配哈希槽，比如节点 1 保存哈希槽 0 和 1，节点 2 保存哈希槽 2 和 3。 12redis-cli -h 192.168.1.10 –p 6379 cluster addslots 0,1redis-cli -h 192.168.1.11 –p 6379 cluster addslots 2,3 然后在集群运行的过程中，key1 和 key2 计算完 CRC16 值后，对哈希槽总个数 4 进行取模，再根据各自的模数结果，就可以被映射到哈希槽 1（对应节点1） 和 哈希槽 2（对应节点2）。 需要注意的是，在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。 集群脑裂导致数据丢失怎么办？ 什么是脑裂？ 总结：由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。 解决方案 当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。 原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了。 等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。 Redis 过期删除与内存淘汰Redis 使用的过期删除策略是什么？Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。 每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个过期字典（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。 当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中： 如果不在，则正常读取键值； 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。 Redis 使用的过期删除策略是「惰性删除+定期删除」这两种策略配和使用。 什么是惰性删除策略？ 惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。 惰性删除策略的优点： 因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。 惰性删除策略的缺点： 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。 什么是定期删除策略？ 定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。 Redis 的定期删除的流程： 从过期字典中随机抽取 20 个 key； 检查这 20 个 key 是否过期，并删除已过期的 key； 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。 可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。 定期删除策略的优点： 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。 定期删除策略的缺点： 难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。 可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。 Redis 持久化时，对过期键会如何处理的？Redis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File），下面我们分别来看过期键在这两种格式中的呈现状态。 RDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。 RDB 文件生成阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键「不会」被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。 RDB 加载阶段：RDB 加载阶段时，要看服务器是主服务器还是从服务器，分别对应以下两种情况： 如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中。所以过期键不会对载入 RDB 文件的主服务器造成影响； 如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。 AOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。 AOF 文件写入阶段：当 Redis 以 AOF 模式持久化时，如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值。 AOF 重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响。 Redis 主从模式中，对过期键会如何处理？当 Redis 运行在主从模式下时，从库不会进行过期扫描，从库对过期的处理是被动的。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。 从库的过期键处理依靠主服务器控制，主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key。 Redis 内存淘汰策略有哪些？Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。 1、不进行数据淘汰的策略 noeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。 2、进行数据淘汰的策略 针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 在设置了过期时间的数据中进行淘汰： volatile-random：随机淘汰设置了过期时间的任意键值； volatile-ttl：优先淘汰更早过期的键值。 volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值； volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值； 在所有数据范围内进行淘汰： allkeys-random：随机淘汰任意键值; allkeys-lru：淘汰整个键值中最久未使用的键值； allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。 **缓存*缓存读写策略有哪几种？**Cache Aside Pattern（旁路缓存模式）写： 先更新 db 然后直接删除 cache 。 读： 从 cache 中读取数据，读取到就直接返回 cache 中读取不到的话，就从 db 中读取数据返回 再把数据放到 cache 中。 问题： 在写数据的过程中，可以先删除 cache ，后更新 db 么？ redis 速度要比 db 快，删 cache 和更新 db 中间可能会穿插读操作 假如使用了该方案可以用双删解决 在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？ 发生情况概率很小，redis 速度要比 db 快。在缓存没有数据情况下，从 db 读数据和更新 cache 中穿插更新 db 操作 Cache Aside 策略适合读多写少的场景，不适合写多的场景 Cache Aside Pattern 的缺陷 缺陷 1：首次请求数据一定不在 cache 的问题 解决办法：可以将热点数据可以提前放入 cache 中。 缺陷 2：写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率 。 解决办法：数据库和缓存数据强一致（加锁同步更新） Read/Write Through Pattern（读写穿透）写（Write Through）： 先查 cache，cache 中不存在，直接更新 db。 cache 中存在，则先更新 cache，然后 cache 服务自己更新 db（同步更新 cache 和 db）。 读(Read Through)： 从 cache 中读取数据，读取到就直接返回 。 读取不到的话，先从 db 加载，写入到 cache 后返回响应。 问题： 和 Cache Aside Pattern 一样， Read-Through Pattern 也有首次请求数据一定不再 cache 的问题，对于热点数据可以提前放入缓存中。 Write Behind Pattern（异步缓存写入）Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。 Write Back 策略特别适合写多的场景 问题：还没同步，缓存宕掉 使用场景：消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制 优势：db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。 缓存穿透根本不存在于缓存中，也不存在于数据库中 解决： 1）缓存空值或者默认值 *2）布隆过滤器 布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成 布隆过滤器会通过 3 个操作完成标记： 第一步，使用 N 个哈希函数分别对数据做哈希计算，得到 N 个哈希值； 第二步，将第一步得到的 N 个哈希值对位图数组的长度取模，得到每个哈希值在位图数组的对应位置。 第三步，将每个哈希值在位图数组的对应位置的值设置为 1； 缺点：存在哈希冲突的可能性，判断可能存在，但是不一定存在；但判断不存在，一定不存在 3）非法请求限制 在 API 入口处我们要判断求请求参数是否合理（请求参数是否含有非法值、请求字段是否存在） 缓存雪崩 缓存在同一时间大面积的失效或者redis宕机，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力。 针对大量缓存失效的情况： 设置不同的失效时间比如随机设置缓存的失效时间。 互斥锁，保证只有一个请求来构建缓存 不给热点数据设置过期时间，由后台异步更新缓存 针对 Redis 故障宕机情况： 采用 Redis 集群。主从节点的方式，如果 Redis 缓存的主节点故障宕机，从节点可以切换成为主节点，继续提供缓存服务 服务熔断。暂停业务应用对缓存服务的访问，直接返回错误 请求限流机制。只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务 缓存击穿请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 可以发现缓存击穿跟缓存雪崩很相似，你可以认为缓存击穿是缓存雪崩的一个子集。 解决： 不给热点数据设置过期时间，由后台异步更新缓存 请求数据库写数据到缓存之前，先获取互斥锁，保证只有一个请求会落到数据库上，减少数据库的压力。 数据库和缓存如何保证一致性？1）先更新数据库，再更新缓存；先更新缓存，再更新数据库 当两个请求并发更新同一条数据的时候，可能会出现缓存和数据库中的数据不一致的现象 2）先更新数据库，还是先删除缓存？ Cache Aside 策略：先更新数据库，再删除缓存 当出现读-更新-删，会出现不一致。但出现不一致的概率低，因为缓存的写入通常要远远快于数据库的写入， 所以，「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的。同时可以还给缓存数据加上了「过期时间」来兜底，达到最终一致。 但是更新数据库和删除缓存不能保证两个操作都执行成功，也就会需要用到过期时间来兜底，这个中间会存在不一致性。 解决方法： 重试机制。 引入消息队列，将第二个操作（删除缓存）要操作的数据加入到消息队列，由消费者来操作数据。 订阅 MySQL binlog，再操作缓存 两种方法都是采用异步操作缓存 为什么是删除缓存，而不是更新缓存呢？删除一个数据，相比更新一个数据更加轻量级，出问题的概率更小。在实际业务中，缓存的数据可能不是直接来自数据库表，也许来自多张底层数据表的聚合。 系统设计中有一个思想叫 Lazy Loading，适用于那些加载代价大的操作，删除缓存而不是更新缓存，就是懒加载思想的一个应用。 Redis 应用基于 Redis 实现分布式锁","link":"/2024/03/14/Redis/"},{"title":"Spring","text":"**Spring 核心之控制反转（IOC）控制反转 - Inversion of Control (IoC)： IoC 不是一种技术，只是一种思想，它能指导我们设计出松耦合、更优良的程序。把创建和查找注入依赖对象的控制权交给了IoC容器，所以对象与对象之间是松散耦合，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。 也就是从原来的由用户管理Bean转变为IoC 容器管理Bean。 谁控制谁，控制什么？ 谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取 为何是反转，哪些方面反转了? 为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 IoC和DI是什么关系控制反转是通过依赖注入实现的，其实它们是同一个概念的不同角度描述。“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。通俗来说就是IoC是设计思想，DI是实现方式。 IoC配置的三种方式xml 配置顾名思义，就是将bean的信息配置.xml文件里，通过Spring加载文件为我们创建bean。这种方式出现很多早前的SSM项目中，将第三方类库或者一些配置工具类都以这种方式进行配置，主要原因是由于第三方类不支持Spring注解。 优点： 可以使用于任何场景，结构清晰，通俗易懂 缺点： 配置繁琐，不易维护，枯燥无味，扩展性差 举例： 配置xx.xml文件 声明命名空间和配置bean Java 配置将类的创建交给我们配置的JavcConfig类来完成，Spring只负责维护和管理，采用纯Java创建方式。其本质上就是把在XML上的配置声明转移到Java配置类中 优点：适用于任何场景，配置方便，因为是纯Java代码，扩展性高，十分灵活 缺点：由于是采用Java类的方式，声明不明显，如果大量配置，可读性比较差 举例： 创建一个配置类， 添加@Configuration注解声明为配置类 创建方法，方法上加上@bean，该方法用于创建实例并返回，该实例创建后会交给spring管理，方法名建议与实例名相同（首字母小写）。注：实例类不需要加任何注解 注解配置通过在类上加注解的方式，来声明一个类交给Spring管理，Spring会自动扫描带有@Component，@Controller，@Service，@Repository这四个注解的类，然后帮我们创建并管理，前提是需要先配置Spring的注解扫描器。 优点：开发便捷，通俗易懂，方便维护。 缺点：具有局限性，对于一些第三方资源，无法添加注解。只能采用XML或JavaConfig的方式配置 举例： 对类添加@Component相关的注解，比如@Controller，@Service，@Repository 设置ComponentScan的basePackage, 比如&lt;context:component-scan base-package='tech.pdai.springframework'&gt;, 或者@ComponentScan(\"tech.pdai.springframework\")注解，或者 new AnnotationConfigApplicationContext(\"tech.pdai.springframework\")指定扫描的basePackage. 依赖注入的三种方式setter方式在XML配置方式中，property都是setter方式注入 本质上包含两步： 第一步，需要new UserServiceImpl()创建对象, 所以需要默认构造函数 第二步，调用setUserDao()函数注入userDao的值, 所以需要setUserDao()函数 构造函数在XML配置方式中，&lt;constructor-arg&gt;是通过构造函数参数注入 注解注入以@Autowired（自动注入）注解注入为例，修饰符有三个属性：Constructor，byType，byName。默认按照byType注入。 constructor：通过构造方法进行自动注入，spring会匹配与构造方法参数类型一致的bean进行注入，如果有一个多参数的构造方法，一个只有一个参数的构造方法，在容器中查找到多个匹配多参数构造方法的bean，那么spring会优先将bean注入到多参数的构造方法中。 1234567public class FooService { private FooFormatter fooFormatter; @Autowired public FooService(FooFormatter fooFormatter) { this.fooFormatter = fooFormatter; }} byName：被注入bean的id名必须与set方法后半截匹配，并且id名称的第一个单词首字母必须小写，这一点与手动set注入有点不同。 byType：查找所有的set方法，将符合符合参数类型的bean注入。 著作权归@pdai所有 原文链接：https://pdai.tech/md/spring/spring-x-framework-ioc.html @Autowired和@Resource以及@Inject等注解注入有何区别？@Autowired Autowired注解源码 在Spring 2.5 引入了 @Autowired 注解 123456@Target({ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Autowired { boolean required() default true;} 从Autowired注解源码上看，可以使用在下面这些地方： 12345@Target(ElementType.CONSTRUCTOR) #构造函数@Target(ElementType.METHOD) #方法@Target(ElementType.PARAMETER) #方法参数@Target(ElementType.FIELD) #字段、枚举的常量@Target(ElementType.ANNOTATION_TYPE) #注解 还有一个value属性，默认是true。 简单总结： 1、@Autowired是Spring自带的注解，通过AutowiredAnnotationBeanPostProcessor 类实现的依赖注入 2、@Autowired可以作用在CONSTRUCTOR、METHOD、PARAMETER、FIELD、ANNOTATION_TYPE 3、@Autowired默认是根据类型（byType ）进行自动装配的 4、如果有多个类型一样的Bean候选者，需要指定按照名称（byName ）进行装配，则需要配合@Qualifier。 简单使用代码： 在字段属性上。 12@Autowiredprivate HelloDao helloDao; 或者 12345678private HelloDao helloDao;public HelloDao getHelloDao() { return helloDao;}@Autowiredpublic void setHelloDao(HelloDao helloDao) { this.helloDao = helloDao;} 或者 123456private HelloDao helloDao;//@Autowiredpublic HelloServiceImpl(@Autowired HelloDao helloDao) { this.helloDao = helloDao;}// 构造器注入也可不写@Autowired，也可以注入成功。 将@Autowired写在被注入的成员变量上，setter或者构造器上，就不用再xml文件中配置了。 如果有多个类型一样的Bean候选者，则默认根据设定的属性名称进行获取。如 HelloDao 在Spring中有 helloWorldDao 和 helloDao 两个Bean候选者。 12@Autowiredprivate HelloDao helloDao; 首先根据类型获取，发现多个HelloDao，然后根据helloDao进行获取，如果要获取限定的其中一个候选者，结合@Qualifier进行注入。 123@Autowired@Qualifier(\"helloWorldDao\")private HelloDao helloDao; 注入名称为helloWorldDao 的Bean组件。@Qualifier(“XXX”) 中的 XX是 Bean 的名称，所以 @Autowired 和 @Qualifier 结合使用时，自动注入的策略就从 byType 转变成 byName 了。 多个类型一样的Bean候选者，也可以@Primary进行使用，设置首选的组件，也就是默认优先使用哪一个。 注意：使用@Qualifier 时候，如何设置的指定名称的Bean不存在，则会抛出异常，如果防止抛出异常，可以使用： 123@Qualifier(\"xxxxyyyy\")@Autowired(required = false)private HelloDao helloDao; 在SpringBoot中也可以使用@Bean+@Autowired进行组件注入，将@Autowired加到参数上，其实也可以省略。 12345@Beanpublic Person getPerson(@Autowired Car car){ return new Person();}// @Autowired 其实也可以省略 @Resource Resource注解源码 123456@Target({TYPE, FIELD, METHOD})@Retention(RUNTIME)public @interface Resource { String name() default \"\"; // 其他省略} 从Resource注解源码上看，可以使用在下面这些地方： 123@Target(ElementType.TYPE) #接口、类、枚举、注解@Target(ElementType.FIELD) #字段、枚举的常量@Target(ElementType.METHOD) #方法 name 指定注入指定名称的组件。 简单总结： 1、@Resource是JSR250规范的实现，在javax.annotation包下 2、@Resource可以作用TYPE、FIELD、METHOD上 3、@Resource是默认根据属性名称进行自动装配的，如果有多个类型一样的Bean候选者，则可以通过name进行指定进行注入 简单使用代码： 12345@Componentpublic class SuperMan { @Resource private Car car;} 按照属性名称 car 注入容器中的组件。如果容器中BMW还有BYD两种类型组件。指定加入BMW。如下代码： 12345@Componentpublic class SuperMan { @Resource(name = \"BMW\") private Car car;} name 的作用类似 @Qualifier @Inject Inject注解源码 1234@Target({ METHOD, CONSTRUCTOR, FIELD })@Retention(RUNTIME)@Documentedpublic @interface Inject {} 从Inject注解源码上看，可以使用在下面这些地方： 123@Target(ElementType.CONSTRUCTOR) #构造函数@Target(ElementType.METHOD) #方法@Target(ElementType.FIELD) #字段、枚举的常量 简单总结： 1、@Inject是JSR330 (Dependency Injection for Java)中的规范，需要导入javax.inject.Inject jar包 ，才能实现注入 2、@Inject可以作用CONSTRUCTOR、METHOD、FIELD上 3、@Inject是根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named； 简单使用代码： 12@Injectprivate Car car; 指定加入BMW组件。 123@Inject@Named(\"BMW\")private Car car; @Named 的作用类似 @Qualifier！ 总结1、@Autowired是Spring自带的，@Resource是JSR250规范实现的，@Inject是JSR330规范实现的 2、@Autowired、@Inject用法基本一样，不同的是@Inject没有required属性 3、@Autowired、@Inject是默认按照类型匹配的，@Resource是按照名称匹配的 4、@Autowired如果需要按照名称匹配需要和@Qualifier一起使用，@Inject和@Named一起使用，@Resource则通过name进行指定 **Spring中 Bean 的生命周期Spring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。 如果 BeanFactoryPostProcessor 和 Bean 关联, 首先尝试从Bean工厂中获取Bean) 如果 InstantiationAwareBeanPostProcessor 和 Bean 关联，则会调用实例化前的方法 根据配置情况调用 Bean 构造方法实例化 Bean。 利用依赖注入完成 Bean 中所有属性值的配置注入。 如果 InstantiationAwareBeanPostProcessor 和 Bean 关联，则会调用实例化后的方法 调用Bean 实现的Aware接口 ，如 BeanNameAware、BeanClassLoaderAware、BeanFactoryAware、ApplicationContextAware，设置当前 Bean 的一些属性 如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的预初始化方法对 Bean 进行加工操作，此处非常重要，Spring 的 AOP 就是利用它实现的。 如果 Bean 实现了 InitializingBean 接口，则 Spring 将调用 afterPropertiesSet() 方法。(或者有执行@PostConstruct注解的方法) 如果在配置文件中通过 init-method 属性指定了初始化方法，则调用该初始化方法。 如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的初始化方法 。此时，Bean 已经可以被应用系统使用了。 如果在 &lt;bean&gt; 中指定了该 Bean 的作用范围为 scope=”singleton”，则将该 Bean 放入 Spring IoC 的缓存池中，将触发 Spring 对该 Bean 的生命周期管理；如果在 &lt;bean&gt; 中指定了该 Bean 的作用范围为 scope=”prototype”，则将该 Bean 交给调用者，调用者管理该 Bean 的生命周期，Spring 不再管理该 Bean。 如果 Bean 实现了 DisposableBean 接口，则 Spring 会调用 destory() 方法将 Spring 中的 Bean 销毁；(或者有执行@PreDestroy注解的方法) 如果在配置文件中通过 destory-method 属性指定了 Bean 的销毁方法，则 Spring 将调用该方法对 Bean 进行销毁。 **Spring如何解决循环依赖问题Spring单例模式下的属性依赖三级缓存： 第一层缓存（singletonObjects）：单例对象缓存池，已经实例化并且属性赋值，这里的对象是成熟对象； 第二层缓存（earlySingletonObjects）：单例对象缓存池，已经实例化但尚未属性赋值，这里的对象是半成品对象； 第三层缓存（singletonFactories）: 单例工厂的缓存 解决循环依赖的关键是通过将实例化但是还没有初始化的对象提高曝光在第三级缓存 singtonfactory 好比“A对象setter依赖B对象，B对象setter依赖A对象”，A首先实例化并将本身提早曝光到singletonFactories中，此时要进行属性赋值，发现本身依赖对象B，此时就尝试去get(B)，发现B尚未被create，因此走create流程，B在属性赋值的时候发现本身依赖了对象A，因而尝试get(A)，尝试一级缓存singletonObjects(确定没有，由于A还没初始化彻底)，尝试二级缓存earlySingletonObjects（也没有），尝试三级缓存singletonFactories，因为A经过ObjectFactory将本身提早曝光了，因此B可以经过ObjectFactory.getObject拿到A对象(半成品)，B拿到A对象后顺利完成了初始化，彻底初始化以后将本身放入到一级缓存singletonObjects中。此时返回A中，A此时能拿到B的对象顺利完成本身的初始化阶段，最终A也完成了初始化，进去了一级缓存singletonObjects中，并且更加幸运的是，因为B拿到了A的对象引用，因此B如今hold住的A对象完成了初始化。 Spring为何不能解决非单例属性之外的循环依赖？Spring为什么不能解决构造器的循环依赖？构造器注入形成的循环依赖： 也就是beanB需要在beanA的构造函数中完成初始化，beanA也需要在beanB的构造函数中完成初始化，这种情况的结果就是两个bean都不能完成初始化，循环依赖难以解决。 Spring解决循环依赖主要是依赖三级缓存，但是的在调用构造方法之前还未将其放入三级缓存之中，因此后续的依赖调用构造方法的时候并不能从三级缓存中获取到依赖的Bean，因此不能解决。 Spring为什么不能解决prototype作用域循环依赖？这种循环依赖同样无法解决，因为spring不会缓存‘prototype’作用域的bean，而spring中循环依赖的解决正是通过缓存来实现的。 Spring为什么不能解决多例的循环依赖？多实例Bean是每次调用一次getBean都会执行一次构造方法并且给属性赋值，根本没有三级缓存，因此不能解决循环依赖。 那么其它循环依赖如何解决？ 生成代理对象产生的循环依赖 这类循环依赖问题解决方法很多，主要有： 使用@Lazy注解，延迟加载 使用@DependsOn注解，指定加载先后关系 修改文件名称，改变循环依赖类的加载顺序 使用@DependsOn产生的循环依赖 这类循环依赖问题要找到@DependsOn注解循环依赖的地方，迫使它不循环依赖就可以解决问题。 多例循环依赖 这类循环依赖问题可以通过把bean改成单例的解决。 构造器循环依赖 这类循环依赖问题可以通过使用@Lazy注解解决。","link":"/2024/03/12/Spring/"}],"tags":[{"name":"面经","slug":"面经","link":"/tags/%E9%9D%A2%E7%BB%8F/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"},{"name":"cs","slug":"cs","link":"/categories/cs/"}],"pages":[]}