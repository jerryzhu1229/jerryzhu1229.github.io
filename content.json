{"posts":[{"title":"面经","text":"3/13 芒星面经 策略模式跟状态模式的区别 黑盒测试、白盒测试 一个项目的研发过程 进程有哪几个状态 如何用 ArrayList 写一个阻塞队列 策略模式封装的是行为，而状态模式封装的是变化。策略是外界给的，策略怎么变，是调用者考虑的事情，系统只是根据所给的策略做事情。状态是系统自身的固有的，由系统本身控制，调用者不能直接指定或改变系统的状态转移。 黑盒测试也称：数据驱动测试，包括功能测试和性能测试。白盒测试也称：逻辑驱动测试，包括语句覆盖、判定覆盖、条件覆盖、路径覆盖等。判定某种方法是否为黑盒测试方法，关键还是看是否针对被测对象内部结构还是针对被测对象的整体进行测试。 策略（Strategy）模式的定义：该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于行为模式。 3/20 腾讯 pcg 一面 os 内核级别 shell 脚本 if-else 如何实现 mysql 如何查看死锁 array 跟 arraylist 的区别 innodb 跟 myisam 的读写性能区别 Linux一般会有7个运行级别（可由init N来切换，init0为关机，init 6为重启系统）0 - 停机1 - 单用户模式2 - 多用户，但是没有NFS ，不能使用网络3 - 完全多用户模式4 - 打酱油的，没有用到5 - X11 图形化登录的多用户模式6 - 重新启动 （如果将默认启动模式设置为6，Linux将会不断重启） 是否死锁 1234567891011121314151617181920use ***db;show ENGINE INNODB STATUS;其他方法：1、查询是否锁表show OPEN TABLES where In_use &gt; 0;2、查询进程show processlist (查询到相对应的进程===然后 kill id )3、查看正在锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 4、查看等待锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; 9/25 中冶赛迪 二面 软件开发流程（需求评审～～～） 如何进行一个软件开发的统筹 锁的类型 kafka 和 rabbitmq 的区别 rpc 定义 规则引擎如何实现，库表设计 接口优化方案 XXL-JOb核心组件admin：任务管理后台，用于配置和管理任务。 executor：任务执行器，用于执行任务。可以有多个Executor节点，实现任务的分布式执行。 jobcore：任务核心配置，包括任务的执行时间、执行器、调度策略等。 jobhandler：任务处理器，实际执行任务的逻辑。 任务调度原理任务在 admin 配置好后，被分配给 executor 节点，executor 节点根据调度策略和执行时间来执行任务。使用数据库来存储任务信息和日志，确保任务的可靠性 如何进行分片处理开发人员需要实现分片任务的逻辑，并在任务处理器（JobHandler）中指定分片参数。Executor节点会根据分片参数来划分任务，并并行执行。 XXL-Job的定时任务和周期性任务有何区别定时任务是指任务在指定的时间点执行一次，而周期性任务是指任务按照固定的时间间隔反复执行。XXL-Job支持配置定时任务和周期性任务，用户可以根据实际需求选择合适的任务类型。 XXL-Job如何保证任务的高可用性 支持多个Executor节点，实现任务的分布式执行，一台Executor节点故障不会影响整个系统。 支持任务的故障转移，如果某个Executor节点故障，任务可以自动切换到其他可用节点执行。 使用数据库来存储任务信息和执行日志，确保任务的可恢复性。 XXL-Job的任务执行失败了，如何排查和处理？如果XXL-Job的任务执行失败，可以采取以下步骤进行排查和处理： 查看任务执行日志，了解失败原因和异常信息。 检查任务处理器（JobHandler）的实现，确保代码逻辑正确。 检查任务的调度策略和参数配置，是否合理。 查看Executor节点的日志，检查Executor是否正常运行。 如果任务执行超时，可以适当调整任务的超时时间。 如果任务执行异常，可以考虑增加报警规则，及时发现和处理异常任务。 XXL-Job如何处理分片任务的失败和重试？XXL-Job对分片任务的失败和重试提供了支持。如果分片任务的某个分片执行失败，XXL-Job会自动进行重试，直到达到最大重试次数或任务成功执行为止。开发人员可以配置分片任务的最大重试次数和重试策略。 XXL-Job的任务调度器是如何实现的？XXL-Job的任务调度器基于Quartz Scheduler实现。Quartz是一个开源的任务调度框架，提供了丰富的调度功能，XXL-Job在其基础上进行了定制化的扩展。","link":"/2024/03/11/%E9%9D%A2%E7%BB%8F/"},{"title":"操作系统","text":"进程和线程进程有哪几种状态创建、就绪、执行、阻塞、结束 java 线程有哪几种状态new、runnable、blocked、waiting、time_waiting、terminated 区别定义： 进程：进程是程序在计算机中的一次执行活动，是操作系统进行资源分配和调度的基本单位。 线程：线程是进程的执行单元，是操作系统能够进行运算调度的最小单位。 资源开销： 进程：每个进程都有独立的地址空间和系统资源，因此创建和销毁进程的开销较大。 线程：线程共享相同的地址空间和系统资源，因此创建和销毁线程的开销较小。线程切换的开销也比进程切换的开销小，因为线程之间共享相同的地址空间。 通信方式： 进程：进程间通信需要借助于操作系统提供的进程间通信（IPC）机制，例如管道、消息队列、信号量、共享内存、套接字等。 线程：线程间通信可以直接通过共享内存进行，因为它们共享相同的地址空间，也可以通过同步机制（如互斥锁、条件变量）进行通信。 并发性： 进程：进程是独立的执行单位，不同进程之间的执行互不干扰，具有较强的隔离性。 线程：线程共享相同的地址空间，可以更方便地进行数据共享和通信，但也增加了线程间竞争和同步的复杂性。 进程的通信方式 管道 匿名管道，通信范围是存在父子关系的进程 命名管道，可以在不相关的进程间也能相互通信 不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，存在通信方式是效率低的问题 消息队列。消息队列是保存在内核中的消息链表，但存在用户态与内核态之间的数据拷贝开销的问题 共享内存。拿出一块虚拟地址空间来，映射到相同的物理内存中，但存在多进程竞争共享资源问题。 信号量。其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据。 互斥信号量： 同步信号量： 信号，对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。信号是进程间通信机制中唯一的异步通信机制，因为可以在任何时候发送信号给某一进程。 过程：用kill函数发送信号，在接收进程里，通过signal函数调用sighandler，来启动对应的函数处理信号消息 Socket，不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信 本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是绑定一个本地文件，这也就是它们之间的最大区别。 线程的通信方式锁机制：包括互斥锁、条件变量、读写锁 互斥锁提供了以排他方式防止数据结构被并发修改的方法。 读写锁允许多个线程同时读共享数据，而对写操作是互斥的。 条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量 信号机制(Signal)：类似进程间的信号处理 僵尸进程和孤儿进程的区别？僵尸进程：一个子进程退出时会处于 ZOMBIE 状态，此时它占用的进程描述符没有被释放，只有等它的父进程调用 wait() 或 waitpid() 获取到子进程的信息后，最后由父进程决定是否将子进程资源释放。如果子进程的资源由于某种原因一直得不到释放，那么就一直处于僵死状态，变成了僵尸进程。 孤儿进程：当父进程退出了，但是它的子进程还没有退出，这些子进程就变成了孤儿进程。孤儿进程只是暂时的，系统会在父进程退出时启动寻父机制，为子进程找到一个新的父亲：首先在当前进程组中寻找，如果找不到就会返回 init (PID=1) 进程作为父进程。 系统中如果驻留大量的僵死进程是危险的，因为会一直占用系统资源，解决的直接办法就是杀死父进程，让他们变成孤儿进程，最后会被新的进程领养，新的父进程会例行调用 wait() 来检查子进程状态，清除相关的僵死进程。 进程是怎么调度的？批处理系统： 先来先服务(FCFS) 短作业优先 交互式系统： 时间片轮转调度 优先级调度 实时系统： 软实时和硬实时。前者可以容忍一定时间的延迟，而后者需要满足绝对的截止时间 进程有哪几种状态，他们是如何转换的？进程主要有三种状态：运行态、就绪态、阻塞态。 进程和线程的创建方式？Linux 中进程的创建主要是通过 fork 系统调用，线程被当做一种特殊的进程，也是用 fork 创建，不过通过传递不同的参数，指明共享父进程的地址空间，打开的文件等资源。 Windows 创建进程执行的是 CreateProcess，而线程创建是 Pthread_create 子进程创建时会拷贝父进程哪些资源？Linux系统中，子进程的创建不会马上拷贝父进程的所有资源，而是以只读的方式共享大部分父进程的资源，当需要修改地址空间资源时，触发只读保护，这时才会拷贝一份地址空间。这种机制叫做 **写时拷贝(copy-on-write)**。这种优化可以避免拷贝大量根本不会使用到的数据。 fork 系统调用实际上只是为子进程创建一个唯一的进程描述符，分配了一个有效的 PID，有的 Linux 系统 fork 调用也会复制一份父进程的页表。 进程上下文切换和线程上下文切换进程上下文切换不仅需要保存虚拟内存、全局变量、文件描述符等用户空间资源，还需要保存内核堆栈、寄存器、程序计数器等内核资源。 线程上下文切换只需要保存自己的线程栈和寄存器内容，比进程切换开销小很多。 什么是系统调用？为什么要有系统调用？系统调用是在一个进程中，由用户态切换到内核态，在内核中执行任务，或者申请操作系统的资源。系统调用是一种保护操作系统的机制，它提供一系列定义良好的 API 接口来和操作系统交互，避免用户程序直接对内核进行操作，保证了系统的稳定、安全、可靠。 内核态和用户态是什么这两种状态其实对应着应用程序访问资源的权限：在用户态只能访问受限的资源，如虚拟内存，全局变量等，而要访问内核等资源需要通过系统调用等方式陷入到内核中；内核态可以访问操作系统的所有资源，包括内存、I/O 等资源。 如何实现进程同步？ 信号量：是一个整型变量，用来实现计数器功能，主要提供 down 和 up 操作（即 P 和 V 操作），这两个操作都是原子性的。当执行 down 操作使信号量值变为 0 时，会导致当前进程睡眠，而执行 up 操作 +1 时，会同时唤醒一个进程。 管程：管程是由一个过程、变量和数据结构组成的一个集合，把需要控制的那部分代码独立出来执行，它有一个重要的特性，同一时刻在管程中只能有一个活跃的进程。为了避免一个进程一直占用管程，引入了条件变量和 wait 和 signal 操作。当发生当前进程无法运行时，执行 wait 操作，将当前进程阻塞，同时调入在管程外等待的另一进程执行，而另一个进程满足条件变量时，会执行 signal 操作将正在睡眠的进程唤醒，然后马上退出管程。 硬中断&amp;&amp;软中断中断处理程序的上部分和下半部可以理解为： 上半部直接处理硬件请求，也就是硬中断，主要是负责耗时短的工作，特点是快速执行； 下半部是由内核触发，也就说软中断，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行； 软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。 虚拟内存物理内存和虚拟内存的区别 内存空间：物理内存是实际存在的计算机内存，又计算机硬件管理。虚拟内存是一个抽象概念，使用硬盘空间来模拟物理内存，以扩展可用内存空间。 访问速度：物理内存的访问速度非常快，通常只需要几纳秒。虚拟内存的访问速度相对较慢，通常需要几毫秒。 大小限制：物理内存的大小通常是固定的，取决于计算机硬件的配置。虚拟内存的大小通常是可变的，取决于操作系统的配置和需要。 如何转变，机制是什么操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存 内存分段和内存分页：段表、页表 为什么需要虚拟内存 让进程的运行内存超过物理内存大小，由于局部性原理，对于没有进程使用的内存可以换出物理内存。 每个进程都有自己的页表，即各自的虚拟内存空间是独立的，解决了多进程之间地址冲突问题。 分段和分页区别 段则是信息的逻辑单位，分成了栈段、堆段、数据段、代码段等不同属性的段。分段的目的是为了能更好地满足用户的需要。页是信息的物理单位，分页是为实现离散分配方式，提高内存的利用率。 页的大小固定，且由系统决定；而段的长度却不固定，决定于用户所编写的程序 分页的地址空间是一维的，程序员只需利用一个记忆符，即可表示一个地址；而分段的作业地址空间是二维的，程序员在标识一个地址时，既需给出段名，又需给出段内地址 为什么要分段分页 分段是将程序分为具有逻辑意义的不同大小的段，方便实现信息的共享与保护。 分页是为了解决分段粒度大，因为段需要整段的加载进内存以及整段换出，造成内存碎片大，不易于管理（不会产生外部碎片） 分段优点： 将程序分为具有逻辑意义的不同大小的段，方便实现信息的共享与保护 分段缺点： 外部内存碎片 内存交换的效率低（段太大，导致内存交换代价高） 分页优点： 利用率高，解决外部碎片，但是存在内部碎片 避免整段换出 分页缺点： 不好实现信息的共享与保护 分页：分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小 页表：虚拟地址与物理地址之间通过页表来映射 多级页表 页表一定要覆盖全部虚拟地址空间 如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表 I/O 多路复用：select/poll/epollselect、poll 和 epoll 是用于在 Unix/Linux 系统中进行 I/O 复用的系统调用。它们的目的都是为了在一个进程中同时监视多个文件描述符的可读、可写和异常事件，以便实现高效的 I/O 操作。它们的主要区别如下： select： 使用一个位图（bitmap）来表示文件描述符集合，文件描述符数量有限。 效率较低，每次调用 select 函数都需要将文件描述符集合从用户态拷贝到内核态，通过轮询的方式去判断该 socket 是否有事件发生，若有则标记为可读或可写，然后内核再将就绪的文件描述符集合从内核态拷贝到用户态，再去通过轮询去找到哪些 socket 可读或者可写，开销较大。 poll： 使用动态数组来表示文件描述符集合，支持的文件描述符数量也有限。 相对于 select，poll 的效率略微提高，但也是轮询查看是否就绪 epoll： 用法：先用epoll_create 创建一个 epoll对象 epfd可以支持非常大的文件描述符集合（上限就为系统定义的进程打开的最大文件描述符个数），再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。 epoll 通过两个方面，很好解决了 select/poll 的问题。 第一点不用来回复制，epoll 在内核里使用红黑树来跟踪进程所有待检测的文件描述符，把需要监控的 socket 通过 epoll_ctl() 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)。 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket 第二点不用轮询判断， epoll 使用事件驱动的机制，内核里通过rdllist双向链表来记录就绪事件，当某个 socket 有事件发生时，通过回调函数内核会将其加入到rdlist中，当用户调用 epoll_wait() 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。 epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。 Socket 编程过程 服务端首先调用 socket() 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket 接着调用 bind() 函数，给这个 Socket 绑定一个 IP 地址和端口，目的是 绑定端口的目的：通过 tcp端口号，来找到对应的应用程序 绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址 绑定完 IP 地址和端口后，就可以调用 listen() 函数进行监听，此时对应 TCP 状态图中的 listen 服务端进入了监听状态后，通过调用 accept() 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。 客户端在创建好 Socket 后，调用 connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。 当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的 Socket 返回应用程序，后续数据传输都用这个 Socket。 连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 read() 和 write() 函数来读写数据。 select，poll，epoll都是IO多路复用机制，即可以监视多个描述符，一旦某个描述符就绪（读或写就绪），能够通知程序进行相应读写操作。 但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。 死锁死锁是指在多线程或多进程并发执行的过程中，由于互相持有对方所需的资源，并且同时等待对方释放资源，导致所有涉及的线程或进程都无法继续执行下去，陷入无限等待的状态。 互斥、不可剥夺、请求保持、循环依赖 解决方案： 主要有一下三种方法： 死锁防止 死锁避免 死锁检测和恢复 死锁防止 破坏互斥：使资源同时访问而非互斥使用 破坏请求保持：采用静态分配的方式，静态分配的方式是指进程必须在执行之前就申请需要的全部资源，且直至所要的资源全部得到满足后才开始执行。 破坏循环等待条件：按序申请、按序释放 死锁避免 银行家算法（Banker’s algorithm）用于避免系统资源分配中的死锁问题，它属于破坏死锁的第四个条件，即破坏循环等待条件。 当一个进程申请使用资源的时候，银行家算法通过先 试探 分配给该进程资源，然后通过安全性算法判断分配后的系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待。","link":"/2024/03/12/cs/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"title":"计算机网络","text":"OSI七层和作用 ✨ 访问网页全过程 ✨ 首先浏览器对 URL 进行解析，生成[HTTP 请求报文](##3.1 HTTP 请求报文)—包括请求行+请求头+请求体。 根据 URL的域名通过 DNS 协议获取对应 IP 地址（DNS 查找过程：浏览器缓存、路由器缓存、DNS 缓存） 根据 IP 地址+端口号，发送 TCP 连接请求建立连接。生成 TCP 报文（若HTTP 报文超出 MSS，需进行分割） 生成IP报文封装源 IP、目标 IP。 根据目标 IP 查找路由表（路由选择协议）得到下一跳 IP，再利用ARP获取对应 MAC 地址，生成 MAC 报文。 网卡驱动程序加上报头和帧校验序列FCS将数字信号转换为电信号传输 路由器对包接收，进行帧校验，检查 MAC 头部中的接收方 MAC 地址是否是自己，若是则查找路由表寻找下一跳路由器IP及其MAC 地址，封装包并转发，重复直至将数据包传输到服务器。 服务器依次拆解报文，得到 HTTP 请求报文，返回封装网页的HTTP 响应报文，重复刚刚的流程发送给浏览器。 浏览器根据 HTTP 响应报文，解析响应体中的 HTML 代码，渲染网页的结构和样式，根据 HTML 中的其他资源的 URL，再次发起 HTTP 请求，获取这些资源 浏览器在不需要和服务器通信时，可以主动关闭 TCP 连接，或者等待服务器的关闭请求 应用层 DNS（Domain Name System）： 解决域名和 IP 地址的映射 HTTP：根据 DNS获取的目标主机的 IP 发送 HTTP 报文 传输层 HTTP 基于 TCP，数据要经过这俩个协议的封装 网络层 应用层、传输层是端到端协议；网络层是中间件协议，主机与中间系统进行交互。 网络层的核心功能：路由选择和分组转发 路由选择：确定分组从源到目的最优路径的过程 分组转发：将分组从路由器的输入端口转移到合适的输出端口 往哪里传输？或者说，要把数据包发到哪个路由器上？（怎么路由） 根据报文的目标 IP 地址跟路由表每个表项的掩码字段做“与”操作，判断是否匹配该表项的目标 IP 地址。 匹配完所有的表项选择掩码最长的匹配项，根据该表项的出接口和下一跳路由 IP将报文转发；若没有匹配到，则查找是否有缺省路由；若都没有，则丢弃报文 路由表项怎么来的？ direct：链路层发现，优点：自动发现，开销小。缺点：只发现接口所属网段 static：静态路由，需要人为调整 缺省路由 动态路由：RIP（Routing Information Protocol使用跳数作为路径距离）、OSPF（Open Shortest Path First链路开销值判断路径长短）和 BGP（Border Gateway Protocol边界网关协议，在路由选择域之间交换网络层可达性信息） 应用层-HTTPHTTP 报文格式 请求报文 请求行：包含请求方法、请求目标（URL 或 URI）和 HTTP 协议版本 请求/消息头：包含请求的附加信息，如 Host、User-Agent、Content-Type 等 空行：用来分隔请求头和请求体 请求/消息体：可选，包含请求数据（POST 请求需要传输的数据） URL和 URI 的区别✨ URL（Uniform Resource Locator）和URI（Uniform Resource Identifier）都是用来标识资源的字符串，但它们的定义和使用场景有所不同： URI：URI是一个更为通用的概念，它可以被用来标识任何类型的资源，不仅仅是网络上的资源。URI有两种形式：URL和URN（Uniform Resource Name）。 URL：URL是URI的一种特定类型，它包含了用于查找某个资源的详细地址信息。一个URL不仅告诉我们资源的标识，还告诉我们如何通过网络获取到这个资源。它通常包含协议（如http或https）、主机名、端口号（如果有）、路径和查询字符串等部分。 总结：所有的URL都是URI，但并非所有的URI都是URL。因为URI还包括了另一种类型：URN，它只标识资源，但不指定如何定位这个资源（如 HTTP 请求报文的请求头中的请求目标）。 响应报文 状态行：HTTP 版本、状态码、短语 响应头：附加信息，如Content-Type,Content-Length 空行 响应体：响应内容，JSON、HTML等 HTTP 请求类型主要四种 GET-查看 POST-创建 PUT-更新 DELETE-删除 HEAD-类似 GET 但只返回资源头部信息 PUT和POST区别 put请求：如果两个请求相同，后一个请求会把第一个请求覆盖掉。（所以PUT用来改资源）。 Post请求：后一个请求不会把第一个请求覆盖掉。（所以Post用来增资源）。 GET和POST区别 关于浏览器缓存：浏览器可以缓存 get请求的数据，也可以将 get 请求保存为书签；post 不可以 关于安全性：post的安全性要相对比get的高。因为get的参数都是放在url中的，可以被缓存，截取直接就能获取数据，所以一般登录密码这些信息不会明文放在url中使用get请求发送。而post的数据都是放在RequestBody，可以进行一次加密，相对安全些。 请求数据上限：get有长度限制而 post 没有。get请求的数据会放在url后面，使用?A=B格式（A是名称，B是参数）发送，这个url一般是有长度限制的，http协议没有对url长度进行限制，这个限定主要是浏览器和服务器的限制，一般是1024字节长度。而post可以将数据放在RequesetBody中传送，这里就没有数据量的上限了，get是无请求体的，所以RequestBody只能使用post方式提交。 关于tcp数据包：对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。实际上get会产生一个tcp数据包，post会产生两个数据，这就会造成响应时间问题，但并不是所有的浏览器都是post发两次包，firefox不是。 HTTP 常见状态码 ✨ 2xx： 「200 OK」请求已正常处理。 「204 No Content」请求处理成功，但响应没有 body 数据，如 PUT 请求更新资源。 「206 Partial Content」表示响应返回的 body 数据并不是资源的全部，而是其中的一部分 3xx： 「301 Moved Permanently」 永久性重定向 「302 Found」临时性重定向 「304 Not Modified」不具有跳转的含义，表示资源未修改 4xx: 「400 Bad Request」表示客户端请求的报文有错误，但只是个笼统的错误 「403 Forbidden」服务器禁止访问资源。权限，未授权IP等 「404 Not Found」服务器上没有请求的资源。路径错误 5xx: 「500 Internal Server Error」服务器发生错误，是个笼统通用的错误码 「501 Not Implemented」表示客户端请求的功能还不支持 「502 Bad Gateway」网关错误 「503 Service Unavailable」表示服务器当前很忙，暂时无法响应客户端 「504 Gateway timeout」 网关超时 301和302 区别301: 旧地址A的资源不可访问了(永久移除), 重定向到网址B，搜索引擎会抓取网址B的内容，同时将旧的网址交换为重定向之后的网址。 302: 旧地址A的资源仍可访问，这个重定向只是临时从旧地址A跳转到B地址，这时搜索引擎会抓取B网址内容，但是会将网址保存为A的。 尽量使用301跳转，以防止网址劫持！ 502 和 504 区别假如 nginx 作为网关 当 nginx 收到无效响应，返回 502 当 nginx 超过自己配置的超时时间，还没有收到请求，返回 504 排查 502 ： 后端服务器故障（ping、telnet、curl） nginx配置问题（nginx.conf、查看日志） 高负载或者资源耗尽（top） nginx与后端服务器通信问题（查看防火墙） 造成 504 的原因 一般指nginx做反向代理服务器时，所连接的服务器tomcat无响应导致的。 为了完成您的 HTTP 请求，该服务器访问一个上游服务器，但没得到及时的响应 nginx超过了自己设置的超时时间 HTTP与HTTPS✨HTTP为什么不安全HTTP 的信息是明文传输，所以存在： 窃听风险 篡改风险 冒充风险 相应 HTTPS 利用 SSL/TLS安全协议实现： 信息加密 校验机制 身份证书 HTTP 与 HTTPS的区别✨ HTTP 是超文本传输协议，信息是明文传输，存在安全隐患。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 默认端口号不一样，HTTP：80 HTTPS：443 HTTP 连接只需要TCP 三次握手；HTTPS 连接还需要 SSL/TLS 四次握手，进入加密报文传输，传输内容对称加密，但对称加密的密钥是用服务器方的证书进行非对称加密 HTTPS协议需要想 CA（证书权威机构）申请数字证书，来保证服务器的身份是可信的 url 前缀：http:// https// SEO（搜索引擎优化Search Engine Optimization）：搜索引擎通常会更青睐使用 HTTPS 协议的网站 SSL/TLS解决了 HTTP 数据透明的问题 主要有两种密钥协商算法： RSA：基于大数因数分解的困难性（大数的质因数很难计算） ECDHE：基于椭圆曲线的密钥协商算法，基于离散对数问题的困难性 区别： 传统的 TLS 握手基本上使用RSA算法作为密钥协商算法，但是存在问题：**使用 RSA 密钥协商算法的最大问题是不支持前向保密**（即私钥泄露，之前的所有加密信息都会被破解）。 基于 RSA 的握手流程 ✨ TLS 第一次握手 客户端发送 client hello（随机数 1、支持的 TLS 版本、支持的密码套件列表） TLS 第二次握手 服务端发送 server hello（随机数 2、确定 TLS 版本、确认的密码套件列表、数字证书） TLS 第三次握手 利用自带的 CA 公钥检验数字证书的真实性，若真实则取出服务器公钥 客户端发送： 随机数 3，并用服务器公钥加密 加密通信算法改变通知，之后信息用「会话密钥」加密通信 客户端握手结束通知，并把之前的数据做个摘要，供服务端校验 会话密钥：随机数 1、2、3 生成 TLS 第四次握手 服务端用私钥解密得到随机数 3 服务端发送： 加密通信算法改变通知，之后信息用「会话密钥」加密通信 服务端握手结束通知，并把之前的数据做个摘要，供客户端校验 之后用 HTTP 进行通信，只不过用「会话密钥」加密内容 基于 ECDHE 的握手流程ECDHE 与 RSA 的区别在于每一次的通信的「会话密钥」是通过椭圆曲线和曲线基点算出来的，是会变化的。 第二次握手 返回给客户端的密码套件就和 RSA 不一样 密钥协商算法使用 ECDHE； 签名算法使用 RSA； 握手后的通信使用 AES 对称算法，密钥长度 256 位，分组模式是 GCM； 摘要算法使用 SHA384； 服务端选择了 ECDHE 密钥协商算法，所以会在发送完证书后，会发送「Server Key Exchange」消息给客户端来确定椭圆曲线、曲线上的基点、私钥1（不公开的随机数）、公钥1（基点 x 私钥1）（用 rsa 加密发送） 第三次握手 客户端收到服务端发来的曲线和基点，然后确定私钥2（不公开的随机数）、公钥2（基点 x 私钥2）（用 rsa 加密发送）发送给服务端。 椭圆曲线上的点符合交换律规则，满足 公钥 1 * 私钥 2 == 公钥 2 * 私钥 1 最终的会话密钥，就是用「客户端随机数 + 服务端随机数 + （ECDHE 算法算出的共享密钥） 」三个材料生成的。 数字证书非对称加密 SSL/TLS 的核心要素是非对称加密。非对称加密采用两个密钥——一个公钥，一个私钥。公钥加密的内容，使用私钥可以解开；而私钥加密的内容，公钥可以解开。目前使用最为广泛的非对称密钥为RSA算法。 对称加密 使用 SSL/TLS 进行通信的双方需要使用非对称加密方案来通信，但是非对称加密设计了较为复杂的数学算法，在实际通信过程中，计算的代价较高，效率太低，因此，SSL/TLS 实际对消息的加密使用的是对称加密。 hash算法加密 它是一种不可逆的加密方式，对一组数据使用哈希算法加密，加密后不能解密 保证公钥传输的信赖性（*数字签名&amp;数字证书） CA 发放证书 服务器把证书内容给 CA，利用 hash算法 + 私钥生成签名。 数字证书=证书内容+证书签名 服务器发送数字证书给客户端 客户端验证证书 hash算法加密证书内容，CA 公钥解密证书签名，判断是否相同 HTTP、Socket 、TCP、RPCHTTP 为什么基于 TCP 协议HTTP协议基于TCP协议是出于对数据传输可靠性和完整性的需求，TCP协议提供了数据传输的可靠性和可控性，而HTTP协议定义了数据的格式和意义，两者协同工作来实现Web应用的各种功能。 HTTP 进行 TCP 连接后，在什么情况下会中断 服务端或客户端执行 close 系统调用，会发送 FIN 报文，进行四次挥手中断 发送方发送数据，超时未收到接收方发来的 ACK 报文，发送方重传次数达到最大次数时，中断 HTTP 长时间未发送请求和响应时，中断 HTTP 和 RPC 有什么区别RPC 定制化程度更高：可以采用体积更小的 Protobuf 或其他序列化协议去保存结构体数据 性能会更好：不需要像 HTTP 那样考虑各种浏览器行为（比如 302 重定向跳转啥的） HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。 HTTP、SOCKET和TCP的区别HTTP 是一种用于传输超文本数据的应用层协议，用于规定客户端和服务端交换的数据格式和规则 socket 是计算机网络中的一种抽象，用于描述通信链路的一端 TCP 是一种提供可靠的、面向连接的传输层协议，负责在通信的两端建立可靠的数据传输连接 HTTP1.0、1.1、2.0、3.0✨HTTP/1.1 的优化连接方式 : HTTP/1.0 为短连接，HTTP/1.1 支持长连接 长短连接实际上是指 TCP 的长短连接，通过两端设置 header 中的 connection 为 keep-alive / close 所以多个 http 请求的源和目标主机的 host 和 port 相同就可以复用同一个 tcp 连接 缺点：可能会导致HTTP级别的头部阻塞。http 请求独占 tcp 连接， 同一时刻只能处理一个 Http 请求，当前请求未结束前（如资源创建缓慢——从数据库查询动态生成的index.html或资源太大），其他请求无法执行， Host 头处理 : HTTP/1.1 在请求头中加入了Host字段（域名系统（DNS）允许多个主机名绑定到同一个 IP 地址上，但是 HTTP/1.0 并没有考虑这个问题） 当服务器收到一个HTTP请求时，仅通过IP地址和端口号是无法确定请求是针对哪个网站的。如果请求中没有包含Host头字段，服务器会返回400（Bad Request）状态码。 状态响应码 : HTTP/1.1 中新加入了大量的状态码，光是错误响应状态码就新增了 24 种。 缓存处理 : 在 HTTP1.0 中主要使用 header 里的 If-Modified-Since,Expires 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，如If-Unmodified-Since If-Modified-Since: 从字面上看, 就是说: 如果从某个时间点算起, 如果文件被修改了…. 如果真的被修改: 那么就开始传输, 服务器返回:200 OK 如果没有被修改: 那么就无需传输, 服务器返回: 304 Not Modified. 用途: 缓存验证: 客户端尝试下载最新版本的文件. 比如网页刷新, 加载大图的时候.很明显，如果从图片下载以后都没有再被修改, 当然就没必要重新下载了! If-Unmodified-Since: 从字面上看, 意思是: 如果从某个时间点算起, 文件没有被修改….. 如果没有被修改: 则开始继续传送文件: 服务器返回: 200 OK 如果文件被修改: 则不传输, 服务器返回: 412 Precondition failed (预处理错误) 用途: 常用于PUT请求，以避免“丢失更新”问题：例如，一个客户端先获取了一个资源，然后进行了一些修改，准备将修改后的资源发送回服务器。但在这个过程中，可能有其他客户端也修改了同一个资源。如果服务器接受这个请求就会覆盖之前的修改。 断点续传(一般会指定Range参数). 要想断点续传, 那么文件就一定不能被修改, 否则就不是同一个文件了, 断续还有啥意义? 带宽优化及网络连接的使用：HTTP1.0 中，存在一些浪费带宽的现象，HTTP1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是 206（Partial Content） HTTP/2 的优化头部压缩：HTTP/1.1 支持Body压缩，Heade不支持压缩。HTTP/2.0 支持对Header压缩，使用了专门为Header压缩而设计的 HPACK算法，减少了网络开销。 HPACK算法：在客户端和服务器同时维护一张头信息表，一个请求的 header 字段存入表中生成一个索引号，用索引号来代替字段来提高速度。 二进制帧：纯文本格式的报文变为二进制格式，头信息和数据体都是二进制，并统称为帧，二进制格式方便计算机直接解析，提高数据传输效率。 HTTP/2 在每个块前面放置一个所谓的数据帧（DATA frame）。这些数据帧主要包含两个关键的元数据。首先：下面的块属于哪个资源。每个资源的“字节流（bytestream）”都被分配了一个唯一的数字，即流id（stream id）。第二：块的大小是多少。使用流id（stream id）来指出这些头（headers）属于哪个响应，这样甚至可以将头（headers）从它们的实际响应数据中分离出来。 使用这些帧，HTTP/2 确实允许在一个连接上正确地复用多个资源 多路复用：串行方式（一个 tcp 连接同一时刻只能被一个 http 请求独占）变为同一连接上可以同时传输多个请求和响应（基于二进制帧） 引入二进制分帧层，传输时数据都会经过该层处理，转化为带有请求ID的帧(Stream id)，在传输完成后会根据 ID 进行组合 这解决了HTTP1.1存在的“应用层”级别的头部阻塞问题，但是仍存在TCP级别的头部阻塞问题： TCP 不知道 HTTP/2 的独立流（streams）这一事实意味着 TCP 层队头阻塞（由于丢失或延迟的数据包）也最终导致 HTTP 队头阻塞！ 服务器推送（Server Push）：HTTP/2.0 支持服务器推送，可以在客户端请求一个资源时，将其他相关资源一并推送给客户端，从而减少了客户端的请求次数和延迟。而 HTTP/1.1 需要客户端自己发送请求来获取相关资源。 HTTP/1.1 vs HTTP/2既然HTTP2存在TCP层的队头阻塞，那么为什么还需要HTTP2。 当比较单个连接上的 HTTP/2 和单个连接上的 HTTP/1.1 时，TCP 队头阻塞是真实存在的，但是它对 Web 性能的影响要比HTTP/1.1 队头阻塞小得多，HTTP/1.1 队头阻塞出现的概率要高的多。 但是现在的浏览器使用 HTTP/1.1 通常会打开多个连接，这使得 HTTP/1.1 不仅在一定程度上减轻了 HTTP 级别，而且减轻了 TCP 级别的队头阻塞。因而在某些情况下，单个连接上的 HTTP/2 很难比6个连接上的 HTTP/1.1 快，甚至与 HTTP/1.1 一样快。 总之，事实上，我们看到（也许出乎意料），HTTP/2 目前部署在浏览器和服务器中，在大多数情况下通常与 HTTP/1.1 一样快或略快。然而，也有一些情况（特别是在数据包丢失率较高的低速网络上），6个连接的 HTTP/1.1 仍然比一个连接的 HTTP/2 更为出色，这通常是由于 TCP 级别的队头阻塞问题造成的。正是这个事实极大地推动了新的 QUIC 传输协议的开发，以取代 TCP。 HTTP/3 的优化传输协议：HTTP/2.0 是基于 TCP 协议实现的，HTTP/3.0 新增了 QUIC（Quick UDP Internet Connections） 协议来实现可靠的传输，提供与 TLS/SSL 相当的安全性，具有较低的连接和传输延迟。你可以将 QUIC 看作是 UDP 的升级版本，在其基础上新增了很多功能比如加密、重传等等。 连接建立：HTTP/2.0 需要经过经典的 TCP 三次握手过程（由于安全的 HTTPS 连接建立还需要 TLS 握手，共需要大约 3 个 RTT）。由于 QUIC 协议的特性（TLS 1.3，TLS 1.3 除了支持 1 个 RTT 的握手，还支持 0 个 RTT 的握手）连接建立仅需 0-RTT 或者 1-RTT。这意味着 QUIC 在最佳情况下不需要任何的额外往返时间就可以建立新连接。 队头阻塞：HTTP/2.0 多请求复用一个 TCP 连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。由于 QUIC 协议的特性，HTTP/3.0 在一定程度上解决了TCP层面的队头阻塞问题 QUIC 受到 HTTP/2 帧方式（framing-approach）的启发，还添加了自己的帧（frames）。在本例中是流帧（STREAM frame）。流id（stream id）以前在 HTTP/2 的数据帧（DATA frame）中，现在被下移到传输层的 QUIC 流帧（STREAM frame）中。这也说明了如果我们想使用 QUIC，我们需要一个新版本的 HTTP 的原因之一：如果我们只在 QUIC 之上运行 HTTP/2，那么我们将有两个（可能冲突的）“流层”（stream layers）。相反，HTTP/3 从 HTTP 层删除了流的概念（它的数据帧（DATA frames）没有流id），而是重新使用底层的 QUIC 流。 与 HTTP/2 的数据帧（DATA frames）非常相似，QUIC 的流帧（STREAM frames）分别跟踪每个流的字节范围。这与 TCP 不同，TCP 只是将所有流数据附加到一个大 blob 中。像以前一样，让我们考虑一下如果 QUIC 数据包2丢失，而 1 和 3 到达会发生什么。与 TCP 类似，数据包1中流1（stream 1）的数据可以直接传递到浏览器。然而，对于数据包3，QUIC 可以比 TCP 更聪明。它查看流1的字节范围，发现这个流帧（STREAM frame）完全遵循流id 1的第一个流帧 STREAM frame（字节 450 跟在字节 449 之后，因此数据中没有字节间隙）。它可以立即将这些数据提供给浏览器进行处理。然而，对于流id 2，QUIC确实看到了一个缺口（它还没有接收到字节0-299，这些字节在丢失的 QUIC 数据包2中）。它将保存该流帧（STREAM frame），直到 QUIC 数据包2的重传（retransmission）到达。再次将其与 TCP 进行对比，后者也将数据流1的数据保留在数据包3中！ 错误恢复：HTTP/3.0 具有更好的错误恢复机制，当出现丢包、延迟等网络问题时，可以更快地进行恢复和重传。而 HTTP/2.0 则需要依赖于 TCP 的错误恢复机制。 安全性：HTTP/2.0 和 HTTP/3.0 在安全性上都有较高的要求，支持加密通信，但在实现上有所不同。HTTP/2.0 使用 TLS 协议进行加密，而 HTTP/3.0 基于 QUIC 协议，包含了内置的加密和身份验证机制，可以提供更强的安全性。 资料：关于队头阻塞（Head-of-Line blocking），看这一篇就足够了 | Lenix Blog (p2hp.com) QUIC 协议HTTP/3 就将传输层从 TCP 替换成了 UDP，并在 UDP 协议上开发了 QUIC 协议，来保证数据的可靠传输。 QUIC 协议的特点： 无队头阻塞，QUIC 连接上的多个 Stream 之间并没有依赖，都是独立的，也不会有底层协议限制，某个流发生丢包了，只会影响该流，其他流不受影响； 建立连接速度快，因为 QUIC 内部包含 TLS 1.3，因此仅需 1 个 RTT 就可以「同时」完成建立连接与 TLS 密钥协商，甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送，达到 0-RTT 的效果。 连接迁移，QUIC 协议没有用四元组的方式来“绑定”连接，而是通过「连接 ID 」来标记通信的两个端点，客户端和服务器可以各自选择一组 ID 来标记自己，因此即使移动设备的网络变化后，导致 IP 地址变化了，只要仍保有上下文信息（比如连接 ID、TLS 密钥等），就可以“无缝”地复用原连接，消除重连的成本； HTTP无状态HTTP到底是不是无状态的？HTTP本身是无状态的，每个请求都是独立的，服务器不会在多个请求之间保留客户端状态的信息。 但是可以通过Cookie和Session来跟踪用户状态来实现状态保持 Cookie是HTTP协议簇的一部分，为什么说HTTP是无状态的？虽然Cookie是HTTP协议簇的一部分，但是HTTP协议在设计初衷上仍然保持无状态特性，即每个请求都是相互独立的，方便web系统更具规模化和简单性。Cookie只是HTTP协议的一种补充机制。 cookie和session有什么区别 位置： 客户端；服务端 大小： 4KB；受服务器内存大小限制 安全性：易受XSS（跨站脚本）、CSPF（跨站请求伪造）攻击；通常更安全，但会出现Session劫持和会话固定攻击 生命周期：设置过期时间，或设置会话Cookie（关闭浏览器自动删除） 性能：影响传输效率；增加服务器负载 有HTTP协议。为什么还要用RPC RPC（Remote Procedure Call，远程过程调用）本质上不算协议，是一种调用方式，而gRPC和thrift这样的具体实现才是协议。它允许一个程序想调用本地方法调用远端的服务方法。PRC有很多实现方式，不一定非要基于TCP。 WebSocket/HTTP/SSEWebSocket为什么需要WebSocketTCP 协议本身是全双工的，但我们最常用的 HTTP/1.1，虽然是基于 TCP 的协议，但它是半双工的，对于大部分需要服务器主动推送数据到客户端的场景，都不太友好，因此我们需要使用支持全双工的 WebSocket 协议。 在 HTTP/1.1 里，只要客户端不问，服务端就不答。基于这样的特点，对于登录页面这样的简单场景，可以使用定时轮询或者长轮询的方式实现服务器推送(comet)的效果。 对于客户端和服务端之间需要频繁交互的复杂场景，比如网页游戏，都可以考虑使用 WebSocket 协议。 如何建立 WebSocket正因为各个浏览器都支持 HTTP协 议，所以 WebSocket 会先利用HTTP协议加上一些特殊的 header 头进行握手升级操作，升级成功后就跟 HTTP 没有任何关系了，之后就用 WebSocket 的数据格式进行收发数据。 SSE（Server-sent Events） sse与长轮询机制类似，区别是每个连接不只发送一个消息。客户端发送一个请求，服务端保持这个连接直到有新消息发送回客户端，仍然保持着连接，这样就可以再次发送消息，由服务器单向发送给客户端。 应用层-DNSDNS是什么DNS全称Domain Name System（域名系统），是互联网中将域名转换为对应IP地址的分布式数据库系统。 域名的层级关系类似一个树状结构： 根DNS服务器（.） 顶级DNS服务器（.com） 权威DNS服务器（server.com） 所有DNS服务器都有根DNS服务器的地址，因此客户端可以直接通过根DNS服务器顺藤摸瓜找到想要的目标服务器地址 域名解析的工作流程✨整体来说分成两个部分： 递归查询：客户端-本地dns服务端 迭代查询：本地dns服务端—外网 域名解析是否都要从根开始先判断是否有缓存，依次为： 浏览器缓存 操作系统缓存 hosts文件 默认端口-53UDP or TCP✨使用UDP 理由： 低延时——UDP无连接 简单快速——UDP无流量控制、拥塞控制，效率高 轻量——UDP头部小，适合DNS这类频繁且短小的数据交换 传输层-TCPTCP 基础TCP 头部格式 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。 控制位： ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。 RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。 SYN：该位为 1 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。 FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位为 1 的 TCP 段。 为什么需要 TCP 协议？ TCP 工作在哪一层？网络层不可靠，如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。 什么是 TCP 连接？用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接 建立一个 TCP 连接是需要客户端与服务端达成上述三个信息的共识。 Socket：由 IP 地址和端口号组成 序列号：用来解决乱序问题等 窗口大小：用来做流量控制 如何唯一确定一个 TCP 连接呢？TCP 四元组可以唯一的确定一个连接，四元组包括如下： 源地址 源端口 目的地址 目的端口 源地址和目的地址的字段（32 位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。 源端口和目的端口的字段（16 位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。 UDP 包大小 原理上，UDP 包长度 16 位，UDP 包的大小为 2^16-1，即 65535 字节 以太网(Ethernet)数据帧的长度必须在==46-1500==字节之间,这是由以太网的物理特性决定的.这个1500字节被称为链路层的MTU(最大传输单元). 但这并不是指链路层的长度被限制在1500字节,其实这这个MTU指的是链路层的数据区.并不包括链路层的首部和尾部的18个字节.又因为UDP数据报的首部8字节,所以UDP数据报的数据区最大长度为1472字节（1500-20-8）. 但在网络编程中，Internet中的路由器可能有设置成不同的值(小于默认值)，鉴于Internet上的标准MTU值为==576==字节,所以我建议在进行Internet的UDP编程时. 最好将UDP的数据长度控件在548字节(576-20-8)以内. 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？ MTU：一个网络包的最大长度，以太网中一般为 1500 字节； MSS：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度； 因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传。因此，可以得知由 IP 层进行分片传输，是非常没有效率的。所以，为了达到最佳的传输效能 TCP 协议在建立连接的时候通常要协商双方的 MSS 值，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。经过 TCP 层分片后，如果一个 TCP 分片丢失后，进行重发时也是以 MSS 为单位，而不用重传所有的分片，大大增加了重传的效率。 UDP 和 TCP 有什么区别呢？分别的应用场景是？✨TCP 和 UDP 区别 面向连接 服务对象（一对一；一对一，一对多，多对多） 可靠性（可靠交付；尽最大努力交付） 拥塞控制、流量控制 首部开销（&gt;=20B，是否使用「选项」字段；8B） 传输方式（面向字节流，没有边界；面向报文，是有边界的） 分片不同 TCP 的数据大小如果大于 MSS （Maximum Segment Size）大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。 UDP 的数据大小如果大于 MTU （Maximum Transmit Unit）大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。 IP 头部长度[20B, 60B] TCP头部长度&gt;20B MTU 是网络层面的参数，决定了网络接口可以传输的最大数据包大小，包括头部和数据。 MSS 是 TCP 层面的参数，决定了每个 TCP 数据段可以承载的最大数据量，不包括 TCP 和 IP 头部。 MSS 的设置通常基于 MTU 计算，以确保数据包在传输过程中不会被分片。 TCP 和 UDP 应用场景由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于： FTP 文件传输； HTTP / HTTPS； ssh远程登录 由于 UDP 面向无连接，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS 等； TFTP简单文件传输协议 语音、电话、视频； TCP 和 UDP 可以使用同一个端口吗？答案：可以的。传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。因此，TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。 TCP 和 UDP 可以同时绑定相同的端口吗？ 可以的。 TCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。 当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。 因此， TCP/UDP 各自的端口号也相互独立，互不影响。 多个 TCP 服务进程可以同时绑定同一个端口吗？ 如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。 如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行 bind() 不会出错。 如何解决服务端重启时，报错“Address already in use”的问题？ 当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。 当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 要解决这个问题，我们可以对 socket 设置 ==SO_REUSEADDR== 属性。 这样即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。 客户端的端口可以重复使用吗？ 在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。 TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。 所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？ 要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？ 打开 net.ipv4.tcp_tw_reuse 这个内核参数。 因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态。 如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。 如何在 Linux 系统中查看 TCP 状态？TCP 的连接状态查看，在 Linux 可以通过 netstat -napt(p—pid，t—tcp，u—udp) 命令查看。 netstat -napt 是 Linux 中用来显示网络连接、路由表、接口统计信息、伪装连接和多播成员信息的命令。以下是各个选项的解释： **-n**：以数字形式显示地址和端口号，而不是解析主机名和端口名。 **-a**：显示所有套接字（包括监听和非监听的）。 **-p**：显示每个套接字所属的进程 ID（PID）和程序名称。 **-t**：仅显示 TCP 连接。 TCP 序列号和确认号是如何变化的？ 公式一：序列号 = 上一次发送的序列号 + len（数据长度）。特殊情况，如果上一次发送的报文是 SYN 报文或者 FIN 报文，则改为 上一次发送的序列号 + 1。 公式二：确认号 = 上一次收到的报文中的序列号 + len（数据长度）。特殊情况，如果收到的是 SYN 报文或者 FIN 报文，则改为上一次收到的报文中的序列号 + 1。 TCP连接✨TCP 三次握手过程是怎样的？✨ ==第三次握手是可以携带数据的，前两次握手是不可以携带数据的== 为什么要三次握手？不是两次或者四次✨不使用「两次握手」和「四次握手」的原因： 「两次握手」：无法防止历史连接的建立（主要原因），会造成双方资源的浪费，也无法可靠的同步双方序列号； 「四次握手」：第二次和第三次握手可以合并，即三次握手就已经理论建立可靠连接，所以不需要使用更多的通信次数。 避免历史连接考虑场景，client 发送 SYN（seq=90）后宕机，同时 SYN 被网络阻塞，服务端没有收到，接着 client 重启，有重新想建立连接，发送 SYN（seq=100）。 二次握手情况 服务端没有中间状态给客户端来阻止历史连接，导致服务端可能建立一个历史连接，造成资源浪费。 三次握手 服务端会通过第二次握手来确认该 SYN 是否正确。 同步双方初始序列号SYN 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。作用是： 接收方去重 接收方按序接收 发送方确认哪些被接收 TCP 三次握手的丢包问题客户端第一次握手发送的 SYN 报文丢失 如果客户端迟迟收不到服务端的 SYN-ACK 报文（第二次握手），就会触发「超时重传」机制，重传 SYN 报文，而且重传的 SYN 报文的序列号都是一样的。 不同版本的操作系统可能超时时间不同，有的 1 秒的，也有 3 秒的，这个超时时间是写死在内核里的，想要更改则需要重新编译内核。 服务端第二次握手回复的 SYN+ACK 报文丢失第二次握手的 SYN-ACK 报文其实有两个目的 ： 第二次握手里的 ACK， 是对第一次握手的确认报文； 第二次握手里的 SYN，是服务端发起建立 TCP 连接的报文； 若 SYN+ACK 报文丢失，会触发两个报文重传： 客户端重传第一次握手的 SYN 报文 服务端重传第二次握手的 SYN+ACK 报文 客户端第三次握手发送的ACK 报文丢失ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文。 当服务端超时重传 2 次 SYN-ACK 报文后，由于 tcp_synack_retries 为 2，已达到最大重传次数，于是再等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到客户端的第三次握手（ACK 报文），那么服务端就会断开连接。 三次握手中服务端的内部情况三次握手中服务端如何存储连接服务端收到客户端发起的 SYN 请求后，内核会把该连接存储到半连接队列，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 全连接队列，等待进程调用 accept 函数时把连接取出来。 什么是 SYN 攻击？假设攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会占满服务端的半连接队列，使得服务端不能为正常用户服务。 如何避免 SYN 攻击？ 增大 TCP 半连接队列 减少 SYN+ACK 重传次数 开启 net.ipv4.tcp_syncookies TCP 断开✨TCP 四次挥手过程是怎样的？ 具体过程： 客户端主动调用关闭连接的函数，发送 FIN 报文，代表客户端不会再发送数据了，进入 FIN_WAIT_1 状态； 服务端收到了 FIN 报文，马上回复一个 ACK 确认报文，此时服务端进入 CLOSE_WAIT 状态。在收到 FIN 报文的时候，TCP 协议栈会为 FIN 包插入一个文件结束符 EOF 到接收缓冲区中，服务端应用程序可以通过 read 调用来感知这个 FIN 包，这个 EOF 会被放在已排队等候的其他已接收的数据之后，所以必须要得继续 read 接收缓冲区已接收的数据； 接着，当服务端在 read 数据的时候，最后自然就会读到 EOF，接着 read() 就会返回 0，这时服务端应用程序如果有数据要发送的话，就发完数据后才调用关闭连接的函数，如果服务端应用程序没有数据要发送的话，可以直接调用关闭连接的函数，这时服务端就会发一个 FIN 包，这个 FIN 报文代表服务端不会再发送数据了，之后处于 LAST_ACK 状态； 客户端接收到服务端的 FIN 包，并发送 ACK 确认包给服务端，此时客户端将进入 TIME_WAIT 状态； 服务端收到 ACK 确认包后，就进入了最后的 CLOSE 状态； 客户端经过 2MSL 时间之后，也进入 CLOSE 状态； 为什么挥手需要四次？再来回顾下四次挥手双方发 FIN 包的过程，就能理解为什么需要四次了。 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务端收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，因此是需要四次挥手。 第二次和第三次挥手能合并嘛当被动关闭方（上图的服务端）在 TCP 挥手过程中，「没有数据要发送」并且「开启了 TCP 延迟确认机制」（默认开启），那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。 什么是 TCP 延迟确认机制？ 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK 第三次挥手一直没发，会发生什么？当主动方收到 ACK 报文后，会处于 FIN_WAIT2 状态，就表示主动方的发送通道已经关闭，接下来将等待对方发送 FIN 报文，关闭对方的发送通道。 这时，分两种情况： 如果连接是用 shutdown 函数关闭的，连接可以一直处于 FIN_WAIT2 状态，因为它可能还可以发送或接收数据。 对于 close 函数关闭的孤儿连接，由于无法再发送和接收数据，所以这个状态不可以持续太久，而 tcp_fin_timeout 控制了这个状态下连接的持续时长，默认值是 60 秒 第二次和第三次挥手之间，主动断开的那端能干什么如果主动断开的一方，是调用了 shutdown 函数来关闭连接，并且只选择了关闭发送能力且没有关闭接收能力的话，那么主动断开的一方在第二次和第三次挥手之间还可以接收数据。 断开连接时客户端 FIN 包丢失，服务端的状态是什么？如果第一次挥手丢失了，那么客户端迟迟收不到被动方的 ACK 的话，也就会触发超时重传机制，重传 FIN 报文，重发次数由 tcp_orphan_retries 参数控制。 当客户端重传 FIN 报文的次数超过 tcp_orphan_retries 后，就不再发送 FIN 报文，则会在等待一段时间（时间为上一次超时时间的 2 倍），如果还是没能收到第二次挥手，那么客户端直接进入到 close 状态，而服务端还是ESTABLISHED状态 为什么第四次挥手客户端需要等待 2*MSL（报文段最长寿命） 确保被动关闭方收到主动关闭方的 ack 确保上一次的 tcp 报文信息不会影响下一次的 tcp 连接 TIME_WAIT 过多有什么危害？ 占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等； 占用端口资源，端口资源也是有限的 服务器出现大量 TIME_WAIT 状态的原因有哪些？ 第一个场景：HTTP 没有使用长连接 根据大多数 Web 服务的实现，不管哪一方禁用了 HTTP Keep-Alive，都是由服务端主动关闭连接，那么此时服务端上就会出现 TIME_WAIT 状态的连接。 第二个场景：HTTP 长连接超时 假设设置了 HTTP 长连接的超时时间是 60 秒，nginx 就会启动一个「定时器」，如果客户端在完后一个 HTTP 请求后，在 60 秒内都没有再发起新的请求，定时器的时间一到，nginx 就会触发回调函数来关闭该连接，那么此时服务端上就会出现 TIME_WAIT 状态的连接。 第三个场景：HTTP 长连接的请求数量达到上限 Web 服务端通常会有个参数（比如 nginx 的 keepalive_requests 这个参数），来定义一条 HTTP 长连接上最大能处理的请求数量，当超过最大限制时，就会主动关闭连接。 对于一些 QPS 比较高的场景，比如超过 10000 QPS，甚至达到 30000 , 50000 甚至更高，如果 keepalive_requests 参数值是 100，这时候就 nginx 就会很频繁地关闭连接，那么此时服务端上就会出大量的 TIME_WAIT 状态。 针对这个场景下，解决的方式也很简单，调大 nginx 的 keepalive_requests 参数就行。 服务器出现大量 CLOSE_WAIT 状态的原因有哪些？当服务端出现大量 CLOSE_WAIT 状态的连接的时候，通常都是代码的问题，这时候我们需要针对具体的代码一步一步的进行排查和定位，主要分析的方向就是服务端为什么没有调用 close。 TCP 如何保证可靠传输✨ 连接管理（三次握手和四次挥手建立可靠连接） 序列号（防止数据丢失；避免数据重复；保证有序；实现多次发送，一次确认） 确认应答（接收方接收数据之后，回传ACK报文，用于告知发送方此次接收数据的情况。在指定时间后，若发送端仍未收到确认应答，就会启动超时重传） 重传机制（超时重传；快速重传） 流量控制（TCP支持根据接收端的处理能力，来决定发送端的发送速度） 拥塞控制（当网络拥堵严重时，发送端减少数据发送） TCP 如何实现流量控制？TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 TCP 为全双工(Full-Duplex, FDX)通信，双方可以进行双向通信，客户端和服务端既可能是发送端又可能是服务端。因此，两端各有一个发送缓冲区与接收缓冲区，两端都各自维护一个发送窗口和一个接收窗口。 TCP 发送窗口可以划分成四个部分： 已经发送并且确认的 TCP 段（已经发送并确认）； 已经发送但是没有确认的 TCP 段（已经发送未确认）； 未发送但是接收方准备接收的 TCP 段（可以发送）； 未发送并且接收方也并未准备接受的 TCP 段（不可发送） TCP 接收窗口可以划分成三个部分： 已经接收并且已经确认的 TCP 段（已经接收并确认）； 等待接收且允许发送方发送 TCP 段（可以接收未确认）； 不可接收且不允许发送方发送 TCP 段（不可接收）。 滑动窗口假如发送一个数据包，要等待 ack 才发送下一个，效率太低。 引入窗口，指定窗口大小，指无需等待确认应答，而可以继续发送数据的最大值。 窗口关闭如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。 只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 TCP 如何实现拥塞控制？拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。 为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。 拥塞控制主要是四个算法: 慢启动：当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。指数性的增长 慢启动门限 ssthresh （slow start threshold）状态变量。 当 cwnd &lt; ssthresh 时，使用慢启动算法。 当 cwnd &gt;= ssthresh 时，就会使用「拥塞避免算法」 拥塞避免：每当收到一个 ACK 时，cwnd 增加 1/cwnd。线性增长 拥塞发生 超时重传：ssthresh 设为 cwnd/2，cwnd 重置为 1 （是恢复为 cwnd 初始化值，我这里假定 cwnd 初始化值 1）。方法太激进，会造成网络卡顿。 快速重传：当接收方发现丢了一个中间包的时候，重复3 次发送前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。这种情况下网络阻塞不严重，将ssthresh慢开始门限设为cwnd/2，cwnd 重置为ssthresh，进入快速恢复算法。 快速恢复：由于发送方现在认为网络很可能没有发生拥塞（如果网络发生了严重拥塞，就不会一连有好几个报文段连续到达接收方，也就不会导致接收方连续发送重复确认）。将 cwnd 变为 ssthresh，开始拥塞避免 如何理解是 TCP 面向字节流协议？如何理解字节流？ 先来说说为什么 UDP 是面向报文的协议？ 当用户消息通过 UDP 协议传输时，操作系统不会对消息进行拆分，也就是每个 UDP 报文就是一个用户消息的边界 再来说说为什么 TCP 是面向字节流的协议？ 当用户消息通过 TCP 协议传输时，消息可能会被操作系统分组成多个的 TCP 报文，也就是一个完整的用户消息被拆分成多个 TCP 报文进行传输。我们不能认为一个用户消息对应一个 TCP 报文，正因为这样，所以 TCP 是面向字节流的协议。 tcp粘包/拆包是什么因为TCP是面向流，没有边界，而操作系统在发送TCP数据时，会通过缓冲区来进行优化，例如缓冲区为1024个字节大小。 如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送，这就形成了粘包问题。 如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包。 tcp粘包怎么解决 固定包的长度，不足用 0 填充。缺点是灵活性低 特殊字符作为边界，遇到特殊字符，就认定读完一个完整信息。如 http 自定义消息结构 tcp 拆包怎么解决 特殊字符作为边界（FTP 协议） TCP 协议有什么缺陷？ 升级 TCP 的工作很困难； TCP 协议是在内核中实现的，应用程序只能使用不能修改，如果要想升级 TCP 协议，那么只能升级内核。内核升级困难。 TCP 建立连接的延迟； 现在大多数网站都是使用 HTTPS 的，这意味着在 TCP 三次握手之后，还需要经过 TLS 四次握手后，才能进行 HTTP 数据的传输，这在一定程序上增加了数据传输的延迟。 TCP 存在队头阻塞问题； TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据。 网络迁移需要重新建立 TCP 连接； 基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接。 如何基于 UDP 协议实现可靠传输？市面上已经有基于 UDP 协议实现的可靠传输协议的成熟方案了，那就是 QUIC 协议，已经应用在了 HTTP/3。 连接迁移：QUIC支持在网络变化时快速迁移连接，例如从WiFi切换到移动数据网络，以保持连接的可靠性。 重传机制：QUIC使用重传机制来确保丢失的数据包能够被重新发送，从而提高数据传输的可靠性。 前向纠错：QUIC可以使用前向纠错技术，在接收端修复部分丢失的数据，降低重传的需求，提高可靠性和传输效率。 拥塞控制：QUIC内置了拥塞控制机制，可以根据网络状况动态调整数据传输速率，以避免网络拥塞和丢包，提高可靠性。 TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？HTTP 的 Keep-Alive 也叫 HTTP 长连接，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。 TCP 的 Keepalive 也叫 TCP 保活机制，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接。 网络攻击DDos 攻击？如何防范？分布式拒绝服务（DDos）攻击是通过大规模互联网流量淹没目标服务器，以破坏目标服务器、服务或网络正常流量的恶意行为。 常见的DDoS攻击包括以下几类： 网络层攻击：比较典型的攻击类型是UDP反射攻击，例如：NTP Flood攻击，这类攻击主要利用大流量拥塞被攻击者的网络带宽，导致被攻击者的业务无法正常响应客户访问。 传输层攻击：比较典型的攻击类型包括SYN Flood攻击、连接数攻击等，这类攻击通过占用服务器的连接池资源从而达到拒绝服务的目的。 会话层攻击：比较典型的攻击类型是SSL连接攻击，这类攻击占用服务器的SSL会话资源从而达到拒绝服务的目的。 应用层攻击：比较典型的攻击类型包括DNS flood攻击、HTTP flood攻击、游戏假人攻击等，这类攻击占用服务器的应用处理资源极大的消耗服务器处理性能从而达到拒绝服务的目的。 为了防范DDoS攻击，可以采取以下措施： 增强网络基础设施：提升网络带宽、增加服务器的处理能力和承载能力，通过增强基础设施的能力来抵御攻击。 使用防火墙和入侵检测系统：配置防火墙规则，限制不必要的网络流量，阻止来自可疑IP地址的流量。入侵检测系统可以帮助及时发现并响应DDoS攻击。 流量清洗和负载均衡：使用专业的DDoS防护服务提供商，通过流量清洗技术过滤掉恶意流量，将合法流量转发给目标服务器。负载均衡可以将流量均匀地分发到多台服务器上，减轻单一服务器的压力。 配置访问控制策略：限制特定IP地址或IP段的访问，设置访问频率限制，防止过多请求集中在单个IP上。 ARP 协议详解(网络层)ARP 协议工作时有一个大前提，那就是 ARP 表。 在一个局域网内，每个网络设备都自己维护了一个 ARP 表，ARP 表记录了某些其他网络设备的 IP 地址-MAC 地址映射关系，该映射关系以 &lt;IP, MAC, TTL&gt; 三元组的形式存储。 ARP 的工作原理将分两种场景讨论： 同一局域网内的 MAC 寻址； 从一个局域网到另一个局域网中的网络设备的寻址。 工作原理：ARP 表、广播问询、单播响应 cookie&amp;session区别 cookie数据存放在客户的浏览器上，session数据放在服务器上 cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗,如果主要考虑到安全应当使用session session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能，如果主要考虑到减轻服务器性能方面，应当使用COOKIE 单个cookie在客户端的限制是3K，就是说一个站点在客户端存放的COOKIE不能3K。 所以：将登陆信息等重要信息存放为SESSION;其他信息如果需要保留，可以放在COOKIE中 session 共享 session 复制 服务器将自己的session数据传送给其他服务器，使得每个服务器都拥有全量的数据。 优点：tomcat原生支持，只需要修改配置文件即可 缺点：网络传输、服务器空间占有 客户端 cookie 保存 用户的信息不再保存在服务器中，而是保存在客户端(浏览器)中。 优点：节省服务器资源 缺点：网络带宽、长度限制、安全隐患 hash一致性 nginx负载均衡的时候采用ip-hash策略，这样同一个客户端每次的请求都会被同一个服务器处理 优点：只需要修改nginx配置，不需要修改应用程序代码；可以支持web-server水平扩展 缺点：web-server重启可能导致部分session丢失，影响业务；如果web-server水平扩展，rehash后session重新分布，会有一部分用户路由不到正确session。 统一存储 将用户的信息存储在第三方中间件上，做到统一存储，如redis中，所有的服务都到redis中获取用户信息，从而实现session共享。 优点： 没有安全隐患 可以水平扩展 服务器重启或扩容都不会造成session的丢失 缺点： 增加了一次网络调用，速度有所下降 代码修改。getSession 变成从 redis 中 get，可以用 Spring Session框架 反向代理是起什么作用的？反向代理是一种位于客户端和后端服务器之间的代理服务器模型，它接收来自客户端的请求，并将其转发到后端服务器，然后将响应返回给客户端。反向代理在多个方面发挥着重要的作用： 负载均衡：反向代理通过将请求分发到多个后端服务器，平衡负载，从而减少单个服务器的负担，提高系统的可伸缩性。对于流量较高的网站，这种分发机制可以确保服务器容量得到充分利用，以处理大量请求。如果某台服务器过载并出现故障，反向代理还可以将流量重定向至其他在线服务器，确保服务的连续性和稳定性。 安全性和访问控制：反向代理通过实现身份验证、授权和防火墙功能，增强了系统的安全性。它可以拦截所有传入请求，为后端服务器提供更高层级的保护。通过阻止来自特定IP地址的可疑流量，反向代理有助于防止恶意访问者滥用网页服务器。此外，反向代理还可以隐藏后端服务器的真实地址，进一步提高系统的安全性。 缓存数据：反向代理可以缓存经常请求的数据，减少对后端服务器的访问次数，提高系统的性能。对于需要存储大量信息流数据的大型用户，缓存机制可以有效降低网站服务器的负载，提高网站的响应速度和用户体验。 服务治理：通过反向代理，管理员可以监控和管理后端服务器的状态，包括健康检查、服务降级等，确保系统的稳定性和可靠性。","link":"/2024/03/11/cs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"Python常用数据结构","text":"列表-List","link":"/2025/06/02/python/python%E5%B8%B8%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"title":"Gson教程","text":"GsonGson 是 Google 提供的用于 Java 语言中序列化和反序列化 JSON 数据的库。 Gson特性 用于 Java 对象 JSON 序列化和反序列化的简单工具。 Java 泛型的广泛支持。 对象的自定义表示。 支持任意复杂的对象。 快速，低内存占用。 允许紧凑的输出和漂亮的打印。 Java Gson 类创建方法: new Gson() new GsonBuilder().create() toJson()与fromJson()toJson()方法将指定的对象序列化为其等效的 JSON 表示形式。 fromJson()方法将指定的 JSON 反序列化为指定类的对象。 12345678910111213141516171819202122232425262728293031import com.google.gson.Gson;public class GsonFromJson { public static void main(String[] args) { Gson gson = new Gson(); String json = gson.toJson(new User(\"Tom\", \"Broody\")); System.out.println(json); User user = gson.fromJson(json, User.class); System.out.println(user); }}class User { private final String firstName; private final String lastName; public User(String firstName, String lastName) { this.firstName = firstName; this.lastName = lastName; } @Override public String toString() { return new StringBuilder().append(\"User{\").append(\"First name: \") .append(firstName).append(\", Last name: \") .append(lastName).append(\"}\").toString(); }} 漂亮打印Gson 有两种输出模式：紧凑和漂亮。 1Gson gson = new GsonBuilder().setPrettyPrinting().create(); 序列化空值默认情况下，Gson 不会将具有空值的字段序列化为 JSON。 如果 Java 对象中的字段为null，则 Gson 会将其排除。 我们可以使用serializeNulls()方法强制 Gson 通过GsonBuilder序列化null值。 1GsonBuilder builder = new GsonBuilder().serializeNulls().create(); 使用@Expose排除字段@Expose注解指示应公开成员以进行 JSON 序列化或反序列化。 @Expose注解可以采用两个布尔参数：serialize和deserialize。 必须使用excludeFieldsWithoutExposeAnnotation()方法显式启用@Expose注解。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import com.google.gson.Gson;import com.google.gson.GsonBuilder;import com.google.gson.annotations.Expose;enum MaritalStatus { SINGLE, MARRIED, DIVORCED, UNKNOWN}class Person { @Expose private String firstName; @Expose private String lastName; private MaritalStatus maritalStatus; public Person(String firstName, String lastName, MaritalStatus maritalStatus) { this.firstName = firstName; this.lastName = lastName; this.maritalStatus = maritalStatus; } public Person() {}}public class GsonExcludeFields { public static void main(String[] args) { Gson gson = new GsonBuilder() .excludeFieldsWithoutExposeAnnotation() .setPrettyPrinting() .create(); Person p = new Person(\"Jack\", \"Sparrow\", MaritalStatus.UNKNOWN); gson.toJson(p, System.out); }} 由于maritalStatus没有被@Expose注解,所以序列化的时候会被排除. @Expose注解通过excludeFieldsWithoutExposeAnnotation()方法启用了字段排除。 Java Gson 树模型 API树模型 API 在内存中创建 JSON 文档的树表示。 它构建JsonElements的树。 JsonElement是代表 Json 元素的类。 它可以是JsonObject，JsonArray，JsonPrimitive或JsonNull。 toJsonTree()toJsonTree()是 Gson 中的一个方法，它可以将 Java 对象序列化为一个 JsonElement 对象。 即将 Java 对象递归转化为 Gson 中的 JsonElement 对象 JsonElement 对象表示一个 JSON 数据结构。JsonElement 可以是一个 JSON 对象、一个 JSON 数组或一个 JSON 基本类型。toJsonTree() 方法通过递归遍历 Java 对象中的字段，将每个字段转化为对应的 JsonElement，并将所有的 JsonElement 组装成一个 JSON 数据结构返回。 例如，假设有以下 Java 类： 12345678arduino复制代码public class Person { private String name; private int age; private List&lt;String&gt; hobbies; // getter and setter methods} 那么调用 toJsonTree() 方法，可以将一个 Person 对象转化为一个 JsonElement 对象： 123456789Person person = new Person();person.setName(\"Alice\");person.setAge(25);List&lt;String&gt; hobbies = new ArrayList&lt;&gt;();hobbies.add(\"reading\");hobbies.add(\"running\");person.setHobbies(hobbies);Gson gson = new Gson();JsonElement jsonTree = gson.toJsonTree(person); 此时 jsonTree 对象将表示以下 JSON 数据结构： 12345678{ \"name\": \"Alice\", \"age\": 25, \"hobbies\": [ \"reading\", \"running\" ]} 需要注意的是，toJsonTree() 方法不会将 Java 对象转化为 JSON 字符串，而是将其转化为一个 Gson 中的 JsonElement 对象。如果需要将 JsonElement 对象转化为 JSON 字符串，可以使用 Gson 中的 toJson() 方法。 JsonElement是Gson库中用来表示JSON元素的基类，它可以是一个JsonObject、JsonArray、JsonPrimitive或者JsonNull。 使用toJsonTree方法而不是toJson的好处是，你可以在将Java对象转换为JSON字符串之前，对生成的JSON结构进行更细粒度的操作和修改。 123456789101112Gson gson = new Gson();MyObject myObject = new MyObject();JsonElement jsonElement = gson.toJsonTree(myObject);// 对jsonElement进行额外操作，比如添加、修改或删除某些属性if (jsonElement.isJsonObject()) { JsonObject jsonObject = jsonElement.getAsJsonObject(); jsonObject.addProperty(\"newProperty\", \"newValue\");}// 最终将JsonElement对象转换为JSON字符串String jsonString = gson.toJson(jsonElement); 在这个例子中，MyObject是一个自定义的Java类。首先，使用toJsonTree方法将myObject实例转换为JsonElement。然后，检查这个JsonElement是否是一个JsonObject，如果是，就将其转换为JsonObject并添加一个新的属性。最后，使用toJson方法将修改后的JsonElement转换为JSON字符串。 这种方法允许你在转换为JSON字符串之前，对JSON数据结构进行复杂的操作，这在需要对生成的JSON进行动态修改的场景下非常有用。","link":"/2024/05/08/Spring/Gson%E4%BD%BF%E7%94%A8/"},{"title":"HttpServletRequest流数据不可重复读","text":"问题背景项目需求中，需要多次读取 Http 请求中的参数，比如前置的 API 身份校验。常规方式是在拦截器中实现该逻辑，但如果在拦截器中通过getInputStream()读取HttpServletRequest中的参数后，之后的 controller 就无法重复读取了，并抛出异常： 12HttpMessageNotReadableException: Required request body is missingIllegalStateException: getInputStream() can't be called after getReader() 解决思路是将请求的数据用包装类缓存起来。 前置原理Controller 接受 HTTP 请求参数时，基本是通过 SpringMVC 封装： POST 发送的multipart/form-data 格式的数据 对于这类数据，SpringMVC 在 DispatchServlet 的 doDispatch() 方法中就会进行处理。 在 controller 中用@PathParam获取参数，或者手动用request.getParameter()获取 POST 发送的application/x-www-form-urlencoded 格式的数据 1234567891011// 非 form-data 的数据，会存储在 HttpServletRequest 的 InputStream 中。// 在第一次调用 getParameterNames() 或 getParameter() 时,// 会调用 parseParameters() 方法对参数进行封装，从 InputStream 中读取数据，并封装到 Map 中。//org.apache.catalina.connector.Request.javapublic String getParameter(String name) { if (!this.parametersParsed) { this.parseParameters(); } return this.coyoteRequest.getParameters().getParameter(name);} POST 发送的application/json 格式的数据 数据会直接会存储在 HttpServletRequest 的 InputStream 中，通过@RequestBody或request.getInputStream()获取。 问题原因如果要在拦截器中和 Controller 中重复读取参数时，会出现 request body 找不到的情况，这是由于 InputStream 这个流数据的特殊性，在 Java 中读取 InputStream 数据时，内部是通过一个指针的移动来读取一个一个的字节数据的，当读完一遍后，这个指针并不会 reset，因此第二遍读的时候就会出现问题了。而之前讲了，HTTP 请求的参数也是封装在 Request 对象中的 InputStream 里，所以当第二次调用 getInputStream() 时会抛出上述异常。 具体问题根据三中不同的格式分成不同情况： 请求方式为 multipart/form-data，在拦截器中手动调用 request.getInputStream() 12// 上文讲了在 doDispatch() 时就会进行处理，因此这里会取不到值log.info(\"input stream content: {}\", new String(StreamUtils.copyToByteArray(request.getInputStream()))); 请求方式为 application/x-www-form-urlencoded，在拦截器中手动调用 request.getInputStream() 123456// 第 1 次可以取到值log.info(\"input stream content: {}\", new String(StreamUtils.copyToByteArray(request.getInputStream())));// 第一次执行 getParameter() 会调用 parseParameters()，parseParameters 进一步调用 getInputStream()// 这里就取不到值了log.info(\"form-data param: {}\", request.getParameter(\"a\"));log.info(\"form-data param: {}\", request.getParameter(\"b\")); 请求方式为 application/json，在拦截器中手动调用 request.getInputStream() 123// 第 1 次可以取到值log.info(\"input stream content: {}\", new String(StreamUtils.copyToByteArray(request.getInputStream())));// 之后再任何地方再调用 getInputStream() 都取不到值，会抛出异常 为了能够多次获取到 HTTP 请求的参数，我们需要将 InputStream 流中的数据缓存起来。 解决方案一、配置过滤器，对HttpServletRequest 进行包装，读取body信息存放到包装类中，后续都从包装类中读取 https://blog.csdn.net/BLACKLOVE7/article/details/130704948 二、springframework 自己就有相应的 wrapper 来解决这个问题，在 org.springframework.web.util 包下有一个 ContentCachingRequestWrapper 的类。这个类的作用就是将 InputStream 缓存到 ByteArrayOutputStream 中，通过调用 ``getContentAsByteArray()` 实现流数据的可重复读取。 https://www.cnblogs.com/Sinte-Beuve/p/13260249.html#%E6%9C%80%E4%BD%B3%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88 总结不管是第一种自己实现 Wrapper 的方案还是使用第二种 Spring 提供的 Wrapper，在过滤器中需要判断Method是否是Post，同时也要判断Content-Type不是multipart/form-data， 因为该类型 SpringMVC 会在 doDispatch()中处理，若在过滤器中先读取了，后续就读取不到了。","link":"/2024/06/12/Spring/HttpServletReque%E6%B5%81%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%8F%AF%E9%87%8D%E5%A4%8D%E8%AF%BB/"},{"title":"Mockito使用","text":"背景项目中，有些函数需要处理某个服务的返回结果，而在对函数单元测试的时候，又不能启动那些服务，这里就可以利用Mockito工具。 基础有如下三种注解：@InjectMocks：创建一个实例，简单的说是这个Mock可以调用真实代码的方法，其余用@Mock（或@Spy）注解创建的mock将被注入到用该实例中。 @Mock：对函数的调用均执行mock（即虚假函数），不执行真正部分。 @Spy：对函数的调用均执行真正部分。 mock方法和spy方法都可以对对象进行mock。但是前者是接管了对象的全部方法，而后者只是将有桩实现（stubbing）的调用进行mock，其余方法仍然是实际调用。 真实情况1 Class A 需要使用 RPC调用 Class C，所以需要对 Class C 的服务进行 mock 流程1234567891011121314@RunWith(SpringRunner.class)public class Test { @Autowired @InjectMocks private ClassA classA; @Mock private ClassB classB; // 此mock将被注入到classA中 @Test public void test(){ Mockito.when(classB.method1()).thenReturn(****); classA.method2(); } 总结InjectMocks，就好比是service层的对象。Mock好比是dao层对象。 Mockito.when(Mock对象.方法).thenReturn(值) 这样，InjectMocks对象调用某方法时候， 真实情况 2假如需要对一个工具类xxxUtils的静态方法method1进行 mock 流程123456789101112131415@RunWith(SpringRunner.class)public class Test { private static MockedStatic&lt;xxxUtils&gt; mockedStatic;//声明 @Before public void init(){ mockedStatic = Mockito.mockStatic(xxxUtils.class);//初始化 when(xxxUtils.method1()).thenReturn(xxx); } @Test public void test(){ xxxUtils.method1(); }","link":"/2024/06/12/Spring/Mockito%E4%BD%BF%E7%94%A8/"},{"title":"Spring面经","text":"Spring、SpringBoot、SpringMVCSpring 是什么？有什么用？是什么： Spring是一个轻量级的控制反转(IoC)和面向切面(AOP)的容器框架。 核心特性/有什么用： IoC 容器：通过控制反转和依赖注入实现松耦合。 AOP：支持面向切面的编程，将应用业务逻辑和系统服务拆分。 事务管理：支持声明式事务。通过声明式方式灵活地进行事务的管理，提高开发效率和质量。 方便集成各种优秀框架。（quartz、mybatis） SpringBoot 是什么？有什么用？是什么： Spring的扩展，简化Spring的配置（xml），简化开发者的使用 有什么用： 快速开发 快速整合 配置简化 内嵌服务容器 Spring MVC和Spring Boot都属于Spring，Spring MVC 是基于Spring的一个 MVC 框架，而Spring Boot 是基于Spring的一套快速开发整合包 SpringMVC是什么？有什么用Spring的一部分，采用MVC架构模式的思想，用于Web应用的开发。 通过Dispatcher Servlet, ModelAndView 和 View Resolver SpringBoot 核心注解@SpringBootApplication 包括三个重要注解： @EnableAutoConfiguration。提供自动配置 @Configuration。代替 xml配置文件 @ComponentScan。开启组件扫描，即自动扫描包路径下的 @Component 注解进行注册 bean 实例到 context 中 Spring 核心之控制反转（IOC）✨如何理解 IoC✨控制反转 - Inversion of Control (IoC) IoC 不是一种技术，而是一种创建和获取对象的技术思想 简单来讲就是从原来的由用户管理Bean转变为IoC 容器管理Bean，大大降低对象之间的耦合度。 谁控制谁 IoC 容器控制对象 控制什么 对象的创建、初始化、销毁 为何是反转 由用户控制对象转变为IoC 容器控制对象 IoC和DI是什么关系✨IoC是设计思想，DI是实现方式。 控制反转是通过依赖注入实现的，“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。 IoC容器配置Bean的三种方式xml 配置顾名思义，就是将bean的信息配置.xml文件里，通过Spring加载文件为我们创建bean。这种方式出现很多早前的SSM项目中，将第三方类库或者一些配置工具类都以这种方式进行配置，主要原因是由于第三方类不支持Spring注解。 优点： 可以使用于任何场景，结构清晰，通俗易懂 缺点： 配置繁琐，不易维护，枯燥无味，扩展性差 举例： 配置xx.xml文件 声明命名空间和配置bean Java 配置将类的创建交给我们配置的JavcConfig类来完成，Spring只负责维护和管理，采用纯Java创建方式。其本质上就是把在XML上的配置声明转移到Java配置类中 优点：适用于任何场景，配置方便，因为是纯Java代码，扩展性高，十分灵活 缺点：由于是采用Java类的方式，声明不明显，如果大量配置，可读性比较差 举例： 创建一个配置类， 添加@Configuration注解声明为配置类 创建方法，方法上加上@bean，该方法用于创建实例并返回，该实例创建后会交给spring管理，方法名建议与实例名相同（首字母小写）。注：实例类不需要加任何注解 注解配置通过在类上加注解的方式，来声明一个类交给Spring管理，Spring会自动扫描带有@Component，@Controller，@Service，@Repository这四个注解的类，然后帮我们创建并管理，前提是需要先配置Spring的注解扫描器。 优点：开发便捷，通俗易懂，方便维护。 缺点：具有局限性，对于一些第三方资源，无法添加注解。只能采用XML或JavaConfig的方式配置 举例： 对类添加@Component相关的注解，比如@Controller，@Service，@Repository 设置ComponentScan的basePackage, 比如&lt;context:component-scan base-package='tech.pdai.springframework'&gt;, 或者@ComponentScan(\"tech.pdai.springframework\")注解，或者 new AnnotationConfigApplicationContext(\"tech.pdai.springframework\")指定扫描的basePackage. 依赖注入的三种方式✨属性注入以@Autowired（自动注入）注解注入为例，默认按照byType注入。 12@Autowiredprivate xxx xxx; setter方式123456priavte xxx yyy;@Autowiredpublic void setXxx(xxx yyy) { this.yyy = yyy;} 构造函数（推荐使用）12345678public class aaa{ private xxx yyy; @Autowired public void setXxx(xxx yyy) { this.yyy = yyy; }} 优势： 依赖不可变 依赖不为空 依赖完全初始化 三种属性注入有何区别？@Autowired Autowired注解源码 在Spring 2.5 引入了 @Autowired 注解 123456@Target({ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Autowired { boolean required() default true;} 从Autowired注解源码上看，可以使用在下面这些地方： 12345@Target(ElementType.CONSTRUCTOR) #构造函数@Target(ElementType.METHOD) #方法@Target(ElementType.PARAMETER) #方法参数@Target(ElementType.FIELD) #字段、枚举的常量@Target(ElementType.ANNOTATION_TYPE) #注解 还有一个value属性，默认是true。 简单总结： 1、@Autowired是Spring自带的注解，通过AutowiredAnnotationBeanPostProcessor 类实现的依赖注入 2、@Autowired可以作用在CONSTRUCTOR、METHOD、PARAMETER、FIELD、ANNOTATION_TYPE 3、@Autowired默认是根据类型（byType ）进行自动装配的 4、如果有多个类型一样的Bean候选者，需要指定按照名称（byName ）进行装配，则需要配合@Qualifier。 简单使用代码： 在字段属性上。 12@Autowiredprivate HelloDao helloDao; 或者 12345678private HelloDao helloDao;public HelloDao getHelloDao() { return helloDao;}@Autowiredpublic void setHelloDao(HelloDao helloDao) { this.helloDao = helloDao;} 或者 123456private HelloDao helloDao;//@Autowiredpublic HelloServiceImpl(@Autowired HelloDao helloDao) { this.helloDao = helloDao;}// 构造器注入也可不写@Autowired，也可以注入成功。 将@Autowired写在被注入的成员变量上，setter或者构造器上，就不用再xml文件中配置了。 如果有多个类型一样的Bean候选者，则默认根据设定的属性名称进行获取。如 HelloDao 在Spring中有 helloWorldDao 和 helloDao 两个Bean候选者。 12@Autowiredprivate HelloDao helloDao; 首先根据类型获取，发现多个HelloDao，然后根据helloDao进行获取，如果要获取限定的其中一个候选者，结合@Qualifier进行注入。 123@Autowired@Qualifier(\"helloWorldDao\")private HelloDao helloDao; 注入名称为helloWorldDao 的Bean组件。@Qualifier(“XXX”) 中的 XX是 Bean 的名称，所以 @Autowired 和 @Qualifier 结合使用时，自动注入的策略就从 byType 转变成 byName 了。 多个类型一样的Bean候选者，也可以@Primary进行使用，设置首选的组件，也就是默认优先使用哪一个。 注意：使用@Qualifier 时候，如何设置的指定名称的Bean不存在，则会抛出异常，如果防止抛出异常，可以使用： 123@Qualifier(\"xxxxyyyy\")@Autowired(required = false)private HelloDao helloDao; 在SpringBoot中也可以使用@Bean+@Autowired进行组件注入，将@Autowired加到参数上，其实也可以省略。 12345@Beanpublic Person getPerson(@Autowired Car car){ return new Person();}// @Autowired 其实也可以省略 @Resource Resource注解源码 123456@Target({TYPE, FIELD, METHOD})@Retention(RUNTIME)public @interface Resource { String name() default \"\"; // 其他省略} 从Resource注解源码上看，可以使用在下面这些地方： 123@Target(ElementType.TYPE) #接口、类、枚举、注解@Target(ElementType.FIELD) #字段、枚举的常量@Target(ElementType.METHOD) #方法 name 指定注入指定名称的组件。 简单总结： 1、@Resource是JSR250规范的实现，在javax.annotation包下 2、@Resource可以作用TYPE、FIELD、METHOD上 3、@Resource是默认根据属性名称进行自动装配的，如果有多个类型一样的Bean候选者，则可以通过name指定进行注入 简单使用代码： 12345@Componentpublic class SuperMan { @Resource private Car car;} 按照属性名称 car 注入容器中的组件。如果容器中BMW还有BYD两种类型组件。指定加入BMW。如下代码： 12345@Componentpublic class SuperMan { @Resource(name = \"BMW\") private Car car;} name 的作用类似 @Qualifier @Inject Inject注解源码 1234@Target({ METHOD, CONSTRUCTOR, FIELD })@Retention(RUNTIME)@Documentedpublic @interface Inject {} 从Inject注解源码上看，可以使用在下面这些地方： 123@Target(ElementType.CONSTRUCTOR) #构造函数@Target(ElementType.METHOD) #方法@Target(ElementType.FIELD) #字段、枚举的常量 简单总结： 1、@Inject是JSR330 (Dependency Injection for Java)中的规范，需要导入javax.inject.Inject jar包 ，才能实现注入 2、@Inject可以作用CONSTRUCTOR、METHOD、FIELD上 3、@Inject是根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named； 简单使用代码： 12@Injectprivate Car car; 指定加入BMW组件。 123@Inject@Named(\"BMW\")private Car car; @Named 的作用类似 @Qualifier！ 总结✨1、@Autowired是Spring自带的，@Resource是JSR250规范实现的，@Inject是JSR330规范实现的 2、@Autowired、@Inject用法基本一样，不同的是@Inject没有required属性 3、@Autowired、@Inject是默认按照类型匹配的，@Resource是按照名称匹配的 4、@Autowired如果需要按照名称匹配需要和@Qualifier一起使用，@Inject和@Named一起使用，@Resource则通过name进行指定 bean 创建的方式 xml配置 1&lt;bean id=\"student\" class=\"cn.com.demo.Student\"/&gt; xml&lt;context:component-scan&gt; +注解@Component（@Controller 、@Service 、@Repository）定义 bean 通过@ComponentScan扫描，去除XML文件 通过@Import创建bean 如果@Import写的是配置类，则配置类会被加载为bean，配置类中有@bean注解的类，同样会被加载为bean， 容器初始化时，通过register或者registerBean进行创建bean 实现ImportSelector接口，并实现selectImports方法 实现ImportBeanDefinitionRegistrar接口，并实现registerBeanDefinitions方法 实现BeanDefinitionRegistryPostProcessor接口 Spring核心之面向切面编程(AOP)✨描述一下Spring AOP？✨AOP为Aspect Oriented Programming的缩写，意为：面向切面编程，是一种开发理念。是 OOP面向对象编程的补充。 在OOP 中最小的单元是 Class 对象，但在 AOP 中最小的单元是“切面”。一个切面可以包含很多类型和对象，对他们进行模块化管理。 ==简单来说就是将代码中重复的部分抽取出来，在需要执行的时候使用动态代理，在不修改源码的基础上对方法进行增强==。 Spring AOP 实现机制✨Spring AOP的实现依赖于动态代理技术。 Spring 根据类是否实现接口来判断动态代理方式： 若实现接口会使用 JDK 的动态代理，核心是 InvocationHandler 接口和 Proxy 类 如果没有实现接口会使用 CGLib 动态代理，CGLiB 是在运行时动态生成被代理类的子类。CGLiB 生成代理类的效率低，但是调用方法的效率高。 常用场景包括日志、统计（方法调用次数）、安防（监控，熔断、限流）、性能（缓存）等 简述 AOP 中几个重要概念 ✨AspectJ：切面，只是一个概念，没有具体的接口或类与之对应，是 Join point，Advice 和 Pointcut 的一个统称。 ✨Join point：连接点，指程序执行过程中的一个点，例如方法调用、异常处理等。在 Spring AOP 中，仅支持方法级别的连接点。 ✨Pointcut：切点，用于匹配连接点，一个 AspectJ 中包含哪些 Join point 需要由 Pointcut 进行筛选。 ✨Advice：通知，切面对某个连接点所产生的动作 Weaving：织入，在切点的引导下，将通知逻辑插入到目标方法上，使得我们的通知逻辑在方法调用时得以执行。 AOP proxy：AOP 代理，指在 AOP 实现框架中实现切面协议的对象。在 Spring AOP 中有两种代理，分别是 JDK 动态代理和 CGLIB 动态代理。 Target object：目标对象，就是被代理的对象。 Spring中有哪些不同的通知类型 前置通知(Before Advice): 在连接点之前执行的Advice，不过除非它抛出异常，否则没有能力中断执行流。使用 @Before 注解使用这个Advice。 返回之后通知(After Retuning Advice): 在连接点正常结束之后执行的Advice。例如，如果一个方法没有抛出异常正常返回。通过 @AfterReturning 关注使用它。 抛出（异常）后执行通知(After Throwing Advice): 如果一个方法通过抛出异常来退出的话，这个Advice就会被执行。通用 @AfterThrowing 注解来使用。 后置通知(After Advice): 无论连接点是通过什么方式退出的(正常返回或者抛出异常)都会执行在结束后执行这些Advice。通过 @After 注解使用。 围绕通知(Around Advice): 围绕连接点执行的Advice，就你一个方法调用。这是最强大的Advice。通过 @Around 注解使用。 CGLIB&amp;&amp;JDKJDK动态代理的实现方案有两种，JDK动态代理和CGLIB动态代理，区别在于JDK自带的动态代理，必须要有接口，而CGLIB动态代理有没有接口都可以。 JDK动态代理：JDK原生的实现方式，需要被代理的目标类必须实现接口。通过反射。核心是 proxy 类和 invocationHandler 接口的 invoke 方法。 主要流程是： 定义一个接口，该接口是被代理对象和代理对象共同实现的。 创建一个实现 InvocationHandler 接口的代理类，并在其中实现 invoke 方法。在 invoke 方法中，可以根据需要执行额外的逻辑，然后调用目标对象的方法。 使用 Proxy 类的静态方法 newProxyInstance 创建代理对象。该方法接收三个参数：ClassLoader、要实现的接口数组以及 InvocationHandler 对象。 通过代理对象调用方法。 缺点： 只能代理接口，无法代理具体类。 被代理的目标对象必须实现至少一个接口。 动态代理的性能相对较低，因为涉及到反射操作。 1234567891011121314151617181920212223public class JDKMatrimonialAgency implements InvocationHandler { //被代理的对象，把引用给保存下来 private Object target; public Object getInstance(Object target) throws Exception{ this.target = target; Class&lt;?&gt; clazz = target.getClass(); return Proxy.newProxyInstance(clazz.getClassLoader(),clazz.getInterfaces(),this); } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { before(); Object obj = method.invoke(this.target,args); after(); return obj; } private void before(){ System.out.println(\"这里是婚介所,请提供你的需求\"); } private void after(){ System.out.println(\"已经找到合适的,尽快安排你相亲\"); }} CGLIBcglib动态代理：通过继承被代理的目标类（认干爹模式）实现代理，所以不需要目标类实现接口。(CGLIB 通过动态生成一个需要被代理类的子类（即被代理类作为父类），该子类重写被代理类的所有不是 final 修饰的方法，并在子类中采用MethodInterceptor接口的 intercept方法拦截父类所有的方法调用，进而织入横切逻辑。) 12345678910111213141516171819202122232425public class CglibMatrimonialAgency implements MethodInterceptor { public Object getInstance(Class&lt;?&gt; clazz) throws Exception{ Enhancer enhancer = new Enhancer(); //要把哪个设置为即将生成的新类的父类 enhancer.setSuperclass(clazz); enhancer.setCallback(this); return enhancer.create(); } @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { //业务的增强 before(); Object obj = methodProxy.invokeSuper(o,objects); after(); return obj; } private void before(){ System.out.println(\"这里是婚介所,请提供你的需求\"); } private void after(){ System.out.println(\"已经找到合适的,尽快安排你相亲\"); }} 对比✨ jdk 实现了被代理类的接口；cglib 是继承了被代理类 jdk 和 cglib 都是在运行期生成字节码，jkd 直接写 class字节码；cglib 使用 asm 框架写 class 字节码，cglib 代理实现更复杂，生成代理类比 jdk 效率低 jdk 调用代理方法，是通过反射机制调用；cglib 是通过 fastclass 机制直接调用方法，cglib 执行效率更高 过滤器&amp;&amp;拦截器&amp;&amp;Spring AOP过滤器过滤器拦截的是 URL，能过滤所有 web 请求 应用场景： 自动登录 同一设置编码格式 访问权限控制 敏感字符过滤 拦截器拦截器拦截的是部分 URL。Java中的拦截器是基于Java反射机制实现的，更准确的划分，是基于JDK实现的动态代理。 应用场景： 日志记录 权限检查：如登录检查 性能检查：检测方法的执行时间 AOP拦截的是类的元数据（类、方法） 相对于拦截器更加细致，而且非常灵活，拦截器只能针对URL做拦截，而AOP针对具体的代码，能够实现更加复杂的业务逻辑 应用场景 事务控制 异常处理 打印日志 对比✨三者功能类似，但各有优势，从过滤器–》拦截器–》AOP，拦截规则越来越细致，执行顺序依次是过滤器、拦截器、切面。一般情况下数据被过滤的时机越早对服务的性能影响越小，因此我们在编写相对比较公用的代码时，优先考虑过滤器，然后是拦截器，最后是aop。 拦截器和过滤器的区别 1、拦截器是Spring自带的，过滤器依赖于Servlet容器。 2、拦截器是基于java的反射机制的，而过滤器是基于函数回调。 3、拦截器只能对Controller中请求或访问static目录下的资源请求起作用，而过滤器则可以对几乎所有的请求起作用。 4、拦截器可以访问controller上下文、值栈里的对象，而过滤器不能访问。 AOP 失效场景✨==首先，Spring的AOP其实是通过动态代理实现的，所以，想要让AOP生效，前提必须是动态代理生效，并且可以调用到代理对象的方法== 非Spring管理的对象 Spring的AOP只能拦截由Spring容器管理的Bean对象。如果您使用了非受Spring管理的对象，则AOP将无法对其进行拦截。 同一个Bean内部方法调用✨ 如果一个Bean内部的方法直接调用同一个Bean内部的另一个方法，AOP将无法拦截这个内部方法调用。因为AOP是基于代理的，只有通过代理对象才能触发AOP拦截。 静态方法 final 方法 CGLIB 通过生成子类来实现代理，final方法是不可重写的 异步方法 对于使用Spring的异步特性（如@Async注解）的方法，AOP拦截器可能无法正常工作。这是因为异步方法在运行时会创建新的线程或使用线程池，AOP拦截器无法跟踪到这些新线程中的方法调用。 经典：针对第二个方法存在事务失效的问题 由于 @Transactional 注解也是通过 Spring AOP 来实现事务管理的增强的 解决方案： 拆分两个方法，把加了事务的方法单独用一个类包装【@Servcie】 类中注入自己，用注入的类来调用加了事务的方法【@Autowired】 获取当前类的代理类来调用方法【AopContext.currentProxy()】 1MyService proxy = (MyService) AopContext.currentProxy(); Spring中 Bean 的生命周期✨Spring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。 如果 BeanFactoryPostProcessor 和 Bean 关联, 首先尝试从Bean工厂中获取Bean 如果 InstantiationAwareBeanPostProcessor 和 Bean 关联，则会调用实例化前的方法 根据配置情况调用 Bean 构造方法实例化 Bean。 利用依赖注入完成 Bean 中所有属性值的配置注入。 如果 InstantiationAwareBeanPostProcessor 和 Bean 关联，则会调用实例化后的方法 调用Bean 实现的Aware接口 ，如 BeanNameAware、BeanClassLoaderAware、BeanFactoryAware、ApplicationContextAware，设置当前 Bean 的一些属性 如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的预初始化方法对 Bean 进行加工操作，此处非常重要，Spring 的 AOP 就是利用它实现的。 如果 Bean 实现了 InitializingBean 接口，则 Spring 将调用 afterPropertiesSet() 方法。(或者有执行@PostConstruct注解的方法) 如果在配置文件中通过 init-method 属性指定了初始化方法，则调用该初始化方法。 如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的初始化方法 。此时，Bean 已经可以被应用系统使用了。 如果在 &lt;bean&gt; 中指定了该 Bean 的作用范围为 scope=”singleton”，则将该 Bean 放入 Spring IoC 的缓存池中，将触发 Spring 对该 Bean 的生命周期管理；如果在 &lt;bean&gt; 中指定了该 Bean 的作用范围为 scope=”prototype”，则将该 Bean 交给调用者，调用者管理该 Bean 的生命周期，Spring 不再管理该 Bean。 如果 Bean 实现了 DisposableBean 接口，则 Spring 会调用 destory() 方法将 Spring 中的 Bean 销毁；(或者有执行@PreDestroy注解的方法) 如果在配置文件中通过 destory-method 属性指定了 Bean 的销毁方法，则 Spring 将调用该方法对 Bean 进行销毁。 Spring如何解决循环依赖问题✨Spring单例模式下的属性依赖三级缓存： 第一层缓存（singletonObjects）：单例对象缓存池，已经实例化并且属性赋值，这里的对象是成熟对象； 第二层缓存（earlySingletonObjects）：单例对象缓存池，已经实例化但尚未属性赋值，这里的对象是半成品对象； 第三层缓存（singletonFactories）: 单例工厂的缓存，仅仅只是实例化的对象 ==解决循环依赖的关键是通过将实例化但是还没有初始化的对象提高曝光在第三级缓存 singtonfactory== 流程： 实例化 A：首先实例化A并提早曝光到三级缓存singletonFactories中 A属性赋值：要进行属性赋值B，发现B尚未被创建，因此要去创建 B， B 属性赋值：B在属性赋值的时候发现本身依赖了对象A，依次从一二三级缓存重找 A，因为A经过singleFactory将本身提早曝光了，因此B可以经singleFactory.getObject拿到A对象(半成品)，B拿到A对象后顺利完成了初始化，彻底初始化以后将本身放入到一级缓存singletonObjects中。 A创建：此时返回A中，A此时能拿到B的对象顺利完成本身的初始化阶段，最终A也完成了初始化，进去了一级缓存singletonObjects中 Spring为何不能解决非单例属性之外的循环依赖？spring 只解决了单例模式下属性注入和 setter 注入的循环依赖问题。同时 springboot 版本2.6 之后默认不支持循环依赖 SpringBoot2.6.0起及新版本中解决循环依赖问题_springboot 2.6-CSDN博客 Spring为什么不能解决构造器的循环依赖？Spring解决循环依赖主要是依赖三级缓存，但是的在调用构造方法之前还未将其放入三级缓存之中，因此后续的依赖调用构造方法的时候并不能从三级缓存中获取到依赖的Bean，因此不能解决。 Spring为什么不能解决prototype作用域循环依赖？这种循环依赖同样无法解决，因为spring不会缓存‘prototype’作用域的bean，而spring中循环依赖的解决正是通过缓存来实现的。 Spring为什么不能解决多例的循环依赖？多实例Bean是每次调用一次getBean都会执行一次构造方法并且给属性赋值，根本没有三级缓存，因此不能解决循环依赖。 那么其它循环依赖如何解决？ 生成代理对象产生的循环依赖 这类循环依赖问题解决方法很多，主要有： 使用@Lazy注解，延迟加载 使用@DependsOn注解，指定加载先后关系 修改文件名称，改变循环依赖类的加载顺序 使用@DependsOn产生的循环依赖 这类循环依赖问题要找到@DependsOn注解循环依赖的地方，迫使它不循环依赖就可以解决问题。 多例循环依赖 这类循环依赖问题可以通过把bean改成单例的解决。 构造器循环依赖 这类循环依赖问题可以通过使用@Lazy注解解决。 MVC 分层介绍✨视图 view：提供使用界面，进行交互 模型 model：包括Service处理逻辑业务以及 Dao 处理数据的读取 控制器 controller：连接 view 和 model SpringMVC 执行流程✨ 用户发送请求到「前端控制器」 「前端控制器」 调用 「映射处理器」。「映射处理器」 根据请求 url 找到具体的处理器，生成 「处理器执行链」（一系列拦截器和处理器对象）返回给 前端控制器 「前端控制器」 根据 「处理器对象」 获取 「处理器适配器」 并执行一些前置操作。如参数封装，数据转换等 执行 「处理器」（Controller，页面控制器）返回 ModelAndView 给 「前端控制器」 「前端控制器」将 ModelAndView 传给 视图解析器 解析返回 「视图view」 「前端控制器」再将 「模型数据Model」 填充到 「视图 view「 中进行渲染 「前端控制器」返回响应 Spring 事务声明式事务底层原理Spring容器在初始化每个单例bean的时候，会遍历容器中的所有BeanPostProcessor实现类，并执行其postProcessAfterInitialization方法，在执行AbstractAutoProxyCreator类的postProcessAfterInitialization方法时会创建代理对象。在创建代理的过程中会获取当前目标方法对应的拦截器，此时会得到TransactionInterceptor实例，在它的invoke方法中实现事务的开启和回滚。 例子： 如果在类A上标注Transactional注解，Spring容器会在启动的时候，为类A创建一个代理类B，类A的所有public方法都会在代理类B中有一个对应的代理方法，调用类A的某个public方法会进入对应的代理方法中进行处理；如果只在类A的b方法(使用public修饰)上标注Transactional注解，Spring容器会在启动的时候，为类A创建一个代理类B，但只会为类A的b方法创建一个代理方法，调用类A的b方法会进入对应的代理方法中进行处理，调用类A的其它public方法，则还是进入类A的方法中处理。在进入代理类的某个方法之前，会先执行TransactionInterceptor类中的invoke方法，完成整个事务处理的逻辑，如是否开启新事务、在目标方法执行期间监测是否需要回滚事务、目标方法执行完成后提交事务等。 事务失效的场景 方法不是 public 方法被 final 修饰 方法被 static 修饰 方法内部调用：有时候我们需要在某个 Service 类的某个方法中，调用另外一个事务方法 未被 spring 管理","link":"/2023/12/28/Spring/Spring/"},{"title":"根据 User-Agent 判断 os","text":"常见 os 的 UA 示例 Windows: 12Chrome: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36Firefox: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0 macOS: 12Chrome: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36Safari: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15 iOS: 12Safari on iPhone: Mozilla/5.0 (iPhone; CPU iPhone OS 14_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1Safari on iPad: Mozilla/5.0 (iPad; CPU OS 14_7 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1 Android: 12Chrome on Android Phone: Mozilla/5.0 (Linux; Android 11; Pixel 3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Mobile Safari/537.36Chrome on Android Tablet: Mozilla/5.0 (Linux; Android 11; SM-T510) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.120 Safari/537.36 匹配规则 Windows用Windows macOS用Macintosh iOS 用iPhone or iPad Android 用Android 注意 mac 和 iOS，iOS 的 UA 中也包括mac，所以不能用 mac来区分是 macos 还是 ios","link":"/2024/06/04/Spring/%E6%A0%B9%E6%8D%AEUser-Agent%20%E5%88%A4%E6%96%ADos/"},{"title":"JVM","text":"java 类加载✨加载过程✨分为三部分:加载、连接、初始化 加载 通过类的全限定名（包名+类名）获取类的.class文件的二进制字节流 将二进制字节流读入内存，在方法区生成该类的运行时数据结构 在堆中创建Class对象,作为.class进入内存后的数据访问入口 将1.7永久代 -&gt;1.8元空间原因 现实使用中存在问题：方法区存储类的元数据信息，我们不清楚一个程序到底有多少类需要被加载，且方法区位于JVM内存，我们不清楚需要给方法区分配多大内存，太小容易PermGen OOM，太大，在触发Full GC时又极其影响性能 连接包括验证、准备、解析 验证: 保证加载的字节流符合 JVM 规范, 包括元数据验证(是否继承了不能继承的类),符号引用验证(引用的其他类是否存在) 准备:为类的类变量开辟空间并赋默认值 解析:将 Class 在常量池的符号引用转变成直接引用(也就是得到类或者字段、方法在内存中的指针或者偏移量) 初始化为类变量初始化值,有两种方式: 声明时直接赋值 在静态代码块中赋值 类加载时机 创建类的实例 创建子类的实例 调用类的静态方法 访问类的静态变量 反射 类加载顺序 先加载、连接当前类 若父类没有加载, 则去加载、连接父类直到 Object 被加载 然后从父类开始初始化(静态变量赋值、静态初始化块) 类加载器✨JVM 中内置了三个重要的 ClassLoader： **BootstrapClassLoader(启动类加载器)**：最顶层的加载类，由 C++实现，通常表示为 null，并且没有父级，主要用来加载 JDK 内部的核心类库（ %JAVA_HOME%/lib目录下的jar 包和类）以及被 -Xbootclasspath参数指定的路径下的所有类。 **ExtensionClassLoader(扩展类加载器)**：主要负责加载 %JRE_HOME%/lib/ext 目录下的 jar 包和类以及被 java.ext.dirs 系统变量所指定的路径下的所有类。 **AppClassLoader(应用程序类加载器)**：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。 双亲委派模型✨ 双亲委派，又叫做父类委托，即在加载一个类时，==自底向上找判断父类加载器是否已经加载该类，如果启动类加载器都没有加载当前类，则自顶向下尝试加载该类==。 使用委派模型的目的是避免重复加载 Java 类型。 双亲委派模型的好处 避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类） 保证 java 核心 api 不被篡改 例子：即使自己写一个 java.lang.Object 类，也会使用启动类加载器加载官方 Object 类 打破双亲委派模型方法自定义类加载器继承ClassLoader，重写 loadClass()方法。若不想打破则重写 findClass()方法 例子：Tomcat。为了实现web应用程序之间的类加载器相互隔离独立的是WebAppClassLoader类加载器。它为什么可以隔离每个web应用程序呢？原因就是它打破了”双亲委派”的机制，如果收到类加载的请求，它会先尝试自己去加载，如果找不到在交给父加载器去加载，这么做的目的就是为了优先加载Web应用程序自己定义的类来实现web应用程序相互隔离独立的。 JVM 内存区域✨JDK 1.8： 线程私有程序计数器 记录程序执行到的位置。字节码解释器就是通过改变这个计数器的值来选取下一条需要执行的字节码指令 是控制流的指示器。分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成 唯一一个不会OutofMemoryError的区域 虚拟机栈✨ 局部变量表：主要用于存储方法参数和定义在方法体内的局部变量 操作数栈：存放方法执行过程中产生的中间计算结果，同时作为计算过程中变量临时的存储空间 动态链接：指向运行时常量池中的方法引用。关联到方法所属类的常量池，以支持动态链接，将指令中的符号引用转换为直接引用。 方法引用 -&gt; 方法区的方法元信息（方法结构体中包含类的引用）-&gt; 方法区的类元信息 （类结构体中包含该类的运行时常量池引用）-&gt; 类所属的常量池 方法返回地址 用来存放调用该方法的 PC 寄存器的值。 一个方法的结束，有两种方式 正常执行完成 出现未处理的异常，非正常退出 无论通过哪种方式退出，在方法退出后都返回到该方法被调用的位置。方法正常退出时，调用者的 PC 计数器的值作为返回地址，即调用该方法的指令的下一条指令的地址。 附加信息 栈高，虚拟机版本 本地方法栈执行本地方法 线程共享堆内存✨存放类实例和数组 JDK 8 版本之后 PermGen(永久代) 已被 Metaspace(元空间) 取代，元空间使用的是本地内存 为了进行高效的垃圾回收，虚拟机把堆内存逻辑上划分成三块区域（分代的唯一理由就是优化 GC 性能） 对象怎么分配✨ 判断是否分配到栈上。进行逃逸分析，如果有些方法中的对象引用没有被返回或没有被外面使用，即没有发生逃逸的对象会优先尝试在栈上分配。 是否大。太大就直接放到「老年代」 TLAB（Thread local allocation buffer）。线程本地分配缓存多线程时，给各线程分配特定的空间 TLAB线程局部分配缓存（Thread Local Allocation Buffer） 对 Eden 区域继续进行划分，JVM 为每个线程分配了一个私有缓存区域，它包含在 Eden 空间内 多线程同时分配内存时，使用 TLAB 可以避免多线程竞争，同时还能提升内存分配的吞吐量，因此我们可以将这种内存分配方式称为快速分配策略 方法区✨ ==（方法区逻辑上在堆中）存储已被虚拟机加载的 类型信息、域（Field）信息、方法信息、运行时常量池、字符串常量池、静态变量、JIT 代码缓存等数据== 类型信息 对每个加载的类型（类 class、接口 interface、枚举 enum、注解 annotation），JVM 必须在方法区中存储以下类型信息 这个类型的完整有效名称（全名=包名.类名） 这个类型直接父类的完整有效名（对于 interface或是 java.lang.Object，都没有父类） 这个类型的修饰符（public，abstract，final 的某个子集） 这个类型直接接口的一个有序列表 域（Field）信息 JVM 必须在方法区中保存类型的所有域的相关信息以及域的声明顺序 域的相关信息包括：域名称、域类型、域修饰符（public、private、protected、static、final、volatile、transient 的某个子集） 方法（Method）信息 方法名称 方法的返回类型 方法参数的数量和类型 方法的修饰符（public，private，protected，static，final，synchronized，native，abstract 的一个子集） 方法的字符码（bytecodes）、操作数栈、局部变量表及大小 运行时常量池：字面量（包括整数、浮点数和字符串字面量）和符号引用（包括对类型、字段、方法的符号引用） 字符串常量池。主要目的是为了避免字符串的重复创建。（逻辑上属于方法区，实际上在堆内存） 运行时常量池（Runtime Constant Pool）是虚拟机规范中是方法区的一部分，在加载类和结构到虚拟机后，就会创建对应的运行时常量池；而字符串常量池是这个过程中常量字符串的存放位置。所以从这个角度，字符串常量池属于虚拟机规范中的方法区，它是一个逻辑上的概念；而堆区，永久代以及元空间是实际的存放位置。 方法区（method area）只是 JVM 规范中定义的一个概念，用于存储类信息、常量池、静态变量、JIT编译后的代码等数据，并没有规定如何去实现它，不同的厂商有不同的实现。而永久代（PermGen）是 Hotspot虚拟机特有的概念， Java8 的时候又被元空间取代了，永久代和元空间都可以理解为方法区的落地实现。 ==永久代物理是堆的一部分，和新生代，老年代地址是连续的（受垃圾回收器管理），而元空间存在于本地内存（我们常说的堆外内存，不受垃圾回收器管理），这样就不受 JVM 限制了，也比较难发生OOM（都会有溢出异常）== JDK 7 之前，只有常量池的概念，都在方法区中。 JDK 7 的时候，字符串常量池从方法区中拿出来放到了堆中，运行时常量池还在方法区中（也就是永久代中）。 JDK 8 的时候，HotSpot 移除了永久代，取而代之的是元空间。字符串常量池还在堆中，而运行时常量池跑到了元空间。 java 内存模型（JMM）JMM是什么？有什么存在作用？✨java 内存模型跟 cpu 缓存模型类似，是基于 cpu 缓存模型来建立的，Java内存模型定义了共享内存系统中多线程程序读写操作行为的规范，是为了解决并发编程问题而存在的。 JMM对内存的划分？✨划分为主内存和工作内存两种 所有的变量都存储在主内存中。 每个线程都有一个私有的工作内存，本地内存中存储了该线程以读/写共享变量的拷贝副本。 线程对变量的所有操作都必须在本地内存中进行，而不能直接读写主内存。 不同的线程之间无法直接访问对方本地内存中的变量。 主内存和工作内存的交互操作 lock：锁定。作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock：解锁。作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read：读取。作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load：载入。作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use：使用。作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign：赋值。作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store：存储。作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write：写入。作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 内存交互基本操作的三个特性的理解？原子性（synchroinzed）、可见性（volatile）、以及有序性（happen-before）。 volatile和synchronized的区别 volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的 volatile仅能实现变量的修改可见性和有序性，不能保证原子性；而synchronized则可以保证变量的修改可见性（内存屏障）、有序性（内存屏障）和原子性 sychronized底层是通过monitorenter的指令来进行加锁的、通过monitorexit指令来释放锁的。 monitorenter指令其实还具有Load屏障的作用。 也就是通过monitorenter指令之后，synchronized内部的共享变量，每次读取数据的时候被强制从主内存读取最新的数据。 同样的道理monitorexit指令也具有Store屏障的作用，也就是让synchronized代码块内的共享变量，如果数据有变更的，强制刷新回主内存。 这样通过这种方式，数据修改之后立即刷新回主内存，其他线程进入synchronized代码块后，使用共享变量的时候强制读取主内存的数据，上一个线程对共享变量的变更操作，它就能立即看到了。 4条禁止重排序的内存屏障分别为： StoreStore屏障：禁止StoreStore屏障的前后Store写操作重排 LoadLoad屏障：禁止LoadLoad屏障的前后Load读操作进行重排 LoadStore屏障：禁止LoadStore屏障的前面Load读操作跟LoadStore屏障后面的Store写操作重排 StoreLoad屏障：禁止LoadStore屏障前面的Store写操作跟后面的Load/Store 读写操作重排 同样的道理啊，也是通过monitorenter、monitorexit指令嵌入上面的内存屏障；monitorenter、monitorexit这两条指令其实就相当于复合指令，既具有加锁、释放锁的功能，同时也具有内存屏障的功能。 volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化 happen-before✨happen-before原则是Java内存模型中定义的两项操作之间的偏序关系。 Happens-Before关系只是描述结果的可见性，并不表示指令执行的先后顺序，也就是说只要不对结果产生影响，仍然允许指令重排序。 为了具体说明，请看前面提到过的计算圆面积的示例代码： 123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面计算圆的面积的示例代码存在三个 happens- before 关系： A happens- before B； B happens- before C； A happens- before C； 由于 A happens- before B，happens- before 的定义会要求：A 操作执行的结果要对 B 可见，且 A 操作的执行顺序排在 B 操作之前。 但是从程序语义的角度来说，对 A 和 B 做重排序即不会改变程序的执行结果，也还能提高程序的执行性能（允许这种重排序减少了对编译器和处理器优化的束缚）。也就是说，上面这 3 个 happens- before 关系中，虽然 2 和 3 是必需要的，但 1 是不必要的。因此，JMM 把 happens- before 要求禁止的重排序分为了下面两类： 会改变程序执行结果的重排序。 不会改变程序执行结果的重排序。 JMM 对这两种不同性质的重排序，采取了不同的策略： 对于会改变程序执行结果的重排序，JMM 要求编译器和处理器必须禁止这种重排序。 对于不会改变程序执行结果的重排序，JMM 对编译器和处理器不作要求（JMM 允许这种重排序）。 下面是 JMM 的设计示意图： Java垃圾回收垃圾判断算法引用计数算法通过在对象头中分配一个空间来保存该对象被引用的次数（Reference Count）。 如果该对象被其它对象引用，则它的引用计数加 1，如果删除对该对象的引用，那么它的引用计数就减 1，当该对象的引用计数为 0 时，那么该对象就会被回收。 但是无法解决循环依赖的问题 可达性分析算法✨基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。 哪些对象可以作为 GC Roots 呢？ 虚拟机栈(栈帧中的局部变量表)中引用的对象 本地方法栈(Native 方法)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 垃圾收集算法标记-清除算法标记-清除（Mark-and-Sweep）算法分为“标记（Mark）”和“清除（Sweep）”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。 这种垃圾收集算法会带来两个明显的问题： 效率问题**：标记和清除两个过程效率都不高。 空间问题**：标记清除后会产生大量不连续的内存碎片。 复制算法为了解决标记-清除算法的效率和内存碎片问题，复制（Copying）收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 虽然改进了标记-清除算法，但依然存在下面这些问题： 可用内存变小：可用内存缩小为原来的一半。 不适合老年代：如果存活对象数量比较大，复制性能会变得很差。 标记-整理算法标记-整理（Mark-and-Compact）算法是根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 由于多了整理这一步，因此效率也不高，适合老年代这种垃圾回收频率不是很高的场景。 分代收集算法✨比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。 而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 大对象直接进入老年代大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 大对象直接进入老年代的行为是由虚拟机动态决定的，它与具体使用的垃圾回收器和相关参数有关。大对象直接进入老年代是一种优化策略，旨在避免将大对象放入新生代，从而减少新生代的垃圾回收频率和成本。 主要进行 gc 的区域针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种： 部分收集 (Partial GC)： 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集； 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集； 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。 整堆收集 (Full GC)：收集整个 Java 堆和方法区。 空间分配担保✨空间分配担保是为了确保在 Minor GC 之前老年代本身还有容纳新生代所有对象的剩余空间。 垃圾收集器✨Serial 收集器新生代采用标记-复制算法，老年代采用标记-整理算法。 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 不良用户体验但简单而高效（与其他收集器的单线程相比） ParNew 收集器ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 Parallel Scavenge 收集器Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 新生代采用标记-复制算法，老年代采用标记-整理算法。 CMS 收集器✨CMS（Concurrent Mark Sweep）收集器是第一个关注停顿时间的收集器。采用标记-清除算法。 之所以能够实现对停顿时间的控制，来源于对可达性分析计算的改进——三色标记法。 初始标记：标记直接与GCRoots 相连的对象，速度很快 并发标记： 从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。这里使用三色标记法。但是由于是并发标记，用户线程会跟GC线程交替执行，会出现标记变动的情况。可能会发生 漏标（该标的没有标而被回收）和 错标（不应该标的标了而没有被回收，变成浮动垃圾） 重新标记：修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录 并发清除：开启用户线程，同时 GC 线程开始对未标记的区域做清扫，这个阶段也是可以与用户线程同时并发的。由于在整个过程中耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一起工作，所以从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。 优点： 并发收集 低停顿 缺点： 吞吐量低: 低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，只能到下一次 GC 时才能进行回收，因此需要预留出一部分内存。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 使用的回收算法-“标记-清除”算法会导致大量空间碎片产生。往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 G1 收集器✨它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 其中初始标记、最终标记、筛选回收这三个步骤仍然需要“Stop The World”。 特点 分代收集：G1(Garbage First)物理内存不再分代，而是由一块一块的Region组成,但是逻辑分代仍然存在。G1 把堆划分成多个大小相等的独立区域(Region)，每个 region 根据需要扮演新生代或老年代，新生代和老年代不再物理隔离， G1 可以直接对新生代和老年代一起回收。 可预测的停顿： 每个Region 可以单独进行垃圾回收，通过对每个 Region 维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region，控制停顿时间。 避免全堆扫描：每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 空间整合：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上（两个 region 来看）来看是基于“标记-复制”算法实现的 步骤 初始标记：暂停所有用户线程，标记 gc roots 直接相连的对象，速度很快。 并发标记：采用三色标记法，以 gc roots直接相连的对象为起点，进行层序遍历，配合原始快照记录用户线程更改引用的关系的原始引用到Remembered Set Logs，并将删除的引用加入到 gc 堆栈中。 最终标记：最终标记阶段需要把 并发标记生成的Remembered Set Logs 的数据合并到 Remembered Set中，然后扫描 gc 堆栈，配合 Rset 执行标记。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 三色标记算法✨ 白色：没有检查（或者检查过了，确实没有引用指向它了） 灰色：自身被检查了，成员没被检查完（可以认为访问到了，但是正在被检查，就是图的遍历里那些在队列中的节点） 黑色：自身和成员都被检查完了 具体流程: 首先创建三个集合：白、灰、黑。 将所有对象放入白色集合中。 然后从根节点开始遍历所有对象（注意这里并不「递归遍历」），把遍历到的对象从白色集合放入灰色集合。 之后遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合 重复 上一步骤 直到灰色中无任何对象 通过write-barrier检测对象有变化，CMS 采用增量更新——在黑色连接白色后，将黑色变为灰色；G1 采用原始快照——将删除灰色到白色的连接记录到 remembered set log 中，将白色引用加入到 gc 堆，这样在后续的重新标记/最终标记的时候进行处理。 收集所有白色对象（垃圾） 第一种问题： 多标标记线程已经确定 b、d 是黑色，然后用户线程将 b、d 断开同时 d 没有其他对象被引用了，但是他已经是黑色不会在去检查，所以就不会被 gc 回收，也就是浮动垃圾。 会在 CMS 出现，因为 cms 采用的是增量更新，不会对引用的删除进行处理。 第二种问题：漏标，或者叫错杀 产生漏标问题的条件有两个： 黑色对象指向了白色对象 灰色对象指向白色对象的引用消失 解决方案： cms：写屏障 + 增量更新 如果新增黑色到白色的引用，那么jvm会通过写屏障，来把黑色置为灰色。但是需要重写扫描这个黑色对象的所有引用，比较费时 如果删除引用，jvm什么都不会做，这个导致了浮动垃圾 G1：写屏障 + SATB （Snapshot At The Beginning） 在开始标记的时候生成一个快照图标记存活对象 在一个引用断开后，要将此引用推到 GC 的堆栈里，保证白色对象(垃圾)还能被 GC 线程扫描到 配合 Rset，去扫描哪些 Region 引用到当前的白色对象，若没有引用到当前对象，则回收 为什么 ThreadLocalMap 的 key 是弱引用，而 value 是强引用？ 问题一：为什么 ThreadLocalMap 的 key 是弱引用？【假设 Entry 的 key 是对 ThreadLocal 对象的强引用】。如果在其他地方都没有对这个 ThreadLocla 对象的引用了，然后在使用 ThreadLocalMap 的过程中又没有正确地在用完后就调用 remove 方法，所以这个 ThreadLocal 对象和所关联的 value 对象就会跟随着线程一直存在，这样就会可能会造成内存泄漏问题。 特别是在使用线程池的时候，核心线程是会一直存在直到程序结束，如果这些线程中的 ThreadLocalMap 中的数据没有被及时清理，就会一直占用内存，而且在线程复用时可能会导致数据错乱的危险。 【Entry 的 key 是对 ThreadLocal 对象的弱引用】：弱引用就意味着，如果没有其他引用对象的强引用关系，那么这个仅被弱引用引用着的对象在下次 GC 时就会被回收掉，这样在一定程度上降低内存泄漏的风险。但同时也引入了新的问题，key 虽然被回收了，但是 value 对象还在，我们无法获取，也无法删除，这样也会存在内存泄漏的风险。虽然 ThreadLocalMap 中在进行 set 和 get 操作时会进行启发式清理和探测式清理，清理一部分 key 为 null 的 Entry 对象，但是这也只是一种后备选择方案，最重要的还是开发人员在编写代码时记得在使用完数据后及时调用 remove() 方法手动清理。 【内存泄漏就是，有些对象已经不再使用了，但是由于没有正确处理对象的引用关系，使得这个无用的对象还一直被 GC Root 直接或间接引用着，垃圾回收时就无法清理掉这些对象，如果这类对象存在很多，就会导致内存泄漏。简单地说就是有些无用对象占用着宝贵的内存空间，但又没办法清理掉它们】 问题二：为什么 ThreadLocalMap 的 value 是强引用？【假设Entry 的 value 是弱引用】：假设 key 所引用的 ThreadLocal 对象还被其他的引用对象强引用着，那么这个 ThreadLocal 对象就不会被 GC 回收，但如果 value 是弱引用且不被其他引用对象引用着，那 GC 的时候就被回收掉了，那线程通过 ThreadLocal 来获取 value 的时候就会获得 null，显然这不是我们希望的结果。因为对我们来说，value 才是我们想要保存的数据，ThreadLcoal 只是用来关联 value 的，如果 value 都没了，还要 ThreadLocal 干嘛呢？所以 value 不能是弱引用。 请问会不会出现：将登录用户信息放入ThreadLocal中，业务代码中还未使用，此时GC把弱引用的key删除了，导致后续业务中获取用户信息失败？？？ 每个线程都会有一个ThreadLocal的强引用在指向着堆中的ThreadLocal对象，知道线程终止key才会失效，一般来说是不会被清理掉的，弱引用是框架层面的思考，加了一层保险 调试排错 - Linux命令文本操作文本查询-grepgrep常用命令： 12345678910111213141516# 基本使用grep yoursearchkeyword f.txt #文件查找grep 'KeyWord otherKeyWord' f.txt cpf.txt #多文件查找, 含空格加引号grep 'KeyWord' /home/admin -r -n #目录下查找所有符合关键字的文件grep 'keyword' /home/admin -r -n -i # -i 忽略大小写grep 'KeyWord' /home/admin -r -n --include *.{vm,java} #指定文件后缀grep 'KeyWord' /home/admin -r -n --exclude *.{vm,java} #反匹配# cat + grepcat f.txt | grep -i keyword # 查找所有keyword且不分大小写 cat f.txt | grep -c 'KeyWord' # 统计Keyword次数# seq + grepseq 10 | grep 5 -A 3 #上匹配seq 10 | grep 5 -B 3 #下匹配seq 10 | grep 5 -C 3 #上下匹配，平时用这个就妥了 Grep的参数： 1234567-i, --ignore-case：忽略字符大小写;-n, --line-number：显示行号;-c, --count：统计匹配到的行数; print a count of matching lines;-B, --before-context=NUM：print NUM lines of leading context 后#行 -A, --after-context=NUM：print NUM lines of trailing context 前#行 -C, --context=NUM：print NUM lines of output context 前后各#行 文本分析 - awkawk基本命令： 12345678910111213# 基本使用awk '{print $4,$6}' f.txtawk '{print NR,$0}' f.txt cpf.txt awk '{print FNR,$0}' f.txt cpf.txtawk '{print FNR,FILENAME,$0}' f.txt cpf.txtawk '{print FILENAME,\"NR=\"NR,\"FNR=\"FNR,\"$\"NF\"=\"$NF}' f.txt cpf.txtecho 1:2:3:4 | awk -F: '{print $1,$2,$3,$4}'# 匹配awk '/ldb/ {print}' f.txt #匹配ldbawk '!/ldb/ {print}' f.txt #不匹配ldbawk '/ldb/ &amp;&amp; /LISTEN/ {print}' f.txt #匹配ldb和LISTENawk '$5 ~ /ldb/ {print}' f.txt #第五列匹配ldb 内建变量 12345`NR`: 已经读出的记录数，就是行号，从1开始，NR可以理解为Number of Record的缩写。`FNR`: 各文件分别计数的行号，FNR可以理解为File Number of Record。`NF`: 一条记录的字段的数目，NF可以理解为Number of Field。 文本处理 - sedsed常用： 1234567891011121314151617181920212223242526# 文本打印sed -n '3p' xxx.log #只打印第三行sed -n '$p' xxx.log #只打印最后一行sed -n '3,9p' xxx.log #只查看文件的第3行到第9行sed -n -e '3,9p' -e '=' xxx.log #打印3-9行，并显示行号sed -n '/root/p' xxx.log #显示包含root的行sed -n '/hhh/,/omc/p' xxx.log # 显示包含\"hhh\"的行到包含\"omc\"的行之间的行# 文本替换sed -i 's/root/world/g' xxx.log # 用world 替换xxx.log文件中的root; s==search 查找并替换, g==global 全部替换, -i: implace# 文本插入sed '1,4i hahaha' xxx.log # 在文件第一行和第四行的每行下面添加hahahased -e '1i happy' -e '$a new year' xxx.log #【界面显示】在文件第一行添加happy,文件结尾添加new yearsed -i -e '1i happy' -e '$a new year' xxx.log #【真实写入文件】在文件第一行添加happy,文件结尾添加new year# 文本删除sed '3,9d' xxx.log # 删除第3到第9行,只是不显示而已sed '/hhh/,/omc/d' xxx.log # 删除包含\"hhh\"的行到包含\"omc\"的行之间的行sed '/omc/,10d' xxx.log # 删除包含\"omc\"的行到第十行的内容# 与find结合find . -name \"*.txt\" |xargs sed -i 's/hhhh/\\hHHh/g'find . -name \"*.txt\" |xargs sed -i 's#hhhh#hHHh#g'find . -name \"*.txt\" -exec sed -i 's/hhhh/\\hHHh/g' {} \\;find . -name \"*.txt\" |xargs cat 文件操作文件监听 - tail最常用的tail -f filename 1234567# 基本使用tail -f xxx.log # 循环监听文件tail -300f xxx.log #倒数300行并追踪文件tail +20 xxx.log #从第 20 行至文件末尾显示文件内容# tailf使用tailf xxx.log #等同于tail -f -n 10 打印最后10行，然后追踪文件 tail的参数 1234-f 循环读取-c&lt;数目&gt; 显示的字节数-n&lt;行数&gt; 显示文件的尾部 n 行内容-s, --sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 文件查找 - find12345678910111213sudo -u admin find /home/admin /tmp /usr -name \\*.log(多个目录去找)find . -iname \\*.txt(大小写都匹配)find . -type d(当前目录下的所有子目录)find /usr -type l(当前目录下所有的符号链接)find /usr -type l -name \"z*\" -ls(符号链接的详细信息 eg:inode,目录)find /home/admin -size +250000k(超过250000k的文件，当然+改成-就是小于了)find /home/admin f -perm 777 -exec ls -l {} \\; (按照权限查询文件)find /home/admin -atime -1 1天内访问过的文件find /home/admin -ctime -1 1天内状态改变过的文件 find /home/admin -mtime -1 1天内修改过的文件find /home/admin -amin -1 1分钟内访问过的文件find /home/admin -cmin -1 1分钟内状态改变过的文件 find /home/admin -mmin -1 1分钟内修改过的文件 查看网络和进程查看所有网络接口的属性1ifconfig 查看防火墙设置1iptables -L 查看路由表1route -n netstat查看所有监听端口 12netstat -lntp-l, --listening display listening server sockets 查看所有已经建立的连接 12netstat -antp -a, --all, --listening display all sockets (default: connected) 查看网络统计信息进程 123netstat -s -s, --statistics display networking statistics (like SNMP) 查看所有进程123ps -ef | grep java-e：显示所有进程。-f：全格式显示。 toptop除了看一些基本信息之外，剩下的就是配合来查询vm的各种问题了 1top -H -p pid 查看磁盘和内存相关查看内存使用12free -h-h或--human-readable 以K，M，G为单位，提高信息的可读性。 查看各分区使用情况12df -h-h或--human-readable 以K，M，G为单位，提高信息的可读性。 查看指定目录的大小123du -sh-s或--summarize 仅显示指定目录或文件的总大小，而不显示其子目录的大小。-h或--human-readable 以K，M，G为单位，提高信息的可读性。 查看内存总量1grep MemTotal /proc/meminfo 查看空闲内存量1grep MemFree /proc/meminfo 查看所有分区1fdisk -l JVM 参数堆内存 1234567891011-Xms2G -Xmx5G-XX:NewSize=256m-XX:MaxNewSize=1024m-XX:PermSize=N #方法区 (永久代) 初始大小-XX:MaxPermSize=N #方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen-XX:MetaspaceSize=N #设置 Metaspace 的初始大小（是一个常见的误区，后面会解释）-XX:MaxMetaspaceSize=N #设置 Metaspace 的最大大小 垃圾回收器1234-XX:+UseSerialGC-XX:+UseParallelGC-XX:+UseParNewGC-XX:+UseG1GC GC日志可以通过在java命令种加入参数来指定对应的gc类型，打印gc日志信息并输出至文件等策略。 123456-XX:+PrintGC 输出GC日志-XX:+PrintGCDetails 输出GC的详细日志-XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式）-XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800）-XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息-Xloggc:../logs/gc.log 日志文件的输出路径 ​ 处理 OOM1234-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=./java_pid&lt;pid&gt;.hprof-XX:OnOutOfMemoryError=\"&lt; cmd args &gt;;&lt; cmd args &gt;\"-XX:+UseGCOverheadLimit 讲一下JVM调优过程？https://zhuanlan.zhihu.com/p/488615913 分析和定位当前系统的瓶颈 1）CPU指标 123456// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pid 2）JVM 内存指标 123456// 查看 Java 进程的配置信息，包括系统属性和JVM命令行标志jinfo pid// 输出 Java 进程当前的 gc 情况jstat -gc pid// 输出 Java 堆详细信息jmap -heap pid 制订优化方案 代码bug：升级修复bug。典型的有：死循环、使用无界队列。 不合理的JVM参数配置：优化 JVM 参数配置。典型的有：年轻代内存配置过小、堆内存配置过小、元空间配置过小。 对比优化前后的指标，统计优化效果 OOM分析、排查堆内存不足1java.lang.OutOfMemoryError: Java heap space 原因 存在大对象分配 存在内存泄漏 解决方法 检查是否有大对象分配，最有可能的是大数组分配 通过jmap 命令，把堆内存 dump 下来，用 mat 工具分析，检查是否存在内存泄露问题 如果没有找到明显的内存泄露，使用 -Xmx 加大最大堆内存 永久代/元空间溢出12java.lang.OutOfMemoryError: PermGen spacejava.lang.OutOfMemoryError: Metaspace 原因 在 java7 之前（字符串常量池还在永久代），频繁使用 String.intern方法 反射类加载、动态代理生成的类加载 解决方法 检查是否空间设置太小，使用 -XX:MetaspaceSize和-XX:MaxMetaspaceSize 加大堆内存 检查代码里面是否有大量反射操作 利用 jmap 命令，dump内存信息，利用 mat 检查是否存在大量代理类 CPU 飙升分析、排查定位问题 top命令查看最耗CPU的进程（输入top命令后键入P，进程按照CPU从高到底排序) top -Hp 进程 id 查看该进程中最耗CPU的线程 将线程号转为16进制printf '%x\\n' 线程号 查看线程在干什么jstack 进程号 | grep 线程号 jmap dump 下堆内存信息 问题分析 内存消耗大，导致 full gc 次数太多 通过：jstack命令可以看到这些线程主要是垃圾回收线程 解决：是否生成大量对象 是否代码有问题 通过：jstack 命令，可直接定位到代码行。是否存在无限循环递归问题 死锁 通过：jstack 命令，会打印出业务死锁的位置","link":"/2023/10/12/java/JVM/"},{"title":"Java中List,Integer[],int[]转换","text":"转 int[]思路 都是先转换成IntStream 然后调用 toArray() 1. List&lt;Integer&gt; 转 int[]流程: List&lt;Integer&gt; 调用stream() 转成 Stream&lt;Integer&gt; 1int[] arr = list.stream().mapToInt(Integer::valueOf).toArray(); 2. Integer[] 转 int[]流程: Integer[] 调用Arrays.stream() 转成 Stream&lt;Integer&gt; 1int[] arr = Arrays.stream(integers1).mapToInt(Integer::valueOf).toArray(); 总结: 区别: 转换成IntStream操作不同 相同: Stream&lt;Integer&gt; 调用mapToInt(Integer::valueOf) 转成IntStream ``IntStream 调用toArray() 转成int[]` int[]转1. int[] 转 List&lt;Integer&gt;流程: 使用Arrays.stream或IntStream.of(arr)将int[]转换成IntStream。 使用IntStream中的boxed()装箱。将IntStream转换成Stream&lt;Integer&gt;。 使用Stream的collect()，将Stream&lt;T&gt;转换成List&lt;T&gt;，因此正是List&lt;Integer&gt;。 1List&lt;Integer&gt; list = IntStream.of(arr).boxed().collect(Collectors.toList()); 2. int[] 转 Integer[]流程: 前两步相同 使用Stream的toArray，传入IntFunction&lt;A[]&gt; generator。 这样就可以返回Integer数组。 不然默认是Object[]。 1Integer[] a1 = Arrays.stream(arr).boxed().toArray(Integer[]::new); List&lt;Integer&gt; 与 Integer[]1. List&lt;Integer&gt; 转 Integer[] 调用toArray。传入参数T[] a。这种用法是目前推荐的。 List&lt;String&gt;转String[]也同理。 123Integer[] arr = list.toArray(new Integer[0]);Integer[] arr = list.toArray(new Integer[list.size()]); 使用集合转数组的方法，必须使用集合的toArray(T[] array)，传入的是类型完全一样的数组，大小就是list.size()。说明：使用toArray带参方法，入参分配的数组空间不够大时，toArray方法内部将重新分配内存空间，并返回新数组地址；如果数组元素个数大于实际所需，下标为[ list.size() ]的数组元素将被置为null，其它数组元素保持原值. 因此最好将方法入参数组大小定义与集合元素个数一致。 反例：直接使用toArray无参方法存在问题，此方法返回值只能是Object[]类，若强转其它类型数组将出现ClassCastException错误。 2. Integer[] 转 List&lt;Integer&gt;最简单的方式。String[]转List&lt;String&gt;也同理。 1List&lt;Integer&gt; list = Arrays.asList(arr); 使用工具类Arrays.asList()把数组转换成集合时，不能使用其修改集合相关的方法，它的add/remove/clear方法会抛出UnsupportedOperationException异常。 说明：asList的返回对象是一个Arrays内部类，并没有实现集合的修改方法。Arrays.asList体现的是适配器模式，只是转换接口，后台的数据仍是数组。 12String[] str = new String[] { \"you\", \"wu\" }; List list = Arrays.asList(str); 第一种情况：list.add(“yangguanbao”); 运行时异常。第二种情况：str[0] = “gujin”; 那么list.get(0)也会随之修改。","link":"/2024/05/03/java/Java%E4%B8%ADList-Integer-int-%E8%BD%AC%E6%8D%A2/"},{"title":"Java基础","text":"基础概念JVM vs JDK vs JRE✨Java 虚拟机（JVM）是运行 Java 字节码的虚拟机。 JRE（Java Runtime Environment） 是 Java 运行时环境。包括 Java 虚拟机（JVM）、Java 基础类库（Class Library）。 JDK（Java Development Kit），是Java开发工具包，是提供给开发者使用，能够开发、编译、调试 Java 程序的开发套件。包括 JRE 和 javac 等开发工具。 什么是字节码?采用字节码的好处是什么?在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。 Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。 为什么说 Java 语言“编译与解释并存”？✨编译性： Java源代码首先被编译成字节码，JIT 会把编译过的机器码保存起来,以备下次使用。 解释性： JVM中一个方法调用计数器，当累计计数大于一定值的时候，就使用JIT进行编译生成机器码文件。否则就是用解释器进行解释执行，然后字节码也是经过解释器进行解释运行的。 AOT 有什么优点？为什么不全部使用 AOT 呢？AOT(ahead of time Compilation)。和 JIT 不同的是，这种编译模式会在程序被执行前就将其编译成机器码，属于静态编译（C、 C++，Rust，Go 等语言就是静态编译） 优点：启动速度、内存占用、打包体积。 缺点：极限处理能力不如 JIT AOT 不能使用反射、动态代理、动态加载，常用框架和库（spring，CGLIB）都需要这些特性质。 基本数据类型八中基本数据类型✨ 总结： java八种基本数据类型的字节数:1字节(byte、boolean)、 2字节(short、char)、4字节(int、float)、8字节(long、double) 浮点数的默认类型为double（如果需要声明一个常量为float型，则必须要在末尾加上f或F） 整数的默认类型为int（声明Long型在末尾加上l或者L） 八种基本数据类型的包装类：除了char的是Character、int类型的是Integer，其他都是首字母大写 char类型是无符号的，不能为负，所以是0开始的 基本类型和包装类型的区别？✨ 基本类型 vs 包装类型 基本类型 包装类型 用途 常量、局部变量 方法参数、对象成员变量（可用于泛型） 存储方式 局部变量存放在 Java 虚拟机栈中的「局部变量表」中成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中（类的实例）；被static 修饰的存放在 Java 虚拟机的堆中（Class对象）。 ==属于对象类型，几乎所有对象实例都存在于堆中== 占用空间 小 大 默认值 有默认值 null 比较方式 == equals() 为什么说是几乎所有对象实例都存在于堆中呢？ ✨这是因为 HotSpot 虚拟机引入了 JIT 优化之后，会对对象进行逃逸分析，如果发现某一个对象并没有逃逸到方法外部，那么就可能通过标量替换来实现栈上分配，而避免堆上分配内存补充部分 基本数据类型是否都存放在栈中？ ✨基本数据类型的存储位置取决于它们的作用域和声明方式。如果它们是局部变量，那么它们会存放在栈中；如果它们是成员变量，那么它们会存放在堆中(是否被 static 修饰都是在堆中) 包装类型的缓存机制了解么？作用：提升性能 Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。 Character 缓存源码: 1234567891011121314public static Character valueOf(char c) { if (c &lt;= 127) { // must cache return CharacterCache.cache[(int)c]; } return new Character(c);}private static class CharacterCache { private CharacterCache(){} static final Character cache[] = new Character[127 + 1]; static { for (int i = 0; i &lt; cache.length; i++) cache[i] = new Character((char)i); }} 如果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。 两种浮点数类型的包装类 Float,Double 并没有实现缓存机制。 自动装箱与拆箱了解吗？原理是什么？什么是自动拆装箱？ 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 装箱其实就是调用了 包装类的valueOf()方法，拆箱其实就是调用了 xxxValue()方法。 因此， Integer i = 10 等价于 Integer i = Integer.valueOf(10) int n = i 等价于 int n = i.intValue(); 注意：如果频繁拆装箱的话，也会严重影响系统的性能。我们应该尽量避免不必要的拆装箱操作。（常量、局部变量用基本数据类型） 为什么浮点数运算的时候会有精度丢失的风险？与计算机保存浮点数机制有关。表示一个数字时，宽度有限，无线循环的小数存储在计算机时只能被截断，所以导致精度丢失。 浮点型从二进制的视角是怎么存储的？✨符号位+指数位+尾号位 这32个二进制位的内存编号从高到低 (从31到0), 共包含如下几个部分: sign: 符号位, 即图中蓝色的方块 biased exponent: 偏移后的指数位（偏移量 127 保证指数非负数）, 即图中绿色的方块 fraction: 尾数位, 即图中红色的方块 例1：求十进制数8.25在内存中的储存方式分析：8.25用二进制形式表示为1000.01，表示成二进制的指数形式为1.00001 * 2^3，用科学计数法则表示为1.00001 * 2E3。因为是正数，符号位即最高位为0；指数位为3 + 127（移位存储） = 130,二进制形式是10000010；尾数部分00001 = 0000100 00000000 00000000（23位）。所以8.25在内存中储存为：0 10000010 00001000000000000000000 例2：二进制1 10000010 00001000000000000000000是一个单精度浮点数，对应的十进制数是多少？分析：最高位为1，表示负数；指数位为10000010（2） = 130（10），130 – 127 = 3；尾数为00001000000000000000000，换成十进制为1 + 1/32。注意这里的1不要忘了加。 （IEEE 754）小数位如何计算出来的？如果我们现在想用浮点数表示 0.2，它的结果会是多少呢？ 0.2 转换为二进制数的过程为，不断乘以 2，直到不存在小数为止，在这个计算过程中，得到的整数部分从上到下排列就是二进制的结果。 1234560.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 10.6 * 2 = 1.2 -&gt; 10.2 * 2 = 0.4 -&gt; 0（发生循环）... 所以 0.2(D) = 0.00110…(B)。 如何解决浮点数运算的精度丢失问题？BigDecimal 可以实现对浮点数的运算，不会造成精度丢失。通常情况下，大部分需要浮点数精确运算结果的业务场景（比如涉及到钱的场景）都是通过 BigDecimal 来做的。 超过 long 整型的数据应该如何表示？基本数值类型都有一个表达范围，如果超过这个范围就会有数值溢出的风险。 在 Java 中，64 位 long 整型是最大的整数类型。 123long l = Long.MAX_VALUE;System.out.println(l + 1); // -9223372036854775808System.out.println(l + 1 == Long.MIN_VALUE); // true BigInteger 内部使用 int[] 数组来存储任意大小的整形数据。 相对于常规整数类型的运算来说，BigInteger 运算的效率会相对较低。 变量成员变量与局部变量的区别？✨ 语法形式：成员变量属于类，局部变量属于代码块、方法定义变量和方法的参数；成员变量可以被访问控制修饰符以及static 修饰、局部变量不可以。 存储方式：成员变量存在于堆内存，局部变量则存在于栈内存。 生存时间：从变量在内存中的生存时间上看，成员变量——对象同步，而局部变量——方法同步。 默认值：从变量是否有默认值来看，成员变量如果没有被赋初始值，则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而==局部变量则不会自动赋值==。 为什么成员变量有默认值？✨ 不考虑变量类型。若没有默认值，变量存储的是内存地址对应的任意随机值，即遗留值，可以会出现意外或信息泄露问题 对于编译器 javac 来说，局部变量没赋值很好判断，可以直接报错。而成员变量可能是运行时赋值，无法判断，误报“没默认值”影响用户体验，所以采用自动赋默认值 静态变量有什么作用？ 所有类的实例共用一份静态变量，节省内存（若被 private 修饰就不能用类名.变量名访问） 通常情况下，静态变量会被 final 关键字修饰成为常量。 字符型常量和字符串常量的区别?形式 : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。 含义 : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)。 占内存大小：字符常量只占 2 个字节; 字符串常量占若干个字节。 ⚠️ 注意 char 在 Java 中占两个字节。 方法静态方法为什么不能调用非静态成员?在类的非静态成员不存在的时候静态方法就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。 静态方法和实例方法有何不同？ 调用方式： 在外部调用静态方法时，可以使用 类名.方法名 的方式，也可以使用 对象.方法名 的方式，而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象 。 为了避免混淆建议静态方法使用前一种 访问类成员是否存在限制： 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），不允许访问实例成员（即实例成员变量和实例方法），而实例方法不存在这个限制。 重载和重写有什么区别？✨重载 发生在同一个类中（或者父类和子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。 编译时多态主要指方法的重载 重写 重写发生在运行期，是子类对父类的允许访问的方法的实现过程进行重新编写。 方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等，抛出的异常范围小于等于父类（更精细），访问修饰符范围大于等于父类。 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明。 构造方法无法被重写 在子类中，你不能重写父类的 static 方法。但是，子类可以定义一个同名的 static 方法。两个方法是独立的 总结 综上：重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变。 方法的重写要遵循“两同两小一大”（以下内容摘录自《疯狂 Java 讲义》，issue#892open in new window ）： “两同”即方法名相同、形参列表相同； “两小”指的是子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等； “一大”指的是子类方法的访问权限应比父类方法的访问权限更大或相等。 ⭐️ 关于 重写的返回值类型 这里需要额外多说明一下，上面的表述不太清晰准确：如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的。 运行时多态有三个条件： 继承 重写 向上转型 面相对象基础面向对象和面向过程的区别两者的主要区别在于解决问题的方式不同： 面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。 面向对象会先抽象出对象，然后用对象执行方法的方式解决问题。 另外，面向对象开发的程序一般更易维护、易复用、易扩展。 对象的相等和引用相等的区别 对象的相等一般比较的是内存中存放的内容是否相等。 引用相等一般比较的是他们指向的内存地址是否相等。 如果一个类没有声明构造方法，该程序能正确执行吗?构造方法是一种特殊的方法，主要作用是完成对象的初始化工作。 如果一个类没有声明构造方法，也可以执行！因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。如果我们自己添加了类的构造方法（无论是否有参），Java 就不会添加默认的无参数的构造方法了。 我们一直在不知不觉地使用构造方法，这也是为什么我们在创建对象的时候后面要加一个括号（因为要调用无参的构造方法）。如果我们重载了有参的构造方法，记得都要把无参的构造方法也写出来（无论是否用到），因为这可以帮助我们在创建对象的时候少踩坑。 构造方法有哪些特点？是否可被 override?构造方法特点如下： 名字与类名相同。 没有返回值，但不能用 void 声明构造函数。 生成类的对象时自动执行，无需调用。 构造方法不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。 面向对象三大特征✨封装封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。但是可以提供一些可以被外界访问的方法来操作属性。 继承继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承，可以快速地创建新的类，可以提高代码的重用，程序的可维护性，节省大量创建新类的时间 ，提高我们的开发效率。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。 多态✨多态，顾名思义，表示一个对象具有多种的状态，具体表现为父类的引用指向子类的实例。 多态的特点: 对象类型和引用类型之间具有继承（类）/实现（接口）的关系； 引用类型变量发出的方法调用的到底是哪个类中的方法，必须在程序运行期间才能确定； 多态不能调用“只在子类存在但在父类不存在”的方法； 如果子类重写了父类的方法，真正执行的是子类覆盖的方法，如果子类没有覆盖父类的方法，执行的是父类的方法。 多态的分类： 编译时多态主要指方法的重载 运行时多态指程序中定义的对象引用指向的具体类型在运行期间确定 运行时多态的三个条件： 继承 重写 向上转型 接口和抽象类有什么共同点和区别？✨共同点： 都不能被实例化。 都可以包含抽象方法。 都可以有默认实现的方法（Java 8 可以用 default 关键字在接口中定义默认方法）。 区别： 接口主要用于对类的行为进行约束，你实现了某个接口就具有了对应的行为。抽象类主要用于代码复用，强调的是所属关系。 一个类只能继承一个类，但是可以实现多个接口。 接口中的成员变量只能是 public static final 类型的，不能被修改且必须有初始值，而抽象类的成员变量默认 default，可在子类中被重新定义，也可被重新赋值。✨ 深拷贝和浅拷贝区别了解吗？什么是引用拷贝？✨关于深拷贝和浅拷贝区别，我这里先给结论： 浅拷贝：浅拷贝会在堆上创建一个新的对象（区别于引用拷贝的一点），不过，如果原对象内部的属性是引用类型的话，浅拷贝会直接复制内部对象的引用地址，也就是说拷贝对象和原对象共用同一个内部对象。 深拷贝：深拷贝会完全复制整个对象，包括这个对象所包含的内部对象。 如何实现深拷贝 实现 Cloneable 接口并重写 clone() 方法 这种方法要求对象及其所有引用类型字段都实现 Cloneable 接口，并且重写 clone() 方法。在 clone() 方法中，通过递归克隆引用类型字段来实现深拷贝。 使用序列化和反序列化 通过将对象序列化为字节流，再从字节流反序列化为对象来实现深拷贝。要求对象及其所有引用类型字段都实现 Serializable 接口。 手动递归复制 针对特定对象结构，手动递归复制对象及其引用类型字段。适用于对象结构复杂度不高的情况。 Object创建对象的方式✨ 使用new关键字 1Student s = new Student(); 使用Class类的newInstance方法（反射） 1Student s = Student.class.newInstance(); 使用Constructor类的newInstance方法 12Constructor&lt;Student&gt; constructor = Student.class.getConstructor();Student s = constructor.newInstance(); 这两种newInstance方法就是大家所说的反射。事实上Class的newInstance方法内部调用Constructor的newInstance方法。 使用clone方法 1Student s1 = s.clone(); 使用反序列化 12ObjectInputStream in = new ObjectInputStream(new FileInputStream(\"xxx.obj\"));Student s = in.readObject(); Object 类的常见方法有哪些？1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * native 方法，用于返回当前运行时对象的 Class 对象，使用了 final 关键字修饰，故不允许子类重写。 */public final native Class&lt;?&gt; getClass()/** * native 方法，用于返回对象的哈希码，主要使用在哈希表中，比如 JDK 中的HashMap。 */public native int hashCode()/** * 用于比较 2 个对象的内存地址是否相等，String 类对该方法进行了重写以用于比较字符串的值是否相等。 */public boolean equals(Object obj)/** * native 方法，用于创建并返回当前对象的一份拷贝。 */protected native Object clone() throws CloneNotSupportedException/** * 返回类的名字实例的哈希码的 16 进制的字符串。建议 Object 所有的子类都重写这个方法。 */public String toString()/** * native 方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。 */public final native void notify()/** * native 方法，并且不能重写。跟 notify 一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 */public final native void notifyAll()/** * native方法，并且不能重写。暂停线程的执行。注意：sleep 方法没有释放锁，而 wait 方法释放了锁 ，timeout 是等待时间。 */public final native void wait(long timeout) throws InterruptedException/** * 多了 nanos 参数，这个参数表示额外时间（以纳秒为单位，范围是 0-999999）。 所以超时的时间还需要加上 nanos 纳秒。。 */public final void wait(long timeout, int nanos) throws InterruptedException/** * 跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念 */public final void wait() throws InterruptedException/** * 实例被垃圾回收器回收的时候触发的操作 */protected void finalize() throws Throwable { } Java值传递和引用传递你是怎么理解的？因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。 例子 类A有个方法f，传递参数为Node node，在方法内node = new Node();，这里会影响到外面的node吗？如果在方法内修改node的参数，他会影响到外面的node吗 == 和 equals() 的区别== 对于基本类型和引用类型的作用效果是不同的： 对于基本数据类型来说，== 比较的是值。 对于引用数据类型来说，== 比较的是对象的内存地址。 equals() 不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。equals()方法存在于Object类中，而Object类是所有类的直接或间接父类，因此所有的类都有equals()方法。 Object 类 equals() 方法： 123public boolean equals(Object obj) { return (this == obj);} equals() 方法存在两种使用情况： 类没有重写 equals()方法：通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象，使用的默认是 Object类equals()方法。 类重写了 equals()方法：一般我们都重写 equals()方法来比较两个对象中的属性是否相等；若它们的属性相等，则返回 true(即，认为这两个对象相等)。 hashCode() 有什么用？hashCode() 的作用是获取哈希码（int 整数），也称为散列码。这个哈希码的作用是确定该对象在哈希表中的索引位置。 hashCode() 定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是：Object 的 hashCode() 方法是本地方法，也就是用 C 语言或 C++ 实现的。 ⚠️ 注意：该方法在 Oracle OpenJDK8 中默认是 “使用线程局部状态来实现 Marsaglia’s xor-shift 随机数生成”, 并不是 “地址” 或者 “地址转换而来”, 不同 JDK/VM 可能不同在 Oracle OpenJDK8 中有六种生成方式 (其中第五种是返回地址), 通过添加 VM 参数: -XX:hashCode=4 启用第五种。 为什么要有 hashCode？在 HashMap 和 HashSet 中都需要用到 hashCode，以 HashSet 为例： ​ 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashCode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashCode 值作比较，如果没有相符的 hashCode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashCode 值的对象，这时会调用 equals() 方法来检查 hashCode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 其实， hashCode() 和 equals()都是用于比较两个对象是否相等。 那为什么 JDK 还要同时提供这两个方法呢？ 这是因为在一些容器（比如 HashMap、HashSet）中，有了 hashCode() 之后，判断元素是否在对应容器中的效率会更高 那为什么不只提供 hashCode() 方法呢？ 这是因为两个对象的hashCode 值相等并不代表两个对象就相等。 那为什么两个对象有相同的 hashCode 值，它们也不一定是相等的？ 因为 hashCode() 所使用的哈希算法也许刚好会让多个对象传回相同的哈希值。越糟糕的哈希算法越容易碰撞，但这也与数据值域分布的特性有关（所谓哈希碰撞也就是指的是不同的对象得到相同的 hashCode )。 总结下来就是： 如果两个对象的hashCode 值相等，那这两个对象不一定相等（哈希碰撞）。 如果两个对象的hashCode 值相等并且equals()方法也返回 true，我们才认为这两个对象相等。 如果两个对象的hashCode 值不相等，我们就可以直接认为这两个对象不相等。 为什么重写 equals() 时必须重写 hashCode() 方法？✨因为两个相等的对象的 hashCode 值必须是相等。也就是说如果 equals 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。 如果重写 equals() 时没有重写 hashCode() 方法的话就可能会导致 equals 方法判断是相等的两个对象，hashCode 值却不相等。 思考：重写 equals() 时没有重写 hashCode() 方法的话，使用 HashMap 可能会出现什么问题。 两个相同的对象加到 HashMap 中，对应的 hashCode 不同，但是 HashMap 是先判断 hashCode 是否相同来判断是否有重复 key，最终会导致 HashMap 存在两个相同的对象同时作为 Key，这与 HashMap 的 key 不可以重复相悖。 总结： equals 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。 两个对象有相同的 hashCode 值，他们也不一定是相等的（哈希碰撞）。 概念对象 Object：表示的是某一类事物的抽象的名词和概念，是对一类事物的抽象表示 类 Class：对象在计算机中的表示，如定义一个“人”的类 实例 Instance：根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 Oop：ordinary object point 对象的创建过程✨ 申请空间，给成员变量赋默认值 调用 init 构造函数，给成员变量赋值 建立引用和对象的连接 单例模式✨饿汉式12345678910/** * 饿汉式单例模式 */public class singletonPattern01 { private static final singletonPattern01 SINGLE = new singletonPattern01(); private singletonPattern01(){}; public static singletonPattern01 getSingle(){ return SINGLE; }} 优点：这种写法比较简单，就是在类加载的时候就完成实例化。避免了线程同步问题。 缺点：在类加载的时候就完成实例化，没有达到Lazy Loading的效果。如果从未使用过这个实例，则会造成内存的浪费。 懒汉式起到了Lazy Loading的效果，但是只能在单线程下使用。 如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以 在多线程环境下不可使用这种方式。 结论：在实际开发中，不要使用这种方式. 1234567891011121314 * 懒汉式单例模式 */public class singletonPattern02 { private static singletonPattern02 SINGLE; private singletonPattern02(){ }; public static singletonPattern02 getInstance(){ if(SINGLE == null){ SINGLE = new singletonPattern02(); } return SINGLE; }} 双重检查锁（DCL double check lock）双重检查概念是多线程开发中常使用到的，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。 这样，实例化代码只会执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象，也避免的反复进行方法同步. 线程安全；延迟加载；效率较高 结论：在实际开发中，推荐使用这种单例设计模式 12345678910111213141516171819public class Dcl { // 防止指令重排导致返回半初始化对象 private static volatile Dcl SINGLE; // 将构造函数私有化防止外部直接构造 private Dcl(){}; public static Dcl getInstance(){ // 懒加载 if(SINGLE == null){ // 多线程下防止两个线程同时创建对象 synchronized (Dcl.class){ if (SINGLE == null){ SINGLE = new Dcl(); } } } return SINGLE; }} DCL要不要加 volatile 要。 假如 new singletonPattern02()时候发生指令重排序，先建立了连接，那么 SINGLE！=null ，多线程时候另一个线程就会直接 返回半初始化的对象。 所以说，这段代码要不要加volatile？必须加！加了volatile的这块内存，对于它的读写访问不可以重排序！ https://blog.csdn.net/zhaoyajie1011/article/details/106812327 静态内部类形式这种方式采用了类加载的机制来保证初始化实例时只有一个线程。 ==静态内部类方式在Singleton类被加载时并不会立即实例化，而是在需要实例化时，调用getSingleTon方法，才会加载Inner类，从而完成Singleton的实例化。== 类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。 优点：避免了线程不安全，利用静态内部类特点实现延迟加载，效率高 结论：推荐使用. 1234567891011public class singletonPattern03 { private singletonPattern03(){}; private static class singletonHolder{ private static singletonPattern03 SINGLE = new singletonPattern03(); } public static singletonPattern03 getInstance(){ return singletonHolder.SINGLE; }} java中的引用类型的对象存放在哪里根据上下文来确定。 123456789void func(){ Object obj = new Object();//这个obj在函数的栈里。}class Test{ private Object obj = new Object();//这个obj随对应的Test对象分配在堆里} 对于方法中的局部变量的引用时存放在java运行时数据区的栈中 对于实例变量则是存放在java运行时数据区的堆中。 Class 实例究竟在 method area 还是在 heap✨「hotspot」 使用了 「OOP-KLASS」 模型来表示 java 对象 main方法中：Object o = new Object(); 当JVM加载一个类时，它会创建一个名为 instanceKlass 的数据结构，用于表示该类的元数据，包括常量池、字段、方法等信息。这些元数据存储在方法区中。instanceKlass 是JVM内部的一种数据结构，表示从字节码（即 .class 文件）加载到方法区的类信息，包括虚拟方法表等，实际上是一个C++对象。 当通过 new 操作创建一个对象时，JVM会生成一个 instanceOopDesc 数据结构用于表示该对象，并将其存储在堆内存中。该对象的引用则存放在栈区。instanceOopDesc 主要用于表示对象的实例信息，尽管它看起来像一个指针，但实际上里面存储的是对象的具体数据；instanceOopDesc 对应于Java中的对象实例。 在HotSpot实现中，并不会将 instanceKlass 直接暴露给Java层，而是会创建一个相应的 instanceOopDesc 来表示 java.lang.Class 对象，并称这个 instanceOopDesc 为 instanceKlass 的“Java镜像”。 对象在内存中的存储布局✨instanceOopDesc，只包含数据信息，它包含三部分： Mark Word，主要存储对象运行时记录信息，如hashcode, GC分代年龄，锁状态标志，线程ID，时间戳等; （64 位 os 就是 64 位即 8 字节） 元数据指针，即指向方法区的instanceKlass实例（压缩前 8 字节，压缩后 4 字节） 如果是数组对象，还多了一个数组长度 实例数据; （成员变量） 对齐填充（满足 8 的倍数） 为什么要对齐数据？字段内存对齐的其中一个原因，是让字段只出现在同一CPU的缓存行中。最终目的是为了计算机高效寻址。 额外扩充：Java对象的内存布局 - JaJian - 博客园 (cnblogs.com) 对象如何定位✨直接使用直接指针访问，Java堆中对象的内存布局就必须考虑如何防止访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。 间接（句柄访问）使用句柄访问，Java堆中将可能会划分出一块内存用来作为句柄池，reference中寸的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息。 两种访问方式的优势句柄访问： 最大的好处是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要被修改。指针访问：最大的好处时速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本。 对象怎么分配✨ 判断是否分配到栈上。进行逃逸分析，没有发生逃逸的对象优先尝试在栈上分配。 是否大。太大就直接放到「老年代」 TLAB（Thread local allocation buffer）。线程本地分配缓存多线程时，给各线程分配特定的空间 为什么 hotspot 不用 c++对象来代表 java 对象c++对象有虚函数表，java 对象的虚函数表在 Class 对象中 StringString、StringBuffer、StringBuilder 的区别？可变性 String 是不可变的（后面会详细分析原因）。 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串，不过没有使用 final 和 private 关键字修饰，最关键的是这个 AbstractStringBuilder 类还提供了很多修改字符串的方法比如 append 方法。 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer String 为什么是不可变的?✨final前瞻：被 final 关键字修饰的类不能被继承，修饰的方法不能被重写，修饰的变量是基本数据类型则值不能改变，修饰的变量是引用类型则不能再指向其他对象。 因此，final 关键字修饰的数组保存字符串并不是 String不可变的根本原因，因为这个数组保存的字符串是可变的（final 修饰引用类型变量的情况）。 String 真正不可变有下面几点原因： 保存字符串的数组被 final 修饰且为私有的，并且String 类没有提供/暴露修改这个字符串的方法。 String 类被 final 修饰导致其不能被继承，进而避免了子类破坏 String 不可变。 不可变的好处 这个最简单地原因，就是为了安全。 再看下面这个HashSet用StringBuilder做元素的场景，问题就更严重了，而且更隐蔽。 123456789101112131415class Test{ public static void main(String[] args){ HashSet&lt;StringBuilder&gt; hs=new HashSet&lt;StringBuilder&gt;(); StringBuilder sb1=new StringBuilder(\"aaa\"); StringBuilder sb2=new StringBuilder(\"aaabbb\"); hs.add(sb1); hs.add(sb2); //这时候HashSet里是{\"aaa\",\"aaabbb\"} StringBuilder sb3=sb1; sb3.append(\"bbb\"); //这时候HashSet里是{\"aaabbb\",\"aaabbb\"} System.out.println(hs); }}//Output://[aaabbb, aaabbb] StringBuilder型变量sb1和sb2分别指向了堆内的字面量”aaa”和”aaabbb”。把他们都插入一个HashSet。到这一步没问题。但如果后面我把变量sb3也指向sb1的地址，再改变sb3的值，因为StringBuilder没有不可变性的保护，sb3直接在原先”aaa”的地址上改。导致sb1的值也变了。这时候，HashSet上就出现了两个相等的键值”aaabbb”。破坏了HashSet键值的唯一性。所以千万不要用可变类型做HashMap和HashSet键值。 字符串拼接用“+” 还是 StringBuilder?Java 语言本身并不支持运算符重载，“+”和“+=”是专门为 String 类重载过的运算符，也是 Java 中仅有的两个重载过的运算符。 字符串对象通过“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 不过，在循环内使用“+”进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用，会导致创建过多的 StringBuilder 对象。 String s1 = new String(“abc”);这句话创建了几个字符串对象？✨会创建 1 或 2 个字符串对象。 1、如果字符串常量池中不存在字符串对象“abc”的引用，那么它会在堆上创建两个字符串对象，其中一个字符串对象的引用会被保存在字符串常量池中。 示例代码（JDK 1.8）： 1String s1 = new String(\"abc\"); 对应的字节码： ldc 命令用于判断字符串常量池中是否保存了对应的字符串对象的引用，如果保存了的话直接返回，如果没有保存的话，会在堆中创建对应的字符串对象并将该字符串对象的引用保存到字符串常量池中。 2、如果字符串常量池中已存在字符串对象“abc”的引用，则只会在堆中创建 1 个字符串对象“abc”。 示例代码（JDK 1.8）： 1234&gt;// 字符串常量池中已存在字符串对象“abc”的引用String s1 = \"abc\";// 下面这段代码只会在堆中创建 1 个字符串对象“abc”String s2 = new String(\"abc\"); 对应的字节码： 这里就不对上面的字节码进行详细注释了，7 这个位置的 ldc 命令不会在堆中创建新的字符串对象“abc”，这是因为 0 这个位置已经执行了一次 ldc 命令，已经在堆中创建过一次字符串对象“abc”了。7 这个位置执行 ldc 命令会直接返回字符串常量池中字符串对象“abc”对应的引用。 String#intern 方法有什么作用?✨String.intern() 是一个 native（本地）方法，其作用是将指定的字符串对象的引用保存在字符串常量池中，可以简单分为两种情况： 如果字符串常量池中保存了对应的字符串对象的引用，就直接返回该引用。 如果字符串常量池中没有保存了对应的字符串对象的引用，那就在常量池中创建一个指向该字符串对象的引用并返回。 应用场景使用方法 1234567public class Person{String name; public void setName(String paramString) { String str = paramString.intern(); }} 这里是一个能展现出inern()实际作用的场景,首先假设我从数据库里读了一个人的信息出来,然后把这个人的名字赋值给这个Person对象.那么,从数据库读数据,毫无疑问得创建一个字符串对象出来,假定读了10个人的数据,其中三个都叫小明,那么在不使用intern()的情况下,对字符串对象的引用情况如图所示 在使用intern的情况下,对字符串对象的引用情况如图所示 很显然,剩下的那两个小明字符串对象,就都可以回收了,大大节省空间. String 类型的变量和常量做“+”运算时发生了什么？ 先来看字符串不加 final 关键字拼接的情况（JDK1.8）： 12345678String str1 = \"str\"; // 字符串常量，存储在字符串常量池中。String str2 = \"ing\"; // 字符串常量，存储在字符串常量池中。String str3 = \"str\" + \"ing\"; // 字符串常量的连接，在编译时优化为\"string\"，存储在常量池中。String str4 = str1 + str2; // 动态连接，运行时执行两个字符串的连接，不在常量池中，而是分配新的对象。String str5 = \"string\"; // 字符串常量，\"string\"直接在字符串常量池中创建。System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 在编译过程中，Javac 编译器（下文中统称为编译器）会进行一个叫做 常量折叠(Constant Folding) 的代码优化。 常量折叠会把常量表达式的值求出来作为常量嵌在最终生成的代码中，这是 Javac 编译器会对源代码做的极少量优化措施之一(代码优化几乎都在即时编译器中进行)。 对于 String str3 = \"str\" + \"ing\"; 编译器会给你优化成 String str3 = \"string\"; 。 并不是所有的常量都会进行折叠，只有编译器在程序编译期就可以确定值的常量才可以： 基本数据类型( byte、boolean、short、char、int、float、long、double)以及字符串常量。 final 修饰的基本数据类型和字符串变量 字符串通过 “+”拼接得到的字符串、基本数据类型之间算数运算（加减乘除）、基本数据类型的位运算（&lt;&lt;、&gt;&gt;、&gt;&gt;&gt; ） 引用的值在程序编译期是无法确定的，编译器无法对其进行优化。 对象引用和“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 我们在平时写代码的时候，尽量避免多个字符串对象拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 不过，字符串使用 final 关键字声明之后，可以让编译器当做常量来处理。 示例代码： 123456final String str1 = \"str\";final String str2 = \"ing\";// 下面两个表达式其实是等价的String c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 常量池中的对象System.out.println(c == d);// true 被 final 关键字修饰之后的 String 会被编译器当做常量来处理，编译器在程序编译期就可以确定它的值，其效果就相当于访问常量。 如果 ，编译器在运行时才能知道其确切值的话，就无法对其优化。 示例代码（str2 在运行时才能确定其值）： 12345678final String str1 = \"str\";final String str2 = getStr();String c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 在堆上创建的新的对象System.out.println(c == d);// falsepublic static String getStr() { return \"ing\";} 异常Exception 和 Error 有什么区别？在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类: Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理)。 Error：程序无法处理的错误 ，不建议通过catch捕获 。例如 Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止 Checked Exception 和 Unchecked Exception 有什么区别？Checked Exception 即 受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 catch或者throws关键字处理的话，就没办法通过编译。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有：IO 相关的异常、ClassNotFoundException、SQLException…。 Unchecked Exception 即 不受检查异常 ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。 RuntimeException 及其子类都统称为非受检查异常，常见的有（建议记下来，日常开发中会经常用到）： NullPointerException(空指针错误) IllegalArgumentException(参数错误比如方法入参类型错误) NumberFormatException（字符串转换为数字格式错误，IllegalArgumentException的子类） ArrayIndexOutOfBoundsException（数组越界错误） ClassCastException（类型转换错误） ArithmeticException（算术错误） SecurityException （安全错误比如权限不够） UnsupportedOperationException(不支持的操作错误比如重复创建同一用户) Throwable 类常用方法有哪些？ String getMessage(): 返回异常发生时的简要描述 String toString(): 返回异常发生时的详细信息 String getLocalizedMessage(): 返回异常对象的本地化信息。使用 Throwable 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 getMessage()返回的结果相同 void printStackTrace(): 在控制台上打印 Throwable 对象封装的异常信息 try-catch-finally 如何使用？ try块：用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块：用于处理 try 捕获到的异常。 finally 块：无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。 注意：不要在 finally 语句块中使用 return! 当 try 语句和 finally 语句中都有 return 语句时，try 语句块中的 return 语句会被忽略。这是因为 try 语句中的 return 返回值会先被暂存在一个本地变量中，当执行到 finally 语句中的 return 之后，这个本地变量的值就变为了 finally 语句中的 return 返回值。 123456789101112131415public static void main(String[] args) { System.out.println(f(2));}public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } }}输出0 finally 中的代码一定会执行吗？不一定的！在某些情况下，finally 中的代码不会被执行。 finally 之前虚拟机被终止运行的话，finally 中的代码就不会被执行。 程序所在的线程死亡。 关闭 CPU。 如何使用 try-with-resources 代替try-catch-finally？适用范围（资源的定义）： 任何实现 java.lang.AutoCloseable或者 java.io.Closeable 的对象 关闭资源和 finally 块的执行顺序： 在 try-with-resources 语句中，任何 catch 或 finally 块在声明的资源关闭后运行 Java 中类似于InputStream、OutputStream、Scanner、PrintWriter等的资源都需要我们调用close()方法来手动关闭，一般情况下我们都是通过try-catch-finally语句来实现这个需求，如下： 1234567891011121314//读取文本文件的内容Scanner scanner = null;try { scanner = new Scanner(new File(\"D://read.txt\")); while (scanner.hasNext()) { System.out.println(scanner.nextLine()); }} catch (FileNotFoundException e) { e.printStackTrace();} finally { if (scanner != null) { scanner.close(); }} 使用 Java 7 之后的 try-with-resources 语句改造上面的代码: 1234567try (Scanner scanner = new Scanner(new File(\"test.txt\"))) { while (scanner.hasNext()) { System.out.println(scanner.nextLine()); }} catch (FileNotFoundException fnfe) { fnfe.printStackTrace();}a 当然多个资源需要关闭的时候，使用 try-with-resources 实现起来也非常简单，如果你还是用try-catch-finally可能会带来很多问题。 通过使用分号分隔，可以在try-with-resources块中声明多个资源。 12345678910try (BufferedInputStream bin = new BufferedInputStream(new FileInputStream(new File(\"test.txt\"))); BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(new File(\"out.txt\")))) { int b; while ((b = bin.read()) != -1) { bout.write(b); }}catch (IOException e) { e.printStackTrace();} 异常使用有哪些需要注意的地方？ 不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。 抛出的异常信息一定要有意义。 建议抛出更加具体的异常比如字符串转换为数字格式错误的时候应该抛出NumberFormatException而不是其父类IllegalArgumentException。 使用日志打印异常之后就不要再抛出异常了（两者不要同时存在一段代码逻辑中）。 …… 泛型什么是泛型？泛型：是一种把明确类型的工作推迟到创建对象或者调用方法的时候才去明确的特殊的类型。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，而这种参数类型可以用在类、方法和接口中，分别被称为「泛型类」、「泛型方法」、「泛型接口」。注意:一般在创建对象时，将未知的类型确定具体的类型。当没有指定泛型时，默认类型为Object类型。 有什么作用？ 避免了类型强转的麻烦。 它提供了编译期的类型安全，确保在泛型类型（通常为泛型集合）上只能使用正确类型的对象，避免了在运行时出现ClassCastException。 泛型的使用方式有哪几种？ 泛型类： 泛型类型用于类的定义中，被称为泛型类。最典型的就是各种集合框架容器类，如：List、Set、Map。 123456789101112131415161718192021222324泛型类的定义格式：修饰符 class 类名&lt;代表泛型的变量&gt; { }怕你不清楚怎么使用，这里我还是做了一个简单的泛型类：/** * @param &lt;T&gt; 这里解释下&lt;T&gt;中的T: * 此处的T可以随便写为任意标识，常见的有T、E等形式的参数表示泛型 * 泛型在定义的时候不具体，使用的时候才变得具体。 * 在使用的时候确定泛型的具体数据类型。即在创建对象的时候确定泛型。 */public class Generic&lt;T&gt;{ private T key; public Generic(T key) { this.key = key; } public T getKey(){ return key; }}泛型在定义的时候不具体，使用的时候才变得具体。在使用的时候确定泛型的具体数据类型。即：在创建对象的时候确定泛型。 泛型接口： 12345定义格式修饰符 interface接口名&lt;代表泛型的变量&gt; { }public interface Generator&lt;T&gt; { public T method();} 实现泛型接口，不指定类型： 123456class GeneratorImpl&lt;T&gt; implements Generator&lt;T&gt;{ @Override public T method() { return null; }} 实现泛型接口，指定类型： 123456class GeneratorImpl&lt;T&gt; implements Generator&lt;String&gt;{ @Override public String method() { return \"hello\"; }} 泛型方法： 123456789101112131415161718192021222324定义格式：修饰符 &lt;代表泛型的变量&gt; 返回值类型 方法名(参数){ }例如：/** * * @param t 传入泛型的参数 * @param &lt;T&gt; 泛型的类型 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间&lt;T&gt;非常重要，可以理解为声明此方法为泛型方法。 * 2）只有声明了&lt;T&gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 * 3）&lt;T&gt;表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。 * 4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E等形式的参数常用于表示泛型。 */ public &lt;T&gt; T genercMethod(T t){ System.out.println(t.getClass()); System.out.println(t); return t;}fanxing fanxing = new fanxing();String string = fanxing.genercMethod(\"string\");Integer integer = fanxing.genercMethod(123); 泛型通配符 当使用泛型类或者接口时，传递的数据中，泛型类型不确定，可以通过通配符&lt;?&gt;表示。但是一旦使用泛型的通配符后，只能使用Object类中的共性方法，集合中元素自身方法无法使用。 12345678static void tongpeifu(List&lt;?&gt; list){ //只能用 Object 修饰，所以也只能用 Object 自带的方法 Object o = list.get(0);}public static void main(String[] args) { tongpeifu(Arrays.asList(\"1111\"));} 通配符基本使用 泛型的通配符:不知道使用什么类型来接收的时候,此时可以使用?,?表示未知通配符。 此时只能接受数据,不能往该集合中存储数据。 12345678// ？代表可以接收任意类型// 泛型不存在继承、多态关系,泛型左右两边要一样。jdk1.7后右边的泛型可以省略//ArrayList&lt;Object&gt; list = new ArrayList&lt;String&gt;();这种是错误的//泛型通配符?:左边写&lt;?&gt; 右边的泛型可以是任意类型ArrayList&lt;?&gt; list = new ArrayList&lt;String&gt;();//编译错误 不可以存储数据list.add(\"1111\"); 泛型通配符?主要应用在参数传递方面 12345678public static void main(String[] args) { ArrayList&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); test(list1); ArrayList&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); test(list2);}public static void test(ArrayList&lt;?&gt; coll){} 通配符高级使用 之前设置泛型的时候，实际上是可以任意设置的，只要是类就可以设置。但是在JAVA的泛型中可以指定一个泛型的上限和下限。 泛型的上限： 格式： 类型名称 &lt;? extends 类 &gt; 对象名称 意义： 只能接收该类型及其子类 泛型的下限： 格式： 类型名称 &lt;? super 类 &gt; 对象名称 意义： 只能接收该类型及其父类型 123456789101112131415161718192021public static void main(String[] args) { Collection&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); Collection&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); Collection&lt;Number&gt; list3 = new ArrayList&lt;Number&gt;(); Collection&lt;Object&gt; list4 = new ArrayList&lt;Object&gt;(); getElement1(list1); getElement1(list2);//报错 getElement1(list3); getElement1(list4);//报错 getElement2(list1);//报错 getElement2(list2);//报错 getElement2(list3); getElement2(list4); }// 泛型的上限：此时的泛型?，必须是Number类型或者Number类型的子类public static void getElement1(Collection&lt;? extends Number&gt; coll){}// 泛型的下限：此时的泛型?，必须是Number类型或者Number类型的父类public static void getElement2(Collection&lt;? super Number&gt; coll){} 反射什么是反射反射在程序运行期间动态获取类和操纵类的一种技术。通过反射机制，可以在运行时动态地创建对象、调用方法、访问和修改属性，以及获取类的信息。 反射有什么作用让 java 代码更灵活 运行时获得类的属于、方法 运行时构造任意类对象 运行时调用任意对象方法 反射的应用场景了解么？ jdk 动态代理 注解 jdbc 链接 反射机制的优缺点优点：可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利 缺点：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点（编译器无法优化，无法使用 JIT），不过，对于框架来说实际是影响不大的。 反射底层原理java 在编译后会生成一个 class 文件，执行反射的时候会将class 文件载入到方法区并在堆中构建一个 class 对象作为方法区Klass 的访问入口，反射通过Class 对象找到其类中的方法和属性等 流程： 1、当我们编写完一个Java项目之后，每个java文件都会被编译成一个.class文件。 2、这些class文件在程序运行时会被ClassLoader加载到JVM中，当一个类被加载以后，JVM就会在内存中自动产生一个Class对象。 3、通过Class对象获取Field/Method/Construcor 要想通过反射获取一个类的信息，首先要获取该类对应的Class类实例（Class 对象）。Class类没有公共的构造方法，Class类对象是在二进制字节流被JVM加载时，通过调用类加载器的defineClass()方法来构建的。 获取 Class 对象的四种方式12345678910111213//1. 知道具体类的情况下可以使用：Class alunbarClass = TargetObject.class;//2. 通过 Class.forName()传入类的全路径获取：Class alunbarClass1 = Class.forName(\"cn.javaguide.TargetObject\");//3. 通过对象实例instance.getClass()获取：TargetObject o = new TargetObject();Class alunbarClass2 = o.getClass();//4. 通过类加载器xxxClassLoader.loadClass()传入类路径获取:ClassLoader.getSystemClassLoader().loadClass(\"cn.javaguide.TargetObject\");//通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一系列步骤，静态代码块和静态对象不会得到执行 反射的一些基本操作 创建一个我们要使用反射操作的类 TargetObject。 123456789101112131415public class TargetObject { private String value; public TargetObject() { value = \"JavaGuide\"; } public void publicMethod(String s) { System.out.println(\"I love \" + s); } private void privateMethod() { System.out.println(\"value is \" + value); }} 使用反射操作这个类的方法以及参数 1234567891011121314151617181920212223242526272829import java.lang.reflect.Field;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class test_targetObject { public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchMethodException, InvocationTargetException, NoSuchFieldException { Class&lt;?&gt; targetObject = Class.forName(\"TargetObject\"); Object o = targetObject.newInstance(); for (Method declaredMethod : targetObject.getDeclaredMethods()) { System.out.println(declaredMethod); } Method publicMethod = targetObject.getDeclaredMethod(\"publicMethod\", String.class); publicMethod.invoke(o, \"fml\"); Field value = targetObject.getDeclaredField(\"value\"); //为了调用private方法我们取消安全检查 value.setAccessible(true); value.set(o, \"fmlzuibang\"); /** * 调用 private 方法 */ Method privateMethod = targetObject.getDeclaredMethod(\"privateMethod\"); privateMethod.setAccessible(true); privateMethod.invoke(o); }} 注解何谓注解Annotation （注解） 是 Java5 开始引入的新特性，可以看作是一种特殊的注释，主要用于修饰类、方法或者变量，提供某些信息供程序在编译或者运行时使用。 注解本质是一个继承了Annotation 的特殊接口： 123456789@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override {}public interface Override extends Annotation{} JDK 提供了很多内置的注解（比如 @Override、@Deprecated），同时，我们还可以自定义注解。 注解的解析方法有哪几种？注解只有被解析之后才会生效，常见的解析方法有两种： 编译期直接扫描：编译器在编译 Java 代码的时候扫描对应的注解并处理，比如某个方法使用@Override 注解，编译器在编译的时候就会检测当前的方法是否重写了父类对应的方法。 运行期通过反射处理：像框架中自带的注解(比如 Spring 框架的 @Value、@Component)都是通过反射来进行处理的。 SPI何谓 SPI?SPI 即 Service Provider Interface ，字面意思就是：“服务提供者的接口”。一种服务发现机制，允许在运行时动态地加载实现特定接口的类，而不需要在代码中显式地指定该类，从而实现解耦和灵活性。 SPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。 SPI 和 API 有什么区别？说到 SPI 就不得不说一下 API 了，从广义上来说它们都属于接口，而且很容易混淆。下面先用一张图说明一下： 一般模块之间都是通过接口进行通讯，那我们在服务调用方和服务实现方（也称服务提供者）之间引入一个“接口”。 API是面向调用方/使用者的接口，提供具体的方法以供调用；而SPI则是面向实现方/服务提供者的接口，允许实现和扩展。 Java SPI 的优缺点？优点： 松耦合度：在运行时动态加载实现类，而无需在编译时将实现类硬编码到代码中 扩展性：可以为同一个接口定义多个实现类。这使得应用程序更容易扩展和适应变化。 缺点： 安全性不足：SPI提供者必须将其实现类名称写入到配置文件中，因此如果未正确配置，则可能存在安全风险。 性能损失：每次查找服务提供者都需要重新读取配置文件，这可能会增加启动时间和内存开销。 上面对Java SPI的缺点说了一下，我们来说一下：Spring的SPI机制相对于Java原生的SPI机制进行了改造和扩展，主要体现在以下几个方面： 支持多个实现类：Spring的SPI机制允许为同一个接口定义多个实现类，而Java原生的SPI机制只支持单个实现类。这使得在应用程序中使用Spring的SPI机制更加灵活和可扩展。 支持自动装配：Spring的SPI机制支持自动装配，可以通过将实现类标记为Spring组件（例如@Component），从而实现自动装配和依赖注入。这在一定程度上简化了应用程序中服务提供者的配置和管理。 支持动态替换：Spring的SPI机制支持动态替换服务提供者，可以通过修改配置文件或者其他方式来切换服务提供者。而Java原生的SPI机制只能在启动时加载一次服务提供者，并且无法在运行时动态替换。 提供了更多扩展点：Spring的SPI机制提供了很多扩展点，例如BeanPostProcessor、BeanFactoryPostProcessor等，可以在服务提供者初始化和创建过程中进行自定义操作。 应用场景Java SPI机制是一种服务提供者发现的机制，适用于需要在多个实现中选择一个进行使用的场景。 常见的应用场景包括： 应用名称 具体应用场景 数据库驱动程序加载 JDBC为了实现可插拔的数据库驱动，在Java.sql.Driver接口中定义了一组标准的API规范，而具体的数据库厂商则需要实现这个接口，以提供自己的数据库驱动程序。在Java中，JDBC驱动程序的加载就是通过SPI机制实现的。 日志框架的实现 流行的开源日志框架，如Log4j、SLF4J和Logback等，都采用了SPI机制。用户可以根据自己的需求选择合适的日志实现，而不需要修改代码。 Spring框架 Spring框架中的Bean加载机制就使用了SPI思想，通过读取classpath下的META-INF/spring.factories文件来加载各种自定义的Bean。 Dubbo框架 Dubbo框架也使用了SPI思想，通过接口注解@SPI声明扩展点接口，并在classpath下的META-INF/dubbo目录中提供实现类的配置文件，来实现扩展点的动态加载。 参考：https://blog.csdn.net/qq_52423918/article/details/130968307 序列化和反序列化什么是序列化?什么是反序列化? 序列化：将数据结构或对象转换成二进制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过 序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中。 序列化协议对应于 TCP/IP 4 层模型的哪一层？ 应用层（表示层） 如果有些字段不想进行序列化怎么办？使用 transient 关键字修饰 关于 transient 还有几点注意： transient 只能修饰变量，不能修饰类和方法。 transient 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 int 类型，那么反序列后结果就是 0。 static 变量因为不属于任何对象(Object)，所以无论有没有 transient 关键字修饰，均不会被序列化。 JDK 自带的序列化方式JDK 自带的序列化，只需实现 java.io.Serializable接口即可。 serialVersionUID 有什么作用？序列化号 serialVersionUID 属于版本控制的作用。反序列化时，会检查 serialVersionUID 是否和当前类的 serialVersionUID 一致。如果 serialVersionUID 不一致则会抛出 InvalidClassException 异常。强烈推荐每个序列化类都手动指定其 serialVersionUID，如果不手动指定，那么编译器会动态生成默认的 serialVersionUID serialVersionUID 不是被 static 变量修饰了吗？为什么还会被“序列化”？static 修饰的变量是静态变量，位于方法区，本身是不会被序列化的。但是，serialVersionUID 的序列化做了特殊处理，在序列化时，会将 serialVersionUID 序列化到二进制字节流中；在反序列化时，也会解析它并做一致性判断。 为什么不推荐使用 JDK 自带的序列化？ 性能差 存在安全问题 不支持跨语言调用 I/OJava IO 流了解吗？IO 即 Input/Output，输入和输出。数据输入到计算机内存的过程即输入，反之输出到外部存储（比如数据库，文件，远程主机）的过程即输出。数据传输过程类似于水流，因此称为 IO 流。IO 流在 Java 中分为输入流和输出流，而根据数据的处理方式又分为字节流和字符流。 Java IO 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 I/O 流为什么要分为字节流和字符流呢?问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？ 字符流是由 Java 虚拟机将字节转换得到的，这个过程还算是比较耗时（字节流优势）； 如果我们不知道编码类型的话，使用字节流的过程中很容易出现乱码问题（字符流优势）。 Java 中 3 种常见 IO 模型BIO (Blocking I/O)同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。 NIO (Non-blocking/New I/O) 存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 I/O 多路复用模型 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间 -&gt; 用户空间）还是阻塞的。 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持。 select 调用：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。 epoll 调用：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。 Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。 AIO (Asynchronous I/O) 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。 零拷贝零拷贝主主要解决操作系统在处理 I/O 操作时频繁复制数据的问题。零拷贝的常见实现技术有： mmap+write、sendfile 下图展示了各种零拷贝技术的对比图： CPU 拷贝 DMA 拷贝 系统调用 上下文切换 传统方法 2 2 read+write 4 mmap+write 1 2 mmap+write 4 sendfile 1 2 sendfile 2 mmapmmap是Linux提供的一种内存映射文件的机制，它实现了将内核中读缓冲区地址与用户空间缓冲区地址进行映射，从而实现内核缓冲区与用户缓冲区的共享。 这样就减少了一次用户态和内核态的CPU拷贝，但是在内核空间内仍然有一次CPU拷贝。 mmap对大文件传输有一定优势，但是小文件可能出现碎片 sendfile建立了两个文件之间的传输通道 sendfile方式只使用一个函数就可以完成之前的read+write 和 mmap+write的功能，少了2次状态切换，由于数据不经过用户缓冲区，因此该数据无法被修改。 kafka和netty都是基于sendfile() 代理模式一种设计模式：使用代理对象来代替对真实对象(real object)的访问，这样就可以在不修改原目标对象的前提下，扩展目标对象的功能。（比如说在目标对象的某个方法执行前后你可以增加一些自定义的操作。） 静态代理 实现和应用角度： ​ 对目标对象的每个方法的增强都是手动完成的，非常不灵活（比如接口一旦新增加方法，目标对象和代理对象都要进行修改）且麻烦(需要对每个目标类都单独写一个代理类） JVM 层面： ​ 静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。 静态代理实现步骤: 定义一个接口及其实现类； 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。 动态代理JDK 动态代理机制在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。 使用步骤： 定义一个接口及其实现类； 定义一个实现 InvocationHandler 并重写invoke方法的类，在 invoke 方法中会调用原生方法（被代理类的方法）并自定义一些处理逻辑； 12345678public interface InvocationHandler { /** * 当你使用代理对象调用方法的时候实际会调用到这个方法 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;} 通过 Proxy.newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) 方法创建代理对象； 12345678910/**loader :类加载器，用于加载代理对象。* interfaces : 被代理类实现的一些接口；* h : 实现了 InvocationHandler 接口的对象；**/public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException{ ......} CGLIB 动态代理机制在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer类是核心。 使用步骤 定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 123456789101112131415161718192021222324import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * 自定义MethodInterceptor */public class DebugMethodInterceptor implements MethodInterceptor { /** * @param o 被代理的对象（需要增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { //调用方法之前，我们可以添加自己的操作 System.out.println(\"before method \" + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\"after method \" + method.getName()); return object; }} 通过 Enhancer 类的 create()创建代理类； 1234567891011121314151617import net.sf.cglib.proxy.Enhancer;public class CglibProxyFactory { public static Object getProxy(Class&lt;?&gt; clazz) { // 创建动态代理增强类 Enhancer enhancer = new Enhancer(); // 设置类加载器 enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类 enhancer.setSuperclass(clazz); // 设置方法拦截器 enhancer.setCallback(new DebugMethodInterceptor()); // 创建代理类 return enhancer.create(); }} JDK 动态代理和 CGLIB 动态代理对比 JDK 动态代理只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。 CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。 就二者的效率来说，大部分情况都是 JDK 动态代理更优秀 静态代理和动态代理的对比 灵活性：动态代理更加灵活，不需要必须实现接口（静态代理、jdk 动态代理），可以直接代理实现类(CGLIB)，并且可以不需要针对每个目标类都创建一个代理类。另外，静态代理中，接口一旦新增加方法，目标对象和代理对象都要进行修改，这是非常麻烦的！ JVM 层面：静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。而动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。","link":"/2023/08/12/java/Java%E5%9F%BA%E7%A1%80/"},{"title":"String.split()的使用","text":"使用 一般采用String.split(String regex)会将末尾空元素去掉,等价于``String.split(regex, 0)` 若需要进一步设置采用String.split(String regex, int limit利用limit来控制输出 limit有三种情况 limit&gt;0,分割limit-1次,相当于生成的list.size()=limit limit=0,不限分割次数,但会把末尾的空结果删去 limit&lt;0,不限分割次数,不会把末尾的空结果删去 针对regex 为 '.'这种情况, 比如分割IPv4, 由于. 在正则表达式中有特殊含义,是特殊字符,用来匹配除换行符\\n之外的任何单字符. 因而要使用\\进行转义,而\\也需要转义,所以最后采用\\\\.来匹配 【推荐】使用索引访问用String的split方法得到的数组时，需做最后一个分隔符后有无内容的检查，否则会有抛IndexOutOfBoundsException的风险。 说明： 123String str = \"a,b,c,,\"; String[] ary = str.split(\",\"); // 预期大于3，结果是3 System.out.println(ary.length); 源码单String参数, 默认limit = 0 123public String[] split(String regex) { return split(regex, 0);} 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public String[] split(String regex, int limit) { /* fastpath if the regex is a (1)one-char String and this character is not one of the RegEx's meta characters \".$|()[{^?*+\\\\\", or (2)two-char String and the first char is the backslash and the second is not the ascii digit or ascii letter. */ char ch = 0; if (((regex.value.length == 1 &amp;&amp; \".$|()[{^?*+\\\\\".indexOf(ch = regex.charAt(0)) == -1) || (regex.length() == 2 &amp;&amp; regex.charAt(0) == '\\\\' &amp;&amp; (((ch = regex.charAt(1))-'0')|('9'-ch)) &lt; 0 &amp;&amp; ((ch-'a')|('z'-ch)) &lt; 0 &amp;&amp; ((ch-'A')|('Z'-ch)) &lt; 0)) &amp;&amp; (ch &lt; Character.MIN_HIGH_SURROGATE || ch &gt; Character.MAX_LOW_SURROGATE)) { int off = 0; int next = 0; boolean limited = limit &gt; 0; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); while ((next = indexOf(ch, off)) != -1) { if (!limited || list.size() &lt; limit - 1) { list.add(substring(off, next)); off = next + 1; } else { // last one //assert (list.size() == limit - 1); list.add(substring(off, value.length)); off = value.length; break; } } // If no match was found, return this if (off == 0) return new String[]{this}; // Add remaining segment if (!limited || list.size() &lt; limit) list.add(substring(off, value.length)); // Construct result int resultSize = list.size(); if (limit == 0) { while (resultSize &gt; 0 &amp;&amp; list.get(resultSize - 1).isEmpty()) { resultSize--; } } String[] result = new String[resultSize]; return list.subList(0, resultSize).toArray(result); } return Pattern.compile(regex).split(this, limit); }","link":"/2024/05/11/java/String.split()%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"title":"异常日志规约","text":"异常处理【强制】Java 类库中定义的可以通过预检查方式规避的RuntimeException异常不应该通过catch 的方式来处理比如：NullPointerException，IndexOutOfBoundsException等等。 说明：无法通过预检查的异常除外，比如，在解析字符串形式的数字时，不得不通过catch NumberFormatException来实现。 正例： 123456//NullPointerExceptionif (obj != null) {...} //IndexOutOfBoundsException 例如分页时if (pageNum &lt; 1) {...}if (pageLimit &lt; 1) {...} 反例： 1try { obj.method() } catch (NullPointerException e) {…} 【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。 【推荐】防止NPE，是程序员的基本修养，注意NPE产生的场景：1）返回类型为基本数据类型，return包装数据类型的对象时，自动拆箱有可能产生NPE。反例：public int f() { return Integer对象}， 如果为null，自动解箱抛NPE。2） 数据库的查询结果可能为null。3） 集合里的元素即使isNotEmpty，取出的数据元素也可能为null。4） 远程调用返回对象时，一律要求进行空指针判断，防止NPE。5） 对于Session中获取的数据，建议NPE检查，避免空指针。6） 级联调用obj.getA().getB().getC()；一连串调用，易产生NPE。正例：使用JDK8的Optional类来防止NPE问题。 日志规约【强制】应用中而应依赖使用日志框架SLF4J中的API应用中不可直接使用日志系统（Log4j、Logback）中的API，而应依赖使用日志框架SLF4J中的API，使用门面模式的日志框架，有利于维护和各个类的日志处理方式统一。 123import org.slf4j.Logger;import org.slf4j.LoggerFactory;private static final Logger logger = LoggerFactory.getLogger(Abc.class);","link":"/2024/05/10/java/%E5%BC%82%E5%B8%B8%E6%97%A5%E5%BF%97%E8%A7%84%E7%BA%A6/"},{"title":"并发基础","text":"线程基础线程有哪几种状态?✨ 线程状态 解释 NEW 尚未启动的线程状态，即线程创建，还未调用start方法 RUNNABLE 就绪状态（调用start，等待调度）+ 正在运行 BLOCKED 等待监视器锁时，陷入阻塞状态 WAITING 等待状态的线程正在等待另一线程执行特定的操作（如notify） TIMED_WAITING 具有指定等待时间的等待状态 TERMINATED 线程完成执行，终止状态 blocked和waiting有啥区别 触发条件 blocked：没有抢到对象锁（monitor lock），通常发生在尝试进入synchronized块或方法时，如果锁已被占用，则线程将被阻塞直到锁可用 waiting：调用Object.wait()方法、Thread.join()方法或LockSupport.park()方法进入waiting状态来等待另一个线程执行某些操作唤醒 唤醒机制 blocked：当一个线程被阻塞等待锁时，一旦锁被释放，线程将有机会重新尝试获取锁。 waiting：线程在WAITING状态中需要被显式唤醒。例如，如果线程调用了Object.wait()，那么它必须等待另一个线程调用同一对象上的Object.notify()或Object.notifyAll()方法才能被唤醒。 notify 选择哪个线程?notify在源码的注释中说到notify选择唤醒的线程是任意的，但是依赖于具体实现的jvm。 JVM有很多实现，比较流行的就是hotspot，hotspot对notofy()的实现并不是我们以为的随机唤醒,，而是“==先进先出==”的顺序唤醒。 线程有哪几种创建方式?✨方式一：继承于Thread类 步骤：1.创建一个继承于Thread类的子类2.重写Thread类的run() –&gt; 将此线程执行的操作声明在run()中3.创建Thread类的子类的对象4.通过此对象调用start()执行线程 1234567891011class MyThread extends Thread { @Override public void run() { // 线程执行的代码 }}public static void main(String[] args) { MyThread t = new MyThread(); t.start();} 方式二：实现Runnable接口 步骤：1.创建一个实现了Runnable接口的类2.实现类去实现Runnable中的抽象方法：run()3.创建实现类的对象4.将此对象作为参数传递到Thread类的构造器中，创建Thread类的对象5.通过Thread类的对象调用start()① 启动线程②调用当前线程的run()–&gt;调用了Runnable类型的target的run() 方式一和方式二的比较： 开发中优先选择实现Runnable接口的方式 原因：（1）实现的方式没有类的单继承性的局限性（2）实现的方式更适合来处理多个线程有共享数据的情况 1234567891011class MyRunnable implements Runnable { @Override public void run() { // 线程执行的代码 }}public static void main(String[] args) { Thread t = new Thread(new MyRunnable()); t.start();} 方式三：实现Callable接口 步骤：1.创建一个实现Callable的实现类2.实现call方法，将此线程需要执行的操作声明在call()中3.创建Callable接口实现类的对象4.将此Callable接口实现类的对象作为传递到FutureTask构造器中，创建FutureTask的对象5.将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread对象，并调用start()6.获取Callable中call方法的返回值 实现Callable接口的方式创建线程的强大之处 call()可以有返回值的 call()可以抛出异常，被外面的操作捕获，获取异常的信息 Callable是支持泛型的 123456789101112131415161718192021class MyCallable implements Callable&lt;Integer&gt; { @Override public Integer call() throws Exception { // 线程执行的代码，这里返回一个整型结果 return 1; }}public static void main(String[] args) { MyCallable task = new MyCallable(); FutureTask&lt;Integer&gt; futureTask = new FutureTask&lt;&gt;(task); Thread t = new Thread(futureTask); t.start(); try { Integer result = futureTask.get(); // 获取线程执行结果 System.out.println(\"Result: \" + result); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); }} 方式四：使用线程池 步骤：1.以方式二或方式三创建好实现了Runnable接口的类或实现Callable的实现类2.实现run或call方法3.创建线程池4.调用线程池的execute方法执行某个线程，参数是之前实现Runnable或Callable接口的对象 线程池好处：1.提高响应速度（减少了创建新线程的时间）2.降低资源消耗（重复利用线程池中线程，不需要每次都创建）3.便于线程管理 1234567891011121314ass Task implements Runnable { @Override public void run() { // 线程执行的代码 }}public static void main(String[] args) { ExecutorService executor = Executors.newFixedThreadPool(10); // 创建固定大小的线程池 for (int i = 0; i &lt; 100; i++) { executor.submit(new Task()); // 提交任务到线程池执行 } executor.shutdown(); // 关闭线程池} 基础线程机制有哪些?线程的中断方式有哪些?使用退出标识终止线程当run方法执行完后，线程就会退出。但有时run方法是永远不会结束的。如在服务端程序中使用线程进行监听客户端请求，或是其他的需要循环处理的任务。在这种情况下，一般是将这些任务放在一个循环中，如while循环。如果想让循环永远运行下去，可以使用while（true）{……}来处理。但要想使while循环在某一特定条件下退出，最直接的方法就是设一个boolean类型的标志，并通过设置这个标志为true或false来控制while循环是否退出。下面给出了一个利用退出标志终止线程的例子。 123456789101112131415161718192021package chapter2; public class ThreadFlag extends Thread{ public volatile boolean exit = false; public void run() { while (!exit); } public static void main(String[] args) throws Exception { ThreadFlag thread = new ThreadFlag(); thread.start(); sleep(5000); // 主线程延迟5秒 thread.exit = true; // 终止线程thread thread.join(); System.out.println(\"线程退出!\"); }} 使用interrupt方法终止线程使用interrupt方法来终端线程可分为两种情况： 线程处于阻塞状态，如使用了sleep方法。 在这种情况下使用interrupt方法，sleep方法将抛出一个InterruptedException例外，而在第二种情况下线程将直接退出。下面的代码演示了在第一种情况下使用interrupt方法。 1234567891011121314151617181920212223242526package chapter2; public class ThreadInterrupt extends Thread{ public void run() { try { sleep(50000); // 延迟50秒 } catch (InterruptedException e) { System.out.println(e.getMessage()); } } public static void main(String[] args) throws Exception { Thread thread = new ThreadInterrupt(); thread.start(); System.out.println(\"在50秒之内按任意键中断线程!\"); System.in.read(); thread.interrupt(); thread.join(); System.out.println(\"线程已经退出!\"); }} 在调用interrupt方法后， sleep方法抛出异常，然后输出错误信息：sleep interrupted. 使用while（！isInterrupted（））{……}来判断线程是否被中断。 使用stop方法终止线程使用stop方法可以强行终止正在运行或挂起的线程。我们可以使用如下的代码来终止线程： 1thread.stop(); 虽然使用上面的代码可以终止线程，但使用stop方法是很危险的，就象突然关闭计算机电源，而不是按正常程序关机一样，可能会产生不可预料的结果，因此，并不推荐使用stop方法来终止线程。 线程的互斥同步方式有哪些? 如何比较和选择?线程之间有哪些协作方式?共享内存在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信，典型的共享内存通信方式就是通过共享对象进行通信。 同步（synchronized关键字） while轮询的方式 消息传递在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信，在java中典型的消息传递方式就是wait()和notify()。 ③wait/notify机制 ④管道通信 并发安全乐观锁&amp;&amp;悲观锁乐观锁认为在访问数据的时候，其他线程不怎么会对该数据进行修改，所以不加锁。只是在修改的时候通过一种验证数据是否被修改的机制来解决并发问题。 有两种方式： volatile + CAS： CAS 是乐观锁的基础。Java 提供了 java.util.concurrent.atomic 包，包含各种原子变量类（如 AtomicInteger、AtomicLong），这些类使用 CAS 操作实现了线程安全的原子操作，可以用来实现乐观锁。 版本号 时间戳 悲观锁认为数据经常会被其他线程修改，所以在某一个线程抢占到该数据时，对其上锁，其他线程想要访问会被阻塞。 volatile + CAS✨CAS的问题 CAS引发的ABA问题 ABA问题是指在CAS操作时，其他线程将变量值A改为了B，但是又被改回了A，等到本线程使用期望值A与当前变量进行比较时，发现变量A没有变，于是CAS就将A值进行了交换操作，但是实际上该值已经被其他线程改变过，这与乐观锁的设计思想不符合。ABA问题的解决思路是，每次变量更新的时候把变量的版本号加1，那么A-B-A就会变成A1-B2-A3，只要变量被某一线程修改过，改变量对应的版本号就会发生递增变化，从而解决了ABA问题。 CAS导致自旋消耗 多个线程争夺同一个资源时，如果自旋一直不成功，将会一直占用CPU。 解决方法：破坏掉for死循环，当超过一定时间或者一定次数时，return退出。 CAS只能单变量 CAS的原子操作只能针对一个共享变量，假如需要针对多个变量进行原子操作也是可以解决的。 方法：CAS操作是针对一个变量的，如果对多个变量操作，1. 可以加锁来解决。2 .封装成对象类解决。 voliatle关键字有什么作用 保证变量对所有线程的可见性。当一个变量被声明为volatile时，它会保证对这个变量的写操作会立即刷新到主存中，而对这个变量的读操作会直接从主存中读取，从而确保了多线程环境下对该变量访问的可见性。 禁止指令重排序优化。volatile关键字在Java中主要通过内存屏障来禁止特定类型的指令重排序。 volatile可以保证线程安全吗volatile关键字可以保证可见性，但不能保证原子性，因此不能完全保证线程安全。volatile关键字用于修饰变量，当一个线程修改了volatile修饰的变量的值，其他线程能够立即看到最新的值，从而避免了线程之间的数据不一致。 但是，volatile并不能解决多线程并发下的复合操作问题，比如i++这种操作不是原子操作，如果多个线程同时对i进行自增操作，volatile不能保证线程安全。 volatile和sychronized比较？ Synchronized: Synchronized是一种排他性的同步机制，保证了多个线程访问共享资源时的互斥性，即同一时刻只允许一个线程访问共享资源。 Volatile: Volatile是一种轻量级的同步机制，用来保证变量的可见性和禁止指令重排序。 Synchronized✨Synchronized 可以作用在哪分为对象锁、类锁 对象锁： 方法锁：synchronized 修饰普通方法，锁对象默认 this 代码块锁：手动指定锁对象 类锁： 方法锁：synchronized 修饰静态方法 代码块锁：手动指定锁对象为 Class 对象 Synchronized本质上是通过什么保证线程安全的加锁、释放锁原理无锁-&gt;偏向锁-&gt;轻量级锁-&gt;重量级锁 可重入原理可重入：同一线程的外层函数获得锁之后，内层函数可以直接再次获取该锁 保证可见性原理Synchronized的happens-before规则，即监视器锁规则：对同一个监视器的解锁，happens-before于对该监视器的加锁。且解锁前的结果对于加锁后的操作是可见的 **锁的升级 Java中锁主要存在四种状态：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，随着竞争的激烈而逐渐升级。==锁只能升级而不能降级== 应用场景首先简单说下先偏向锁、轻量级锁、重量级锁三者各自的应用场景： 偏向锁：只有一个线程进入临界区；轻量级锁：多个线程交替进入临界区；重量级锁：多个线程同时进入临界区。 偏向锁流程： ​ ==JVM使用CAS操作把线程ID记录到对象的Mark Word当中，并修改标识位==。此时进入偏向模式，若接下来没有其他线程进入临界区，该线程可以不用执行任何同步操作进入临界区。 ​ 线程不会自己释放偏向锁，当其他线程想要抢占锁时，会进行判断之前的占有线程是否完成，（1）如果在运行中，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁升级到轻量级锁（可能多个线程交替进入临界区）；（2）如果不在运行中，则利用 CAS 更改对象头的线程 ID 轻量级锁​ 轻量级锁是由偏向级锁升级来的，一段时间内有两个线程进入临界区，偏向锁就会升级为轻量级锁。 流程： 首先，JVM会将锁对象的Mark Word恢复成为无锁状态，在当前两线程的栈桢中各自分配一个空间，叫做Lock Record，把锁对象的Mark Word在两线程的栈桢中各自复制了一份，官方称为：Displaced Mark Word 然后一个线程尝试使用CAS将对象头中的Mak Word替换为指向锁记录的指针，如果替换成功，则当前线程获得锁，如果失败，则当前线程自旋重新尝试获取锁。当自旋获取锁失败超过 10 次时，表示竞争比较激烈，则轻量级锁会膨胀成重量级锁 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头中，如果成功，则表示没有发生竞争关系。如果失败，表示当前锁存在竞争关系。锁就会膨胀成重量级锁。 重量级锁当多个线程竞争同一个锁时，会导致除锁的拥有者外，其余线程都会自旋，这将导致自旋次数过多，cpu效率下降，所以会将锁升级为重量级锁。 重量级锁需要操作系统的介入，依赖操作系统底层的Mutex Lock。JVM会创建一个monitor对象，把这个对象的地址更新到Mark Word中。 当一个线程获取了该锁后，其余线程想要获取锁，必须等到这个线程释放锁后才可能获取到，没有获取到锁的线程，就进入了阻塞状态。 具体流程： 加锁、释放锁通过「monitor 对象」。在并发抢占时，一个对象对应一个 monitor，同一时间只有一个线程能获得 monitor。 主要指令有「monitorenter」尝试获取monitor 所有权 和 「monitorexit」释放monitor 所有权 调用「monitorenter」时有三种情况： 当 monitor 计数器为 0，说明没有其他线程抢占，enter 成功，线程获取该 monitor，计数器+1 已经获取该monitor的线程，可以重复执行 enter，执行依次计数器+1 若 monitor 计数器不为 0，说明锁已经被其他线程获取，需要等待锁的释放。 调用「monitorexit」时： 释放对于monitor的所有权，将monitor的计数器减1，如果减完以后，计数器不是0，则代表刚才是重入进来的，当前线程还继续持有这把锁的所有权，如果计数器变成0，则代表当前线程不再拥有该monitor的所有权，即释放锁。 该图可以看出，任意线程获取Object锁的时候，首先要获得Object的监视器，如果获取失败，该线程就进入同步状态，线程状态变为BLOCKED，当Object的监视器占有者释放后，在同步队列中得线程就会有机会重新获取该监视器。 Synchronized有什么样的缺陷？Lock解决相应问题？如何选择？ 灵活性低：加锁释放锁的时机单一 效率低：锁的释放情况少，只有代码完成或者异常退出的时候才释放；不能中断一个正在使用锁的进程，不能对试图获取锁的进程设置超时。Lock 可以中断和超时 无法知道是否成功获得锁：Lock 可以拿到是否获取到锁的状态 Lock类这里不做过多解释，主要看里面的4个方法: lock(): 加锁 unlock(): 解锁 tryLock(): 尝试获取锁，返回一个boolean值 tryLock(long,TimeUtil): 尝试获取锁，可以设置超时 建议： 1.如果可以的话，尽量就不使用lock也不要使用Synchronized关键字而是使用java.Util.concurrent中的各种包中的类2.如果Synchronized在程序中适用，那么我们就优先选择Synchronized，因为这样可以减少我们所需要编写的代码，可以减少出错率3.如果特别需要用的lock的时候，我们再使用lock Synchronized在使用时有何注意事项?锁对象不能为空，作用域不宜过大，避免死锁 锁对象不能为空：就是我们如果指定了一个对象为我们的锁对象，那么他就必须是被实例化过的，就是被new过的，不是一个空对象，因为我们的Synchronized是放在我们的对象头中修饰的，如果这个对象为空，更没有对象头，那么这个锁是没有办法工作的作用域不宜过大：作用域是指被Synchronized代码块所包括的范围，如果我们尽量多的代码都被Synchronized所修饰，那么我们代码就会达到安全的目的，但是对于效率就会降低，我们多线程编程，目的是为了提高效率，再不需要安全的情况下，我们并行去执行，是可以提高运行的效率，所以如果我们把Synchronized作用域设置的过大，就会影响我们程序的执行效率的。 多个线程等待同一个Synchronized锁的时候，JVM如何选择下一个获取锁的线程? 队列： Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：候选者队列，Contention List中那些有资格成为候选资源的线程被移动到Entry List中； WaitSet：阻塞队列，哪些调用wait方法被阻塞的线程被放置在这里，直到某个时刻通过notify或者notifyAll唤醒，会重新进去EntryList中； 线程： OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck，从 EntryList 中选出。 Owner：从刚刚申请的线程和 OnDeck 选一个最终获得锁，当前已经获取到所资源的线程被称为Owner； 根据 JVM 随机选择，要么是候选者队列中的线程，要么是刚刚申请获取锁的线程 所以——synchronized实际上是非公平的，新来的线程先尝试自旋获取锁，如果获取不到就进入竞争队列，而在等待区中等候已久的线程可能再次等待，这样有利于提高性能，但是也可能会导致饥饿现象。 AQSAQS核心思想？如何实现？底层数据结构？核心思想：如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制。这个机制主要用的是 CLH 队列的变体实现的，将暂时获取不到锁的线程加入到队列中。 实现： state：volatile int state变量，表示同步状态 通过内置的 FIFO 队列来完成资源获取的排队工作 通过 CAS 完成对 State 值的修改。 state： 独占模式 state=0，说明可以抢资源，&gt;0 说明不能 共享模式 state&gt;0，说明可以抢资源，=0 说明不能 设计模式： 模版方法 底层数据结构： 同步队列： CLH 锁是对自旋锁的一种改进，是单向链表，AQS又对 CLH 锁进行改进, 变成一个==虚拟的双向队列，即同步队列==（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系），AQS 是通过将每条请求共享资源的线程封装成一个Node节点来实现锁的分配；当获取同步状态成功的线程释放同步状态的时候，唤醒队列中下一个节点去抢同步状态。 共享模式的区别在于，唤醒的时候，可能会唤醒队列里一批的节点 在 CLH 队列锁中，一个节点表示一个线程，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）。 AQS 每个节点的状态如下所示，在源码中如下所示： 1volatile int waitStatus; AQS 同样提供了该状态变量的原子读写操作，但和同步器状态不同的是，节点状态在 AQS 中被清晰的定义，如下表所示： 状态名 描述 SIGNAL 表示该节点正常等待 PROPAGATE 应将 releaseShared 传播到其他节点 CONDITION 该节点位于条件队列，不能用于同步队列节点 CANCELLED 由于超时、中断或其他原因，该节点被取消 condition等待队列： AQS通过内部类ConditionObject构建等待队列（可有多个），当Condition调用wait()方法后，线程将会加入等待队列中，而当Condition调 用signal()方法后 ，线程将从等待队列转移动同步队列中进行锁竞争 。 AQS 的核心原理图 AQS 使用 int 成员变量 state 表示同步状态，通过内置的 CLH 队列来完成获取资源线程的排队工作。 state 变量由 volatile 修饰，用于展示当前临界资源的获锁情况。 同步状态 状态信息 state 可以通过 protected 类型的getState()、setState()和compareAndSetState() 进行操作。并且，这几个方法都是 final 修饰的，在子类中无法被重写。 123456789101112//返回同步状态的当前值protected final int getState() { return state;} // 设置同步状态的值protected final void setState(int newState) { state = newState;}//原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值）protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update);} AQS定义什么样的资源获取方式?==可以通过修改 State 字段表示的同步状态来实现多线程的独占模式和共享模式（加锁过程）。== AQS有哪些核心的方法?acquire该方法以独占模式获取(资源)，忽略中断，即线程在aquire过程中，中断此线程是无效的。 1234public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} release以独占模式释放对象 12345678910public final boolean release(int arg) { if (tryRelease(arg)) { // 释放成功 // 保存头节点 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) // 头节点不为空并且头节点状态不为0 unparkSuccessor(h); //释放头节点的后继结点 return true; } return false;} **ReentrantLock修改 State 字段为 0 来实现独占方式 再实现两个 AQS 提供的模板方法 tryAcquire和tryRelease。 流程 如何实现可重入比较拥有同步状态的线程是否和当前请求同步状态的线程相同，若相同则cas 让 state+1， 有公平锁和非公平锁两种 公平锁：在请求共享资源时发现资源被占用，该线程就会添加到sync queue中的尾部，而不会先尝试获取资源。 非公平锁：每一次都会尝试去获取资源，如果此时该资源恰好被释放，则会被当前线程获取，这就造成了不公平的现象，当获取不成功，再加入队列尾部。 默认用非公平锁 三个内部类： 说明: ReentrantLock类内部总共存在Sync、NonfairSync、FairSync三个类，NonfairSync与FairSync类继承自Sync类，Sync类继承自AbstractQueuedSynchronizer抽象类。 ReentrantLock和 Synchornized 关系相同 独占锁 可重入 不同 syn 是关键字，reen 是类 syn 基于 monitor，reen 基于AQS syn 只有非公平方式，reen 有公平方式和非公平方式 syn不可以响应中断，reen 可以同时获取锁时可以限时等待 syn 利用 object 的 wait() 和 notify() 实现线程间的等待通知机制；reen 使用Condition的 await() 和 signal() 实现 syn 自动释放 monitor，reen 需要显式调用unlock（）释放锁 syn 只能关联一个条件队列（waitset），reen可以关联多个条件队列（lock.newCondition()生成多个 condition 对象） 应用场景syn 由于使用方便简单，建议在竞争不激烈的时候使用。在竞争激烈的时候 reen 性能会好点 reen 由于具有 syn 所不具有的灵活性、可中断、有限等待等特性，可以用于时间锁等待、可中断锁等待 CountdownLatch和CyclicBarrier的区别使用场景与具体实现CountdownLatch和CyclicBarrier都属于线程同步的工具 CountdownLatch使用场景顾名思义CountdownLatch可以当做一个计数器来使用,比如某线程需要等待其他几个线程都执行过某个时间节点后才能继续执行 我们来模拟一个场景,某公司一共有十个人,门卫要等十个人都来上班以后,才可以休息,代码实现如下 1234567891011121314151617181920212223242526272829public static void main(String[] args) { final CountDownLatch latch = new CountDownLatch(10); for (int i = 0; i &lt; 10; i++) { //lambda中只能只用final的变量 final int times = i; new Thread(() -&gt; { try { System.out.println(\"子线程\" + Thread.currentThread().getName() + \"正在赶路\"); Thread.sleep(1000 * times); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"到公司了\"); //调用latch的countDown方法使计数器-1 latch.countDown(); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"开始工作\"); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } try { System.out.println(\"门卫等待员工上班中...\"); //主线程阻塞等待计数器归零 latch.await(); System.out.println(\"员工都来了,门卫去休息了\"); } catch (InterruptedException e) { e.printStackTrace(); } } 可以看到子线程并没有因为调用latch.countDown而阻塞,会继续进行该做的工作,只是通知计数器-1,即完成了我们如上说的场景,只需要在所有进程都进行到某一节点后才会执行被阻塞的进程.如果我们想要多个线程在同一时间进行就要用到CyclicBarrier了 原理CountDownLatch是共享锁的一种实现，通过 AQS 实现，内部使用 syn类。将 state 设置为 count。主要有两个重要方法countDown()和 await()，countDown()调用tryReleaseShared()以 CAS 的方式减少 state，直至 state 为 0 。调用 await()则是会一直阻塞线程，直至 state 为 0，线程才会被唤醒，执行后续语句。 我们先来看看CountdownLatch的构造方法 1234public CountDownLatch(int count) { if (count &lt; 0) throw new IllegalArgumentException(\"count &lt; 0\"); this.sync = new Sync(count); } Sync是CountdownLatch的静态内部类,继承了AbstractQueuedSynchronizer(即AQS,提供了一种实现阻塞锁和一系列依赖FIFO等待队列的同步器的工具,回头单讲)抽象类, 在Sync的构造方法中,调用了setState方法,可以视作初始化了一个标记来记录当前计数器的数量 CountdownLatch的两个核心方法,await和countdown,先来看await 1234public void await() throws InterruptedException { //可以视作将线程阻塞 sync.acquireSharedInterruptibly(1); } await调用的是AQS的方法,可以视作阻塞线程 countdown方法 123public void countDown() { sync.releaseShared(1); } 调用了sync的一个方法,再来看看这个方法的实现 1234567public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 再来看这个tryReleaseShared方法 123456789101112protected boolean tryReleaseShared(int releases) { for (;;) { //获取标记位 int c = getState(); if (c == 0) return false; int nextc = c-1; //用cas的方式更新标记位 if (compareAndSetState(c, nextc)) return nextc == 0; } } 可以看到在调用tryReleaseShared实际上是将标记位-1并且返回标记位是否为0,如果标记位为0 那么调用的doReleaseShared可以视作将阻塞的线程放行,这样整个的流程就通了 CyclicBarrier使用场景我们重新模拟一个新的场景,就用已经被说烂的跑步场景吧,十名运动员各自准备比赛,需要等待所有运动员都准备好以后,裁判才能说开始然后所有运动员一起跑,代码实现如下 123456789101112131415161718192021222324public static void main(String[] args) { final CyclicBarrier cyclicBarrier = new CyclicBarrier(10,()-&gt;{ System.out.println(\"所有人都准备好了裁判开始了\"); }); for (int i = 0; i &lt; 10; i++) { //lambda中只能只用final的变量 final int times = i; new Thread(() -&gt; { try { System.out.println(\"子线程\" + Thread.currentThread().getName() + \"正在准备\"); Thread.sleep(1000 * times); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"准备好了\"); cyclicBarrier.await(); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"开始跑了\"); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }).start(); } } 可以看到所有线程在其他线程没有准备好之前都在被阻塞中,等到所有线程都准备好了才继续执行 我们在创建CyclicBarrier对象时传入了一个方法,当调用CyclicBarrier的await方法后,当前线程会被阻塞等到所有线程都调用了await方法后 调用传入CyclicBarrier的方法,然后让所有的被阻塞的线程一起运行 原理CyclicBarrier是利用ReentrantLock的condition的 await()和 signalAll()来进行线程的阻塞和唤醒【类似Object.wait()和notifyAll()】在count不为0时阻塞,在count=0时唤醒所有线程 二者区别 CountDownLatch减计数，CyclicBarrier加计数。 CountDownLatch是一次性的，CyclicBarrier可以重用。 CountDownLatch和CyclicBarrier都有让多个线程等待同步然后再开始下一步动作的意思，但是CountDownLatch的下一步的动作实施者是主线程，具有不可重复性；而CyclicBarrier的下一步动作实施者还是“其他线程”本身，具有往复多次实施动作的特点。 线程池什么是线程池、为什么要用、底层实现线程池是一种池化技术，主要思想是资源复用。 优点：减少创建销毁的开销；任务响应快；限制线程数量，防止资源利用率过高 底层逻辑是使用线程+阻塞队列实现，队列中有任务线程就会消费，假如没有线程就会被阻塞，等待队列中新的任务的到来，假如队列满了，则会出发设定的饱和策略（包括抛出异常、原线程执行、丢弃任务、丢弃最早未处理任务） 如何创建1234567public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) 一般使用 TreadPoolExecutor 建立线程池，有三个重要参数（核心线程数、最大线程数、阻塞队列）。不使用 Executors 去创建的原因：fixedThreadPool 阻塞队列的长度是 Integer.MAX_VALUE，cachedThreadPool 的线程数量最大是Integer.MAX_VALUE。都会导致 OOM 阻塞队列workQueue: 用来保存等待被执行的任务的阻塞队列 LinkedBlockingQueue无界队列: 基于链表结构的阻塞队列，按FIFO排序任务； SynchronousQueue同步队列: 没有容量，不存储元素，目的是保证对于提交的任务，如果有空闲线程，则使用空闲线程来处理；否则新建一个线程来处理任务。CachedThreadPool 的最大线程数是 Integer.MAX_VALUE ，可以理解为线程数是可以无限扩展的，可能会创建大量线程，从而导致 OOM DelayQueue（延迟阻塞队列）：DelayQueue 的内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构，可以保证每次出队的任务都是当前队列中执行时间最靠前的。 饱和策略handler 线程池的饱和策略，当阻塞队列满了，且没有空闲的工作线程，如果继续提交任务，必须采取一种策略处理该任务，线程池提供了4种策略: AbortPolicy: 直接抛出异常，默认策略； CallerRunsPolicy: 用调用者所在的线程来执行任务； DiscardOldestPolicy: 丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy: 直接丢弃任务； 线程池处理任务的流程 submit()or execute()一个任务的流程 当前线程数量&lt;核心线程数，新建线程处理任务 阻塞队列未满，加到阻塞队列等待执行 阻塞队列满&amp;当前线程数&lt;最大线程数，新建线程处理 达到最大线程数，使用饱和策略 提交任务的方法submit()submit()方法可以接收Callable、Runnable两种类型的参数 submit()方法也用于启动任务的执行，但是启动之后会返回Future对象，代表一个异步执行实例，可以通过该异步执行实例去获取结果。 submit()方法返回的Future对象（异步执行实例），可以进行异步执行过程中的异常捕获。 execute()Execute()方法只能接收Runnable类型的参数 execute()方法主要用于启动任务的执行，而任务的执行结果和可能的异常调用者并不关心。 execute()方法在启动任务执行后，任务执行过程中可能发生的异常调用者并不关心 联系在ThreadPoolExecutor类的实现中，内部核心的任务提交方法是execute()方法，虽然用户程序通过submit()也可以提交任务，但是实际上submit()方法中最终调用的还是execute()方法。 Callable类型的任务是可以返回执行结果的，而Runnable类型的任务不可以返回执行结果。 Runnable和Callable的主要区别为：Callable允许有返回值，Runnable不允许有返回值；Runnable不允许抛出异常，Callable允许抛出异常。 内置线程池类型FixedThreadPool12345return new ThreadPoolExecutor( nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); 核心线程数和最大线程数一样，且线程没有过期时间，线程池的线程数量达corePoolSize后，即使线程池没有可执行任务时，也不会释放线程。 使用了LinkedBlockingQueue无界队列, 所以FixedThreadPool永远不会拒绝, 即饱和策略失效 SingleThreadExecutor12345return new ThreadPoolExecutor( 1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); 初始化的线程池中只有一个线程 使用了LinkedBlockingQueue无界队列, 所以SingleThreadPool永远不会拒绝, 即饱和策略失效 CachedThreadPool12345return new ThreadPoolExecutor( 0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); 最大线程数为Integer.MAX_VALUE 使用SynchronousQueue同步队列； 线程有过期时间，空闲会释放线程 ScheduledThreadPool12345super( corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); 最大线程数为Integer.MAX_VALUE 使用无界的DelayedWorkQueue延迟阻塞队列 关闭线程池遍历线程池中的所有线程，然后逐个调用线程的interrupt方法来中断线程. 关闭方式 - shutdown将线程池里的线程状态设置成SHUTDOWN状态, 然后中断所有没有正在执行任务的线程. 关闭方式 - shutdownNow将线程池里的线程状态设置成STOP状态, 然后停止所有正在执行或暂停任务的线程. 只要调用这两个关闭方法中的任意一个, isShutDown() 返回true. 当所有任务都成功关闭了, isTerminated()返回true. 任务的执行==execute –&gt; addWorker –&gt;runworker (getTask)== addWorker方法：主要负责创建新的线程并执行任务 线程池创建新线程执行任务时，需要获取全局锁: 线程池的工作线程通过Woker类实现，启动线程本质是执行了Worker的runWorker方法。 线程执行完任务怎么通知阻塞队列？ firstTask执行完成之后，通过==getTask==方法从阻塞队列中获取等待的任务，如果队列中没有任务，getTask方法会被阻塞并挂起，不会占用cpu资源； CompletableFuture是 JDK1.8 中引入的一个基于事件驱动的异步回调类。 当使用异步线程去执行一个任务时，在任务完成后触发一个后续动作。可以实现任务的编排，主要方法有： thenCombine。两个任务都执行结束后触发事件回调 thenCompose。第一个任务执行玩后自动触发执行第二个任务","link":"/2023/11/12/java/%E5%B9%B6%E5%8F%91/"},{"title":"路径中斜杠&#x2F;与反斜杠\\的区别","text":"在文件路径中，斜杠（/）和反斜杠（\\）是两种常见的路径分隔符，它们在不同的操作系统和编程环境中有不同的使用方式和含义。 斜杠/ 在 Unix、Linux 和 macOS 等类 Unix 系统中，斜杠是路径分隔符的标准，用于分隔目录和文件名。 在 URL 中，斜杠用于分隔协议、主机名和路径等部分。 反斜杠\\ 在 Windows 系统中，反斜杠是路径分隔符的标准，用于分隔目录和文件名。 在字符串中，反斜杠可以用作转义字符，用于表示特殊字符或字符编码。 在正则表达式中，反斜杠用于表示元字符的转义字符。 在 Windows 操作系统中，使用反斜杠（\\）作为路径分隔符的历史原因主要有以下几点： DOS 的影响：Windows 操作系统的前身是 MS-DOS（Microsoft Disk Operating System），而 MS-DOS 是在早期的计算机系统中开发的。在 MS-DOS 中，使用反斜杠作为路径分隔符，这种约定被保留下来，并延续到了后来的 Windows 系统中。 兼容性考虑：为了保持与早期的 MS-DOS 和 Windows 系统的兼容性，Windows 仍然使用反斜杠作为路径分隔符。这样可以确保旧的 DOS 和 Windows 程序能够在新的 Windows 系统上继续运行。 随着发展，DOS系统已经被淘汰了，命令提示符也用的很少，斜杆和反斜杠在大多数情况下可以互换，没有影响。 总结 浏览器地址栏网址使用 斜杆/ ; windows文件浏览器上使用 反斜杠\\ ; 出现在html url() 属性中的路径，指定的路径是网络路径，所以必须用 斜杆/ ; 出现在普通字符串中的路径，如果代表的是windows文件路径，则使用 斜杆/ 和 反斜杠\\ 是一样的；如果代表的是网络文件路径，则必须使用 斜杆/ ; 12&lt;img src=\".\\Image/Control/ding.jpg\" /&gt; // 本地文件路径，/ 和 \\ 是等效的&lt;img src=\"./Image\\Control\\cai.jpg\" /&gt;","link":"/2024/05/11/java/%E8%B7%AF%E5%BE%84%E4%B8%AD%E6%96%9C%E6%9D%A0%EF%80%A2%E4%B8%8E%E5%8F%8D%E6%96%9C%E6%9D%A0%EF%80%A6%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"集合基础","text":"ListArrayList 和 Array的区别✨ArrayList 内部基于动态数组实现，比 Array（静态数组） 使用起来更加灵活： 动态地扩容或缩容 使用泛型来确保类型安全 只能存储对象（Array 都可以） 不需要指定大小 LinkedList 为什么不能实现 RandomAccess 接口？RandomAccess 是一个标记接口，用来表明实现该接口的类支持随机访问（即可以通过索引快速访问元素）。由于 LinkedList 底层数据结构是链表，内存地址不连续，只能通过指针来定位，不支持随机快速访问，所以不能实现 RandomAccess 接口。 RandomAccess 接口只是标识，并不是说ArrayList实现 RandomAccess` 接口才具有快速随机访问功能的！是为了能够更好地判断集合是否ArrayList或者LinkedList，从而能够更好选择更优的遍历方式，提高性能！（Collections类中的binarySearch（）） ArrayList 与 LinkedList 区别?✨ ArrayList LinkedList 底层数据结构 数组 双向链表 插入和删除性能 末尾O(1)（在扩容时可能为 O(n)）在中间或开始O(n)，因为需要移动大量数据 O(1) ，但需要 O(n) 的时间找到插入或删除位置 访问性能 支持高效的随机访问—— O(1) 遍历链表以访问第 n 个元素—— O(n) 内存空间占用 只存数据，且空间连续，但可能出现内存浪费 除了数据还需要存储前后指针，空间分散 使用场景 多读取，需要快速随机访问 多插入、删除 ArrayList 的扩容机制✨ 先从 ArrayList 的构造函数说起 有三种构造方法 1234567891011121314/** * 带初始容量参数的构造函数。（用户自己指定容量） */public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) {//初始容量大于0 //创建initialCapacity（10）大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) {//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else {//初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \" + initialCapacity); }} 以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。 add 方法 这里以无参构造函数创建的 ArrayList 为例分析，即一开始为空的数组。 当我们要 add 进第 1 个元素到 ArrayList 时，内部存放数据的数组长度为 0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以此次add操作需要的 minCapacity 为 10。此时，minCapacity 大于 数组长度，所以会进入 grow(minCapacity) 方法来扩充数组。 当 add 第 2 个元素时，minCapacity 为 2，此时 elementData.length(容量)在添加第一个元素后扩容成 10了。此时，minCapacity 小于等于 数组长度，所以不会进入 （执行）grow(minCapacity)方法。 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。 直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。 grow 方法 12345678910111213141516171819202122232425262728/** * 要分配的最大数组大小 */private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;/** * ArrayList扩容的核心方法。 */private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; // 将oldCapacity 右移一位，其效果相当于oldCapacity /2， // 我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， // 如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);} int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数. 需要保证newCapacity&gt;=minCapacity 假如newCapacity &gt; MAX_ARRAY_SIZE需要进一步进入判断minCapacity是否真的大于MAX_ARRAY_SIZE 1234567891011private static int hugeCapacity(int minCapacity) { if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); // 对minCapacity和MAX_ARRAY_SIZE进行比较 // 若minCapacity大，将Integer.MAX_VALUE作为新数组的大小 // 若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小 // MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE;} == 如果没有MAX_ARRAY_SIZE，某些需要-8的虚拟机，会在数组长度大于Integer.MAX_VALUE的2/3的时候，一扩容就 报错。== 无序性和不可重复性的含义是什么 无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。 不可重复性是指添加的元素按照 equals() 判断时 ，返回 false，需要同时重写 equals() 方法和 hashCode() 方法。 ArrayList线程安全吗？把ArrayList变成线程安全有哪些方法？✨ 使用Collections类的synchronizedList方法将ArrayList包装成线程安全的List 1List&lt;String&gt; synchronizedList = Collections.synchronizedList(arrayList); 使用CopyOnWriteArrayList类代替ArrayList，它是一个线程安全的List实现 1CopyOnWriteArrayList&lt;String&gt; copyOnWriteArrayList = new CopyOnWriteArrayList&lt;&gt;(arrayList); 为什么ArrayList不是线程安全的，具体来说是哪里不安全？✨在高并发添加数据下，ArrayList会暴露三个问题: 部分值为null（我们并没有add null进去） size与我们add的数量不符 索引越界异常 从add源码出发： 12345678public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! // 线程切换点 导致问题3，ab线程都认为不需要扩容 elementData[size] = e; // 线程切换点 导致问题1和2，ab线程在同一个位置进行赋值，导致覆盖之前的赋值 size++; return true;} Set比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同同： HashSet、LinkedHashSet 和 TreeSet 都是 Set 接口的实现类，都能保证元素唯一，并且都不是线程安全的。 异： HashSet、LinkedHashSet 和 TreeSet 的主要区别在于底层数据结构不同。HashSet 的底层数据结构是哈希表（基于 HashMap 实现）。LinkedHashSet 的底层数据结构是链表和哈希表，元素的插入和取出顺序满足 FIFO。TreeSet 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序。 底层数据结构不同又导致这三者的应用场景不同。HashSet 用于不需要保证元素插入和取出顺序的场景，LinkedHashSet 用于保证元素的插入和取出顺序满足 FIFO 的场景，TreeSet 用于支持对元素自定义排序规则的场景。 关于hashCode和equals的处理遵循如下规则：1） 只要重写equals，就必须重写hashCode。2） 因为Set存储的是不重复的对象，依据hashCode和equals进行判断，所以Set存储的对象必须重写这两个方法。3） 如果自定义对象作为Map的键，那么必须重写hashCode和equals。说明：String重写了hashCode和equals方法，所以我们可以非常愉快地使用String对象作为key来使用。 QueueQueue 与 Deque 的区别Queue 是单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循 先进先出（FIFO） 规则。 Deque 是双端队列，在队列的两端均可以插入或删除元素。 Deque 还提供有 push() 和 pop() 等其他方法，可用于模拟栈。 1Deque&lt;Integer&gt; a = new LinkedList&lt;&gt;(); ArrayDeque 与 LinkedList 的区别ArrayDeque 和 LinkedList 都实现了 Deque 接口，两者都具有队列的功能，但两者有什么区别呢？ ArrayDeque 是基于可变长的数组和双指针来实现，而 LinkedList 则通过链表来实现。 ArrayDeque 不支持存储 NULL 数据，但 LinkedList 支持。 ArrayDeque 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 LinkedList 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。 从性能的角度上，选用 ArrayDeque 来实现队列要比 LinkedList 更好。此外，ArrayDeque 也可以用于实现栈。 说一说 PriorityQueue PriorityQueue 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据 PriorityQueue 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。 PriorityQueue 是非线程安全的，且不支持存储 NULL 和 non-comparable 的对象。 PriorityQueue 默认是小顶堆，但可以接收一个 Comparator 作为构造参数，从而来自定义元素优先级的先后。 PriorityQueue 在面试中可能更多的会出现在手撕算法的时候，典型例题包括堆排序、求第 K 大的数、带权图的遍历等，所以需要会熟练使用才行。 Map✨✨HashMap 的底层实现✨JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashcode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法影响性能 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 1234567 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。 TreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 HashMap 的 put过程✨ 根据要添加的键的哈希码计算在数组中的位置（索引）。 检查该位置是否为空（即没有键值对存在） 如果为空，则直接在该位置创建一个新的Entry对象来存储键值对。将要添加的键值对作为该Entry的键和值，并保存在数组的对应位置。将HashMap的修改次数（modCount）加1，以便在进行迭代时发现并发修改。 如果该位置已经存在其他键值对，检查该位置的第一个键值对的哈希码和键是否与要添加的键值对相同？ 如果相同，则表示找到了相同的键，直接将新的值替换旧的值，完成更新操作。 如果第一个键值对的哈希码和键不相同，则需要遍历链表或红黑树来查找是否有相同的键： 对于链表，从头遍历查找是否有相同key，若有直接更新value，若无则将新entry添加到链表头(JKD1.7) 对于红黑树，查找是否有相同key，若有直接更新value，若无则将新entry添加到红黑树中 查看链表长度是否大于8，数组长度是否大于等于64，若是，则将链表转换成红黑树 检查负载因子是否大于0.75，若是则进行扩容 创建一个新的两倍大小的数组。 将旧数组中的键值对重新计算哈希码并分配到新数组中的位置。 更新HashMap的数组引用和阈值参数。 HashMap一般用什么做Key？为啥String适合做Key呢？用 string 做 key，因为 String对象是不可变的，一旦创建就不能被修改，这确保了Key的稳定性。如果Key是可变的，可能会导致hashCode和equals方法的不一致，进而影响HashMap的正确性。 为什么HashMap要用红黑树而不是平衡二叉树？ 平衡二叉树追求的是一种 “完全平衡” 状态：==任何结点的左右子树的高度差不会超过 1==优势是树的结点是很平均分配的。这个要求实在是太严了，导致每次进行插入/删除节点的时候，几乎都会破坏平衡树的第二个规则，进而我们都需要通过左旋和右旋来进行调整，使之再次成为一颗符合要求的平衡树。 红黑树不追求这种完全平衡状态，而是追求一种 “弱平衡” 状态：==整个树最长路径不会超过最短路径的 2 倍==优势是虽然牺牲了一部分查找的性能效率，但是能够换取一部分维持树平衡状态的成本。与平衡树不同的是，红黑树在插入、删除等操作，不会像平衡树那样，频繁着破坏红黑树的规则，所以不需要频繁着调整，这也是我们为什么大多数情况下使用红黑树的原因。 HashMap 为什么线程不安全✨jdk1.7 hashmap的为头插法导致死循环JDK1.7 及之前版本的 HashMap 在多线程环境下扩容操作可能存在死循环问题，这是由于当一个桶位中有多个元素需要进行扩容时，多个线程同时对链表进行操作，头插法可能会导致链表中的节点指向错误的位置，从而形成一个环形链表，进而使得查询元素的操作陷入死循环无法结束。 当两个线程同时put元素时候触发了扩容，需要执行transfer方法： 1234567891011121314151617181920void transfer(Entry[] newTable) { Entry[] src = table; int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) { Entry&lt;K,V&gt; e = src[j]; if (e != null) {//两个线程都先进入if src[j] = null; do { Entry&lt;K,V&gt; next = e.next; // 计算节点在新数组中的下标 int i = indexFor(e.hash, newCapacity); //&nbsp;将旧节点插入到新节点的头部 e.next = newTable[i]; //线程1 这里还没执行 停下 newTable[i] = e; e = next; } while (e != null); } } } 两个线程同时进入if判断后，线程1继续执行头插法 在此时线程1被挂起，执行线程2 此时线程2执行e.next = newTable[i]; 而e是a，newTable[i]是b，从而形成循环 为了解决这个问题，JDK1.8 版本的 HashMap 采用了尾插法而不是头插法来避免链表倒置，使得插入的节点永远都是放在链表的末尾，避免了链表中的环形结构。但是还是不建议在多线程下使用 HashMap，因为多线程下使用 HashMap 还是会存在数据覆盖的问题。并发环境下，推荐使用 ConcurrentHashMap 。 HashMap 的数据丢失JDK1.7 及之前版本，在多线程环境下，HashMap 扩容时会造成死循环和数据丢失的问题。 数据丢失这个在 JDK1.7 和 JDK 1.8 中都存在 情况 1：哈希冲突，导致两个线程同时进行 hash 判断都可以插入，a 线程判断可以插入后挂起，b 线程执行插入，完成后另a 线程继续执行插入导致数据被覆盖 情况 2：多线程同时执行 put 操作，如果计算出来的索引位置是相同的，那会造成前一个 key 被后一个 key 覆盖，从而导致元素的丢失。 HashMap的扩容机制介绍一下✨hashMap默认的负载因子是0.75，即如果hashmap中的元素个数超过了总容量75%，则会触发扩容，扩容分为两个步骤： 第1步是对哈希表长度的扩展（2倍） 第2步是将旧哈希表中的数据放到新的哈希表中。 因为我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。 这样设计的好处： 省去了重新计算hash值的时间， 由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。 HashMap 的长度为什么是 2 的幂次方✨Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。散列值是不能直接拿来用的。 用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。正常来说应该是用hash % n来计算，但是效率太低，所以hashMap使用—— (n - 1) &amp; hash（n 代表数组长度），这就要求n必须是2的幂次方 解释：只有 n 为 2 的幂次方，n-1 用二进制表示时低位才可能都是 1，与 hash 做按位&amp; 才能得到全部低位数据 HashMap 和 Hashtable 的区别线程是否安全 效率 对 Null key 和 Null value 的支持 初始容量大小和每次扩充容量大小的不同: ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小 底层数据结构:JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间 HashMap 和 HashSet 区别如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 HashMap 和 TreeMap 区别相比于HashMap来说 TreeMap 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。 ConcurrentHashMap 和 Hashtable 的区别✨实现线程安全的方式（重要） 在 JDK1.7 的时候，ConcurrentHashMap 对整个桶数组进行了分割分段(Segment，分段锁)，每一把锁只锁容器其中一部分数据（下面有示意图） 到了 JDK1.8 的时候，ConcurrentHashMap 已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并使用 synchronized 、CAS 、node来保证并发安全。 （未发生 hash 冲突时，采用 CAS加入新 node 到数组中；若发生 hash冲突时，采用 synchronized 来添加该节点到链表 or 红黑树） Hashtable 数据结构：Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 线程安全的方式：使用 synchronized 来保证线程安全，锁住整个数组，效率底下。 JDK1.7 的 ConcurrentHashMap 数据结构：JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现 线程安全的方式：在 JDK1.7 的时候，ConcurrentHashMap 对整个桶数组进行了分割分段(Segment，分段锁)，每一把锁只锁容器其中一部分数据（下面有示意图） Segment 数组中的每个元素包含一个 HashEntry 数组，每个 HashEntry 数组属于链表结构。 JDK 1.7 ConcurrentHashMap 分段锁技术将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问，能够实现真正的并发访问。 JDK1.8 的 ConcurrentHashMap 数据结构：JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样，数组+链表/红黑二叉树。 线程安全的方式：到了 JDK1.8 的时候，ConcurrentHashMap 已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并使用 synchronized 、volatile、CAS 、node来保证并发安全。 （未发生 hash 冲突时，采用volatile + CAS加入新 node 到数组中；若发生 hash冲突时，采用 synchronized 来添加该节点到链表 or 红黑树） JDK1.8 的 ConcurrentHashMap 不再是 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。不过，Node 只能用于链表的情况，红黑树的情况需要使用 **TreeNode**。当冲突链表达到一定长度时，链表会转换成红黑树。 已经用了synchronized，为什么还要用CAS呢？在某些操作中使用synchronized，还是使用CAS，主要是根据锁竞争程度来判断的。 当锁竞争程度不大，使用CAS来put 当存在hash碰撞的时候，说明容量不够或者并发度高，采用synchronized来处理hash碰撞效率比CAS高 ConcurrentHashMap用了悲观锁还是乐观锁?悲观锁和乐观锁都有用到。 添加元素时首先会判断容器是否为空： 如果为空则使用 volatile 加 CAS （乐观锁） 来初始化。 如果容器不为空，则根据存储的元素计算该位置是否为空。 如果根据存储的元素计算结果为空，则利用 CAS（乐观锁） 设置该节点； 如果根据存储的元素计算结果不为空，则使用 synchronized（悲观锁） ，然后，遍历桶中的数据，并替换或新增节点到桶中，最后再判断是否需要转为红黑树，这样就能保证并发访问时的线程安全了。 ConcurrentHashMap 为什么 key 和 value 不能为 null？避免二义性： 值没有在集合中 ； 值本身就是 null。 单线程下可以容忍歧义，而多线程下无法容忍 HashMap采用containsKey来避免二义性 123public boolean containsKey(Object key) { return getNode(hash(key), key) != null;} 如果存在key为null的元素（key=null对应的hash值=0），getNode获取到值不为null； 如果不存在key为null的元素，此时hash值=0对应的下标元素为null，即getNode获取到的值为null； ConcurrentHashMap为什么不能解决二义性问题 因为ConcurrentHashMap是线程安全的，一般使用在并发环境下，你一开始get方法获取到null之后，再去调用containsKey方法，没法确保get方法和containsKey方法之间，没有别的线程来捣乱，刚好把你要查询的对象设置了进去或者删除掉了。","link":"/2023/12/12/java/%E9%9B%86%E5%90%88/"},{"title":"持久化kv存储-cellar","text":"TairTair(Taobao Pair)是淘宝开发的分布式Key-Value存储引擎 Tair采用可插拔存储引擎设计，存储引擎可以很方便的替换： 非持久化：分布式缓存使用 Memcached（mdb）、Redis（rdb） 持久化：LevelDB（ldb） 一个Tair集群主要包括client、Config server和Dataserver 三个不同的应用。 Client在初始化时，从Config server处获取数据的分布信息，根据分布信息和相应的Data server交互完成用户的请求。 Config Server通过和Data Server的心跳（HeartBeat）维护集群中可用的节点，并根据可用的节点，构建数据的在集群中的分布信息 Data server负责数据的存储，并按照Config server的指示完成数据的复制和迁移工作。 Cellar cellar是美团基于Tair自研的。 跟Tair的不同 加入ob，实时与中心节点的 Master 同步最新的路由表，客户端的路由表都是从 OB 去拿。 这样做的好处主要有两点，第一，把大量的业务客户端跟集群的大脑 Master 做了天然的隔离，防止路由表请求影响集群的管理。第二，因为 OB 只供路由表查询，不参与集群的管理，所以它可以进行水平扩展，极大地提升了我们路由表的查询能力。 引入了 ZooKeeper 做分布式仲裁，解决Master、Slave 在网络分割情况下的“脑裂”问题，并且通过把集群的元数据存储到 ZooKeeper，我们保证了元数据的高可靠。 采取的存储引擎 快的原因：将随机写变为顺序写 内存一般使用跳表这种查找和插入都很方便的有序数据结构，数据写入 MemTable 中其实就是往 SkipList 中插入一条数据。需要在内存中将数据进行排序的原因是便于查找以及 compaction 文件时需要进行归并排序，所以需要 sstable 有序。 读取的话采用暴力二分查找，顺序为memtable-&gt;immemtable-&gt;每一层 存储引擎(三):LSM-tree类存储引擎 - 知乎 (zhihu.com) 跨地域容灾 热点 Key 实现给一些节点加入热点区域 如果客户端有一个写操作到了 A 节点，A 节点处理完成后，会根据实时的热点统计结果判断写入的 Key 是否为热点。如果这个 Key 是一个热点，那么它会在做集群内复制的同时，还会把这个数据复制有热点区域的节点，也就是图中的 C、D 节点。同时，存储节点在返回结果给客户端时，会告诉客户端，这个 Key 是热点，这时客户端内会缓存这个热点 Key。当客户端有这个 Key 的读请求时，它就会直接去热点区域做数据的读取。 优点 做到只对热点数据做扩容 通过这种实时的热点数据复制，我们很好地解决了类似客户端缓存热点 KV 方案造成的一致性问题。","link":"/2024/08/03/%E4%B8%AD%E9%97%B4%E4%BB%B6/cellar/"},{"title":"Redis","text":"Redis优缺点优点 读写性能好 支持多个数据类型 数据持久化 支持事务 缺点 成本高 数据安全和持久性：虽然Redis支持持久化，但如果在主节点宕机前数据未能及时同步到从节点，可能会导致数据丢失。 内存容量限制 复杂度高：在分布式系统中，维护Redis集群可能会增加系统的复杂性，尤其是在处理数据分片和复制时 Redis 线程模型Redis 采用单线程为什么还这么快✨？ 大部分操作在内存中处理，并且采用了高效的数据结构。因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU。 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。 采用了I/O 多路复用机制处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流， select/epoll 机制。 Redis 单线程模式是怎样的？Redis 单线程指的是「接收客户端请求-&gt;解析请求 -&gt;进行数据读写等操作-&gt;发送数据给客户端」这个过程是由一个线程（主线程）来完成的，这也是我们常说 Redis 是单线程的原因。 Redis 多线程的使用Redis 程序并不是单线程的，Redis 在启动的时候，是会启动后台线程的： Redis 在 2.6 版本，会启动 2 个后台线程，分别处理关闭文件、AOF 刷盘这两个任务； Redis 在 4.0 版本之后，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程，例如删除大 key 使用 unlink 而不是 del 之所以 Redis 为「关闭文件、AOF 刷盘、释放内存」这些任务创建单独的线程来处理，是因为这些任务的操作都是很耗时的，如果把这些任务都放在主线程来处理，那么 Redis 主线程就很容易发生阻塞，这样就无法处理后续的请求了。 后台线程相当于一个消费者，生产者把耗时任务丢到任务队列中，消费者不停轮询这个队列，拿出任务就去执行对应的方法即可。 Redis 在 6.0 版本之后，采用了多个 I/O 线程来处理网络请求，这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。 默认开启 3 个 I/O 多线程 Redis 如何实现 IO 多路复用以 epoll 为例，多个客户端连接服务端时，Redis 将客户端的 socket 对应的文件描述符fd注册到 epoll对象中的红黑树中（ epoll_ctl()函数），然后epoll 同时监听多个fd是否有数据到来，若有则回调 fd 绑定的事件处理器进行处理，避免服务端一直等待某客户端给数据的情况 Redis 的网络模型是怎样的？Redis 6.0 版本之前，是用的是单Reactor单线程的模式 缺点： 无法充分利用 多核 CPU 的性能 Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，如果业务处理耗时比较长，那么就造成响应的延迟； 到 Redis 6.0 之后，就将网络IO的处理改成多线程的方式了，目的是为了这是因为随着网络硬件的性能提升，Redis 的性能瓶颈有时会出现在网络 I/O 的处理上。 所以为了提高网络 I/O 的并行度，Redis 6.0 对于网络 I/O 采用多线程来处理。但是对于命令的执行，Redis 仍然使用单线程来处理 Redis 数据结构Redis 数据类型以及使用场景分别是什么？常见的有五种数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）、Zset（有序集合）。 Redis 五种数据类型的应用场景： String 类型的应用场景 需要存储常规数据的场景 举例：缓存 Session、Token、图片地址、序列化后的对象(相比较于 Hash 存储更节省内存)。 相关命令：SET、GET。 需要计数的场景 举例：用户单位时间的请求数（简单限流可以用到）、页面单位时间的访问数。 相关命令：SET、GET、 INCR、DECR 。 分布式锁 利用 SETNX key value 命令可以实现一个最简易的分布式锁（存在一些缺陷，通常不建议这样实现分布式锁）。 List 类型的应用场景： 信息流展示 举例：最新文章、最新动态。 相关命令：LPUSH、LRANGE。 消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。 Hash 类型： 对象数据存储场景 举例：用户信息、商品信息、文章信息、购物车信息。 相关命令：HSET （设置单个字段的值）、HMSET（设置多个字段的值）、HGET（获取单个字段的值）、HMGET（获取多个字段的值）。 Set 类型： 需要存放的数据不能重复的场景 举例：网站 UV 统计（数据量巨大的场景还是 HyperLogLog更适合一些）、文章点赞、动态点赞等场景。 相关命令：SCARD（获取集合数量） 。 需要获取多个数据源交集、并集和差集的场景 举例：共同好友(交集)、共同粉丝(交集)、共同关注(交集)、好友推荐（差集）、音乐推荐（差集）、订阅号推荐（差集+交集） 等场景。 相关命令：SINTER（交集）、SINTERSTORE （交集）、SUNION （并集）、SUNIONSTORE（并集）、SDIFF（差集）、SDIFFSTORE （差集）。 需要随机获取数据源中的元素的场景 举例：抽奖系统、随机点名等场景。 相关命令：SPOP（随机获取集合中的元素并移除，适合不允许重复中奖的场景）、SRANDMEMBER（随机获取集合中的元素，适合允许重复中奖的场景）。 Zset 类型： 需要随机获取数据源中的元素根据某个权重进行排序的场景 举例：各种排行榜比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等。 相关命令：ZRANGE (从小到大排序)、 ZREVRANGE （从大到小排序）、ZREVRANK (指定元素排名)。 特殊数据类型 BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等； HyperLogLog（2.8 版新增）： 数量量巨大（百万、千万级别以上）的计数场景 举例：热门网站每日/每周/每月访问 ip 数统计、热门帖子 uv 统计、 相关命令：PFADD、PFCOUNT 。 GEO（3.2 版新增）： 需要管理使用地理空间数据的场景 举例：附近的人，滴滴叫车。 相关命令: GEOADD、GEORADIUS、GEORADIUSBYMEMBER 。 Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。 常见的 Redis 数据类型 String 类型内部实现String 类型的底层的数据结构实现主要是 SDS（简单动态字符串）。 为什么不用 c 语言中的字符串 **SDS 通过成员变量len获取字符串长度的时间复杂度是 O(1)**。 C 语言的字符串长度获取 strlen 函数，需要通过遍历的方式来统计字符串长度，时间复杂度是 O（N）； SDS 不仅可以保存文本数据，还可以保存二进制数据。 因为 SDS 使用 len 属性的值而不是“\\0”来判断字符串是否结束。所以 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。 Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求（alloc-len），如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。 List 类型内部实现List 类型的底层数据结构是由双向链表或压缩列表实现的： 如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用压缩列表作为 List 类型的底层数据结构； 如果列表的元素不满足上面的条件，Redis 会使用双向链表作为 List 类型的底层数据结构； 但是在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表。 Hash 类型内部实现✨Hash 类型的底层数据结构是由压缩列表或哈希表实现的： 如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用压缩列表作为 Hash 类型的底层数据结构； 如果哈希类型元素不满足上面条件，Redis 会使用哈希表作为 Hash 类型的底层数据结构。 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 实际redis 定义的dict结构下有两个哈希表dictht，一个是用来存数据的，另一个是用来 rehash 的。每个哈希表是一个数组，每一个元素是指向「哈希表节点」的指针。 Hash 扩容 ✨rehash 的触发条件 若负载因子&gt;=1，若没有执行bgsave 或者 bgrewriteaof，才会立即进行 rehash 若负载因子&gt;=5，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作。 rehashdict 下的两个 dictht 哈希表，正常情况下只会往「哈希表 1」中插入数据，「哈希表 2」是不会分配空间。 当触发了 rehash 条件时： 给「哈希表 2」分配比「哈希表 1」 大一倍的空间 将「哈希表 1 」的数据迁移到「哈希表 2」 中； 迁移完成后，「哈希表 1 」的空间会被释放，并把「哈希表 2」 设置为「哈希表 1」，然后在「哈希表 2」 新创建一个空白的哈希表，为下次 rehash 做准备。 缺点：当数据量特别大时，会造成redis 长时间的阻塞 渐进 rehash数据从一次性迁移完成，变成分批迁移 当触发了 rehash 条件时： 给「哈希表 2」分配比「哈希表 1」 大一倍的空间 在 rehash 期间，每次对于哈希表元素的新增、修改和查找操作时，redis除了执行对应操作之外，还会将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上； 随着对哈希表的请求操作的增加，最后完成数据的迁移 在 rehash 时需要满足： 查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。 新增一个 key-value 时，会被保存到「哈希表 2 」里面，而「哈希表 1」 则不再进行任何添加操作，这样保证了「哈希表 1 」的 key-value 数量只会减少，随着 rehash 操作的完成，最终「哈希表 1 」就会变成空表。 哈希表扩容的时候，有读请求怎么查？查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。 Set 类型内部实现Set 类型的底层数据结构是由哈希表或整数集合实现的： 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Redis 会使用整数集合作为 Set 类型的底层数据结构； 如果集合中的元素不满足上面条件，则 Redis 使用哈希表作为 Set 类型的底层数据结构。 整数集合 本质是一块连续的内存区域 12345678typedef struct intset { //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[];} intset; 根据 encoding 来决定 contents[]的真正类型 encoding 有三种 int16_t、 int32_t、 int64_t 升级操作 假如一开始都是存 int16，然后加入一个只有 int32 才能存的数，那么就会触发升级操作。 流程包括： 数组扩充 将元素从后向前转换类型放在对应位置 好处： 节省内存资源 缺点： 但不支持降级操作 ZSet 类型内部实现✨Zset 类型的底层数据结构是由压缩列表或跳表实现的： 当有序集合对象同时满足以下两个条件时，使用 ziplist： ZSet 保存的键值对数量少于 128 个； 每个元素的长度小于 64 字节。 如果不满足上述两个条件，那么使用 skiplist 。 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 底层数据结构ziplist✨压缩列表是 Redis 为了节约内存而开发的，它是由连续内存块组成的顺序型数据结构，有点类似于数组。 压缩列表在表头有三个字段： zlbytes，记录整个压缩列表占用的内存字节数； zltail，记录压缩列表「尾部」节点距离起始地址由多少字节，也就是列表尾的偏移量； zllen，记录压缩列表包含的节点数量； zlend，标记压缩列表的结束点，固定值 0xFF（十进制255）。 压缩列表节点包含三部分内容： prevlen，记录了「前一个节点」的长度，目的是为了实现从后向前遍历； encoding，记录了当前节点实际数据的「类型和长度」，类型主要有两种：字符串和整数。 data，记录了当前节点的实际数据，类型和长度都由 encoding 决定； 当我们往压缩列表中插入数据时，压缩列表就会根据数据类型是字符串还是整数，以及数据的大小，会使用不同空间大小的 prevlen 和 encoding 这两个元素里保存的信息，这种根据数据大小和类型进行不同的空间大小分配的设计思想，正是 Redis 为了节省内存而采用的。 缺陷：存在连锁更新问题。多个连续的长度为 250～253 的元素，当插入一个长度大于254 （1111 1111）的元素时，会让后面的元素的 prevlen 从之前的 1 字节变成 5 字节，而不断发生内存重新分配的问题。 quicklistquicklist 的结构体跟链表的结构体类似，都包含了表头和表尾，区别在于 quicklist 的节点是 quicklistNode。quicklistNode 结构体里包含了前一个节点和下一个节点指针，这样每个 quicklistNode 形成了一个双向链表。但是链表节点的元素不再是单纯保存元素值，而是保存了一个压缩列表。 quicklist 会控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来规避潜在的连锁更新的风险，但是这并没有完全解决连锁更新的问题。 listpack✨quicklist 虽然通过控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来减少连锁更新带来的性能影响，但是并没有完全解决连锁更新的问题。 因为 quicklistNode 还是用了压缩列表来保存元素，压缩列表连锁更新的问题，来源于它的结构设计，所以要想彻底解决这个问题，需要设计一个新的数据结构。 于是，Redis 在 5.0 新设计一个数据结构叫 listpack，目的是替代压缩列表，它最大特点是 listpack 中每个节点不再包含前一个节点的长度了，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患。 listpack 头包含两个属性，分别记录了 listpack 总字节数和元素数量，然后 listpack 末尾也有个结尾标识。 Listpack entry主要包含三个方面内容： encoding，定义该元素的编码类型，会对不同长度的整数和字符串进行编码； data，实际存放的数据； len，encoding+data的总长度； 可以看到，listpack 没有压缩列表中记录前一个节点长度的字段了，listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题。 为什么用跳表实现有序集合zset结构体中有两个数据结构：跳表、哈希表。好处是既能进行高效的范围查询，又能进行高效的单点查询 或者是在数据量少的时候用压缩列表 跳表的建立跳表可以理解为在原始链表基础上，建立「多层」有序链表，将增删改查的时间复杂度变为O(log n)。 12345678typedef struct zskiplist { //跳表的头尾节点，便于在O(1)时间复杂度内访问跳表的头节点和尾节点； struct zskiplistNode *header, *tail; //跳表的长度，便于在O(1)时间复杂度获取跳表节点的数量； unsigned long length; //跳表的最大层数，便于在O(1)时间复杂度获取跳表中层高最大的那个节点的层数量； int level;} zskiplist; 1234567891011121314typedef struct zskiplistNode { //Zset 对象的元素值 sds ele; //元素权重值 double score; //后向指针 struct zskiplistNode *backward; //节点的level数组，保存每层上的前向指针和跨度 struct zskiplistLevel { struct zskiplistNode *forward; unsigned long span; } level[];} zskiplistNode; 跳表节点查询查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，判断是走这一层下一个节点还是走当前节点的下一层的下一个节点，共有两个判断条件： 如果下一个节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。 如果下一个节点的权重「等于」要查找的权重时，并且下一个节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。 如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。 跳表节点层数设置理想情况是每一层索引是下一层元素个数的二分之一。 Redis 则采用一种巧妙的方法是，跳表在创建节点的时候，随机生成每个节点的层数，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。 具体的做法是，跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。 这样的做法，相当于每增加一层的概率不超过 25%，层数越高，概率越低，层高最大限制是 64。 虽然我前面讲解跳表的时候，图中的跳表的「头节点」都是 3 层高，但是其实如果层高最大限制是 64，那么在创建跳表「头节点」的时候，就会直接创建 64 层高的头节点。 和其余三种数据结构的比较平衡树 vs 跳表平衡条件必须满足（所有节点的左右子树高度差不超过 1，即平衡因子为范围为 [-1,1]）。平衡树的插入、删除和查询的时间复杂度和跳表一样都是 **O(log n)**。 对于范围查询来说，它也可以通过中序遍历的方式达到和跳表一样的效果。但是它的每一次插入或者删除操作都需要保证整颗树左右节点的绝对平衡，只要不平衡就要通过旋转操作来保持平衡，这个过程是比较耗时的。 红黑树 vs 跳表红黑树查询性能略微逊色于 AVL 树，但插入和删除效率更高。红黑树的插入、删除和查询的时间复杂度和跳表一样都是 **O(log n)**。 红黑树是一个黑平衡树，即从任意节点到另外一个叶子节点，它所经过的黑节点是一样的。当对它进行插入操作时，需要通过旋转和染色（红黑变换）来保证黑平衡。不过，相较于 AVL 树为了维持平衡的开销要小一些。 相比较于红黑树来说，跳表的实现也更简单一些。并且，按照区间来查找数据这个操作，红黑树的效率没有跳表高。 红黑树（Red-Black Tree）是一种自平衡的二叉查找树，它在每个节点上增加了一个额外的属性，即节点的颜色，可以是红色或黑色。红黑树具有以下特点： 节点颜色：每个节点要么是红色，要么是黑色。 根节点和叶子节点：根节点是黑色的，叶子节点（NIL节点）是黑色的。 颜色约束：红色节点的子节点必须是黑色的。换句话说，不能有两个相邻的红色节点，即红色节点不能连续存在。 黑色高度约束：从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点，这个数目称为黑色高度。 平衡性：保证了树的黑色高度相对平衡，确保了最长路径不超过最短路径的两倍。 通过这些约束，红黑树能够在插入和删除节点时自动调整结构，以保持这些约束，从而保证了树的平衡性，使得查找、插入和删除等操作的最坏情况时间复杂度都是 O(log n)。 B+树 vs 跳表✨内存数据库它不可能存储大量的数据，所以对于索引不需要通过 B+树这种方式进行维护。 使用跳表实现 zset 时相较前者来说更简单一些，在进行插入时只需通过索引将数据插入到链表中合适的位置再随机维护一定高度的索引即可，也不需要像 B+树那样插入时发现失衡时还需要对节点分裂与合并。 总结： 从内存占用上来比较，跳表比平衡树更灵活一些。平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。 在做范围查找的时候，跳表比平衡树操作要简单。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。 从算法实现难度上来比较，跳表比平衡树要简单得多。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速 Redis 持久化Redis 共有三种数据持久化的方式： AOF （append only file）日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里； RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘； 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点； AOF 日志是如何实现的？Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。 「$3 set」表示这部分有 3 个字节，也就是「set」命令这个字符串的长度。 为什么先执行命令，再把数据写入日志呢？好处： 避免额外的检查开销（语法检查） 不会阻塞当前写操作命令的执行 风险： 数据可能会丢失 可能阻塞后续操作 AOF 写回策略有几种？✨Redis 写入 AOF 日志的过程 redis 执行完写操作后，将命令追加到 server.aof_buf 缓冲区 然后通过 write()系统调用将缓冲区的内容写到内核缓冲区page cache，等待内核将数据写入硬盘 具体内核缓冲区什么时候写入硬盘有三种写回策略 这三种策略只是在控制 fsync() 函数的调用时机。 AOF 日志过大，会触发什么机制？✨Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。 AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。 为什么重写 AOF 的时候，不直接复用现有的 AOF 文件，而是先写到新的 AOF 文件再覆盖过去？如果 AOF 重写过程中失败了，现有的 AOF 文件就会造成污染，可能无法用于恢复使用。 重写 AOF 日志的过程是怎样的？✨redis 的重写 AOF 过程是由后台子进程 bgrewriteaof 来完成的 好处： 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生「写时复制（Copy On Write）」，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 写时复制： 主进程在通过 fork 系统调用生成子进程时，操作系统会把主进程的「页表」复制一份给子进程，这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。 这样一来，子进程就共享了父进程的物理内存数据了，这样能够节约物理内存资源，页表对应的页表项的属性会标记该物理内存的权限为只读。 不过，当父进程或者子进程在向这个内存发起写操作时，CPU 就会触发写保护中断，这个写保护中断是由于违反权限导致的，然后操作系统会在「写保护中断处理函数」里进行物理内存的复制，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写，最后才会对内存进行写操作，这个过程被称为「**写时复制(*Copy On Write*)**」。 写时复制顾名思义，在发生写操作的时候，操作系统才会去复制物理内存，这样是为了防止 fork 创建子进程时，由于物理内存数据的复制时间过长而导致父进程长时间阻塞的问题。 但是重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？ 为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」，这样一来可以保证： 现有的 AOF 功能会继续执行，即使在 AOF 重写期间发生停机，也不会有任何数据丢失。 所有对数据库进行修改的命令都会被记录到 AOF 重写缓存中。 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作： 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 RDB 快照是如何实现的呢？AOF 缺点：因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。 RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。 因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。 RDB 做快照时会阻塞线程吗？Redis 提供了两个命令来生成 RDB 文件，分别是 save 和bgsave，他们的区别就在于是否在「主线程」里执行： 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程； 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞； RDB 快照缺点 频率难把握：Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。 宕机会丢失数据：在 RDB 执行快照的时候，主线程对内存的修改只能到下一次 RDB 的时候才能持久化。假如在完成本次 RDB 快照后 redis 宕机，就会丢失在快照期间修改的数据 另外，写时复制的时候会出现这么个极端的情况。 在 Redis 执行 RDB 持久化期间，刚 fork 时，主进程和子进程共享同一物理内存，但是途中主进程处理了写操作，修改了共享内存，于是当前被修改的数据的物理内存就会被复制一份。 那么极端情况下，如果所有的共享内存都被修改，则此时的内存占用是原先的 2 倍。 所以，针对写操作多的场景，我们要留意下快照过程中内存的变化，防止内存被占满了。 RDB 在执行快照的时候，数据能修改吗？✨可以的，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）。 执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。 如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。 为什么会有混合持久化？RDB 优点是数据恢复速度快，但是快照的频率不好把握。 AOF 优点是丢失数据少，但是数据恢复不快。 AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。 这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。 加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。 混合持久化优点： 混合持久化工作在 AOF 日志重写过程，既保证了 Redis 重启速度，又降低数据丢失风险。 混合持久化缺点： AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性差； 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。 Redis 集群/高可用主从复制主从复制是 Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是「读写分离」的方式。 主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。 也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。 主从复制共有三种模式：全量复制、基于长连接的命令传播、增量复制。 主从服务器第一次同步的时候，就是采用全量复制，此时主服务器会两个耗时的地方，分别是生成 RDB 文件和传输 RDB 文件。 第一次同步完成后，主从服务器都会维护着一个长连接，主服务器在接收到写操作命令后，就会通过这个连接将写命令传播给从服务器，来保证主从服务器的数据一致性。 如果遇到网络断开，增量复制就可以上场了，不过这个还跟 repl_backlog_size 这个大小有关系。 如果它配置的过小，主从服务器网络恢复时，可能发生「从服务器」想读的数据已经被覆盖了，那么这时就会导致主服务器采用全量复制的方式。所以为了避免这种情况的频繁发生，要调大这个参数的值，以降低主从服务器断开后全量同步的概率。 全量复制/全量同步✨ 第一阶段是建立链接、协商同步【从服务器发送psync命令请求同步，主服务器发送 runID 和复制进度 offset】； 第二阶段是主服务器同步数据给从服务器【bgsave 生成 RDB】； 第三阶段是主服务器发送新写操作命令给从服务器【将第二阶段期间新增的写命令存放在 replication buffer，然后发送给从服务器执行】。 基于长连接的命令传播 增量复制✨主从服务器在完成第一次同步后，就会基于长连接进行命令传播。 若网络中断则会使用增量复制重新同步。 主要有三个步骤： 从服务器在恢复网络后，会发送 psync 命令给主服务器，此时的 psync 命令里的 offset 参数不是 -1； 主服务器收到该命令后，然后用 CONTINUE 响应命令告诉从服务器接下来采用增量复制的方式同步数据； 然后主服务将主从服务器断线期间，所执行的写命令发送给从服务器，然后从服务器执行这些命令 主服务器怎么知道要将哪些增量数据发送给从服务器呢？ 答案藏在这两个东西里： replication_backlog_buffer，是一个「环形」缓冲区，用于主从服务器断连后，从中找到差异的数据； replication offset，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 master_repl_offset 来记录自己「写」到的位置，从服务器使用 slave_repl_offset 来记录自己「读」到的位置。 怎么判断 Redis 某个节点是否正常工作？基本都是通过互相的 ping-pong 心态检测机制，如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。 Redis 主从节点发送的心态间隔是不一样的，而且作用也有一点区别： Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数repl-ping-slave-period控制发送频率。 Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量，目的是为了： 实时监测主从节点网络状态； 上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据。 主从复制架构中，过期key如何处理？主节点处理了一个key或者通过淘汰算法淘汰了一个key，这个时间主节点模拟一条del命令发送给从节点，从节点收到该命令后，就进行删除key的操作。 Redis 是同步复制还是异步复制？Redis 主节点每次收到写命令之后，先写到内部的缓冲区，然后异步发送给从节点。 主从复制中replication buffer 、repl backlog buffer有什么区别？✨ 出现的阶段不一样： repl backlog buffer 是在增量复制阶段出现，一个主节点只分配一个 repl backlog buffer； replication buffer 是在全量复制阶段和增量复制阶段都会出现，主节点会给每个新连接的从节点，分配一个 replication buffer； 这两个 Buffer 都有大小限制的，当缓冲区满了之后，发生的事情不一样： 当 repl backlog buffer 满了，因为是环形结构，会直接覆盖起始位置数据; 当 replication buffer 满了，会导致连接断开，删除缓存，从节点重新连接，重新开始全量复制。 哨兵模式为什么要有哨兵机制在使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复。 为了解决这个问题，Redis 增加了哨兵模式（Redis Sentinel），因为哨兵模式做到了可以监控主从服务器，并且提供主从节点故障转移的功能。 哨兵机制如何工作✨如何判断主节点故障—监控 如果主节点或者从节点没有在规定的时间内响应哨兵的 PING 命令，哨兵就会将它们标记为「主观下线」。 当一个哨兵判断主节点为「主观下线」后，就会向其他哨兵发起命令，其他哨兵收到这个命令后，就会根据自身和主节点的网络状况，做出赞成投票或者拒绝投票的响应来判定主节点是否「客观下线」。 当这个哨兵的赞同票数达到哨兵配置文件中的 quorum 配置项设定的值后，这时主节点就会被该哨兵标记为「客观下线」。 （quorum 的值一般设置为哨兵个数的二分之一加 1，例如 3 个哨兵就设置 2） 由哪个哨兵进行主从故障转移如果需要从redis集群选举一个节点为主节点，首先需要从Sentinel集群中选举一个Sentinel节点作为Leader。 成为Leader候选人的条件是判断主节点客观下线 候选人成为Leader条件：如果一个Sentinel节点获得的选举票数达到Leader最低票数(quorum和Sentinel节点数/2+1的最大值)，则该Sentinel节点选举为Leader；否则重新进行选举。 这时候有的同学就会问了，如果某个时间点，刚好有两个哨兵节点判断到主节点为客观下线，那这时不就有两个候选者了？这时该如何决定谁是 Leader 呢？ 每位候选者都会先给自己投一票，然后向其他哨兵发起投票请求。如果投票者先收到「候选者 A」的投票请求，就会先投票给它，如果投票者用完投票机会后，收到「候选者 B」的投票请求后，就会拒绝投票。这时，候选者 A 先满足了上面的那两个条件，所以「候选者 A」就会被选举为 Leader。 主从故障转移的过程是怎样的—选主当Sentinel集群选举出Sentinel Leader后，由Sentinel Leader从redis从节点中选择一个redis节点作为主节点： 过滤故障的节点 选择优先级slave-priority最大的从节点作为主节点，如不存在则继续 选择复制偏移量（数据写入量的字节，记录写了多少数据。主服务器会把偏移量同步给从服务器，当主从的偏移量一致，则数据是完全同步）最大的从节点作为主节点，如不存在则继续 选择runid（redis每次启动的时候生成随机的runid作为redis的标识）最小的从节点作为主节点 Redis 的发布者/订阅者机制—通知客户端和哨兵建立连接后，客户端会订阅哨兵提供的频道。主从切换完成后，哨兵就会向 +switch-master 频道发布新主节点的 IP 地址和端口的消息，这个时候客户端就可以收到这条信息，然后用这里面的新主节点的 IP 地址和端口进行通信了。 Redis Cluster切片集群模式当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 Redis 切片集群（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。 **Redis Cluster 是如何分片的?**✨Redis Cluster 方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 【2^14】个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步： 根据键值对的 key，按照 CRC16 算法 (opens new window)计算一个 16 bit 的值。 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。 接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案： 平均分配： 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。 手动分配： 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。 为了方便你的理解，我通过一张图来解释数据、哈希槽，以及节点三者的映射分布关系。 上图中的切片集群一共有 2 个节点，假设有 4 个哈希槽（Slot 0～Slot 3）时，我们就可以通过命令手动分配哈希槽，比如节点 1 保存哈希槽 0 和 1，节点 2 保存哈希槽 2 和 3。 12redis-cli -h 192.168.1.10 –p 6379 cluster addslots 0,1redis-cli -h 192.168.1.11 –p 6379 cluster addslots 2,3 然后在集群运行的过程中，key1 和 key2 计算完 CRC16 值后，对哈希槽总个数 4 进行取模，再根据各自的模数结果，就可以被映射到哈希槽 1（对应节点1） 和 哈希槽 2（对应节点2）。 需要注意的是，在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。 一个最基本的 Redis Cluster 架构是怎样的?✨为了保证高可用，Redis Cluster 至少需要 3 个 master 以及 3 个 slave，也就是说每个 master 必须有 1个 slave。master 和 slave 之间做主从复制，slave 会实时同步 master 上的数据。 不同于普通的 Redis 主从架构，这里的 slave 不对外提供读服务，主要用来保障 master 的高可用，当master 出现故障的时候替代它。 如果 master 有多个 slave 的话，Redis Cluster 中的其他节点会从这个 master 的所有 slave 中选出一 个替代 master 继续提供服务。Redis Cluster 总是希望数据最完整的 slave 被提升为新的 master。 **Redis Cluster 是去中心化的(各个节点基于 Gossip 进行通信)**，任何一个 master 出现故障，其它的 master 节点不受影响，因为 key 找的是哈希槽而不是 Redis 节点。不过，Redis Cluster 至少要保证宕 机的 master 有一个 slave 可用。 Gossip是什么Gossip协议是一个通信协议，一种传播消息的方式，灵感来自于：瘟疫、社交网络等。使用Gossip协议的有：Redis Cluster、Consul、Apache Cassandra等。 六度分隔理论说到社交网络，就不得不提著名的六度分隔理论。1967年，哈佛大学的心理学教授Stanley Milgram想要描绘一个连结人与社区的人际连系网。做过一次连锁信实验，结果发现了“六度分隔”现象。简单地说：“你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过六个人你就能够认识任何一个陌生人。 数学解释该理论：若每个人平均认识260人，其六度就是260↑6 =1,188,137,600,000。消除一些节点重复，那也几乎覆盖了整个地球人口若干多多倍，这也是Gossip协议的雏形。 原理Gossip协议基本思想就是：一个节点想要分享一些信息给网络中的其他的一些节点。于是，它周期性的随机选择一些节点，并把信息传递给这些节点。这些收到信息的节点接下来会做同样的事情，即把这些信息传递给其他一些随机选择的节点。一般而言，信息会周期性的传递给N个目标节点，而不只是一个。这个N被称为fanout（这个单词的本意是扇出）。 用途Gossip协议的主要用途就是信息传播和扩散：即把一些发生的事件传播到全世界。它们也被用于数据库复制，信息扩散，集群成员身份确认，故障探测等。 基于Gossip协议的一些有名的系统：Apache Cassandra，Redis（Cluster模式），Consul等。 Redis Cluster 扩容和缩容本质是进行重新分片，动态迁移哈希槽。为了保证 Redis Cluster 在扩容和缩容期间依然能够对外正常提供服务，Redis Cluster 提供了重定向机制，两种不同的类型: ASK 重定向 ASK 重定向并不会同步更新客户端缓存的哈希槽分配信息 MOVED 重定向。 如果客户端请求的 key 对应的哈希槽应该迁移完成的话，就会返回 -MOVED 重定向错误，告知客户端当 前哈希槽是由哪个节点负责，客户端向目标节点发送请求并更新缓存的哈希槽分配信息。 Redis Cluster中的节点是怎么进行通信的?Redis Cluster 中的各个节点基于 Gossip 协议 来进行通信共享信息，每个 Redis 节点都维护了一份集群的状态信息。 Redis Cluster 相当于是内置了 Sentinel 机制，Redis Cluster 内部的各个 Redis 节点通过 Gossip 协议互相探测健康状态，在故障时可 以自动切换。 集群脑裂导致数据丢失怎么办？什么是脑裂？总结：由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。 解决方案当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。 原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了。 等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。 Redis 过期删除与内存淘汰Redis 的过期删除策略Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。 每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个过期字典（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。 当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中： 如果不在，则正常读取键值； 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。 Redis 使用的过期删除策略是「惰性删除+定期删除」这两种策略配和使用。 惰性删除策略✨惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。 惰性删除策略的优点： 因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。 惰性删除策略的缺点： 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。 定期删除策略✨定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。 Redis 的定期删除的流程： 默认每秒执行10次检查（可通过 Redis 的配置文件 redis.conf 进行配置） 从过期字典中随机抽取 20 个 key； 检查这 20 个 key 是否过期，并删除已过期的 key； 如果本轮检查的已过期 key 的数量，超过 5 个，也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。 可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。 定期删除策略的优点： 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。 定期删除策略的缺点： 难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。 redis的缓存的失败会不会立即删除✨可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。 那为什么我不过期立即删除？✨过期key太多的情况下，删除过期key会造成需要占用很大的CPU时间，在内存不紧张单CPU时间紧张的情况下，立即删除过期key并不会会对服务器的响应时间和吞吐量造成影响 Redis 持久化时，对过期键会如何处理的？Redis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File），下面我们分别来看过期键在这两种格式中的呈现状态。 RDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。 RDB 文件生成阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键「不会」被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。 RDB 加载阶段：RDB 加载阶段时，要看服务器是主服务器还是从服务器，分别对应以下两种情况： 如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中。所以过期键不会对载入 RDB 文件的主服务器造成影响； 如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。 AOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。 AOF 文件写入阶段：当 Redis 以 AOF 模式持久化时，如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值。 AOF 重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响。 Redis 主从模式中，对过期键会如何处理？当 Redis 运行在主从模式下时，从库不会进行过期扫描，从库对过期的处理是被动的。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。 从库的过期键处理依靠主服务器控制，主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key。 Redis 内存淘汰策略？虽然redis的确是不断的删除一些过期数据，但是很多没有设置过期时间的数据也会越来越多，那么redis内存不够用的时候是怎么处理的呢？这里我们就会谈到淘汰策略。 ==当redis的内存超过最大允许的内存之后，Redis会触发内存淘汰策略，删除一些不常用的数据，以保证redis的正常运行== Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。 不进行数据淘汰的策略noeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。 进行数据淘汰的策略✨针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 在设置了过期时间的数据中进行淘汰： volatile-random：随机淘汰设置了过期时间的任意键值； volatile-ttl：优先淘汰更早过期的键值。 volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值； volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值； 在所有数据范围内进行淘汰： allkeys-random：随机淘汰任意键值; allkeys-lru：淘汰整个键值中最久未使用的键值； allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。 内存淘汰策略可以通过配置文件来修改，redis.conf对应的配置项是maxmemory-policy 修改对应的值就行，默认是noeviction Redis 缓存缓存读写策略有哪几种？Cache Aside Pattern（旁路缓存模式）✨写： 先更新 db 然后直接删除 cache 。 读： 从 cache 中读取数据，读取到就直接返回 cache 中读取不到的话，就从 db 中读取数据返回 再把数据放到 cache 中。 问题： 在写数据的过程中，可以先删除 cache ，后更新 db 么？ redis 速度要比 db 快，删 cache 和更新 db 中间可能会穿插读操作，就会引入脏数据 假如使用了该方案可以用双删解决 在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？ 发生情况概率很小，redis 速度要比 db 快。在缓存没有数据情况下，从 db 读数据和更新 cache 中穿插更新 db 操作 Cache Aside 策略适合读多写少的场景，不适合写多的场景 Cache Aside Pattern 的缺陷 缺陷 1：首次请求数据一定不在 cache 的问题 解决办法：可以将热点数据可以提前放入 cache 中。 缺陷 2：写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率 。 解决办法：数据库和缓存数据强一致（加锁同步更新） Read/Write Through Pattern（读写穿透）写（Write Through）： 先查 cache，cache 中不存在，直接更新 db。 cache 中存在，则先更新 cache，然后 cache 服务自己更新 db（同步更新 cache 和 db）。 读(Read Through)： 从 cache 中读取数据，读取到就直接返回 。 读取不到的话，先从 db 加载，写入到 cache 后返回响应。 问题： 和 Cache Aside Pattern 一样， Read-Through Pattern 也有首次请求数据一定不再 cache 的问题，对于热点数据可以提前放入缓存中。 Write Behind Pattern（异步缓存写入）Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。 Write Back 策略特别适合写多的场景 问题：还没同步，缓存宕掉 使用场景：消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制 优势：db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。 缓存雪崩✨ 缓存在同一时间大面积的失效或者redis宕机，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力。 针对大量缓存失效的情况 均匀设置过期时间：设置不同的失效时间比如随机设置缓存的失效时间。 互斥锁：保证只有一个请求来构建缓存（避免相同的后续请求打到数据库） 后台更新缓存：不给热点数据设置过期时间，由后台异步更新缓存 针对 Redis 故障宕机情况 采用 Redis 集群。主从节点的方式，如果 Redis 缓存的主节点故障宕机，从节点可以切换成为主节点，继续提供缓存服务 服务熔断。暂停业务应用对缓存服务的访问，直接返回错误 请求限流机制。只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务 缓存击穿✨请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 可以发现缓存击穿跟缓存雪崩很相似，你可以认为缓存击穿是缓存雪崩的一个子集。 解决（同缓存雪崩）： 互斥锁：保证只有一个请求来构建缓存（避免相同的后续请求打到数据库） 后台更新缓存：不给热点数据设置过期时间，由后台异步更新缓存 缓存穿透✨根本不存在于缓存中，也不存在于数据库中 解决： 非法请求限制 在 API 入口处我们要判断求请求参数是否合理（请求参数是否含有非法值、请求字段是否存在） 缓存空值或者默认值 布隆过滤器(重要) 布隆过滤器✨布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成 布隆过滤器会通过 3 个操作完成标记： 第一步，使用 N 个哈希函数分别对数据做哈希计算，得到 N 个哈希值； 第二步，将第一步得到的 N 个哈希值对位图数组的长度取模，得到每个哈希值在位图数组的对应位置。 第三步，将每个哈希值在位图数组的对应位置的值设置为 1； 缺点：存在哈希冲突的可能性，判断可能存在，但是不一定存在；但判断不存在，一定不存在 扩容：因为布隆过滤器的不可逆，我们没法重新建一个更大的布隆过滤器然后去把数据重新导入。这边采取的扩容的方法是，保留原有的布隆过滤器，建立一个更大的，新增数据都放在新的布隆过滤器中，去重的时候检查所有的布隆过滤器。 数据库和缓存如何保证一致性？1）先更新数据库，再更新缓存；先更新缓存，再更新数据库 当两个请求并发更新同一条数据的时候，可能会出现缓存和数据库中的数据不一致的现象 2）先更新数据库，还是先删除缓存？ Cache Aside 策略：先更新数据库，再删除缓存 当出现读-更新-删，会出现不一致。但出现不一致的概率低，因为缓存的写入通常要远远快于数据库的写入， 所以，「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的。同时可以还给缓存数据加上了「过期时间」来兜底，达到最终一致。 但是更新数据库和删除缓存不能保证两个操作都执行成功，也就会需要用到过期时间来兜底，这个中间会存在不一致性。 解决方法： 重试机制。 引入消息队列，将第二个操作（删除缓存）要操作的数据加入到消息队列，由消费者来操作数据。 如果应用删除缓存失败，可以从消息队列中重新读取数据，然后再次删除缓存，这个就是重试机制。当然，如果重试超过的一定次数，还是没有成功，我们就需要向业务层发送报错信息了。 如果删除缓存成功，就要把数据从消息队列中移除，避免重复操作，否则就继续重试。 订阅 MySQL binlog，再操作缓存 更新数据库成功，就会产生一条变更日志，记录在 binlog 里。可以通过订阅 binlog 日志，拿到具体要操作的数据，然后再执行缓存删除，阿里巴巴开源的 Canal 中间件就是基于这个实现的。 将binlog日志采集发送到MQ队列里面，然后编写一个简单的缓存删除消息者订阅binlog日志，根据更新log删除缓存，并且通过ACK机制确认处理这条更新log，保证数据缓存一致性 Canal 模拟 MySQL 主从复制的交互协议，把自己伪装成一个 MySQL 的从节点，向 MySQL 主节点发送 dump 请求，MySQL 收到请求后，就会开始推送 Binlog 给 Canal，Canal 解析 Binlog 字节流之后，转换为便于读取的结构化数据，供下游程序订阅使用。 ​ 两种方法都是采用异步操作缓存 为什么是删除缓存，而不是更新缓存呢？删除一个数据，相比更新一个数据更加轻量级，出问题的概率更小。在实际业务中，缓存的数据可能不是直接来自数据库表，也许来自多张底层数据表的聚合。 系统设计中有一个思想叫 Lazy Loading，适用于那些加载代价大的操作，删除缓存而不是更新缓存，就是懒加载思想的一个应用。 Redis 场景应用什么是 Redis 基于 c 开发 NoSQL 数据库 内存数据库，支持持久化 KY键值对数据 为什么要用 Redis？主要是因为 Redis 具备「高性能」和「高并发」两种特性。 1、高性能：直接操作内存 2、高并发：单台设备的 Redis 的 QPS（Query Per Second，每秒钟处理完请求的次数） 是 MySQL 的 10 倍 为什么redis比mysql要快？ 内存存储：Redis 是基于内存存储的 NoSQL 数据库，而 MySQL 是基于磁盘存储的关系型数据库。由于内存存储速度快，Redis 能够更快地读取和写入数据，而无需像 MySQL 那样频繁进行磁盘 I/O 操作。 简单数据结构：Redis 是基于键值对存储数据的，支持简单的数据结构（字符串、哈希、列表、集合、有序集合）。相比之下，MySQL 需要定义表结构、索引等复杂的关系型数据结构，因此在某些场景下 Redis 的数据操作更为简单高效，比如 Redis 用哈希表查询， 只需要O1 时间复杂度，而MySQL引擎的底层实现是B+Tree，时间复杂度是O(logn) 线程模型：Redis 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。 基于 Redis 实现分布式锁✨实现 Redis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁： 如果 key 不存在，则显示插入成功，可以用来表示加锁成功； 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。 用 EX 参数设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作 1SET lock_key unique_value NX PX 10000 lock_key 就是 key 键； unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作； NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作； PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁 基于 Redis 实现分布式锁的优点： 性能高效（这是选择缓存实现分布式锁最核心的出发点）。 实现方便。很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。 基于 Redis 实现分布式锁的缺点： 超时时间不好设置 。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效，注意 A 线程没执行完，后续线程 B 又意外的持有了锁，意味着可以操作共享资源，那么两个线程之间的共享资源就没办法进行保护了。 那么如何合理设置超时时间呢？ 我们可以基于续约的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂。 Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。 单机redis挂了怎么办，主从集群如何保证分布式锁RedLock 是对集群的每个节点进行加锁，如果大多数节点（N/2+1）加锁成功，则才会认为加锁成功。 这样即使集群中有某个节点挂掉了，因为大部分集群节点都加锁成功了，所以分布式锁还是可以继续使用的。 存在问题 RedLock 主要存在以下两个问题： 性能问题：RedLock 要等待大多数节点返回之后，才能加锁成功，而这个过程中可能会因为网络问题，或节点超时的问题，影响加锁的性能。 并发安全性问题：当客户端加锁时，如果遇到 GC 可能会导致加锁失效，但 GC 后误认为加锁成功的安全事故，例如以下流程： 客户端 A 请求 3 个节点进行加锁。 在节点回复处理之前，客户端 A 进入 GC 阶段（存在 STW，全局停顿）。 之后因为加锁时间的原因，锁已经失效了。 客户端 B 请求加锁（和客户端 A 是同一把锁），加锁成功。 客户端 A GC 完成，继续处理前面节点的消息，误以为加锁成功。 此时客户端 B 和客户端 A 同时加锁成功，出现并发安全性问题。 其余的分布式锁的实现基于数据库实现分布式锁：主要是利用数据库的唯一索引来实现，唯一索引天然具有排他性，这刚好符合我们对锁的要求：同一时刻只能允许一个竞争者获取锁。加锁时我们在数据库中插入一条锁记录，利用业务id进行防重。当第一个竞争者加锁成功后，第二个竞争者再来加锁就会抛出唯一索引冲突，如果抛出这个异常，我们就判定当前竞争者加锁失败。防重业务id需要我们自己来定义，例如我们的锁对象是一个方法，则我们的业务防重id就是这个方法的名字，如果锁定的对象是一个类，则业务防重id就是这个类名。 基于Zookeeper：Zookeeper一般用作配置中心，其实现分布式锁的原理和Redis类似，我们在Zookeeper中创建==瞬时节点==，利用节点不能重复创建的特性来保证排他性。 基于Redis实现延时任务两种方案： Redis 过期事件监听 Redisson 内置的延时队列 Redis 过期事件监听实现延时任务功能的原理？发布订阅 (pub/sub) 功能。在 pub/sub 中，引入了一个叫做 channel（频道） 的概念，有点类似于消息队列中的 topic（主题）。 在 pub/sub 模式下，生产者需要指定消息发送到哪个 channel 中，而消费者则订阅对应的 channel 以获取消息。 Redis 中有很多默认的 channel，这些 channel 是由 Redis 本身向它们发送消息的，而不是我们自己编写的代码。其中，__keyevent@0__:expired 就是一个默认的 channel，负责监听 key 的过期事件。也就是说，当一个 key 过期之后，Redis 会发布一个 key 过期的事件到__keyevent@&lt;db&gt;__:expired这个 channel 中。 我们只需要监听这个 channel，就可以拿到过期的 key 的消息，进而实现了延时任务功能。 Redis 过期事件监听实现延时任务功能有什么缺陷？1、时效性差：过期事件消息是在 Redis 服务器删除 key 时发布的，而不是一个 key 过期之后就会就会直接发布。因此，就会存在我设置了 key 的过期时间，但到了指定时间 key 还未被删除，进而没有发布过期事件的情况。 2、丢消息：Redis 的 pub/sub 模式中的消息并不支持持久化，这与消息队列不同 3、多服务实例下消息重复消费：Redis 的 pub/sub 模式目前只有广播模式，这意味着当生产者向特定频道发布一条消息时，所有订阅相关频道的消费者都能够收到该消息。 Redisson 延迟队列原理是什么？有什么优势？Redisson 的延迟队列 RDelayedQueue 是基于 Redis 的 SortedSet 来实现的。SortedSet 是一个有序集合，其中的每个元素都可以设置一个分数，代表该元素的权重。Redisson 利用这一特性，将需要延迟执行的任务插入到 SortedSet 中，并给它们设置相应的过期时间作为分数。 优势： 减少了丢消息的可能：DelayedQueue 中的消息会被持久化，即使 Redis 宕机了，根据持久化机制，也只可能丢失一点消息，影响不大。当然了，你也可以使用扫描数据库的方法作为补偿机制。 消息不存在重复消费问题：每个客户端都是从同一个目标队列中获取任务的，不存在重复消费的问题。 大 key 如何处理✨什么是 Redis 大 key大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。 一般而言，下面这两种情况被称为大 key： String 类型的值大于 10 KB； Hash、List、Set、ZSet 类型的元素的个数超过 5000个； 大 key 问题 内存占用过高。大Key占用过多的内存空间，可能导致可用内存不足，从而触发内存淘汰策略。在极端情况下，可能导致内存耗尽，Redis实例崩溃，影响系统的稳定性。 性能下降。大Key会占用大量内存空间，导致内存碎片增加，进而影响Redis的性能。对于大Key的操作，如读取、写入、删除等，都会消耗更多的CPU时间和内存资源，进一步降低系统性能。 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令，从而影响系统的响应时间和吞吐量。。 主从同步延迟。当Redis实例配置了主从同步时，大Key可能导致主从同步延迟。由于大Key占用较多内存，同步过程中需要传输大量数据，这会导致主从之间的网络传输延迟增加，进而影响数据一致性。 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多。 大 key 如何产生 一直往 value 塞数据，没有删除机制 没有合理做分片，将大 key 变小 key 如何找到大 key1、redis-cli –bigkeys 查找大key 2、使用 SCAN 命令查找大 key 3、使用 RdbTools 工具查找大 key 解决大 key 问题 删除大 key场景 对大Key进行清理。将不适用Redis能力的数据存至其它存储，并在Redis中删除此类数据。注意，要使用异步删除。 对过期数据进行定期清。堆积大量过期数据会造成大Key的产生，例如在HASH数据类型中以增量的形式不断写入大量数据而忽略了数据的时效性。可以通过定时任务的方式对失效数据进行清理。 删除大key方法 unlink 异步惰性非阻塞删除 scan 游标式迭代扫描删除 压缩和拆分 key 场景 对大Key进行拆分。例如将含有数万成员的一个HASH Key拆分为多个HASH Key，并确保每个Key的成员数量在合理范围。在Redis集群架构中，拆分大Key能对数据分片间的内存平衡起到显著作用。 压缩和拆分 key方法 对于 string采用序列化、压缩算法 对于 string 压缩后还是大 key，则进行拆分，使用 multiget 实现事务读取 对于 list/set 等集合，进行分片 热key✨什么是热key通常以其接收到的Key被请求频率来判定： QPS集中在特定的Key：Redis实例的总QPS（每秒查询率）为10,000，而其中一个Key的每秒访问量达到了7,000。 带宽使用率集中在特定的Key：对一个拥有上千个成员且总大小为1 MB的HASH Key每秒发送大量的HGETALL操作请求。 CPU使用时间占比集中在特定的Key：对一个拥有数万个成员的Key（ZSET类型）每秒发送大量的ZRANGE操作请求。 热key怎么解决 在Redis集群架构中对热Key进行复制。可以将对应热Key进行复制并迁移至其他数据分片来解决单个数据分片的热Key压力 使用读写分离架构。可以将实例改造成读写分离架构来降低每个数据分片的读请求压力，甚至可以不断地增加从节点。 Redis 管道有什么用？管道技术（Pipeline）是客户端提供的一种批处理技术，用于一次处理多个 Redis 命令，从而提高整个交互的性能。 使用管道技术可以解决多个命令执行时的网络等待，它是把多个命令整合到一起发送给服务器端处理之后统一返回给客户端，这样就免去了每条命令执行后都要等待的情况，从而有效地提高了程序的执行效率。 但使用管道技术也要注意避免发送的命令过大，或管道内的数据太多而导致的网络阻塞。 要注意的是，管道技术本质上是客户端提供的功能，而非 Redis 服务器端的功能。 Redis 事务Redis事务的概念事务提供了一种将多个命令打包，然后一次性、有序地执行的机制。 多个命令会被人队到事务队列中， 然后按先进先出(FIFO)的顺序执行。 事务在执行过程中不会被中断，当事务队列中的所有命令都被执行完毕之后，事务才会结束。 Redis事务不支持回滚机制。 Redis 事务没有原子性 有持久性和一致性 至于隔离性, 都不存在多个事务的情况 毕竟单线程 Redis事务没有隔离级别批量操作在发送 EXEC 命令前被放入队列缓存，并不会被实际执行，也就不存在事务内的查询要看到事务里的更新，事务外查询不能看到。 Redis事务是否保证原子性Redis 对事务原子性属性的保证情况： Redis 事务正常执行，可以保证原子性； Redis 事务执行中某一个操作执行失败，不保证原子性； Redis事务的三个阶段（1）开始事务 （2）命令入队 （3）执行事务 Redis事务命令 Redis事务案例正常执行 放弃事务 全体连坐在事务队列中存在命令性错误（类似于java编译性错误），则执行EXEC命令时，所有命令都不会执行。 比如 命令写错 冤头债主在事务队列中存在语法性错误（类似于java的1/0的运行时异常），则执行EXEC命令时，其他正确命令会被执行，错误命令抛出异常。 比如 对 string incr 使用watch案例一：使用watch检测balance，事务期间balance数据未变动，事务执行成功。 案例二：使用watch检测balance，在开启事务后（标注1处），在新窗口执行标注2中的操作，更改balance的值，模拟其他客户端在事务执行期间更改watch监控的数据，然后再执行标注1后命令，执行EXEC后，事务未成功执行。 一但执行 EXEC 开启事务的执行后，无论事务使用执行成功， WARCH 对变量的监控都将被取消。故当事务执行失败后，需重新执行WATCH命令对变量进行监控，并开启新的事务进行操作。","link":"/2024/02/14/%E4%B8%AD%E9%97%B4%E4%BB%B6/Redis/"},{"title":"kafka","text":"技术选型rabbitMQ、rocketMQ、kafka三大消息中间件的区别 优点 缺点 使用场景 kafka 吞吐量非常大；性能非常好；集群高可用 会丢数据；功能单一 大数据；日志分析 RabbitMQ 消息可靠性高；支持多种消息协议；适合做任务队列，比如定时任务 吞吐量低；消息积累会影响性能 适合需要复杂消息路由的场景 RocketMQ 支持有序消息、事物消息、消息回溯 客户端只支持java 适合对消息顺序和事务性要求高的场景 Kafkakafka 是什么分布式流处理平台，常用于企业级消息引擎 kafka特点高吞吐量、低延迟、持久性、可扩展性和容错性等特点 kafka 高性能原理使用 page cache，不依赖 jvm 内存，如果使用 jvm 内存，会增加 gc 负担，增加延迟，创建对象的开销也比较高。 kafka的架构Kafka 的架构主要由以下几个核心组件组成： Producer（生产者） : 产生消息的一方。 Consumer（消费者） : 消费消息的一方。 Broker（代理） : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。 Topic（主题） : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。 Partition（分区） : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 。 Zookeeper：Kafka 使用 Zookeeper 来管理集群的元数据、进行 Leader 选举和协调 Broker 节点。 AR(Assigned Replicas): AR 是主题被创建后，分区创建时被分配的副本集合，副本个数由副本因子决定。 ISR(In-Sync Replicas): Kafka 中特别重要的概念，指代的是 AR 中那些与 Leader 保持同步的副本集合。在 AR 中的副本可能不在 ISR 中，但 Leader 副本天然就包含在 ISR 中。 在 Kafka 的架构中，Producer 将消息发布到 Topic 中，Broker 存储并处理消息，并且通过 Partition 将 Topic 分割成多个部分，消费者通过 Consumer Group 消费消息。整个架构通过 Zookeeper 进行管理和协调，保证了系统的可靠性和稳定性。 什么是消费组消费者组是 Kafka 提供的可扩展且具有容错性的消费者机制 可扩展： 当生产者向 Topic 写入消息的速度超过了现有消费者的处理速度，此时需要对消费者进行横向伸缩，用多个消费者从同一个主题读取消息，对消息进行分流。同一个分区不能被一个组中的多个 consumer 消费。 容错性 在消费者组中，多个实例共同订阅若干个主题，实现共同消费。当某个实例挂掉的时候，其他实例会自动地承担起它负责消费的分区。 Kafka 的多副本机制了解吗？带来了什么好处？分区（Partition）中的多个副本之间会有一个 leader ，其他副本称为 follower。发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。 Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？ 多分区——一个 topic 可以指定多个 分布在不同 broker 的partition，提高并发能力（负载均衡）。 多副本——提高容灾能力，提高了消息存储的安全性 zookeeper 的作用 注册 broker 注册 topic 负载均衡 位移 offset 的作用在 Kafka 中，每个主题分区下的每条消息都被赋予了一个唯一的 ID 数值，用于标识它在分区中的位置。这个 ID 数值，就被称为位移，或者叫偏移量。一旦消息被写入到分区日志，它的位移值将不能被修改。 保证消费顺序⭐总结一下，对于如何保证 Kafka 中消息的顺序，有了下面两种方法： 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key/Partition。 每次添加消息到 Partition(分区) 的时候都会采用尾加法，如上图所示。 Kafka 只能为我们保证 Partition(分区) 中的消息有序。 消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。Kafka 通过偏移量（offset）来保证消息在分区内的顺序性。 保证消息不丢失⭐生产者丢失消息情况生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。 所以，我们不能默认在调用send方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。 回调机制：send()后添加回调函数 123ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(topic, o);future.addCallback(result -&gt; logger.info(\"生产者成功发送消息到topic:{} partition:{}的消息\", result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -&gt; logger.error(\"生产者发送消失败，原因：{}\", ex.getMessage())); 重试机制。通过配置重试次数和重试间隔，可以确保消息在发生失败时有机会重新发送 消息确认机制。 acks=0：生产者不会等待任何确认，直接发送下一条消息。 acks=1：生产者会等待 leader 副本确认消息后再发送下一条消息。 acks=all：生产者会等待所有 ISR（In-Sync Replicas，同步副本）确认消息后再发送下一条消息。 消费者丢失消息情况消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 解决办法也比较粗暴，我们手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。 Kafka Broker 丢失消息情况假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。 解决办法就是我们设置 acks = all。acks 是 Kafka 生产者(Producer) 很重要的一个参数。当我们配置 acks = all 表示只有所有 ISR 列表的副本全部收到消息时，生产者才会接收到来自服务器的响应。 保证消息不重复消费⭐服务端侧已经消费的数据没有成功提交 offset（根本原因）。 解决方案： 做幂等校验，Redis 的 set，mysql 的主键 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底。 重试机制Kafka 消费者在默认配置下会进行最多 10 次 的重试，每次重试的时间间隔为 0，即立即进行重试。如果在 10 次重试后仍然无法成功消费消息，则不再进行重试，消息将被视为消费失败。 重试失败后的数据如何再次处理？当达到最大重试次数后，数据会直接被跳过，继续向后进行。当代码修复后，如何重新消费这些重试失败的数据呢？ 死信队列（Dead Letter Queue，简称 DLQ） 是消息中间件中的一种特殊队列。它主要用于处理无法被消费者正确处理的消息，通常是因为消息格式错误、处理失败、消费超时等情况导致的消息被”丢弃”或”死亡”的情况。当消息进入队列后，消费者会尝试处理它。如果处理失败，或者超过一定的重试次数仍无法被成功处理，消息可以发送到死信队列中，而不是被永久性地丢弃。在死信队列中，可以进一步分析、处理这些无法正常消费的消息，以便定位问题、修复错误，并采取适当的措施。 @RetryableTopic 是 Spring Kafka 中的一个注解,它用于配置某个 Topic 支持消息重试，更推荐使用这个注解来完成重试。 1234567891011121314// 重试 5 次，重试间隔 100 毫秒,最大间隔 1 秒@RetryableTopic( attempts = \"5\", backoff = @Backoff(delay = 100, maxDelay = 1000))@KafkaListener(topics = {KafkaConst.TEST_TOPIC}, groupId = \"apple\")private void customer(String message) { log.info(\"kafka customer:{}\", message); Integer n = Integer.parseInt(message); if (n % 5 == 0) { throw new RuntimeException(); } System.out.println(n);} 当达到最大重试次数后，如果仍然无法成功处理消息，消息会被发送到对应的死信队列中。对于死信队列的处理，既可以用 @DltHandler 处理，也可以使用 @KafkaListener 重新消费。 消息积压三种情况 线上有时因为发送方发送消息速度过快，或者消费方处理消息过慢，可能会导致broker积压大量未消费消息 解决方案：此种情况如果积压了上百万未消费消息需要紧急处理，可以修改消费端程序，让其将收到的消息快速转发到其他topic(可以设置很多分区)，然后再启动多个消费者同时消费新主题的不同分区。如图所示： 由于消息数据格式变动或消费者程序有bug，导致消费者一直消费不成功，也可能导致broker积压大量未消费消息。解决方案：此种情况可以将这些消费不成功的消息转发到其它队列里去(类似死信队列)，后面再慢慢分析死信队列里的消息处理问题。这个死信队列，kafka并没有提供，需要整合第三方插件！ 设置 Kafka 能接收的最大消息的大小？ Broker 端参数: message.max.bytes, Consumer 端参数: fetch.message.max.bytes 监控 Kafka 的框架都有哪些？ JMX 监控: 由于 Kafka 提供的监控指标都是基于 JMX 的 Kafka Manager Kafka Monitor Broker 的 Heap Size 如何设置？任何 Java 进程 JVM 堆大小的设置都需要仔细地进行考量和测试。一个常见的做法是，以默认的初始 JVM 堆大小运行程序，当系统达到稳定状态后，手动触发一次 Full GC，然后通过 JVM 工具查看 GC 后的存活对象大小。之后，将堆大小设置成存活对象总大小的 1.5~2 倍。对于 Kafka 而言，这个方法也是适用的。不过，业界有个最佳实践，那就是将 Broker 的 Heap Size 固定为 6GB。经过很多公司的验证，这个大小是足够且良好的。 Kafka 能手动删除消息吗？Kafka 不需要用户手动删除消息。它本身提供了留存策略，能够自动删除过期消息。当然，它是支持手动删除消息的。 使用 kafka-delete-records 命令，通过将分区 Log Start Offset 值抬高的方式间接删除消息。 __consumer_offsets 是做什么用的？这是一个内部主题，主要用于存储消费者的偏移量，以及消费者的元数据信息 (消费者实例，消费者id 等等) 分区 Leader 选举策略有几种？ OfflinePartition Leader 选举: 每当有分区上线时，就需要执行 Leader 选举。 ReassignPartition Leader 选举: 当你手动运行 kafka-reassign-partitions 命令，或者是调用 Admin 的 alterPartitionReassignments 方法执行分区副本重分配时，可能触发此类选举。 PreferredReplicaPartition Leader 选举: 当你手动运行 kafka-preferred-replica-election 命令，或自动触发了 Preferred Leader 选举时，该类策略被激活 ControlledShutdownPartition Leader 选举: 当 Broker 正常关闭时，该 Broker 上的所有 Leader 副本都会下线，因此，需要为受影响的分区执行相应的 Leader 选举。 从 AR 中挑选首个在 ISR 中的副本，作为新 Leader。 Kafka 的哪些场景中使用了零拷贝（Zero Copy） 基于 mmap 的索引 日志文件读写所用的 TransportLayer Kafka 为什么不支持读写分离？Leader/Follower 模型并没有规定 Follower 副本不可以对外提供读服务。很多框架都是允许这么做的，只是 Kafka 最初为了避免不一致性的问题，而采用了让 Leader 统一提供服务的方式。 不过，自 Kafka 2.4 之后，Kafka 提供了有限度的读写分离，也就是说，Follower 副本能够对外提供读服务。 如何调优 Kafka？ Producer 端:启用压缩，关闭重试 Broker 端：避免 Broker Full GC Consumer: 增加fetch.min.bytes 为什么 redis Pub/Sub 比 kafka 更快一些？二者如何选取Redis是一个内存数据库，其Pub/Sub功能将消息保存在内存中。由于内存访问速度通常远快于磁盘访问速度，因此Redis在处理实时性较高的消息推送时具有优势；Redis的Pub/Sub模型相对简单，使得它在处理发布和订阅操作时的开销较小。 Kafka是一个完整的系统，提供了高吞吐量、分布式的提交日志。它旨在处理大规模数据流，具有强大的持久化能力和容错性。Kafka的分布式架构和分区机制使得它能够在多个消费者之间实现负载均衡，从而提高整体处理能力。 Redis PUB/SUB使用场景： 消息持久性需求不高 吞吐量要求不高 可以忍受数据丢失 数据量不大 Kafka使用场景：(上面以外的其他场景) 高可靠性 高吞吐量 持久性高 多样化的消费处理模型","link":"/2024/03/26/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka/"},{"title":"git","text":"reset与revert作用：回滚代码 区别：reset是用来”回退”版本，而revert是用来”还原”某次或者某几次提交 123456789//使用reset命令，Git会把要回退版本之后提交的修改都删除掉。使用git log命令，查看分支提交历史，确认需要回退的版本使用git reset --hard commit_id命令，进行版本回退使用git push origin命令，推送至远程分支//使用revert命令，Git会把要回退版本之后提交的修改都保留，剔除回退版本的修改，产生新的commit_id。使用git log命令，查看分支提交历史，确认需要回退的版本使用git revert -n commit_id命令，进行版本回退使用git commit -m \"恢复xxx修改\" 命令 在实际生产环境中，代码是基于master分支发布到线上的，会有多人进行提交。可能会碰到自己或团队其他成员开发的某个功能在上线之后有Bug,需要及时做代码回滚的操作。 在确认要回滚的版本之后，如果别人没有最新提交，那么就可以直接用reset命令进行版本回退，否则，就可以考虑使用revert命令进行还原修改，不能影响到别人的提交。 使用reset还是revert，需要考虑实际的适用场景，没有绝对化。 上面提的并不是真正的物理删除，是因为Git会把分支的每次修改记录都会保留下来，比如有某次的commit,某次的reset等。而使用git reflog show命令,可以查看完整的提交历史， 只要有commit_id，我们就能恢复任意版本的代码，在各版本之间来回穿梭。 Merge与Rebase作用: 更新代码 ​ 一般在将本地代码提交到远程仓库时，最好先更新下远程仓库的代码到本地，从而避免不必要的冲突。【养成良好的习惯，每天到公司就先更新代码】 更新时有两种方式: Merge incoming changes into the current brance Rebase the current branch on top of incoming changes 总结: Merge具有更高的可追溯性，而Rebase则更整洁且易于审核。 假如想要将本地以及远程的版本回退123git log //查看回退的idgit reset --hard id //回退git push -f origin &lt;branch_id&gt; //强制推送到origin stash使用场景：在 a 分支开发，临时b 分支有修改需求，但是不想提交 a 分支写的内容。这时候就可以用到 stash git stash会把所有未提交的修改（包括暂存的和非暂存的）都保存起来，用于后续恢复当前工作目录。通过git stash命令推送一个新的储藏，当前的工作目录就干净了。 可以通过git stash pop命令恢复之前缓存的工作目录","link":"/2024/04/27/%E4%B8%AD%E9%97%B4%E4%BB%B6/git/"},{"title":"kafka部署","text":"docker部署1docker run -d --name kafka -p 9092:9092 -e KAFKA_BROKER_ID=0 -e KAFKA_ZOOKEEPER_CONNECT=192.168.198.128:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://10.20.186.167:9092 -e KAFKA_LISTENERS=PLAINTEXT://192.168.198.128:9092 -t wurstmeister/kafka 测试是否成功 关注listeners和advertised.listeners，根据注释大概可以看出，前者是对内的监听器，后者是对外的监听器。也就是说，只要配置好advertised.listeners，就足够让外部能够访问 https://blog.csdn.net/tuguai7887/article/details/103260224 进入容器Kafka安装完毕后,还要进入到容器中启动生产者和消费者,这样可以验证kafka功能是否正常,顺序执行命令如下: 123# docker ps -a #查看kafka镜像的容器ID# docker exec -it 容器ID /bin/bash #进入到容器内部# cd /opt/kafka/bin # 切到容器内部kafka执行目录下 执行最后的结果如图: 进入到kafka镜像内部 启动生产者在容器里执行以下命令启动生产者: 1./kafka-console-producer.sh --broker-list localhost:9092 --topic [你的topic名称] 1./kafka-console-producer.sh --broker-list localhost:9092 --topic test123 我这里自己起了一个topic名称,名字为test123, 如图: 启动生产者脚本 生产者脚本启动成功后,会有一个”&gt;”提示符。 启动消费者为了看到生产者和消费者之间的消息传递效果,这里需要另开一个终端,按照上面的方法进入容器对应目录,并执行以下命令: 1./kafka-console-consumer.sh --bootstrap-server [你的IP地址]:9092 --topic [你的topic名称] 注意,这里有两个变量需要自己调整,一个是IP地址,另一个是上面建立的Topic名称, 我这里填入信息后的完整命令如下: 1./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test123 执行过程如图: 启动消费者脚本 生产者与消费者测试切换到生产者窗口,连续输入一些信息,如图: 在生产者端发送消息 再切换回消费者窗口, 正常的话已经可以收到生产者发送的信息了,如图: 在消费者端接收消息","link":"/2024/05/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/kafka%E9%83%A8%E7%BD%B2/"},{"title":"redis部署","text":"本章是基于CentOS7下的Redis集群教程，包括： 单机安装Redis Redis主从 Redis分片集群 1.单机安装Redis首先需要安装Redis所需要的依赖： 1yum install -y gcc tcl 将Redis安装包上传到虚拟机的任意目录： 解压缩： 1tar -xvf redis-6.2.4.tar.gz 进入redis目录： 1cd redis-6.2.4 运行编译命令： 1make &amp;&amp; make install 如果没有出错，应该就安装成功了。 然后修改redis.conf文件中的一些配置： 1234# 绑定地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问bind 0.0.0.0# 数据库数量，设置为1databases 1 启动Redis： 1redis-server redis.conf 停止redis服务： 1redis-cli shutdown docker部署1bind 127.0.0.1 -::1 # 这个的话改成 0.0.0.0 即可，127.0.0.1 是只允许本机访问 1docker run -p 6379:6379 --name redis --privileged=true -v /root/redis/conf/redis.conf:/etc/redis/redis.conf -v /root/redis/data:/data -d redis redis-server /etc/redis/redis.conf 2.Redis主从集群2.1.集群结构我们搭建的主从集群结构如图： 共包含三个节点，一个主节点，两个从节点。 这里我们会在同一台虚拟机中开启3个redis实例，模拟主从集群，信息如下： IP PORT 角色 192.168.150.101 7001 master 192.168.150.101 7002 slave 192.168.150.101 7003 slave 2.2.准备实例和配置要在同一台虚拟机开启3个实例，必须准备三份不同的配置文件和目录，配置文件所在目录也就是工作目录。 1）创建目录 我们创建三个文件夹，名字分别叫7001、7002、7003： 1234# 进入/tmp目录cd /tmp# 创建目录mkdir 7001 7002 7003 如图： 2）恢复原始配置 修改redis-6.2.4/redis.conf文件，将其中的持久化模式改为默认的RDB模式，AOF保持关闭状态。 12345678# 开启RDB# save \"\"save 3600 1save 300 100save 60 10000# 关闭AOFappendonly no 3）拷贝配置文件到每个实例目录 然后将redis-6.2.4/redis.conf文件拷贝到三个目录中（在/tmp目录执行下列命令）： 123456# 方式一：逐个拷贝cp redis-6.2.4/redis.conf 7001cp redis-6.2.4/redis.conf 7002cp redis-6.2.4/redis.conf 7003# 方式二：管道组合命令，一键拷贝echo 7001 7002 7003 | xargs -t -n 1 cp redis-6.2.4/redis.conf 4）修改每个实例的端口、工作目录 修改每个文件夹内的配置文件，将端口分别修改为7001、7002、7003，将rdb文件保存位置都修改为自己所在目录（在/tmp目录执行下列命令）： 123sed -i -e 's/6379/7001/g' -e 's/dir .\\//dir \\/tmp\\/7001\\//g' 7001/redis.confsed -i -e 's/6379/7002/g' -e 's/dir .\\//dir \\/tmp\\/7002\\//g' 7002/redis.confsed -i -e 's/6379/7003/g' -e 's/dir .\\//dir \\/tmp\\/7003\\//g' 7003/redis.conf 5）修改每个实例的声明IP 虚拟机本身有多个IP，为了避免将来混乱，我们需要在redis.conf文件中指定每一个实例的绑定ip信息，格式如下： 12# redis实例的声明 IPreplica-announce-ip 192.168.150.101 每个目录都要改，我们一键完成修改（在/tmp目录执行下列命令）： 1234567# 逐一执行sed -i '1a replica-announce-ip 192.168.150.101' 7001/redis.confsed -i '1a replica-announce-ip 192.168.150.101' 7002/redis.confsed -i '1a replica-announce-ip 192.168.150.101' 7003/redis.conf# 或者一键修改printf '%s\\n' 7001 7002 7003 | xargs -I{} -t sed -i '1a replica-announce-ip 192.168.150.101' {}/redis.conf 2.3.启动为了方便查看日志，我们打开3个ssh窗口，分别启动3个redis实例，启动命令： 123456# 第1个redis-server 7001/redis.conf# 第2个redis-server 7002/redis.conf# 第3个redis-server 7003/redis.conf 启动后： 如果要一键停止，可以运行下面命令： 1printf '%s\\n' 7001 7002 7003 | xargs -I{} -t redis-cli -p {} shutdown 2.4.开启主从关系现在三个实例还没有任何关系，要配置主从可以使用replicaof 或者slaveof（5.0以前）命令。 有临时和永久两种模式： 修改配置文件（永久生效） 在redis.conf中添加一行配置：slaveof &lt;masterip&gt; &lt;masterport&gt; 使用redis-cli客户端连接到redis服务，执行slaveof命令（重启后失效）： 1slaveof &lt;masterip&gt; &lt;masterport&gt; 注意：在5.0以后新增命令replicaof，与salveof效果一致。 这里我们为了演示方便，使用方式二。 通过redis-cli命令连接7002，执行下面命令： 1234# 连接 7002redis-cli -p 7002# 执行slaveofslaveof 192.168.150.101 7001 通过redis-cli命令连接7003，执行下面命令： 1234# 连接 7003redis-cli -p 7003# 执行slaveofslaveof 192.168.150.101 7001 然后连接 7001节点，查看集群状态： 1234# 连接 7001redis-cli -p 7001# 查看状态info replication 结果： 2.5.测试执行下列操作以测试： 利用redis-cli连接7001，执行set num 123 利用redis-cli连接7002，执行get num，再执行set num 666 利用redis-cli连接7003，执行get num，再执行set num 888 可以发现，只有在7001这个master节点上可以执行写操作，7002和7003这两个slave节点只能执行读操作。 docker 部署从节点配置主节点ip 端口 密码 123slaveof 192.168.125.130 7001masterauth 123456 #验证master的密码，此密码为master的密码requirepass 123456 #给slave设置密码，可设置可不设置，自己选择 1docker run -p 7001:7001 --name redisCluster1 --privileged=true -v /root/redisCluster1/conf/redis.conf:/etc/redis/redis.conf -v /root/redisCluster1/data:/data -d redis:6.2.6 redis-server /etc/redis/redis.conf 1docker run -p 7002:7002 --name redisCluster2 --privileged=true -v /root/redisCluster2/conf/redis.conf:/etc/redis/redis.conf -v /root/redisCluster2/data:/data -d redis:6.2.6 redis-server /etc/redis/redis.conf 1docker run -p 7003:7003 --name redisCluster3 --privileged=true -v /root/redisCluster3/conf/redis.conf:/etc/redis/redis.conf -v /root/redisCluster3/data:/data -d redis:6.2.6 redis-server /etc/redis/redis.conf 测试 1234docker exec -it redisCluster1 bashredis-cli -h 127.0.0.1 -p 7001auth 123456info replication 3.搭建哨兵集群3.1.集群结构这里我们搭建一个三节点形成的Sentinel集群，来监管之前的Redis主从集群。如图： 三个sentinel实例信息如下： 节点 IP PORT s1 192.168.150.101 27001 s2 192.168.150.101 27002 s3 192.168.150.101 27003 3.2.准备实例和配置要在同一台虚拟机开启3个实例，必须准备三份不同的配置文件和目录，配置文件所在目录也就是工作目录。 我们创建三个文件夹，名字分别叫s1、s2、s3： 1234# 进入/tmp目录cd /tmp# 创建目录mkdir s1 s2 s3 如图： 然后我们在s1目录创建一个sentinel.conf文件，添加下面的内容： 123456port 27001sentinel announce-ip 192.168.150.101sentinel monitor mymaster 192.168.150.101 7001 2sentinel down-after-milliseconds mymaster 5000sentinel failover-timeout mymaster 60000dir \"/tmp/s1\" 解读： port 27001：是当前sentinel实例的端口 sentinel monitor mymaster 192.168.150.101 7001 2：指定主节点信息 mymaster：主节点名称，自定义，任意写 192.168.150.101 7001：主节点的ip和端口 2：选举master时的quorum值 然后将s1/sentinel.conf文件拷贝到s2、s3两个目录中（在/tmp目录执行下列命令）： 12345# 方式一：逐个拷贝cp s1/sentinel.conf s2cp s1/sentinel.conf s3# 方式二：管道组合命令，一键拷贝echo s2 s3 | xargs -t -n 1 cp s1/sentinel.conf 修改s2、s3两个文件夹内的配置文件，将端口分别修改为27002、27003： 12sed -i -e 's/27001/27002/g' -e 's/s1/s2/g' s2/sentinel.confsed -i -e 's/27001/27003/g' -e 's/s1/s3/g' s3/sentinel.conf 3.3.启动为了方便查看日志，我们打开3个ssh窗口，分别启动3个redis实例，启动命令： 123456# 第1个redis-sentinel s1/sentinel.conf# 第2个redis-sentinel s2/sentinel.conf# 第3个redis-sentinel s3/sentinel.conf 启动后： 3.4.测试尝试让master节点7001宕机，查看sentinel日志： 查看7003的日志： 查看7002的日志： docker1234使用 查看ip地址docker inspect redisCluster1 docker inspect redisCluster2docker inspect redisCluster3 172.17.0.3:7001 172.17.0.4:7002 172.17.0.5:7003 设置sentinel的port 27001 27002 27003 分别对3个节点进行配置sentinel 1234567891011121314151617181920【master容器配置哨兵】:docker exec -it redisCluster1 bashcd /sed -i 's/deb.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list #换源apt-get update #更新依赖apt-get install -y vim #安装vim#vim sentinel.conf #建立sentinel（哨兵模式）的配置文件，保存退出，内容如下：port 27001 #指定哨兵监听端口dir \"/data\" #sentinel工作目录logfile \"sentinel.log\" #日志文件sentinel monitor name 172.17.0.3 7001 2 #启动哨兵(名称：name) 监听master 2是达到变成主节点需要vote的数量sentinel auth-pass mymaster 123456daemonize yes #后台运行##启动哨兵：redis-sentinel sentinel.conf#关闭哨兵redis-cli -p 27001 shutdown 123456789101112131415161718192021222324252627282930313233343536373839【slave1容器配置哨兵】:docker exec -it redisCluster2 bashcd /sed -i 's/deb.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list #换源apt-get update #更新依赖apt-get install -y vim #安装vim#vim sentinel.conf #建立sentinel（哨兵模式）的配置文件，保存退出，内容如下：port 27002 #指定哨兵监听端口dir \"/data\" #sentinel工作目录logfile \"sentinel.log\" #日志文件sentinel monitor name 172.17.0.3 7001 2 #启动哨兵(名称：name) 监听master 2是达到变成主节点需要vote的数量daemonize yes #后台运行##启动哨兵：redis-sentinel sentinel.conf#关闭哨兵redis-cli -p 27001 shutdown【slave2容器配置哨兵】:docker exec -it redisCluster2 bashcd /sed -i 's/deb.debian.org/mirrors.ustc.edu.cn/g' /etc/apt/sources.list #换源apt-get update #更新依赖apt-get install -y vim #安装vim#vim sentinel.conf #建立sentinel（哨兵模式）的配置文件，保存退出，内容如下：port 27003 #指定哨兵监听端口dir \"/data\" #sentinel工作目录logfile \"sentinel.log\" #日志文件sentinel monitor name 172.17.0.3 7001 2 #启动哨兵(名称：name) 监听master 2是达到变成主节点需要vote的数量daemonize yes #后台运行##启动哨兵：redis-sentinel sentinel.conf#关闭哨兵redis-cli -p 27001 shutdown 配置完成后主节点的conf信息 现在宕机主节点 1docker stop redisCluster1 节点3变成主节点 节点2认定节点3是主节点 然后重启节点1，节点1也是从节点 ，其主节点是节点3 修改要端口映射的容器的配置文件12345678910111213141516171819202122#1、查看容器的信息docker ps -a#2、查看容器的端口映射情况，在容器外执行：docker port 容器ID 或者 docker port 容器名称#3、查找要修改容器的全IDdocker inspect 容器ID |grep Id#4、进到/var/lib/docker/containers 目录下找到与全 Id 相同的目录，修改 其中的hostconfig.json 和 config.v2.json文件：#注意：若该容器还在运行中，需要先停掉docker stop 容器ID#再停掉docker服务systemctl stop docker#5、修改hostconfig.json如下# 格式如：\"{容器内部端口}/tcp\":[{\"HostIp\":\"\",\"HostPort\":\"映射的宿主机端口\"}]\"PortBindings\":{\"22/tcp\":[{\"HostIp\":\"\",\"HostPort\":\"3316\"}],\"80/tcp\":[{\"HostIp\":\"\",\"HostPort\":\"180\"}]}#6、修改config.v2.json在ExposedPorts中加上要暴露的端口# 格式如：\"{容器内部端口}/tcp\":{}\"ExposedPorts\":{\"22/tcp\":{},\"80/tcp\":{}} 4.搭建分片集群4.1.集群结构分片集群需要的节点数量较多，这里我们搭建一个最小的分片集群，包含3个master节点，每个master包含一个slave节点，结构如下： 这里我们会在同一台虚拟机中开启6个redis实例，模拟分片集群，信息如下： IP PORT 角色 192.168.150.101 7001 master 192.168.150.101 7002 master 192.168.150.101 7003 master 192.168.150.101 8001 slave 192.168.150.101 8002 slave 192.168.150.101 8003 slave 4.2.准备实例和配置删除之前的7001、7002、7003这几个目录，重新创建出7001、7002、7003、8001、8002、8003目录： 123456# 进入/tmp目录cd /tmp# 删除旧的，避免配置干扰rm -rf 7001 7002 7003# 创建目录mkdir 7001 7002 7003 8001 8002 8003 在/tmp下准备一个新的redis.conf文件，内容如下： 123456789101112131415161718192021port 6379# 开启集群功能cluster-enabled yes# 集群的配置文件名称，不需要我们创建，由redis自己维护cluster-config-file /tmp/6379/nodes.conf# 节点心跳失败的超时时间cluster-node-timeout 5000# 持久化文件存放目录dir /tmp/6379# 绑定地址bind 0.0.0.0# 让redis后台运行daemonize yes# 注册的实例ipreplica-announce-ip 192.168.150.101# 保护模式protected-mode no# 数据库数量databases 1# 日志logfile /tmp/6379/run.log 将这个文件拷贝到每个目录下： 1234# 进入/tmp目录cd /tmp# 执行拷贝echo 7001 7002 7003 8001 8002 8003 | xargs -t -n 1 cp redis.conf 修改每个目录下的redis.conf，将其中的6379修改为与所在目录一致： 1234# 进入/tmp目录cd /tmp# 修改配置文件printf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -I{} -t sed -i 's/6379/{}/g' {}/redis.conf 4.3.启动因为已经配置了后台启动模式，所以可以直接启动服务： 1234# 进入/tmp目录cd /tmp# 一键启动所有服务printf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -I{} -t redis-server {}/redis.conf 通过ps查看状态： 1ps -ef | grep redis 发现服务都已经正常启动： 如果要关闭所有进程，可以执行命令： 1ps -ef | grep redis | awk '{print $2}' | xargs kill 或者（推荐这种方式）： 1printf '%s\\n' 7001 7002 7003 8001 8002 8003 | xargs -I{} -t redis-cli -p {} shutdown 4.4.创建集群虽然服务启动了，但是目前每个服务之间都是独立的，没有任何关联。 我们需要执行命令来创建集群，在Redis5.0之前创建集群比较麻烦，5.0之后集群管理命令都集成到了redis-cli中。 1）Redis5.0之前 Redis5.0之前集群命令都是用redis安装包下的src/redis-trib.rb来实现的。因为redis-trib.rb是有ruby语言编写的所以需要安装ruby环境。 123# 安装依赖yum -y install zlib ruby rubygemsgem install redis 然后通过命令来管理集群： 1234# 进入redis的src目录cd /tmp/redis-6.2.4/src# 创建集群./redis-trib.rb create --replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003 2）Redis5.0以后 我们使用的是Redis6.2.4版本，集群管理以及集成到了redis-cli中，格式如下： 1redis-cli --cluster create --cluster-replicas 1 192.168.150.101:7001 192.168.150.101:7002 192.168.150.101:7003 192.168.150.101:8001 192.168.150.101:8002 192.168.150.101:8003 命令说明： redis-cli --cluster或者./redis-trib.rb：代表集群操作命令 create：代表是创建集群 --replicas 1或者--cluster-replicas 1 ：指定集群中每个master的副本个数为1，此时节点总数 ÷ (replicas + 1) 得到的就是master的数量。因此节点列表中的前n个就是master，其它节点都是slave节点，随机分配到不同master 运行后的样子： 这里输入yes，则集群开始创建： 通过命令可以查看集群状态： 1redis-cli -p 7001 cluster nodes 4.5.测试尝试连接7001节点，存储一个数据： 12345678# 连接redis-cli -p 7001# 存储数据set num 123# 读取数据get num# 再次存储set a 1 结果悲剧了： 集群操作时，需要给redis-cli加上-c参数才可以： 1redis-cli -c -p 7001 这次可以了： redis.conf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311411511611711811912012112212312412512612712812913013113213313413513613713813914014114214314414514614714814915015115215315415515615715815916016116216316416516616716816917017117217317417517617717817918018118218318418518618718818919019119219319419519619719819920020120220320420520620720820921021121221321421521621721821922022122222322422522622722822923023123223323423523623723823924024124224324424524624724824925025125225325425525625725825926026126226326426526626726826927027127227327427527627727827928028128228328428528628728828929029129229329429529629729829930030130230330430530630730830931031131231331431531631731831932032132232332432532632732832933033133233333433533633733833934034134234334434534634734834935035135235335435535635735835936036136236336436536636736836937037137237337437537637737837938038138238338438538638738838939039139239339439539639739839940040140240340440540640740840941041141241341441541641741841942042142242342442542642742842943043143243343443543643743843944044144244344444544644744844945045145245345445545645745845946046146246346446546646746846947047147247347447547647747847948048148248348448548648748848949049149249349449549649749849950050150250350450550650750850951051151251351451551651751851952052152252352452552652752852953053153253353453553653753853954054154254354454554654754854955055155255355455555655755855956056156256356456556656756856957057157257357457557657757857958058158258358458558658758858959059159259359459559659759859960060160260360460560660760860961061161261361461561661761861962062162262362462562662762862963063163263363463563663763863964064164264364464564664764864965065165265365465565665765865966066166266366466566666766866967067167267367467567667767867968068168268368468568668768868969069169269369469569669769869970070170270370470570670770870971071171271371471571671771871972072172272372472572672772872973073173273373473573673773873974074174274374474574674774874975075175275375475575675775875976076176276376476576676776876977077177277377477577677777877978078178278378478578678778878979079179279379479579679779879980080180280380480580680780880981081181281381481581681781881982082182282382482582682782882983083183283383483583683783883984084184284384484584684784884985085185285385485585685785885986086186286386486586686786886987087187287387487587687787887988088188288388488588688788888989089189289389489589689789889990090190290390490590690790890991091191291391491591691791891992092192292392492592692792892993093193293393493593693793893994094194294394494594694794894995095195295395495595695795895996096196296396496596696796896997097197297397497597697797897998098198298398498598698798898999099199299399499599699799899910001001100210031004100510061007100810091010101110121013101410151016101710181019102010211022102310241025102610271028102910301031103210331034103510361037103810391040104110421043104410451046104710481049105010511052105310541055105610571058105910601061106210631064106510661067106810691070107110721073107410751076107710781079108010811082108310841085108610871088108910901091109210931094109510961097109810991100110111021103110411051106110711081109111011111112111311141115111611171118111911201121112211231124112511261127112811291130113111321133113411351136113711381139114011411142114311441145114611471148114911501151115211531154115511561157115811591160116111621163116411651166116711681169117011711172117311741175117611771178117911801181118211831184118511861187118811891190119111921193119411951196119711981199120012011202120312041205120612071208120912101211121212131214121512161217121812191220122112221223122412251226122712281229123012311232123312341235123612371238123912401241124212431244124512461247124812491250125112521253125412551256125712581259126012611262126312641265126612671268126912701271127212731274127512761277127812791280128112821283128412851286128712881289129012911292129312941295129612971298129913001301130213031304130513061307130813091310131113121313131413151316131713181319132013211322132313241325132613271328132913301331133213331334133513361337133813391340134113421343134413451346134713481349135013511352135313541355135613571358135913601361136213631364136513661367136813691370137113721373137413751376137713781379138013811382138313841385138613871388138913901391139213931394139513961397139813991400140114021403140414051406140714081409141014111412141314141415141614171418141914201421142214231424142514261427142814291430143114321433143414351436143714381439144014411442144314441445144614471448144914501451145214531454145514561457145814591460146114621463146414651466146714681469147014711472147314741475147614771478147914801481148214831484148514861487148814891490149114921493149414951496149714981499150015011502150315041505150615071508150915101511151215131514151515161517151815191520152115221523152415251526152715281529153015311532153315341535153615371538153915401541154215431544154515461547154815491550155115521553155415551556155715581559156015611562156315641565156615671568156915701571157215731574157515761577157815791580158115821583158415851586158715881589159015911592159315941595159615971598159916001601160216031604160516061607160816091610161116121613161416151616161716181619162016211622162316241625162616271628162916301631163216331634163516361637163816391640164116421643164416451646164716481649165016511652165316541655165616571658165916601661166216631664166516661667166816691670167116721673167416751676167716781679168016811682168316841685168616871688168916901691169216931694169516961697169816991700170117021703170417051706170717081709171017111712171317141715171617171718171917201721172217231724172517261727172817291730173117321733173417351736173717381739174017411742174317441745174617471748174917501751175217531754175517561757175817591760176117621763176417651766176717681769177017711772177317741775177617771778177917801781178217831784178517861787178817891790179117921793179417951796179717981799180018011802180318041805180618071808180918101811181218131814181518161817181818191820182118221823182418251826182718281829183018311832183318341835183618371838183918401841184218431844184518461847184818491850185118521853185418551856185718581859186018611862186318641865186618671868186918701871187218731874187518761877187818791880188118821883188418851886188718881889189018911892189318941895189618971898189919001901190219031904190519061907190819091910191119121913191419151916191719181919192019211922192319241925192619271928192919301931193219331934193519361937193819391940194119421943194419451946194719481949195019511952195319541955195619571958195919601961196219631964196519661967196819691970197119721973197419751976197719781979198019811982198319841985198619871988198919901991199219931994199519961997199819992000200120022003200420052006200720082009201020112012201320142015201620172018201920202021202220232024202520262027202820292030203120322033203420352036203720382039204020412042204320442045204620472048204920502051# Redis configuration file example.## Note that in order to read the configuration file, Redis must be# started with the file path as first argument:## ./redis-server /path/to/redis.conf# Note on units: when memory size is needed, it is possible to specify# it in the usual form of 1k 5GB 4M and so forth:## 1k =&gt; 1000 bytes# 1kb =&gt; 1024 bytes# 1m =&gt; 1000000 bytes# 1mb =&gt; 1024*1024 bytes# 1g =&gt; 1000000000 bytes# 1gb =&gt; 1024*1024*1024 bytes## units are case insensitive so 1GB 1Gb 1gB are all the same.################################## INCLUDES #################################### Include one or more other config files here. This is useful if you# have a standard template that goes to all Redis servers but also need# to customize a few per-server settings. Include files can include# other files, so use this wisely.## Note that option \"include\" won't be rewritten by command \"CONFIG REWRITE\"# from admin or Redis Sentinel. Since Redis always uses the last processed# line as value of a configuration directive, you'd better put includes# at the beginning of this file to avoid overwriting config change at runtime.## If instead you are interested in using includes to override configuration# options, it is better to use include as the last line.## include /path/to/local.conf# include /path/to/other.conf################################## MODULES ###################################### Load modules at startup. If the server is not able to load modules# it will abort. It is possible to use multiple loadmodule directives.## loadmodule /path/to/my_module.so# loadmodule /path/to/other_module.so################################## NETWORK ###################################### By default, if no \"bind\" configuration directive is specified, Redis listens# for connections from all available network interfaces on the host machine.# It is possible to listen to just one or multiple selected interfaces using# the \"bind\" configuration directive, followed by one or more IP addresses.# Each address can be prefixed by \"-\", which means that redis will not fail to# start if the address is not available. Being not available only refers to# addresses that does not correspond to any network interfece. Addresses that# are already in use will always fail, and unsupported protocols will always BE# silently skipped.## Examples:## bind 192.168.1.100 10.0.0.1 # listens on two specific IPv4 addresses# bind 127.0.0.1 ::1 # listens on loopback IPv4 and IPv6# bind * -::* # like the default, all available interfaces## ~~~ WARNING ~~~ If the computer running Redis is directly exposed to the# internet, binding to all the interfaces is dangerous and will expose the# instance to everybody on the internet. So by default we uncomment the# following bind directive, that will force Redis to listen only on the# IPv4 and IPv6 (if available) loopback interface addresses (this means Redis# will only be able to accept client connections from the same host that it is# running on).## IF YOU ARE SURE YOU WANT YOUR INSTANCE TO LISTEN TO ALL THE INTERFACES# JUST COMMENT OUT THE FOLLOWING LINE.# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~bind 0.0.0.0# Protected mode is a layer of security protection, in order to avoid that# Redis instances left open on the internet are accessed and exploited.## When protected mode is on and if:## 1) The server is not binding explicitly to a set of addresses using the# \"bind\" directive.# 2) No password is configured.## The server only accepts connections from clients connecting from the# IPv4 and IPv6 loopback addresses 127.0.0.1 and ::1, and from Unix domain# sockets.## By default protected mode is enabled. You should disable it only if# you are sure you want clients from other hosts to connect to Redis# even if no authentication is configured, nor a specific set of interfaces# are explicitly listed using the \"bind\" directive.protected-mode no# Accept connections on the specified port, default is 6379 (IANA #815344).# If port 0 is specified Redis will not listen on a TCP socket.port 6379# TCP listen() backlog.## In high requests-per-second environments you need a high backlog in order# to avoid slow clients connection issues. Note that the Linux kernel# will silently truncate it to the value of /proc/sys/net/core/somaxconn so# make sure to raise both the value of somaxconn and tcp_max_syn_backlog# in order to get the desired effect.tcp-backlog 511# Unix socket.## Specify the path for the Unix socket that will be used to listen for# incoming connections. There is no default, so Redis will not listen# on a unix socket when not specified.## unixsocket /run/redis.sock# unixsocketperm 700# Close the connection after a client is idle for N seconds (0 to disable)timeout 0# TCP keepalive.## If non-zero, use SO_KEEPALIVE to send TCP ACKs to clients in absence# of communication. This is useful for two reasons:## 1) Detect dead peers.# 2) Force network equipment in the middle to consider the connection to be# alive.## On Linux, the specified value (in seconds) is the period used to send ACKs.# Note that to close the connection the double of the time is needed.# On other kernels the period depends on the kernel configuration.## A reasonable value for this option is 300 seconds, which is the new# Redis default starting with Redis 3.2.1.tcp-keepalive 300################################# TLS/SSL ###################################### By default, TLS/SSL is disabled. To enable it, the \"tls-port\" configuration# directive can be used to define TLS-listening ports. To enable TLS on the# default port, use:## port 0# tls-port 6379# Configure a X.509 certificate and private key to use for authenticating the# server to connected clients, masters or cluster peers. These files should be# PEM formatted.## tls-cert-file redis.crt # tls-key-file redis.key## If the key file is encrypted using a passphrase, it can be included here# as well.## tls-key-file-pass secret# Normally Redis uses the same certificate for both server functions (accepting# connections) and client functions (replicating from a master, establishing# cluster bus connections, etc.).## Sometimes certificates are issued with attributes that designate them as# client-only or server-only certificates. In that case it may be desired to use# different certificates for incoming (server) and outgoing (client)# connections. To do that, use the following directives:## tls-client-cert-file client.crt# tls-client-key-file client.key## If the key file is encrypted using a passphrase, it can be included here# as well.## tls-client-key-file-pass secret# Configure a DH parameters file to enable Diffie-Hellman (DH) key exchange:## tls-dh-params-file redis.dh# Configure a CA certificate(s) bundle or directory to authenticate TLS/SSL# clients and peers. Redis requires an explicit configuration of at least one# of these, and will not implicitly use the system wide configuration.## tls-ca-cert-file ca.crt# tls-ca-cert-dir /etc/ssl/certs# By default, clients (including replica servers) on a TLS port are required# to authenticate using valid client side certificates.## If \"no\" is specified, client certificates are not required and not accepted.# If \"optional\" is specified, client certificates are accepted and must be# valid if provided, but are not required.## tls-auth-clients no# tls-auth-clients optional# By default, a Redis replica does not attempt to establish a TLS connection# with its master.## Use the following directive to enable TLS on replication links.## tls-replication yes# By default, the Redis Cluster bus uses a plain TCP connection. To enable# TLS for the bus protocol, use the following directive:## tls-cluster yes# By default, only TLSv1.2 and TLSv1.3 are enabled and it is highly recommended# that older formally deprecated versions are kept disabled to reduce the attack surface.# You can explicitly specify TLS versions to support.# Allowed values are case insensitive and include \"TLSv1\", \"TLSv1.1\", \"TLSv1.2\",# \"TLSv1.3\" (OpenSSL &gt;= 1.1.1) or any combination.# To enable only TLSv1.2 and TLSv1.3, use:## tls-protocols \"TLSv1.2 TLSv1.3\"# Configure allowed ciphers. See the ciphers(1ssl) manpage for more information# about the syntax of this string.## Note: this configuration applies only to &lt;= TLSv1.2.## tls-ciphers DEFAULT:!MEDIUM# Configure allowed TLSv1.3 ciphersuites. See the ciphers(1ssl) manpage for more# information about the syntax of this string, and specifically for TLSv1.3# ciphersuites.## tls-ciphersuites TLS_CHACHA20_POLY1305_SHA256# When choosing a cipher, use the server's preference instead of the client# preference. By default, the server follows the client's preference.## tls-prefer-server-ciphers yes# By default, TLS session caching is enabled to allow faster and less expensive# reconnections by clients that support it. Use the following directive to disable# caching.## tls-session-caching no# Change the default number of TLS sessions cached. A zero value sets the cache# to unlimited size. The default size is 20480.## tls-session-cache-size 5000# Change the default timeout of cached TLS sessions. The default timeout is 300# seconds.## tls-session-cache-timeout 60################################# GENERAL ###################################### By default Redis does not run as a daemon. Use 'yes' if you need it.# Note that Redis will write a pid file in /var/run/redis.pid when daemonized.# When Redis is supervised by upstart or systemd, this parameter has no impact.daemonize no# If you run Redis from upstart or systemd, Redis can interact with your# supervision tree. Options:# supervised no - no supervision interaction# supervised upstart - signal upstart by putting Redis into SIGSTOP mode# requires \"expect stop\" in your upstart job config# supervised systemd - signal systemd by writing READY=1 to $NOTIFY_SOCKET# on startup, and updating Redis status on a regular# basis.# supervised auto - detect upstart or systemd method based on# UPSTART_JOB or NOTIFY_SOCKET environment variables# Note: these supervision methods only signal \"process is ready.\"# They do not enable continuous pings back to your supervisor.## The default is \"no\". To run under upstart/systemd, you can simply uncomment# the line below:## supervised auto# If a pid file is specified, Redis writes it where specified at startup# and removes it at exit.## When the server runs non daemonized, no pid file is created if none is# specified in the configuration. When the server is daemonized, the pid file# is used even if not specified, defaulting to \"/var/run/redis.pid\".## Creating a pid file is best effort: if Redis is not able to create it# nothing bad happens, the server will start and run normally.## Note that on modern Linux systems \"/run/redis.pid\" is more conforming# and should be used instead.pidfile /var/run/redis_6379.pid# Specify the server verbosity level.# This can be one of:# debug (a lot of information, useful for development/testing)# verbose (many rarely useful info, but not a mess like the debug level)# notice (moderately verbose, what you want in production probably)# warning (only very important / critical messages are logged)loglevel notice# Specify the log file name. Also the empty string can be used to force# Redis to log on the standard output. Note that if you use standard# output for logging but daemonize, logs will be sent to /dev/nulllogfile \"\"# To enable logging to the system logger, just set 'syslog-enabled' to yes,# and optionally update the other syslog parameters to suit your needs.# syslog-enabled no# Specify the syslog identity.# syslog-ident redis# Specify the syslog facility. Must be USER or between LOCAL0-LOCAL7.# syslog-facility local0# To disable the built in crash log, which will possibly produce cleaner core# dumps when they are needed, uncomment the following:## crash-log-enabled no# To disable the fast memory check that's run as part of the crash log, which# will possibly let redis terminate sooner, uncomment the following:## crash-memcheck-enabled no# Set the number of databases. The default database is DB 0, you can select# a different one on a per-connection basis using SELECT &lt;dbid&gt; where# dbid is a number between 0 and 'databases'-1databases 16# By default Redis shows an ASCII art logo only when started to log to the# standard output and if the standard output is a TTY and syslog logging is# disabled. Basically this means that normally a logo is displayed only in# interactive sessions.## However it is possible to force the pre-4.0 behavior and always show a# ASCII art logo in startup logs by setting the following option to yes.always-show-logo no# By default, Redis modifies the process title (as seen in 'top' and 'ps') to# provide some runtime information. It is possible to disable this and leave# the process name as executed by setting the following to no.set-proc-title yes# When changing the process title, Redis uses the following template to construct# the modified title.## Template variables are specified in curly brackets. The following variables are# supported:## {title} Name of process as executed if parent, or type of child process.# {listen-addr} Bind address or '*' followed by TCP or TLS port listening on, or# Unix socket if only that's available.# {server-mode} Special mode, i.e. \"[sentinel]\" or \"[cluster]\".# {port} TCP port listening on, or 0.# {tls-port} TLS port listening on, or 0.# {unixsocket} Unix domain socket listening on, or \"\".# {config-file} Name of configuration file used.#proc-title-template \"{title} {listen-addr} {server-mode}\"################################ SNAPSHOTTING ################################# Save the DB to disk.## save &lt;seconds&gt; &lt;changes&gt;## Redis will save the DB if both the given number of seconds and the given# number of write operations against the DB occurred.## Snapshotting can be completely disabled with a single empty string argument# as in following example:## save \"\"## Unless specified otherwise, by default Redis will save the DB:# * After 3600 seconds (an hour) if at least 1 key changed# * After 300 seconds (5 minutes) if at least 100 keys changed# * After 60 seconds if at least 10000 keys changed## You can set these explicitly by uncommenting the three following lines.## save 3600 1# save 300 100# save 60 10000# By default Redis will stop accepting writes if RDB snapshots are enabled# (at least one save point) and the latest background save failed.# This will make the user aware (in a hard way) that data is not persisting# on disk properly, otherwise chances are that no one will notice and some# disaster will happen.## If the background saving process will start working again Redis will# automatically allow writes again.## However if you have setup your proper monitoring of the Redis server# and persistence, you may want to disable this feature so that Redis will# continue to work as usual even if there are problems with disk,# permissions, and so forth.stop-writes-on-bgsave-error yes# Compress string objects using LZF when dump .rdb databases?# By default compression is enabled as it's almost always a win.# If you want to save some CPU in the saving child set it to 'no' but# the dataset will likely be bigger if you have compressible values or keys.rdbcompression yes# Since version 5 of RDB a CRC64 checksum is placed at the end of the file.# This makes the format more resistant to corruption but there is a performance# hit to pay (around 10%) when saving and loading RDB files, so you can disable it# for maximum performances.## RDB files created with checksum disabled have a checksum of zero that will# tell the loading code to skip the check.rdbchecksum yes# Enables or disables full sanitation checks for ziplist and listpack etc when# loading an RDB or RESTORE payload. This reduces the chances of a assertion or# crash later on while processing commands.# Options:# no - Never perform full sanitation# yes - Always perform full sanitation# clients - Perform full sanitation only for user connections.# Excludes: RDB files, RESTORE commands received from the master# connection, and client connections which have the# skip-sanitize-payload ACL flag.# The default should be 'clients' but since it currently affects cluster# resharding via MIGRATE, it is temporarily set to 'no' by default.## sanitize-dump-payload no# The filename where to dump the DBdbfilename dump.rdb# Remove RDB files used by replication in instances without persistence# enabled. By default this option is disabled, however there are environments# where for regulations or other security concerns, RDB files persisted on# disk by masters in order to feed replicas, or stored on disk by replicas# in order to load them for the initial synchronization, should be deleted# ASAP. Note that this option ONLY WORKS in instances that have both AOF# and RDB persistence disabled, otherwise is completely ignored.## An alternative (and sometimes better) way to obtain the same effect is# to use diskless replication on both master and replicas instances. However# in the case of replicas, diskless is not always an option.rdb-del-sync-files no# The working directory.## The DB will be written inside this directory, with the filename specified# above using the 'dbfilename' configuration directive.## The Append Only File will also be created inside this directory.## Note that you must specify a directory here, not a file name.dir ./################################# REPLICATION ################################## Master-Replica replication. Use replicaof to make a Redis instance a copy of# another Redis server. A few things to understand ASAP about Redis replication.## +------------------+ +---------------+# | Master | ---&gt; | Replica |# | (receive writes) | | (exact copy) |# +------------------+ +---------------+## 1) Redis replication is asynchronous, but you can configure a master to# stop accepting writes if it appears to be not connected with at least# a given number of replicas.# 2) Redis replicas are able to perform a partial resynchronization with the# master if the replication link is lost for a relatively small amount of# time. You may want to configure the replication backlog size (see the next# sections of this file) with a sensible value depending on your needs.# 3) Replication is automatic and does not need user intervention. After a# network partition replicas automatically try to reconnect to masters# and resynchronize with them.## replicaof &lt;masterip&gt; &lt;masterport&gt;# If the master is password protected (using the \"requirepass\" configuration# directive below) it is possible to tell the replica to authenticate before# starting the replication synchronization process, otherwise the master will# refuse the replica request.## masterauth &lt;master-password&gt;## However this is not enough if you are using Redis ACLs (for Redis version# 6 or greater), and the default user is not capable of running the PSYNC# command and/or other commands needed for replication. In this case it's# better to configure a special user to use with replication, and specify the# masteruser configuration as such:## masteruser &lt;username&gt;## When masteruser is specified, the replica will authenticate against its# master using the new AUTH form: AUTH &lt;username&gt; &lt;password&gt;.# When a replica loses its connection with the master, or when the replication# is still in progress, the replica can act in two different ways:## 1) if replica-serve-stale-data is set to 'yes' (the default) the replica will# still reply to client requests, possibly with out of date data, or the# data set may just be empty if this is the first synchronization.## 2) If replica-serve-stale-data is set to 'no' the replica will reply with# an error \"SYNC with master in progress\" to all commands except:# INFO, REPLICAOF, AUTH, PING, SHUTDOWN, REPLCONF, ROLE, CONFIG, SUBSCRIBE,# UNSUBSCRIBE, PSUBSCRIBE, PUNSUBSCRIBE, PUBLISH, PUBSUB, COMMAND, POST,# HOST and LATENCY.#replica-serve-stale-data yes# You can configure a replica instance to accept writes or not. Writing against# a replica instance may be useful to store some ephemeral data (because data# written on a replica will be easily deleted after resync with the master) but# may also cause problems if clients are writing to it because of a# misconfiguration.## Since Redis 2.6 by default replicas are read-only.## Note: read only replicas are not designed to be exposed to untrusted clients# on the internet. It's just a protection layer against misuse of the instance.# Still a read only replica exports by default all the administrative commands# such as CONFIG, DEBUG, and so forth. To a limited extent you can improve# security of read only replicas using 'rename-command' to shadow all the# administrative / dangerous commands.replica-read-only yes# Replication SYNC strategy: disk or socket.## New replicas and reconnecting replicas that are not able to continue the# replication process just receiving differences, need to do what is called a# \"full synchronization\". An RDB file is transmitted from the master to the# replicas.## The transmission can happen in two different ways:## 1) Disk-backed: The Redis master creates a new process that writes the RDB# file on disk. Later the file is transferred by the parent# process to the replicas incrementally.# 2) Diskless: The Redis master creates a new process that directly writes the# RDB file to replica sockets, without touching the disk at all.## With disk-backed replication, while the RDB file is generated, more replicas# can be queued and served with the RDB file as soon as the current child# producing the RDB file finishes its work. With diskless replication instead# once the transfer starts, new replicas arriving will be queued and a new# transfer will start when the current one terminates.## When diskless replication is used, the master waits a configurable amount of# time (in seconds) before starting the transfer in the hope that multiple# replicas will arrive and the transfer can be parallelized.## With slow disks and fast (large bandwidth) networks, diskless replication# works better.repl-diskless-sync no# When diskless replication is enabled, it is possible to configure the delay# the server waits in order to spawn the child that transfers the RDB via socket# to the replicas.## This is important since once the transfer starts, it is not possible to serve# new replicas arriving, that will be queued for the next RDB transfer, so the# server waits a delay in order to let more replicas arrive.## The delay is specified in seconds, and by default is 5 seconds. To disable# it entirely just set it to 0 seconds and the transfer will start ASAP.repl-diskless-sync-delay 5# -----------------------------------------------------------------------------# WARNING: RDB diskless load is experimental. Since in this setup the replica# does not immediately store an RDB on disk, it may cause data loss during# failovers. RDB diskless load + Redis modules not handling I/O reads may also# cause Redis to abort in case of I/O errors during the initial synchronization# stage with the master. Use only if you know what you are doing.# -----------------------------------------------------------------------------## Replica can load the RDB it reads from the replication link directly from the# socket, or store the RDB to a file and read that file after it was completely# received from the master.## In many cases the disk is slower than the network, and storing and loading# the RDB file may increase replication time (and even increase the master's# Copy on Write memory and salve buffers).# However, parsing the RDB file directly from the socket may mean that we have# to flush the contents of the current database before the full rdb was# received. For this reason we have the following options:## \"disabled\" - Don't use diskless load (store the rdb file to the disk first)# \"on-empty-db\" - Use diskless load only when it is completely safe.# \"swapdb\" - Keep a copy of the current db contents in RAM while parsing# the data directly from the socket. note that this requires# sufficient memory, if you don't have it, you risk an OOM kill.repl-diskless-load disabled# Replicas send PINGs to server in a predefined interval. It's possible to# change this interval with the repl_ping_replica_period option. The default# value is 10 seconds.## repl-ping-replica-period 10# The following option sets the replication timeout for:## 1) Bulk transfer I/O during SYNC, from the point of view of replica.# 2) Master timeout from the point of view of replicas (data, pings).# 3) Replica timeout from the point of view of masters (REPLCONF ACK pings).## It is important to make sure that this value is greater than the value# specified for repl-ping-replica-period otherwise a timeout will be detected# every time there is low traffic between the master and the replica. The default# value is 60 seconds.## repl-timeout 60# Disable TCP_NODELAY on the replica socket after SYNC?## If you select \"yes\" Redis will use a smaller number of TCP packets and# less bandwidth to send data to replicas. But this can add a delay for# the data to appear on the replica side, up to 40 milliseconds with# Linux kernels using a default configuration.## If you select \"no\" the delay for data to appear on the replica side will# be reduced but more bandwidth will be used for replication.## By default we optimize for low latency, but in very high traffic conditions# or when the master and replicas are many hops away, turning this to \"yes\" may# be a good idea.repl-disable-tcp-nodelay no# Set the replication backlog size. The backlog is a buffer that accumulates# replica data when replicas are disconnected for some time, so that when a# replica wants to reconnect again, often a full resync is not needed, but a# partial resync is enough, just passing the portion of data the replica# missed while disconnected.## The bigger the replication backlog, the longer the replica can endure the# disconnect and later be able to perform a partial resynchronization.## The backlog is only allocated if there is at least one replica connected.## repl-backlog-size 1mb# After a master has no connected replicas for some time, the backlog will be# freed. The following option configures the amount of seconds that need to# elapse, starting from the time the last replica disconnected, for the backlog# buffer to be freed.## Note that replicas never free the backlog for timeout, since they may be# promoted to masters later, and should be able to correctly \"partially# resynchronize\" with other replicas: hence they should always accumulate backlog.## A value of 0 means to never release the backlog.## repl-backlog-ttl 3600# The replica priority is an integer number published by Redis in the INFO# output. It is used by Redis Sentinel in order to select a replica to promote# into a master if the master is no longer working correctly.## A replica with a low priority number is considered better for promotion, so# for instance if there are three replicas with priority 10, 100, 25 Sentinel# will pick the one with priority 10, that is the lowest.## However a special priority of 0 marks the replica as not able to perform the# role of master, so a replica with priority of 0 will never be selected by# Redis Sentinel for promotion.## By default the priority is 100.replica-priority 100# -----------------------------------------------------------------------------# By default, Redis Sentinel includes all replicas in its reports. A replica# can be excluded from Redis Sentinel's announcements. An unannounced replica# will be ignored by the 'sentinel replicas &lt;master&gt;' command and won't be# exposed to Redis Sentinel's clients.## This option does not change the behavior of replica-priority. Even with# replica-announced set to 'no', the replica can be promoted to master. To# prevent this behavior, set replica-priority to 0.## replica-announced yes# It is possible for a master to stop accepting writes if there are less than# N replicas connected, having a lag less or equal than M seconds.## The N replicas need to be in \"online\" state.## The lag in seconds, that must be &lt;= the specified value, is calculated from# the last ping received from the replica, that is usually sent every second.## This option does not GUARANTEE that N replicas will accept the write, but# will limit the window of exposure for lost writes in case not enough replicas# are available, to the specified number of seconds.## For example to require at least 3 replicas with a lag &lt;= 10 seconds use:## min-replicas-to-write 3# min-replicas-max-lag 10## Setting one or the other to 0 disables the feature.## By default min-replicas-to-write is set to 0 (feature disabled) and# min-replicas-max-lag is set to 10.# A Redis master is able to list the address and port of the attached# replicas in different ways. For example the \"INFO replication\" section# offers this information, which is used, among other tools, by# Redis Sentinel in order to discover replica instances.# Another place where this info is available is in the output of the# \"ROLE\" command of a master.## The listed IP address and port normally reported by a replica is# obtained in the following way:## IP: The address is auto detected by checking the peer address# of the socket used by the replica to connect with the master.## Port: The port is communicated by the replica during the replication# handshake, and is normally the port that the replica is using to# listen for connections.## However when port forwarding or Network Address Translation (NAT) is# used, the replica may actually be reachable via different IP and port# pairs. The following two options can be used by a replica in order to# report to its master a specific set of IP and port, so that both INFO# and ROLE will report those values.## There is no need to use both the options if you need to override just# the port or the IP address.## replica-announce-ip 5.5.5.5# replica-announce-port 1234############################### KEYS TRACKING ################################## Redis implements server assisted support for client side caching of values.# This is implemented using an invalidation table that remembers, using# a radix key indexed by key name, what clients have which keys. In turn# this is used in order to send invalidation messages to clients. Please# check this page to understand more about the feature:## https://redis.io/topics/client-side-caching## When tracking is enabled for a client, all the read only queries are assumed# to be cached: this will force Redis to store information in the invalidation# table. When keys are modified, such information is flushed away, and# invalidation messages are sent to the clients. However if the workload is# heavily dominated by reads, Redis could use more and more memory in order# to track the keys fetched by many clients.## For this reason it is possible to configure a maximum fill value for the# invalidation table. By default it is set to 1M of keys, and once this limit# is reached, Redis will start to evict keys in the invalidation table# even if they were not modified, just to reclaim memory: this will in turn# force the clients to invalidate the cached values. Basically the table# maximum size is a trade off between the memory you want to spend server# side to track information about who cached what, and the ability of clients# to retain cached objects in memory.## If you set the value to 0, it means there are no limits, and Redis will# retain as many keys as needed in the invalidation table.# In the \"stats\" INFO section, you can find information about the number of# keys in the invalidation table at every given moment.## Note: when key tracking is used in broadcasting mode, no memory is used# in the server side so this setting is useless.## tracking-table-max-keys 1000000################################## SECURITY #################################### Warning: since Redis is pretty fast, an outside user can try up to# 1 million passwords per second against a modern box. This means that you# should use very strong passwords, otherwise they will be very easy to break.# Note that because the password is really a shared secret between the client# and the server, and should not be memorized by any human, the password# can be easily a long string from /dev/urandom or whatever, so by using a# long and unguessable password no brute force attack will be possible.# Redis ACL users are defined in the following format:## user &lt;username&gt; ... acl rules ...## For example:## user worker +@list +@connection ~jobs:* on &gt;ffa9203c493aa99## The special username \"default\" is used for new connections. If this user# has the \"nopass\" rule, then new connections will be immediately authenticated# as the \"default\" user without the need of any password provided via the# AUTH command. Otherwise if the \"default\" user is not flagged with \"nopass\"# the connections will start in not authenticated state, and will require# AUTH (or the HELLO command AUTH option) in order to be authenticated and# start to work.## The ACL rules that describe what a user can do are the following:## on Enable the user: it is possible to authenticate as this user.# off Disable the user: it's no longer possible to authenticate# with this user, however the already authenticated connections# will still work.# skip-sanitize-payload RESTORE dump-payload sanitation is skipped.# sanitize-payload RESTORE dump-payload is sanitized (default).# +&lt;command&gt; Allow the execution of that command# -&lt;command&gt; Disallow the execution of that command# +@&lt;category&gt; Allow the execution of all the commands in such category# with valid categories are like @admin, @set, @sortedset, ...# and so forth, see the full list in the server.c file where# the Redis command table is described and defined.# The special category @all means all the commands, but currently# present in the server, and that will be loaded in the future# via modules.# +&lt;command&gt;|subcommand Allow a specific subcommand of an otherwise# disabled command. Note that this form is not# allowed as negative like -DEBUG|SEGFAULT, but# only additive starting with \"+\".# allcommands Alias for +@all. Note that it implies the ability to execute# all the future commands loaded via the modules system.# nocommands Alias for -@all.# ~&lt;pattern&gt; Add a pattern of keys that can be mentioned as part of# commands. For instance ~* allows all the keys. The pattern# is a glob-style pattern like the one of KEYS.# It is possible to specify multiple patterns.# allkeys Alias for ~*# resetkeys Flush the list of allowed keys patterns.# &amp;&lt;pattern&gt; Add a glob-style pattern of Pub/Sub channels that can be# accessed by the user. It is possible to specify multiple channel# patterns.# allchannels Alias for &amp;*# resetchannels Flush the list of allowed channel patterns.# &gt;&lt;password&gt; Add this password to the list of valid password for the user.# For example &gt;mypass will add \"mypass\" to the list.# This directive clears the \"nopass\" flag (see later).# &lt;&lt;password&gt; Remove this password from the list of valid passwords.# nopass All the set passwords of the user are removed, and the user# is flagged as requiring no password: it means that every# password will work against this user. If this directive is# used for the default user, every new connection will be# immediately authenticated with the default user without# any explicit AUTH command required. Note that the \"resetpass\"# directive will clear this condition.# resetpass Flush the list of allowed passwords. Moreover removes the# \"nopass\" status. After \"resetpass\" the user has no associated# passwords and there is no way to authenticate without adding# some password (or setting it as \"nopass\" later).# reset Performs the following actions: resetpass, resetkeys, off,# -@all. The user returns to the same state it has immediately# after its creation.## ACL rules can be specified in any order: for instance you can start with# passwords, then flags, or key patterns. However note that the additive# and subtractive rules will CHANGE MEANING depending on the ordering.# For instance see the following example:## user alice on +@all -DEBUG ~* &gt;somepassword## This will allow \"alice\" to use all the commands with the exception of the# DEBUG command, since +@all added all the commands to the set of the commands# alice can use, and later DEBUG was removed. However if we invert the order# of two ACL rules the result will be different:## user alice on -DEBUG +@all ~* &gt;somepassword## Now DEBUG was removed when alice had yet no commands in the set of allowed# commands, later all the commands are added, so the user will be able to# execute everything.## Basically ACL rules are processed left-to-right.## For more information about ACL configuration please refer to# the Redis web site at https://redis.io/topics/acl# ACL LOG## The ACL Log tracks failed commands and authentication events associated# with ACLs. The ACL Log is useful to troubleshoot failed commands blocked # by ACLs. The ACL Log is stored in memory. You can reclaim memory with # ACL LOG RESET. Define the maximum entry length of the ACL Log below.acllog-max-len 128# Using an external ACL file## Instead of configuring users here in this file, it is possible to use# a stand-alone file just listing users. The two methods cannot be mixed:# if you configure users here and at the same time you activate the external# ACL file, the server will refuse to start.## The format of the external ACL user file is exactly the same as the# format that is used inside redis.conf to describe users.## aclfile /etc/redis/users.acl# IMPORTANT NOTE: starting with Redis 6 \"requirepass\" is just a compatibility# layer on top of the new ACL system. The option effect will be just setting# the password for the default user. Clients will still authenticate using# AUTH &lt;password&gt; as usually, or more explicitly with AUTH default &lt;password&gt;# if they follow the new protocol: both will work.## The requirepass is not compatable with aclfile option and the ACL LOAD# command, these will cause requirepass to be ignored.#requirepass 123456# New users are initialized with restrictive permissions by default, via the# equivalent of this ACL rule 'off resetkeys -@all'. Starting with Redis 6.2, it# is possible to manage access to Pub/Sub channels with ACL rules as well. The# default Pub/Sub channels permission if new users is controlled by the # acl-pubsub-default configuration directive, which accepts one of these values:## allchannels: grants access to all Pub/Sub channels# resetchannels: revokes access to all Pub/Sub channels## To ensure backward compatibility while upgrading Redis 6.0, acl-pubsub-default# defaults to the 'allchannels' permission.## Future compatibility note: it is very likely that in a future version of Redis# the directive's default of 'allchannels' will be changed to 'resetchannels' in# order to provide better out-of-the-box Pub/Sub security. Therefore, it is# recommended that you explicitly define Pub/Sub permissions for all users# rather then rely on implicit default values. Once you've set explicit# Pub/Sub for all existing users, you should uncomment the following line.## acl-pubsub-default resetchannels# Command renaming (DEPRECATED).## ------------------------------------------------------------------------# WARNING: avoid using this option if possible. Instead use ACLs to remove# commands from the default user, and put them only in some admin user you# create for administrative purposes.# ------------------------------------------------------------------------## It is possible to change the name of dangerous commands in a shared# environment. For instance the CONFIG command may be renamed into something# hard to guess so that it will still be available for internal-use tools# but not available for general clients.## Example:## rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52## It is also possible to completely kill a command by renaming it into# an empty string:## rename-command CONFIG \"\"## Please note that changing the name of commands that are logged into the# AOF file or transmitted to replicas may cause problems.################################### CLIENTS ##################################### Set the max number of connected clients at the same time. By default# this limit is set to 10000 clients, however if the Redis server is not# able to configure the process file limit to allow for the specified limit# the max number of allowed clients is set to the current file limit# minus 32 (as Redis reserves a few file descriptors for internal uses).## Once the limit is reached Redis will close all the new connections sending# an error 'max number of clients reached'.## IMPORTANT: When Redis Cluster is used, the max number of connections is also# shared with the cluster bus: every node in the cluster will use two# connections, one incoming and another outgoing. It is important to size the# limit accordingly in case of very large clusters.## maxclients 10000############################## MEMORY MANAGEMENT ################################# Set a memory usage limit to the specified amount of bytes.# When the memory limit is reached Redis will try to remove keys# according to the eviction policy selected (see maxmemory-policy).## If Redis can't remove keys according to the policy, or if the policy is# set to 'noeviction', Redis will start to reply with errors to commands# that would use more memory, like SET, LPUSH, and so on, and will continue# to reply to read-only commands like GET.## This option is usually useful when using Redis as an LRU or LFU cache, or to# set a hard memory limit for an instance (using the 'noeviction' policy).## WARNING: If you have replicas attached to an instance with maxmemory on,# the size of the output buffers needed to feed the replicas are subtracted# from the used memory count, so that network problems / resyncs will# not trigger a loop where keys are evicted, and in turn the output# buffer of replicas is full with DELs of keys evicted triggering the deletion# of more keys, and so forth until the database is completely emptied.## In short... if you have replicas attached it is suggested that you set a lower# limit for maxmemory so that there is some free RAM on the system for replica# output buffers (but this is not needed if the policy is 'noeviction').## maxmemory &lt;bytes&gt;# MAXMEMORY POLICY: how Redis will select what to remove when maxmemory# is reached. You can select one from the following behaviors:## volatile-lru -&gt; Evict using approximated LRU, only keys with an expire set.# allkeys-lru -&gt; Evict any key using approximated LRU.# volatile-lfu -&gt; Evict using approximated LFU, only keys with an expire set.# allkeys-lfu -&gt; Evict any key using approximated LFU.# volatile-random -&gt; Remove a random key having an expire set.# allkeys-random -&gt; Remove a random key, any key.# volatile-ttl -&gt; Remove the key with the nearest expire time (minor TTL)# noeviction -&gt; Don't evict anything, just return an error on write operations.## LRU means Least Recently Used# LFU means Least Frequently Used## Both LRU, LFU and volatile-ttl are implemented using approximated# randomized algorithms.## Note: with any of the above policies, when there are no suitable keys for# eviction, Redis will return an error on write operations that require# more memory. These are usually commands that create new keys, add data or# modify existing keys. A few examples are: SET, INCR, HSET, LPUSH, SUNIONSTORE,# SORT (due to the STORE argument), and EXEC (if the transaction includes any# command that requires memory).## The default is:## maxmemory-policy noeviction# LRU, LFU and minimal TTL algorithms are not precise algorithms but approximated# algorithms (in order to save memory), so you can tune it for speed or# accuracy. By default Redis will check five keys and pick the one that was# used least recently, you can change the sample size using the following# configuration directive.## The default of 5 produces good enough results. 10 Approximates very closely# true LRU but costs more CPU. 3 is faster but not very accurate.## maxmemory-samples 5# Eviction processing is designed to function well with the default setting.# If there is an unusually large amount of write traffic, this value may need to# be increased. Decreasing this value may reduce latency at the risk of # eviction processing effectiveness# 0 = minimum latency, 10 = default, 100 = process without regard to latency## maxmemory-eviction-tenacity 10# Starting from Redis 5, by default a replica will ignore its maxmemory setting# (unless it is promoted to master after a failover or manually). It means# that the eviction of keys will be just handled by the master, sending the# DEL commands to the replica as keys evict in the master side.## This behavior ensures that masters and replicas stay consistent, and is usually# what you want, however if your replica is writable, or you want the replica# to have a different memory setting, and you are sure all the writes performed# to the replica are idempotent, then you may change this default (but be sure# to understand what you are doing).## Note that since the replica by default does not evict, it may end using more# memory than the one set via maxmemory (there are certain buffers that may# be larger on the replica, or data structures may sometimes take more memory# and so forth). So make sure you monitor your replicas and make sure they# have enough memory to never hit a real out-of-memory condition before the# master hits the configured maxmemory setting.## replica-ignore-maxmemory yes# Redis reclaims expired keys in two ways: upon access when those keys are# found to be expired, and also in background, in what is called the# \"active expire key\". The key space is slowly and interactively scanned# looking for expired keys to reclaim, so that it is possible to free memory# of keys that are expired and will never be accessed again in a short time.## The default effort of the expire cycle will try to avoid having more than# ten percent of expired keys still in memory, and will try to avoid consuming# more than 25% of total memory and to add latency to the system. However# it is possible to increase the expire \"effort\" that is normally set to# \"1\", to a greater value, up to the value \"10\". At its maximum value the# system will use more CPU, longer cycles (and technically may introduce# more latency), and will tolerate less already expired keys still present# in the system. It's a tradeoff between memory, CPU and latency.## active-expire-effort 1############################# LAZY FREEING ##################################### Redis has two primitives to delete keys. One is called DEL and is a blocking# deletion of the object. It means that the server stops processing new commands# in order to reclaim all the memory associated with an object in a synchronous# way. If the key deleted is associated with a small object, the time needed# in order to execute the DEL command is very small and comparable to most other# O(1) or O(log_N) commands in Redis. However if the key is associated with an# aggregated value containing millions of elements, the server can block for# a long time (even seconds) in order to complete the operation.## For the above reasons Redis also offers non blocking deletion primitives# such as UNLINK (non blocking DEL) and the ASYNC option of FLUSHALL and# FLUSHDB commands, in order to reclaim memory in background. Those commands# are executed in constant time. Another thread will incrementally free the# object in the background as fast as possible.## DEL, UNLINK and ASYNC option of FLUSHALL and FLUSHDB are user-controlled.# It's up to the design of the application to understand when it is a good# idea to use one or the other. However the Redis server sometimes has to# delete keys or flush the whole database as a side effect of other operations.# Specifically Redis deletes objects independently of a user call in the# following scenarios:## 1) On eviction, because of the maxmemory and maxmemory policy configurations,# in order to make room for new data, without going over the specified# memory limit.# 2) Because of expire: when a key with an associated time to live (see the# EXPIRE command) must be deleted from memory.# 3) Because of a side effect of a command that stores data on a key that may# already exist. For example the RENAME command may delete the old key# content when it is replaced with another one. Similarly SUNIONSTORE# or SORT with STORE option may delete existing keys. The SET command# itself removes any old content of the specified key in order to replace# it with the specified string.# 4) During replication, when a replica performs a full resynchronization with# its master, the content of the whole database is removed in order to# load the RDB file just transferred.## In all the above cases the default is to delete objects in a blocking way,# like if DEL was called. However you can configure each case specifically# in order to instead release memory in a non-blocking way like if UNLINK# was called, using the following configuration directives.lazyfree-lazy-eviction nolazyfree-lazy-expire nolazyfree-lazy-server-del noreplica-lazy-flush no# It is also possible, for the case when to replace the user code DEL calls# with UNLINK calls is not easy, to modify the default behavior of the DEL# command to act exactly like UNLINK, using the following configuration# directive:lazyfree-lazy-user-del no# FLUSHDB, FLUSHALL, and SCRIPT FLUSH support both asynchronous and synchronous# deletion, which can be controlled by passing the [SYNC|ASYNC] flags into the# commands. When neither flag is passed, this directive will be used to determine# if the data should be deleted asynchronously.lazyfree-lazy-user-flush no################################ THREADED I/O ################################## Redis is mostly single threaded, however there are certain threaded# operations such as UNLINK, slow I/O accesses and other things that are# performed on side threads.## Now it is also possible to handle Redis clients socket reads and writes# in different I/O threads. Since especially writing is so slow, normally# Redis users use pipelining in order to speed up the Redis performances per# core, and spawn multiple instances in order to scale more. Using I/O# threads it is possible to easily speedup two times Redis without resorting# to pipelining nor sharding of the instance.## By default threading is disabled, we suggest enabling it only in machines# that have at least 4 or more cores, leaving at least one spare core.# Using more than 8 threads is unlikely to help much. We also recommend using# threaded I/O only if you actually have performance problems, with Redis# instances being able to use a quite big percentage of CPU time, otherwise# there is no point in using this feature.## So for instance if you have a four cores boxes, try to use 2 or 3 I/O# threads, if you have a 8 cores, try to use 6 threads. In order to# enable I/O threads use the following configuration directive:## io-threads 4## Setting io-threads to 1 will just use the main thread as usual.# When I/O threads are enabled, we only use threads for writes, that is# to thread the write(2) syscall and transfer the client buffers to the# socket. However it is also possible to enable threading of reads and# protocol parsing using the following configuration directive, by setting# it to yes:## io-threads-do-reads no## Usually threading reads doesn't help much.## NOTE 1: This configuration directive cannot be changed at runtime via# CONFIG SET. Aso this feature currently does not work when SSL is# enabled.## NOTE 2: If you want to test the Redis speedup using redis-benchmark, make# sure you also run the benchmark itself in threaded mode, using the# --threads option to match the number of Redis threads, otherwise you'll not# be able to notice the improvements.############################ KERNEL OOM CONTROL ############################### On Linux, it is possible to hint the kernel OOM killer on what processes# should be killed first when out of memory.## Enabling this feature makes Redis actively control the oom_score_adj value# for all its processes, depending on their role. The default scores will# attempt to have background child processes killed before all others, and# replicas killed before masters.## Redis supports three options:## no: Don't make changes to oom-score-adj (default).# yes: Alias to \"relative\" see below.# absolute: Values in oom-score-adj-values are written as is to the kernel.# relative: Values are used relative to the initial value of oom_score_adj when# the server starts and are then clamped to a range of -1000 to 1000.# Because typically the initial value is 0, they will often match the# absolute values.oom-score-adj no# When oom-score-adj is used, this directive controls the specific values used# for master, replica and background child processes. Values range -2000 to# 2000 (higher means more likely to be killed).## Unprivileged processes (not root, and without CAP_SYS_RESOURCE capabilities)# can freely increase their value, but not decrease it below its initial# settings. This means that setting oom-score-adj to \"relative\" and setting the# oom-score-adj-values to positive values will always succeed.oom-score-adj-values 0 200 800#################### KERNEL transparent hugepage CONTROL ####################### Usually the kernel Transparent Huge Pages control is set to \"madvise\" or# or \"never\" by default (/sys/kernel/mm/transparent_hugepage/enabled), in which# case this config has no effect. On systems in which it is set to \"always\",# redis will attempt to disable it specifically for the redis process in order# to avoid latency problems specifically with fork(2) and CoW.# If for some reason you prefer to keep it enabled, you can set this config to# \"no\" and the kernel global to \"always\".disable-thp yes############################## APPEND ONLY MODE ################################ By default Redis asynchronously dumps the dataset on disk. This mode is# good enough in many applications, but an issue with the Redis process or# a power outage may result into a few minutes of writes lost (depending on# the configured save points).## The Append Only File is an alternative persistence mode that provides# much better durability. For instance using the default data fsync policy# (see later in the config file) Redis can lose just one second of writes in a# dramatic event like a server power outage, or a single write if something# wrong with the Redis process itself happens, but the operating system is# still running correctly.## AOF and RDB persistence can be enabled at the same time without problems.# If the AOF is enabled on startup Redis will load the AOF, that is the file# with the better durability guarantees.## Please check https://redis.io/topics/persistence for more information.appendonly yes# The name of the append only file (default: \"appendonly.aof\")appendfilename \"appendonly.aof\"# The fsync() call tells the Operating System to actually write data on disk# instead of waiting for more data in the output buffer. Some OS will really flush# data on disk, some other OS will just try to do it ASAP.## Redis supports three different modes:## no: don't fsync, just let the OS flush the data when it wants. Faster.# always: fsync after every write to the append only log. Slow, Safest.# everysec: fsync only one time every second. Compromise.## The default is \"everysec\", as that's usually the right compromise between# speed and data safety. It's up to you to understand if you can relax this to# \"no\" that will let the operating system flush the output buffer when# it wants, for better performances (but if you can live with the idea of# some data loss consider the default persistence mode that's snapshotting),# or on the contrary, use \"always\" that's very slow but a bit safer than# everysec.## More details please check the following article:# http://antirez.com/post/redis-persistence-demystified.html## If unsure, use \"everysec\".# appendfsync alwaysappendfsync everysec# appendfsync no# When the AOF fsync policy is set to always or everysec, and a background# saving process (a background save or AOF log background rewriting) is# performing a lot of I/O against the disk, in some Linux configurations# Redis may block too long on the fsync() call. Note that there is no fix for# this currently, as even performing fsync in a different thread will block# our synchronous write(2) call.## In order to mitigate this problem it's possible to use the following option# that will prevent fsync() from being called in the main process while a# BGSAVE or BGREWRITEAOF is in progress.## This means that while another child is saving, the durability of Redis is# the same as \"appendfsync none\". In practical terms, this means that it is# possible to lose up to 30 seconds of log in the worst scenario (with the# default Linux settings).## If you have latency problems turn this to \"yes\". Otherwise leave it as# \"no\" that is the safest pick from the point of view of durability.no-appendfsync-on-rewrite no# Automatic rewrite of the append only file.# Redis is able to automatically rewrite the log file implicitly calling# BGREWRITEAOF when the AOF log size grows by the specified percentage.## This is how it works: Redis remembers the size of the AOF file after the# latest rewrite (if no rewrite has happened since the restart, the size of# the AOF at startup is used).## This base size is compared to the current size. If the current size is# bigger than the specified percentage, the rewrite is triggered. Also# you need to specify a minimal size for the AOF file to be rewritten, this# is useful to avoid rewriting the AOF file even if the percentage increase# is reached but it is still pretty small.## Specify a percentage of zero in order to disable the automatic AOF# rewrite feature.auto-aof-rewrite-percentage 100auto-aof-rewrite-min-size 64mb# An AOF file may be found to be truncated at the end during the Redis# startup process, when the AOF data gets loaded back into memory.# This may happen when the system where Redis is running# crashes, especially when an ext4 filesystem is mounted without the# data=ordered option (however this can't happen when Redis itself# crashes or aborts but the operating system still works correctly).## Redis can either exit with an error when this happens, or load as much# data as possible (the default now) and start if the AOF file is found# to be truncated at the end. The following option controls this behavior.## If aof-load-truncated is set to yes, a truncated AOF file is loaded and# the Redis server starts emitting a log to inform the user of the event.# Otherwise if the option is set to no, the server aborts with an error# and refuses to start. When the option is set to no, the user requires# to fix the AOF file using the \"redis-check-aof\" utility before to restart# the server.## Note that if the AOF file will be found to be corrupted in the middle# the server will still exit with an error. This option only applies when# Redis will try to read more data from the AOF file but not enough bytes# will be found.aof-load-truncated yes# When rewriting the AOF file, Redis is able to use an RDB preamble in the# AOF file for faster rewrites and recoveries. When this option is turned# on the rewritten AOF file is composed of two different stanzas:## [RDB file][AOF tail]## When loading, Redis recognizes that the AOF file starts with the \"REDIS\"# string and loads the prefixed RDB file, then continues loading the AOF# tail.aof-use-rdb-preamble yes################################ LUA SCRIPTING ################################ Max execution time of a Lua script in milliseconds.## If the maximum execution time is reached Redis will log that a script is# still in execution after the maximum allowed time and will start to# reply to queries with an error.## When a long running script exceeds the maximum execution time only the# SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be# used to stop a script that did not yet call any write commands. The second# is the only way to shut down the server in the case a write command was# already issued by the script but the user doesn't want to wait for the natural# termination of the script.## Set it to 0 or a negative value for unlimited execution without warnings.lua-time-limit 5000################################ REDIS CLUSTER ################################ Normal Redis instances can't be part of a Redis Cluster; only nodes that are# started as cluster nodes can. In order to start a Redis instance as a# cluster node enable the cluster support uncommenting the following:## cluster-enabled yes# Every cluster node has a cluster configuration file. This file is not# intended to be edited by hand. It is created and updated by Redis nodes.# Every Redis Cluster node requires a different cluster configuration file.# Make sure that instances running in the same system do not have# overlapping cluster configuration file names.## cluster-config-file nodes-6379.conf# Cluster node timeout is the amount of milliseconds a node must be unreachable# for it to be considered in failure state.# Most other internal time limits are a multiple of the node timeout.## cluster-node-timeout 15000# A replica of a failing master will avoid to start a failover if its data# looks too old.## There is no simple way for a replica to actually have an exact measure of# its \"data age\", so the following two checks are performed:## 1) If there are multiple replicas able to failover, they exchange messages# in order to try to give an advantage to the replica with the best# replication offset (more data from the master processed).# Replicas will try to get their rank by offset, and apply to the start# of the failover a delay proportional to their rank.## 2) Every single replica computes the time of the last interaction with# its master. This can be the last ping or command received (if the master# is still in the \"connected\" state), or the time that elapsed since the# disconnection with the master (if the replication link is currently down).# If the last interaction is too old, the replica will not try to failover# at all.## The point \"2\" can be tuned by user. Specifically a replica will not perform# the failover if, since the last interaction with the master, the time# elapsed is greater than:## (node-timeout * cluster-replica-validity-factor) + repl-ping-replica-period## So for example if node-timeout is 30 seconds, and the cluster-replica-validity-factor# is 10, and assuming a default repl-ping-replica-period of 10 seconds, the# replica will not try to failover if it was not able to talk with the master# for longer than 310 seconds.## A large cluster-replica-validity-factor may allow replicas with too old data to failover# a master, while a too small value may prevent the cluster from being able to# elect a replica at all.## For maximum availability, it is possible to set the cluster-replica-validity-factor# to a value of 0, which means, that replicas will always try to failover the# master regardless of the last time they interacted with the master.# (However they'll always try to apply a delay proportional to their# offset rank).## Zero is the only value able to guarantee that when all the partitions heal# the cluster will always be able to continue.## cluster-replica-validity-factor 10# Cluster replicas are able to migrate to orphaned masters, that are masters# that are left without working replicas. This improves the cluster ability# to resist to failures as otherwise an orphaned master can't be failed over# in case of failure if it has no working replicas.## Replicas migrate to orphaned masters only if there are still at least a# given number of other working replicas for their old master. This number# is the \"migration barrier\". A migration barrier of 1 means that a replica# will migrate only if there is at least 1 other working replica for its master# and so forth. It usually reflects the number of replicas you want for every# master in your cluster.## Default is 1 (replicas migrate only if their masters remain with at least# one replica). To disable migration just set it to a very large value or# set cluster-allow-replica-migration to 'no'.# A value of 0 can be set but is useful only for debugging and dangerous# in production.## cluster-migration-barrier 1# Turning off this option allows to use less automatic cluster configuration.# It both disables migration to orphaned masters and migration from masters# that became empty.## Default is 'yes' (allow automatic migrations).## cluster-allow-replica-migration yes# By default Redis Cluster nodes stop accepting queries if they detect there# is at least a hash slot uncovered (no available node is serving it).# This way if the cluster is partially down (for example a range of hash slots# are no longer covered) all the cluster becomes, eventually, unavailable.# It automatically returns available as soon as all the slots are covered again.## However sometimes you want the subset of the cluster which is working,# to continue to accept queries for the part of the key space that is still# covered. In order to do so, just set the cluster-require-full-coverage# option to no.## cluster-require-full-coverage yes# This option, when set to yes, prevents replicas from trying to failover its# master during master failures. However the replica can still perform a# manual failover, if forced to do so.## This is useful in different scenarios, especially in the case of multiple# data center operations, where we want one side to never be promoted if not# in the case of a total DC failure.## cluster-replica-no-failover no# This option, when set to yes, allows nodes to serve read traffic while the# the cluster is in a down state, as long as it believes it owns the slots. ## This is useful for two cases. The first case is for when an application # doesn't require consistency of data during node failures or network partitions.# One example of this is a cache, where as long as the node has the data it# should be able to serve it. ## The second use case is for configurations that don't meet the recommended # three shards but want to enable cluster mode and scale later. A # master outage in a 1 or 2 shard configuration causes a read/write outage to the# entire cluster without this option set, with it set there is only a write outage.# Without a quorum of masters, slot ownership will not change automatically. ## cluster-allow-reads-when-down no# In order to setup your cluster make sure to read the documentation# available at https://redis.io web site.########################## CLUSTER DOCKER/NAT support ######################### In certain deployments, Redis Cluster nodes address discovery fails, because# addresses are NAT-ted or because ports are forwarded (the typical case is# Docker and other containers).## In order to make Redis Cluster working in such environments, a static# configuration where each node knows its public address is needed. The# following four options are used for this scope, and are:## * cluster-announce-ip# * cluster-announce-port# * cluster-announce-tls-port# * cluster-announce-bus-port## Each instructs the node about its address, client ports (for connections# without and with TLS) and cluster message bus port. The information is then# published in the header of the bus packets so that other nodes will be able to# correctly map the address of the node publishing the information.## If cluster-tls is set to yes and cluster-announce-tls-port is omitted or set# to zero, then cluster-announce-port refers to the TLS port. Note also that# cluster-announce-tls-port has no effect if cluster-tls is set to no.## If the above options are not used, the normal Redis Cluster auto-detection# will be used instead.## Note that when remapped, the bus port may not be at the fixed offset of# clients port + 10000, so you can specify any port and bus-port depending# on how they get remapped. If the bus-port is not set, a fixed offset of# 10000 will be used as usual.## Example:## cluster-announce-ip 10.1.1.5# cluster-announce-tls-port 6379# cluster-announce-port 0# cluster-announce-bus-port 6380################################## SLOW LOG #################################### The Redis Slow Log is a system to log queries that exceeded a specified# execution time. The execution time does not include the I/O operations# like talking with the client, sending the reply and so forth,# but just the time needed to actually execute the command (this is the only# stage of command execution where the thread is blocked and can not serve# other requests in the meantime).## You can configure the slow log with two parameters: one tells Redis# what is the execution time, in microseconds, to exceed in order for the# command to get logged, and the other parameter is the length of the# slow log. When a new command is logged the oldest one is removed from the# queue of logged commands.# The following time is expressed in microseconds, so 1000000 is equivalent# to one second. Note that a negative number disables the slow log, while# a value of zero forces the logging of every command.slowlog-log-slower-than 10000# There is no limit to this length. Just be aware that it will consume memory.# You can reclaim memory used by the slow log with SLOWLOG RESET.slowlog-max-len 128################################ LATENCY MONITOR ############################### The Redis latency monitoring subsystem samples different operations# at runtime in order to collect data related to possible sources of# latency of a Redis instance.## Via the LATENCY command this information is available to the user that can# print graphs and obtain reports.## The system only logs operations that were performed in a time equal or# greater than the amount of milliseconds specified via the# latency-monitor-threshold configuration directive. When its value is set# to zero, the latency monitor is turned off.## By default latency monitoring is disabled since it is mostly not needed# if you don't have latency issues, and collecting data has a performance# impact, that while very small, can be measured under big load. Latency# monitoring can easily be enabled at runtime using the command# \"CONFIG SET latency-monitor-threshold &lt;milliseconds&gt;\" if needed.latency-monitor-threshold 0############################# EVENT NOTIFICATION ############################### Redis can notify Pub/Sub clients about events happening in the key space.# This feature is documented at https://redis.io/topics/notifications## For instance if keyspace events notification is enabled, and a client# performs a DEL operation on key \"foo\" stored in the Database 0, two# messages will be published via Pub/Sub:## PUBLISH __keyspace@0__:foo del# PUBLISH __keyevent@0__:del foo## It is possible to select the events that Redis will notify among a set# of classes. Every class is identified by a single character:## K Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.# E Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.# g Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...# $ String commands# l List commands# s Set commands# h Hash commands# z Sorted set commands# x Expired events (events generated every time a key expires)# e Evicted events (events generated when a key is evicted for maxmemory)# t Stream commands# d Module key type events# m Key-miss events (Note: It is not included in the 'A' class)# A Alias for g$lshzxetd, so that the \"AKE\" string means all the events# (Except key-miss events which are excluded from 'A' due to their# unique nature).## The \"notify-keyspace-events\" takes as argument a string that is composed# of zero or multiple characters. The empty string means that notifications# are disabled.## Example: to enable list and generic events, from the point of view of the# event name, use:## notify-keyspace-events Elg## Example 2: to get the stream of the expired keys subscribing to channel# name __keyevent@0__:expired use:## notify-keyspace-events Ex## By default all notifications are disabled because most users don't need# this feature and the feature has some overhead. Note that if you don't# specify at least one of K or E, no events will be delivered.notify-keyspace-events \"\"############################### GOPHER SERVER ################################## Redis contains an implementation of the Gopher protocol, as specified in# the RFC 1436 (https://www.ietf.org/rfc/rfc1436.txt).## The Gopher protocol was very popular in the late '90s. It is an alternative# to the web, and the implementation both server and client side is so simple# that the Redis server has just 100 lines of code in order to implement this# support.## What do you do with Gopher nowadays? Well Gopher never *really* died, and# lately there is a movement in order for the Gopher more hierarchical content# composed of just plain text documents to be resurrected. Some want a simpler# internet, others believe that the mainstream internet became too much# controlled, and it's cool to create an alternative space for people that# want a bit of fresh air.## Anyway for the 10nth birthday of the Redis, we gave it the Gopher protocol# as a gift.## --- HOW IT WORKS? ---## The Redis Gopher support uses the inline protocol of Redis, and specifically# two kind of inline requests that were anyway illegal: an empty request# or any request that starts with \"/\" (there are no Redis commands starting# with such a slash). Normal RESP2/RESP3 requests are completely out of the# path of the Gopher protocol implementation and are served as usual as well.## If you open a connection to Redis when Gopher is enabled and send it# a string like \"/foo\", if there is a key named \"/foo\" it is served via the# Gopher protocol.## In order to create a real Gopher \"hole\" (the name of a Gopher site in Gopher# talking), you likely need a script like the following:## https://github.com/antirez/gopher2redis## --- SECURITY WARNING ---## If you plan to put Redis on the internet in a publicly accessible address# to server Gopher pages MAKE SURE TO SET A PASSWORD to the instance.# Once a password is set:## 1. The Gopher server (when enabled, not by default) will still serve# content via Gopher.# 2. However other commands cannot be called before the client will# authenticate.## So use the 'requirepass' option to protect your instance.## Note that Gopher is not currently supported when 'io-threads-do-reads'# is enabled.## To enable Gopher support, uncomment the following line and set the option# from no (the default) to yes.## gopher-enabled no############################### ADVANCED CONFIG ################################ Hashes are encoded using a memory efficient data structure when they have a# small number of entries, and the biggest entry does not exceed a given# threshold. These thresholds can be configured using the following directives.hash-max-ziplist-entries 512hash-max-ziplist-value 64# Lists are also encoded in a special way to save a lot of space.# The number of entries allowed per internal list node can be specified# as a fixed maximum size or a maximum number of elements.# For a fixed maximum size, use -5 through -1, meaning:# -5: max size: 64 Kb &lt;-- not recommended for normal workloads# -4: max size: 32 Kb &lt;-- not recommended# -3: max size: 16 Kb &lt;-- probably not recommended# -2: max size: 8 Kb &lt;-- good# -1: max size: 4 Kb &lt;-- good# Positive numbers mean store up to _exactly_ that number of elements# per list node.# The highest performing option is usually -2 (8 Kb size) or -1 (4 Kb size),# but if your use case is unique, adjust the settings as necessary.list-max-ziplist-size -2# Lists may also be compressed.# Compress depth is the number of quicklist ziplist nodes from *each* side of# the list to *exclude* from compression. The head and tail of the list# are always uncompressed for fast push/pop operations. Settings are:# 0: disable all list compression# 1: depth 1 means \"don't start compressing until after 1 node into the list,# going from either the head or tail\"# So: [head]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[tail]# [head], [tail] will always be uncompressed; inner nodes will compress.# 2: [head]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[tail]# 2 here means: don't compress head or head-&gt;next or tail-&gt;prev or tail,# but compress all nodes between them.# 3: [head]-&gt;[next]-&gt;[next]-&gt;node-&gt;node-&gt;...-&gt;node-&gt;[prev]-&gt;[prev]-&gt;[tail]# etc.list-compress-depth 0# Sets have a special encoding in just one case: when a set is composed# of just strings that happen to be integers in radix 10 in the range# of 64 bit signed integers.# The following configuration setting sets the limit in the size of the# set in order to use this special memory saving encoding.set-max-intset-entries 512# Similarly to hashes and lists, sorted sets are also specially encoded in# order to save a lot of space. This encoding is only used when the length and# elements of a sorted set are below the following limits:zset-max-ziplist-entries 128zset-max-ziplist-value 64# HyperLogLog sparse representation bytes limit. The limit includes the# 16 bytes header. When an HyperLogLog using the sparse representation crosses# this limit, it is converted into the dense representation.## A value greater than 16000 is totally useless, since at that point the# dense representation is more memory efficient.## The suggested value is ~ 3000 in order to have the benefits of# the space efficient encoding without slowing down too much PFADD,# which is O(N) with the sparse encoding. The value can be raised to# ~ 10000 when CPU is not a concern, but space is, and the data set is# composed of many HyperLogLogs with cardinality in the 0 - 15000 range.hll-sparse-max-bytes 3000# Streams macro node max size / items. The stream data structure is a radix# tree of big nodes that encode multiple items inside. Using this configuration# it is possible to configure how big a single node can be in bytes, and the# maximum number of items it may contain before switching to a new node when# appending new stream entries. If any of the following settings are set to# zero, the limit is ignored, so for instance it is possible to set just a# max entries limit by setting max-bytes to 0 and max-entries to the desired# value.stream-node-max-bytes 4096stream-node-max-entries 100# Active rehashing uses 1 millisecond every 100 milliseconds of CPU time in# order to help rehashing the main Redis hash table (the one mapping top-level# keys to values). The hash table implementation Redis uses (see dict.c)# performs a lazy rehashing: the more operation you run into a hash table# that is rehashing, the more rehashing \"steps\" are performed, so if the# server is idle the rehashing is never complete and some more memory is used# by the hash table.## The default is to use this millisecond 10 times every second in order to# actively rehash the main dictionaries, freeing memory when possible.## If unsure:# use \"activerehashing no\" if you have hard latency requirements and it is# not a good thing in your environment that Redis can reply from time to time# to queries with 2 milliseconds delay.## use \"activerehashing yes\" if you don't have such hard requirements but# want to free memory asap when possible.activerehashing yes# The client output buffer limits can be used to force disconnection of clients# that are not reading data from the server fast enough for some reason (a# common reason is that a Pub/Sub client can't consume messages as fast as the# publisher can produce them).## The limit can be set differently for the three different classes of clients:## normal -&gt; normal clients including MONITOR clients# replica -&gt; replica clients# pubsub -&gt; clients subscribed to at least one pubsub channel or pattern## The syntax of every client-output-buffer-limit directive is the following:## client-output-buffer-limit &lt;class&gt; &lt;hard limit&gt; &lt;soft limit&gt; &lt;soft seconds&gt;## A client is immediately disconnected once the hard limit is reached, or if# the soft limit is reached and remains reached for the specified number of# seconds (continuously).# So for instance if the hard limit is 32 megabytes and the soft limit is# 16 megabytes / 10 seconds, the client will get disconnected immediately# if the size of the output buffers reach 32 megabytes, but will also get# disconnected if the client reaches 16 megabytes and continuously overcomes# the limit for 10 seconds.## By default normal clients are not limited because they don't receive data# without asking (in a push way), but just after a request, so only# asynchronous clients may create a scenario where data is requested faster# than it can read.## Instead there is a default limit for pubsub and replica clients, since# subscribers and replicas receive data in a push fashion.## Both the hard or the soft limit can be disabled by setting them to zero.client-output-buffer-limit normal 0 0 0client-output-buffer-limit replica 256mb 64mb 60client-output-buffer-limit pubsub 32mb 8mb 60# Client query buffers accumulate new commands. They are limited to a fixed# amount by default in order to avoid that a protocol desynchronization (for# instance due to a bug in the client) will lead to unbound memory usage in# the query buffer. However you can configure it here if you have very special# needs, such us huge multi/exec requests or alike.## client-query-buffer-limit 1gb# In the Redis protocol, bulk requests, that are, elements representing single# strings, are normally limited to 512 mb. However you can change this limit# here, but must be 1mb or greater## proto-max-bulk-len 512mb# Redis calls an internal function to perform many background tasks, like# closing connections of clients in timeout, purging expired keys that are# never requested, and so forth.## Not all tasks are performed with the same frequency, but Redis checks for# tasks to perform according to the specified \"hz\" value.## By default \"hz\" is set to 10. Raising the value will use more CPU when# Redis is idle, but at the same time will make Redis more responsive when# there are many keys expiring at the same time, and timeouts may be# handled with more precision.## The range is between 1 and 500, however a value over 100 is usually not# a good idea. Most users should use the default of 10 and raise this up to# 100 only in environments where very low latency is required.hz 10# Normally it is useful to have an HZ value which is proportional to the# number of clients connected. This is useful in order, for instance, to# avoid too many clients are processed for each background task invocation# in order to avoid latency spikes.## Since the default HZ value by default is conservatively set to 10, Redis# offers, and enables by default, the ability to use an adaptive HZ value# which will temporarily raise when there are many connected clients.## When dynamic HZ is enabled, the actual configured HZ will be used# as a baseline, but multiples of the configured HZ value will be actually# used as needed once more clients are connected. In this way an idle# instance will use very little CPU time while a busy instance will be# more responsive.dynamic-hz yes# When a child rewrites the AOF file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.aof-rewrite-incremental-fsync yes# When redis saves RDB file, if the following option is enabled# the file will be fsync-ed every 32 MB of data generated. This is useful# in order to commit the file to the disk more incrementally and avoid# big latency spikes.rdb-save-incremental-fsync yes# Redis LFU eviction (see maxmemory setting) can be tuned. However it is a good# idea to start with the default settings and only change them after investigating# how to improve the performances and how the keys LFU change over time, which# is possible to inspect via the OBJECT FREQ command.## There are two tunable parameters in the Redis LFU implementation: the# counter logarithm factor and the counter decay time. It is important to# understand what the two parameters mean before changing them.## The LFU counter is just 8 bits per key, it's maximum value is 255, so Redis# uses a probabilistic increment with logarithmic behavior. Given the value# of the old counter, when a key is accessed, the counter is incremented in# this way:## 1. A random number R between 0 and 1 is extracted.# 2. A probability P is calculated as 1/(old_value*lfu_log_factor+1).# 3. The counter is incremented only if R &lt; P.## The default lfu-log-factor is 10. This is a table of how the frequency# counter changes with a different number of accesses with different# logarithmic factors:## +--------+------------+------------+------------+------------+------------+# | factor | 100 hits | 1000 hits | 100K hits | 1M hits | 10M hits |# +--------+------------+------------+------------+------------+------------+# | 0 | 104 | 255 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 1 | 18 | 49 | 255 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 10 | 10 | 18 | 142 | 255 | 255 |# +--------+------------+------------+------------+------------+------------+# | 100 | 8 | 11 | 49 | 143 | 255 |# +--------+------------+------------+------------+------------+------------+## NOTE: The above table was obtained by running the following commands:## redis-benchmark -n 1000000 incr foo# redis-cli object freq foo## NOTE 2: The counter initial value is 5 in order to give new objects a chance# to accumulate hits.## The counter decay time is the time, in minutes, that must elapse in order# for the key counter to be divided by two (or decremented if it has a value# less &lt;= 10).## The default value for the lfu-decay-time is 1. A special value of 0 means to# decay the counter every time it happens to be scanned.## lfu-log-factor 10# lfu-decay-time 1########################### ACTIVE DEFRAGMENTATION ######################### What is active defragmentation?# -------------------------------## Active (online) defragmentation allows a Redis server to compact the# spaces left between small allocations and deallocations of data in memory,# thus allowing to reclaim back memory.## Fragmentation is a natural process that happens with every allocator (but# less so with Jemalloc, fortunately) and certain workloads. Normally a server# restart is needed in order to lower the fragmentation, or at least to flush# away all the data and create it again. However thanks to this feature# implemented by Oran Agra for Redis 4.0 this process can happen at runtime# in a \"hot\" way, while the server is running.## Basically when the fragmentation is over a certain level (see the# configuration options below) Redis will start to create new copies of the# values in contiguous memory regions by exploiting certain specific Jemalloc# features (in order to understand if an allocation is causing fragmentation# and to allocate it in a better place), and at the same time, will release the# old copies of the data. This process, repeated incrementally for all the keys# will cause the fragmentation to drop back to normal values.## Important things to understand:## 1. This feature is disabled by default, and only works if you compiled Redis# to use the copy of Jemalloc we ship with the source code of Redis.# This is the default with Linux builds.## 2. You never need to enable this feature if you don't have fragmentation# issues.## 3. Once you experience fragmentation, you can enable this feature when# needed with the command \"CONFIG SET activedefrag yes\".## The configuration parameters are able to fine tune the behavior of the# defragmentation process. If you are not sure about what they mean it is# a good idea to leave the defaults untouched.# Enabled active defragmentation# activedefrag no# Minimum amount of fragmentation waste to start active defrag# active-defrag-ignore-bytes 100mb# Minimum percentage of fragmentation to start active defrag# active-defrag-threshold-lower 10# Maximum percentage of fragmentation at which we use maximum effort# active-defrag-threshold-upper 100# Minimal effort for defrag in CPU percentage, to be used when the lower# threshold is reached# active-defrag-cycle-min 1# Maximal effort for defrag in CPU percentage, to be used when the upper# threshold is reached# active-defrag-cycle-max 25# Maximum number of set/hash/zset/list fields that will be processed from# the main dictionary scan# active-defrag-max-scan-fields 1000# Jemalloc background thread for purging will be enabled by defaultjemalloc-bg-thread yes# It is possible to pin different threads and processes of Redis to specific# CPUs in your system, in order to maximize the performances of the server.# This is useful both in order to pin different Redis threads in different# CPUs, but also in order to make sure that multiple Redis instances running# in the same host will be pinned to different CPUs.## Normally you can do this using the \"taskset\" command, however it is also# possible to this via Redis configuration directly, both in Linux and FreeBSD.## You can pin the server/IO threads, bio threads, aof rewrite child process, and# the bgsave child process. The syntax to specify the cpu list is the same as# the taskset command:## Set redis server/io threads to cpu affinity 0,2,4,6:# server_cpulist 0-7:2## Set bio threads to cpu affinity 1,3:# bio_cpulist 1,3## Set aof rewrite child process to cpu affinity 8,9,10,11:# aof_rewrite_cpulist 8-11## Set bgsave child process to cpu affinity 1,10,11# bgsave_cpulist 1,10-11# In some cases redis will emit warnings and even refuse to start if it detects# that the system is in bad state, it is possible to suppress these warnings# by setting the following config which takes a space delimited list of warnings# to suppress## ignore-warnings ARM64-COW-BUG","link":"/2023/11/05/%E4%B8%AD%E9%97%B4%E4%BB%B6/redis%E9%83%A8%E7%BD%B2/"},{"title":"CORS跨域资源共享","text":"什么是同源如果一个请求地址里面的协议、域名和端口号都相同，就属于同源。 同源策略是浏览器的一个安全功能，不同源的客户端脚本在没有明确授权的情况下，不能读写对方资源。 同源策略是浏览器安全的基石。 什么是跨域依据浏览器同源策略，非同源脚本不可操作其他源下面的对象。想要操作其他源下的对象就需要跨域。 CORS 技术为了解决浏览器跨域问题，W3C 提出了跨源资源共享方案，即 CORS(Cross-Origin Resource Sharing)。 CORS 将请求分为两类：简单请求和非简单请求，分别对跨域通信提供了支持。 简单请求在CORS出现前，发送HTTP请求时在头信息中不能包含任何自定义字段，且 HTTP 头信息不超过以下几个字段： Accept Accept-Language Content-Language Last-Event-ID Content-Type （仅限于 [application/x-www-form-urlencoded 、multipart/form-data、text/plain ] 类型） 对于简单请求，CORS的策略是请求时在请求头中增加一个Origin字段，服务器收到请求后，根据该字段判断是否允许该请求访问。 如果允许，则在 HTTP 头信息中添加 Access-Control-Allow-Origin 字段，并返回正确的结果 ； 如果不允许，则不在 HTTP 头信息中添加 Access-Control-Allow-Origin 字段 。 非简单请求对于非简单请求的跨源请求，浏览器会在真实请求发出前，增加一次OPTION请求，称为预检请求(preflight request)。预检请求将真实请求的信息，包括请求方法、自定义头字段、源信息添加到 HTTP 头信息字段中，询问服务器是否允许这样的操作。 例如一个GET请求： 12345OPTIONS /test HTTP/1.1Origin: http://www.test.comAccess-Control-Request-Method: GETAccess-Control-Request-Headers: X-Custom-HeaderHost: www.test.com 与 CORS 相关的字段有： 请求使用的 HTTP 方法 Access-Control-Request-Method 请求中包含的自定义头字段 Access-Control-Request-Headers 服务器收到请求时，需要分别对 Origin、Access-Control-Request-Method、Access-Control-Request-Headers 进行验证，验证通过后，会在返回 HTTP头信息中添加 ： 12345Access-Control-Allow-Origin: http://www.test.comAccess-Control-Allow-Methods: GET, POST, PUT, DELETEAccess-Control-Allow-Headers: X-Custom-HeaderAccess-Control-Allow-Credentials: trueAccess-Control-Max-Age: 1728000 他们的含义分别是： Access-Control-Allow-Methods: 真实请求允许的方法 Access-Control-Allow-Headers: 服务器允许使用的字段 Access-Control-Allow-Credentials: 是否允许用户发送、处理 cookie Access-Control-Max-Age: 预检请求的有效期，单位为秒。有效期内，不会重复发送预检请求","link":"/2024/05/27/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/CORS%E8%B7%A8%E5%9F%9F%E8%B5%84%E6%BA%90%E5%85%B1%E4%BA%AB/"},{"title":"MySQL基础原理","text":"基础SQL 和 NOSQL 区别 ACID vs BASE 关系型数据库支持 ACID 即原子性，一致性，隔离性和持续性。相对而言，NoSQL 采用更宽松的模型 BASE ， 即基本可用，软状态和最终一致性。 扩展性对比 NoSQL数据之间无关系，这样就非常容易扩展。相反关系型数据库的数据之间存在关联性，水平扩展较难。 三大范式✨第一范式（1NF)：每列的原子性【不可再分】 第二范式（2NF)：确保表中每列与主键相关，而不能只与主键的某部分相关（主要针对联合主键而言）【消除部分函数依赖】 第三范式（3NF): 非主属性不依赖于其他非主属性【消除传递依赖】 MySQL如何避免重复插入数据？ 建表时，使用 UNIQUE 约束 在表的相关列上添加UNIQUE约束，确保每个值在该列中唯一 12345CREATE TABLE users ( id INT PRIMARY KEY AUTO_INCREMENT, email VARCHAR(255) UNIQUE, name VARCHAR(255)); 插入时，使用 insert …… on duplicate key update 这种语句允许在插入记录时处理重复键的情况。如果插入的记录与现有记录冲突，可以选择更新现有记录 123INSERT INTO users (email, name) VALUES ('example@example.com', 'John Doe')ON DUPLICATE KEY UPDATE name = VALUES(name); 插入时，使用 insert ignore 该语句会在插入记录时忽略那些因重复键而导致的插入错误 12INSERT IGNORE INTO users (email, name) VALUES ('example@example.com', 'John Doe'); 总结： 选择哪种方法取决于具体的需求： 如果需要保证全局唯一性，使用UNIQUE约束是最佳做法。 如果需要插入和更新结合可以使用ON DUPLICATE KEY UPDATE。 对于快速忽略重复插入，INSERT IGNORE是合适的选择。 CHAR 和 VARCHAR有什么区别？ CHAR是固定长度的字符串类型，定义时需要指定固定长度，存储时会在末尾补足空格。CHAR适合存储长度固定的数据，如固定长度的代码、状态等，存储空间固定，对于短字符串效率较高。 VARCHAR是可变长度的字符串类型，定义时需要指定最大长度，实际存储时根据实际长度占用存储空间。VARCHAR适合存储长度可变的数据，如用户输入的文本、备注等，节约存储空间。 Text数据类型可以无限大吗？MySQL 3 种text类型的最大长度如下： TEXT：65,535 bytes ~64kb MEDIUMTEXT：16,777,215 bytes ~16Mb LONGTEXT：4,294,967,295 bytes ~4Gb 说一下外键约束外键约束的作用是维护表与表之间的关系，确保数据的完整性和一致性。 比如，students表中的course_id字段是一个外键，它指向courses表中的id字段。这个外键约束确保了每个学生所选的课程在courses表中都存在，从而维护了数据的完整性和一致性。 123456CREATE TABLE students ( id INT PRIMARY KEY, name VARCHAR(50), course_id INT, FOREIGN KEY (course_id) REFERENCES courses(id)); 多表查询（各种join连接详解）A）内连接：join=inner join B）外连接：left join=left outer join，right join=right outer join，union C）交叉连接：cross join 2.1 内连接（只有一种场景） 123select a.*, b.* from tablea ainner join tableb bon a.id = b.id 这种场景下得到的是满足某一条件的A，B内部的数据；正因为得到的是内部共有数据，所以连接方式称为内连接。 2.2 外连接（六种场景） 2.2.1 left join 或者left outer join(等同于left join) 123select a.*, b.* from tablea aleft outer join tableb bon a.id = b.id 左外连接返回左表中的所有行，即使在右表中没有匹配的行。未匹配的右表列会包含NULL 2.2.2 [ left join 或者left outer join(等同于left join) ] + [ where B.column is null ] 1234select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idWhere b.id is null left join表a的数据全部显示，匹配表b的数据也显示，而b.id再次过滤掉 表b的id为空的。 2.2.3 right join 或者right outer join(等同于right join) 123select a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.id 2.2.5 full join （mysql不支持，但是可以用 left join union right join代替） 1234567select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idunionselect a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.id 这种场景下得到的是满足某一条件的公共记录，和独有的记录 2.2.6 full join + is null（mysql不支持，但是可以用 （left join + is null） union （right join+is null代替） 123456789select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idwhere b.id is nullunionselect a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.idwhere a.id is null 2.3 交叉连接 （cross join） 2.3.1 实际应用中还有这样一种情形，想得到A，B记录的排列组合，即笛卡儿积，这个就不好用集合和元素来表示了。需要用到cross join： 12select a.id aid,a.age,b.id bid,b.name from tablea across join tableb b 2.3.2 还可以为cross join指定条件 （where）： 123select a.id aid,a.age,b.id bid,b.name from tablea across join tableb bwhere a.id = b.id 这种情况下实际上实现了内连接的效果 MySQL的关键字in和existININ 用于检查左边的表达式是否存在于右边的列表或子查询的结果集中。如果存在，则IN 返回TRUE，否则返回FALSE。 EXISTSEXISTS 用于判断子查询是否至少能返回一行数据。它不关心子查询返回什么数据，只关心是否有结果。如果子查询有结果，则EXISTS 返回TRUE，否则返回FALSE。 一种通俗的可以理解为：将外查询表的每一行，代入内查询作为检验，如果内查询返回的结果取非空值，则EXISTS子句返回TRUE。 总结 相同点：IN 和 EXISTS 都是用来处理子查询的关键词 区别与选择： 性能差异：在很多情况下，EXISTS 的性能优于 IN，特别是当子查询的表很大时。这是因为EXISTS 一旦找到匹配项就会立即停止查询，而IN可能会扫描整个子查询结果集。 使用场景：如果子查询结果集较小且不频繁变动，IN 可能更直观易懂。而当子查询涉及外部查询的每一行判断，并且子查询的效率较高时，EXISTS 更为合适。 NULL值处理：IN 能够正确处理子查询中包含NULL值的情况，而EXISTS 不受子查询结果中NULL值的影响，因为它关注的是行的存在性，而不是具体值。 SQL查询语句的执行顺序是怎么样的？ 所有的查询语句都是从FROM开始执行，在执行过程中，每个步骤都会生成一个虚拟表，这个虚拟表将作为下一个执行步骤的输入，最后一个步骤产生的虚拟表即为输出结果。 123456789101112(9) SELECT (10) DISTINCT &lt;column&gt;,(6) AGG_FUNC &lt;column&gt; or &lt;expression&gt;, ...(1) FROM &lt;left_table&gt; (3) &lt;join_type&gt;JOIN&lt;right_table&gt; (2) ON&lt;join_condition&gt;(4) WHERE &lt;where_condition&gt;(5) GROUP BY &lt;group_by_list&gt;(7) WITH {CUBE|ROLLUP}(8) HAVING &lt;having_condtion&gt;(11) ORDER BY &lt;order_by_list&gt;(12) LIMIT &lt;limit_number&gt;; 存储引擎执行一条 SQL 查询语句，期间发生了什么?✨ 连接器：建立连接、管理链接、校验个人身份 查询缓存：key-value形式——查询语句是否命中 解析器：词法解析、语法解析，建立语法树 执行 SQL： 预处理阶段：判断是否存在表面、字段名；将*替换成全部列 优化阶段：基于查询成本，选择最佳的执行计划 执行阶段：根据执行计划执行 SQL查询语句，从存储引擎读取记录，返回给客户端 MySQL 一行记录是怎么存储的？ MySQL 的 NULL 值是怎么存放的？ MySQL 的 Compact 行格式中会用「NULL值列表」来标记值为 NULL 的列，NULL 值并不会存储在行格式中的真实数据部分。 NULL值列表会占用 1 字节空间，当表中所有字段都定义成 NOT NULL，行格式中就不会有 NULL值列表，这样可节省 1 字节的空间。 MySQL 怎么知道 varchar(n) 实际占用数据的大小？ MySQL 的 Compact 行格式中会用「变长字段长度列表」存储变长字段实际占用的数据大小。 varchar(n) 中 n 最大取值为多少？ 一行记录最大能存储 65535 字节的数据，但是这个是包含「变长字段字节数列表所占用的字节数」和「NULL值列表所占用的字节数」。所以， 我们在算 varchar(n) 中 n 最大值时，需要减去这两个列表所占用的字节数。 如果一张表只有一个 varchar(n) 字段，且允许为 NULL，字符集为 ascii。varchar(n) 中 n 最大取值为 65532。 计算公式：65535 - 变长字段字节数列表所占用的字节数 - NULL值列表所占用的字节数 = 65535 - 2 - 1 = 65532。 如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 &lt;= 65535。 行溢出后，MySQL 是怎么处理的？ 如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。 Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。 Compressed 和 Dynamic 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。 MySql 数据的存储结构✨聚簇索引（Innodb）和非聚簇索引（myISAM） 对于InnoDB引擎来说，是按照聚簇索引的形式存储数据 对于MyISAM引擎来说，是按照非聚簇索引的形式存储数据： 存储引擎是InnoDB, 在data目录下会看到2类文件：.frm、.ibd（1）.frm–表结构的文件。（2）.ibd–表数据文件 存储引擎是MyISAM, 在data目录下会看到3类文件：.frm、.myi、.myd（1）.frm–表定义，是描述表结构的文件。（2）.MYD–”D”数据信息文件，是表的数据文件。（3）.MYI–”I”索引信息文件，是表数据文件中任何索引的数据树 聚簇索引和非聚簇索引的存储方式区别： 在MyISAM引擎索引和数据是分开存储的，而InnoDB是索引和数据是一起以idb文件的形式进行存储的。 在访问速度上，聚簇索引比非聚簇索引快。非聚簇索引需要先查询一遍索引文件，得到索引，跟据索引获取数据。而聚簇索引的索引树的叶子节点的直接指向要查找的数据行。 MyISAM 和 InnoDB ✨MyISAM 和 InnoDB有什么区别？ 事务支持：MyISAM 不提供事务支持。InnoDB 提供事务支持，实现了 SQL 标准定义了四个隔离级别。（默认引擎更换重要原因） MyISAM 不支持数据库异常崩溃后的安全恢复，而 InnoDB 支持。 表锁差异：InnoDB 支持行级别的锁粒度，MyISAM 不支持，只支持表级别的锁粒度(一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限) 读写过程：MyISAM在读写过程中相互阻塞；InnoDB读写阻塞与事务隔离级别相关 读写性能：MyISAM读取性能优越，但是写入性能差（如果执行大量的select，MyISAM是更好的选择）；InnoDB写入性能较强（如果执行大量的insert或者update，InnoDB是更好的选择） 外键支持：MyISAM 不支持外键，而 InnoDB 支持。 存储结构：MyISAM 不支持聚簇索引（数据保存在连续内存中，数据文件是分离的，索引保存的是数据文件的指针, 主键索引：主键列值+行号，二级索引：索引列值+行号）；InnoDB 的数据是存储在主键索引==聚簇索引中 innoDB 比 MyISAM 好在哪？为什么是默认引擎事务支持 表锁差异 外键支持 数据库异常崩溃后的安全恢复(redolog日志进行崩溃恢复) 各自适合什么场景MyISAM读取性能优越，但是写入性能差（如果执行大量的select，MyISAM是更好的选择）； InnoDB写入性能较强（如果执行大量的insert或者update，InnoDB是更好的选择） Memory：Memory引擎将数据存储在内存中，适用于对性能要求较高的读操作，但是在服务器重启或崩溃时数据会丢失。它不支持事务、行级锁和外键约束 InnoDB的底层结构InnoDB的底层结构主要由2部分组成：内存结构和磁盘结构。 2、内存结构由缓冲池（Buffer Pool），写缓冲（Change Buffer），日志缓冲（ Log Buffer），自适应hash索引（Adaptive Hash Index）组成。3、缓冲池（Buffer Pool）主要是缓存表数据与索引数据，加快访问速度。内部采用基于LRU算法的变体算法来管理缓存对象。4、写缓冲（Change Buffer）主要是缓存辅助索引的更新操作，加快辅助索引的更新速度。5、日志缓冲（ Log Buffer）使大型事务可以运行，而无需在事务提交之前将redo日志数据写入磁盘，节省了磁盘I/O。注意事务提交时刷redo log有三种策略。 索引索引的分类是什么？按照数据结构: B+树索引 hash索引 full-text索引 按照物理存储: 聚簇索引 二级索引 按照字段特性: 主键索引 唯一索引 普通索引 前缀索引 按照字段个数: 单列索引 联合索引 聚簇索引与二级索引的区别✨ 数据存储: 聚簇索引是一种特殊类型的索引，它定义了表中数据的物理排序方式。在聚簇索引中，数据行按照索引的顺序存储在磁盘上。即数据行本身存储在聚簇索引中。 唯一性: 如果叶子节点存储的是实际数据的就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。 效率: 在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。所以, 对于范围查询和排序查询, 聚簇索引通常更有效率 为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？✨B+Tree vs B Tree B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的非叶子节点能存放更多索引，因此 B+树更加「矮胖」，查询底层节点的磁盘 I/O 次数更少，查找的性能更加稳定 B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树需要通过遍历来完成范围查询，会涉及多个节点的磁盘 I/O，效率低。 B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化； B+Tree vs 二叉树对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。 在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 34 层左右，也就是说一次数据查询操作只需要做 34 次的磁盘 I/O 操作就能查询到目标数据。根节点可以包含的关键字数量范围是 [2, m-1]。 非根节点至少包含m/2个关键字，至多包含m-1个关键字。 而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。 B+Tree vs HashHash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。 但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。 B+树叶子节点数据如何存储，以及如何查找某一条数据✨数据库表中的数据都是存储在页里 一个页可以存放多少条记录取决于一行记录的大小是多少（假如一行数据大小是1k，那么理论上一页就可以放16条数据） InnoDB 存储引擎通过 B+树来构建聚簇索引来查询数据：按照每张表的主键构造一颗B+树，叶子节点存放的是整行记录数据，在非叶子节点上存放的是键值以及指向数据页的指针，同时每个数据页之间都通过一个双向链表来进行链接。 数据记录按照主键排序，放在不同的页中。 当需要通过主键来查询时，先找到根页面（page offset=3），然后通过二分查找主键所在的页的指针 把所在页载入到内存，再通过二分查找定位到具体的记录 联合索引范围查询联合索引的最左匹配原则，在遇到范围查询（如 &gt;、&lt;）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。 注意，对于 &gt;=、&lt;=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配 (对于符合=情况, 后面的字段是有序的, 因而不会停止匹配) 参考:[【MySQL | 第十篇】重新认识MySQL索引匹配过程-CSDN博客](https://blog.csdn.net/weixin_61440595/article/details/138393806#:~:text=正确结论：,联合索引的最左匹配原则，在遇到范围查询（如 &gt;、&lt;）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段后面的字段无法用到联合索引。) 索引下推 在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个回表，到「主键索引」上找出数据行，再对比 b 字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 什么时候创建索引？✨需要： 字段唯一；经常用于where查询、 groupby条件和 orderby条件 的字段 不需要：区分度小的；数据少的；经常更新的 区分度: 指字段在数据库中的不重复比 区分度的计算规则如下: 字段去重后的总数与全表总记录数的商。 1select count(distinct(name))/count(*) from t_base_user; 有什么优化索引的方法？✨ 前缀索引优化； 提高索引的查询速度；减小索引项的大小 覆盖索引优化； 不需要查询出包含整行记录的所有信息，也就减少了大量的 I/O 操作。 主键索引最好是自增的； 避免页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。 防止索引失效； 使用左或者左右模糊匹配的时候，也就是 like %xx 或者 like %xx%这两种方式 在查询条件中对索引列做了计算（+ - * /）、函数、类型转换操作 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。 在 WHERE 子句中，or 的前后字段都要上索引 列和列对比，这种情况会被认为还不如走全表扫描。 存在NULL值条件(is null/ is not null)，索引中无法存储NULL值，所以where条件判断如果对字段进行了NULL值判断（is NULL/ is not null），则数据库放弃索引而进行全表查询 优化案例SELECT * FROM t_order ORDER BY text LIMIT 1000000, 101、使用覆盖索引 SELECT id, ‘text’ FROM t_order ORDER BY text LIMIT 1000000, 10 因为实际开发中，用SELECT查询一两列操作是非常少的，因此上述的覆盖索引的适用范围就比较有限。 2、子查询优化 可以通过把分页的SQL语句改写成子查询的方法获得性能上的提升。 SELECT * FROM t_order where id&gt;=(select id from t_order order by text limit 1000000, 1) LIMIT 10 但是这种优化方法也有局限性：这种写法，要求主键ID必须是连续的 3、延迟关联 和上述的子查询做法类似，我们可以使用JOIN，先在索引列上完成分页操作，然后再回表获取所需要的列。 SELECT a.* FROM t_order as a inner join ( select id from t_order order by text limit 1000000, 10) as b on a.id = b.id 4、记录上次查询结束的位置 和上面使用的方法都不同，记录上次结束位置优化思路是使用某种变量记录上一次数据的位置，下次分页时直接从这个变量的位置开始扫描，从而避免MySQL扫描大量的数据再抛弃的操作。 SELECT * FROM t_order where id&gt;=1000000 LIMIT 10 MySQL 使用 like “%x“，索引一定会失效吗？从这个思考题我们知道了，使用左模糊匹配（like “%xx”）并不一定会走全表扫描，关键还是看数据表中的字段。 如果数据库表中的字段只有主键+二级索引，那么即使使用了左模糊匹配，也不会走全表扫描（type=all），而是走全扫描二级索引树(type=index)。 count(*) 和 count(1) 有什么区别？哪个性能最好？count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。 所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。 再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。 事务事务有哪些特性？原子性（Atomicity） 一个事务中的所有操作，要么全部完成，要么全部不完成 一致性（Consistency） 数据满足完整性约束，数据库保持一致性状态 隔离性（Isolation） 隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致 持久性（Durability） 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？原子性是通过 undo log（回滚日志）; 一致性则是通过持久性+原子性+隔离性来保证； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 持久性是通过 redo log （重做日志）来保证的； 并行事务会引发什么问题？✨ 脏读：读到其他事务未提交的数据； 不可重复读：前后读取的数据不一致； 幻读：前后读取的记录数量不一致。 脏读如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。 不可重复读在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。 幻读在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。 事务的隔离级别有哪些？各自的应用场景✨ 读未提交(Read Uncommitted) 特点：一个事务还没提交时，它做的变更就能被其他事务看到 应用场景：对于一些对数据一致性要求不高的场景，比如读取系统的实时监控数据。 读已提交(Read Committed) 特点：一个事务提交之后，它做的变更才能被其他事务看到；事务读取数据时会生成新的快照数据Read View。 应用场景：适用于大部分常规业务场景，能够保证读取的数据具有较高的一致性。 可重复读(Repeatable Read) ==默认级别== 特点：事务中的修改操作需要等待事务提交后才生效；事务读取数据时只能读取事务开始时的快照数据Read View，对其他事务对数据的修改不可见。 应用场景：适用于需要保证读取数据一致性的应用，例如订单交易等。 串行化(Serializable) 特点：事务中的修改操作需要等待事务提交后才生效；会对记录加上读写锁，在多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行； 应用场景：适用于对数据一致性要求极高的场景，例如金融领域的转账操作。 Read View Read View 中四个字段作用； 聚簇索引记录中两个跟事务有关的隐藏列； trx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里； roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。 MVCC✨核心包括：undo log、版本链、read view ==通过「事务的 Read View 里的事务 id」和「版本链中记录的事务 id」的比对,来控制并发事务访问一个记录的行为叫 MVCC（多版本并发控制）== 在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况： 一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况： 如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，可见 如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，不可见 如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：在——不可见；不在——可见 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同： 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View 「可重复读」隔离级别是启动事务时生成一个 Read View 对于幻读现象，不建议将隔离级别升级为串行化。MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读 针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读 MySQL 可重复读隔离级别，完全解决幻读了吗？✨可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。 场景一 在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。 对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。 场景二T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id &gt; 100 得到了 3 条记录。 T2 时刻：事务 B 往插入一个 id= 200 的记录并提交； T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id &gt; 100 for update 就会得到 4 条记录，此时也发生了幻读现象。 所以，MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。 如何解决幻读的场景要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。 锁mysql有哪些锁？根据加锁的范围，可以分为全局锁、表级锁和行锁三类。 表锁和行锁的作用表锁作用： 整体控制 粒度大：可能会引起锁竞争和性能问题 适用于大批量操作：表的重建、大量数据的加载 行锁作用： 细粒度控制 减少锁冲突 适用于频繁蛋黄操作 全局锁要使用全局锁，则要执行这条命令： 1flush tables with read lock 执行后，整个数据库就处于只读状态了，这时其他线程执行以下操作，都会被阻塞： 对数据的增删改操作，比如 insert、delete、update等语句； 对表结构的更改操作，比如 alter table、drop table 等语句。 如果要释放全局锁，则要执行这条命令： 1unlock tables 当然，当会话断开了，全局锁会被自动释放。 全局锁应用场景全局锁主要应用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。 加全局锁的缺点加上全局锁，意味着整个数据库都是只读状态。 那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而不能更新数据，这样会造成业务停滞。 既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？如果数据库的引擎支持的事务支持可重复读的隔离级别，那么==在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作，不会影响备份的数据==。 备份数据库的工具是 mysqldump，在使用 mysqldump 时加上 –single-transaction 参数的时候，就会在备份数据库之前先开启事务。这种方法只适用于支持「可重复读隔离级别的事务」的存储引擎。 InnoDB 存储引擎默认的事务隔离级别正是可重复读，因此可以采用这种方式来备份数据库。 但是，对于 MyISAM 这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法。 表级锁MySQL 里面表级别的锁有这几种： 表锁表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。 也就是说如果本线程对学生表加了「共享表锁」，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放。 优势：开销小；加锁快；无死锁 劣势：锁粒度大，发生锁冲突的概率高，并发能力差 加锁方法：自动加锁。查询操作（SELECT），会自动给涉及的所有表加读锁，更新操作（UPDATE、DELETE、INSERT），会自动给涉及的表加写锁。也可以显示加锁： 123共享读锁：lock tables tableName read;独占写锁：lock tables tableName write;批量解锁：unlock tables; 元数据锁（MDL）我们不需要显示的使用 MDL，因为当我们对数据库表进行操作时，会自动给这个表加上 MDL： 对一张表进行 CRUD 操作时，加的是 MDL 读锁； 对一张表做结构变更操作的时候，加的是 MDL 写锁； MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。 MDL 不需要显示调用，那它是在什么时候释放的?MDL 是在事务提交后才会释放，这意味着事务执行期间，MDL 是一直持有的。 意向锁意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables … read）和独占表锁（lock tables … write）发生冲突。 意向锁的目的是为了快速判断表里是否有记录被加锁，从而判断是否能加表锁。 如果没有「意向锁」，那么加「独占表锁」时，就需要遍历表里所有记录，查看是否有记录存在独占锁，这样效率会很慢。 那么有了「意向锁」，由于在对记录加独占锁前，先会加上表级别的意向独占锁，那么在加「独占表锁」时，直接查该表是否有意向独占锁，如果有就意味着表里已经有记录被加了独占锁，这样就不用去遍历表里的记录。 AUTO-INC 锁在插入数据时，会加一个表级别的 AUTO-INC 锁，然后为被 AUTO_INCREMENT 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。 保证插入数据时，被 AUTO_INCREMENT 修饰的字段的值是连续递增的。 缺点：AUTO-INC 锁再对大量数据进行插入的时候，会影响插入性能，因为另一个事务中的插入会被阻塞。 解决： 在 MySQL 5.1.22 版本开始，InnoDB 存储引擎提供了一种轻量级的锁来实现自增。一样也是在插入数据的时候，会为被 AUTO_INCREMENT 修饰的字段加上轻量级锁，然后给该字段赋值一个自增的值，就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁。 行级锁✨不同隔离级别下，行级锁的种类是不同的。 在读已提交隔离级别下，行级锁的种类只有记录锁（解决脏读），也就是仅仅把一条记录锁上。 在可重复读隔离级别下，行级锁的种类除了有记录锁，还有间隙锁（避免幻读），所以行级锁的种类主要有三类： Record Lock-记录锁锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的 Gap Lock-间隙锁锁定一个范围，但是不包含记录本身；间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，==因为间隙锁的目的是防止插入幻影记录而提出的==。 Next-Key Lock-临键锁Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。 什么 SQL 语句会加行级锁？InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁，所以后面的内容都是基于 InnoDB 引擎 的。 普通的 select 语句是不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过 MVCC（多版本并发控制）实现的。 如果要在查询时对记录加行级锁，可以使用下面这两个方式，这两种查询会加锁的语句称为锁定读。 12345//对读取的记录加共享锁(S型锁)select ... lock in share mode;//对读取的记录加独占锁(X型锁)select ... for update; 上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin 或者 start transaction 开启事务的语句。 **除了上面这两条锁定读语句会加行级锁之外，update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)**。 12345//对操作的记录加独占锁(X型锁)update table .... where id = 1;//对操作的记录加独占锁(X型锁)delete from table where id = 1; 共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。 MySQL 行级锁的加锁规则唯一索引等值查询： 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「记录锁」。 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成「间隙锁」。 非唯一索引等值查询： 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。 当查询的记录「不存在」时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。 唯一索引的范围查询 对每一个扫描到的索引加 next-key 锁 唯一索引在满足一些条件的时候，索引的 next-key lock 退化为间隙锁或者记录锁。 「大于等于」找到满足的记录加上 next-key lock，找到等值的记录会退化成记录锁 「小于等于、小于」出现第一个不匹配的会退化成间隙锁 非唯一索引的范围查询 对每一个扫描到的非唯一索引加 next-key 锁（不会退化），并对匹配的主键索引加记录锁 其实理解 MySQL 为什么要这样加锁，主要要以避免幻读角度去分析，这样就很容易理解这些加锁的规则了。 update 没加索引会锁全表？还有一件很重要的事情，在线上在执行 update、delete、select … for update 等具有加锁性质的语句，一定要检查语句是否走了索引，**==如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了==**，这是挺严重的问题。 解决方案： 打开 MySQL sql_safe_updates 参数 使用 force index([index_name]) 可以告诉优化器使用哪个索引 MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读吗？在 MySQL 的可重复读隔离级别下，针对当前读的语句会对索引加记录锁+间隙锁，这样可以避免其他事务执行增、删、改时导致幻读的问题。 MySQL 死锁了，怎么办？情况一 两个事务中的锁定读各自会在（1006， +∞）生成间隙锁，间隙锁是兼容的。但假如有记录锁，就会存在 xs、xx 互斥，那么事务 B 的锁定读就会被阻塞 而事务 A 在进行 insert 的时候会尝试在插入间隙上获取「插入意向锁」，但是间隙上已经有事务 B 的间隙锁，所以insert 被阻塞 同理，事务 B 的 insert 也会被阻塞 因为当我们执行插入语句时，会在插入间隙上获取插入意向锁，而插入意向锁与间隙锁是冲突的，所以当其它事务持有该间隙的间隙锁时，需要等待其它事务释放间隙锁之后，才能获取到插入意向锁。而间隙锁与间隙锁之间是兼容的，所以两个事务中 select ... for update 语句并不会相互影响。 避免死锁： 设置事务等待锁的超时时间 开启主动死锁检测 情况二 两个事务即使生成的间隙锁的范围是一样的，也不会发生冲突，因为间隙锁目的是为了防止其他事务插入数据，因此间隙锁与间隙锁之间是相互兼容的。 在执行插入语句时，如果插入的记录在其他事务持有间隙锁范围内，插入语句就会被阻塞，因为插入语句在碰到间隙锁时，会生成一个插入意向锁，然后插入意向锁和间隙锁之间是互斥的关系。 如果两个事务分别向对方持有的间隙锁范围内插入一条记录，而插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：互斥、占有且等待、不可强占用、循环等待，因此发生了死锁。 MySQL的update阻塞问题MySQL两个线程的update语句同时处理一条数据，会不会有阻塞？如果是两个事务同时更新了 id = 1，比如 update … where id = 1，那么是会阻塞的。因为 InnoDB 存储引擎实现了行级锁。 当A事务对 id =1 这行记录进行更新时，会对主键 id 为 1 的记录加X类型的记录锁，这样第二事务对 id = 1 进行更新时，发现已经有记录锁了，就会陷入阻塞状态。 两条update语句处理一张表的不同的主键范围的记录，一个&lt;10，一个&gt;15，会不会遇到阻塞？底层是为什么的？不会，因为锁住的范围不一样，不会形成冲突。 第一条 update sql 的话（ id&lt;10），锁住的范围是（-♾️，10） 第二条 update sql 的话（id &gt;15），锁住的范围是（15，+♾️） 如果2个范围不是主键或索引？还会阻塞吗？如果2个范围查询的字段不是索引的话，那就代表 update 没有用到索引，这时候触发了全表扫描，全部索引都会加行级锁，这时候第二条 update 执行的时候，就会阻塞了。 因为如果 update 没有用到索引，在扫描过程中会对索引加锁，所以全表扫描的场景下，所有记录都会被加锁，也就是这条 update 语句产生了 4 个记录锁和 5 个间隙锁，相当于锁住了全表。 日志三种日志： undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。（存放在Buffer Pool里面的undo页中，其持久性受redo log控制） redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复。（物理日志） binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制； undo log✨undo log 是一种用于撤销回退的日志，它保证了事务的 ACID 特性中的原子性（Atomicity）。 具体实现在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。如下图： 每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，比如： 在插入一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了； 在删除一条记录时，要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了； 在更新一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列更新为旧值就好了。 在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。 两大作用 实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。 实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。 刷盘策略undo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。 buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。 为什么需要 Buffer Pool？Innodb 存储引擎设计了一个缓冲池（Buffer Pool），来提高数据库的读写性能。 有了 Buffer Poo 后： 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。 redo log✨有了undolog为啥还需要redolog？undo log只是保证了原子性，但是不能保证持久性。 为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。 后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。 ==WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。== 什么是 redo log？redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。 在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。 当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。 被修改 Undo 页面，需要记录对应 redo log 吗？需要的。 开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。 不过，在内存修改该 Undo 页面后，需要记录对应的 redo log。 redo log 和 undo log 区别在哪？✨这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于： redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值； undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值； ==事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务==。 所以==有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）==。可以看出来， redo log 保证了事务四大特性中的持久性。 redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？可以说这是 WAL 技术的另外一个优点：MySQL 的写操作从磁盘的「随机写」变成了「顺序写」，提升语句的执行性能。 redo log怎么保证持久性？✨ Write-ahead logging（WAL）：在事务提交之前，将事务所做的修改操作记录到redo log中，然后再将数据写入磁盘。这样即使在数据写入磁盘之前发生了宕机，系统可以通过redo log中的记录来恢复数据。（如何实现） 将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。（为什么这么实现） Checkpoint机制：MySQL会定期将内存中的数据刷新到磁盘，同时将最新的LSN（Log Sequence Number）记录到磁盘中，这个LSN可以确保redo log中的操作是按顺序执行的。在恢复数据时，系统会根据LSN来确定从哪个位置开始应用redo log。（如何定位） 产生的 redo log 是直接写入磁盘的吗？不是的。 实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。 所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘 redo log 什么时候刷盘？✨主要有下面几个时机： MySQL 正常关闭时； 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘； InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。 提交事务：每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（由innodb_flush_log_at_trx_commit控制） 当设置该参数为 0 时，表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。 当设置该参数为 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。 当设置该参数为 2 时，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到操作系统的文件缓存（page cache）。 InnoDB 的后台线程每隔 1 秒： 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过调用 write() 写到操作系统的 Page Cache，然后调用 fsync() 持久化到磁盘。所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失; 针对参数 2 ：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 这三个参数的数据安全性和写入性能的比较如下： 数据安全性：参数 1 &gt; 参数 2 &gt; 参数 0 写入性能：参数 0 &gt; 参数 2&gt; 参数 1 redo log 文件写满了怎么办？默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成 重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。 图中的： write pos 和 checkpoint 的移动都是顺时针方向； write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作； check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录； 如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。 binlog为什么需要 binlog ？binlog 是 server 层实现的日志，所有存储引擎都能用；而最开始 MySQL 中并没有 InnoDB，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有崩溃恢复能力，所以通过插件的方式引入 InnoDB， 通过 redo log 来实现 crash-safe（崩溃恢复） binlog 只用于归档：binlog 文件是记录了所有数据库表结构变更和表数据修改的操作日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。 能不能只用binlog不用redo log？不行，binlog是 server 层的日志，记录所有的表结构变动和表数据变动的语句，是全量日志，没有 checkpoint 机制来判断哪些脏页还没有刷盘，redolog 是存储引擎层的日志，可以记录哪些脏页还没有刷盘，这样崩溃恢复的时候，就能恢复那些还没有被刷盘的脏页数据。 为什么 redo log 具有 crash-safe 的能力，是 binlog 无法替代的？-CSDN博客 redo log 和 binlog 有什么区别？为什么有了 binlog， 还要有 redo log？ 适用对象不同： binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用； redo log 是 Innodb 存储引擎实现的日志； 文件格式不同： binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。 问题：但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致； ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。 问题：但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已； MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新； 写入方式不同： binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。 redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。 用途不同： binlog 用于备份恢复、主从复制； redo log 用于掉电等故障恢复。 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。 因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。 binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。 主从复制是怎么实现？✨MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。 这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。 MySQL 集群的主从复制过程梳理成 3 个阶段： 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。 在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。 在主库端一旦有新的日志产生后，立刻会发送一次广播，Binglog Dump线程在收到广播后，则会读取二进制日志并通过网络向备库传输日志，所以这是一个主库向备库不断推送的过程。 从库是不是越多越好？不是的。 因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。 所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。 MySQL 主从复制还有哪些模型？主要有三种： 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。 binlog 什么时候刷盘？ 什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。如下图： MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog =N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在MySQL中系统默认的设置是 sync_binlog = 0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦主机发生异常重启，还没持久化到磁盘的数据就会丢失。 而当 sync_binlog 设置为 1 的时候，是最安全但是性能损耗最大的设置。因为当设置为 1 的时候，即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。 如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值。 为什么需要两阶段提交？✨事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。 举个例子，假设 id = 1 这行数据的字段 name 的值原本是 ‘jay’，然后执行 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况： 如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入。 如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入。 ==在持久化 redo log 和 binlog 这两份日志的时候，如果出现半成功的状态，就会造成主从环境的数据不一致性。这是因为 redo log 影响主库的数据，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致。== 两阶段提交的过程是怎样的？ 从图中可看出，事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下： prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）； commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功； 异常重启会出现什么现象？ 在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID： 如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。 如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。对应时刻 B 崩溃恢复的情况。 可以看到，对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了。 所以说，两阶段提交是以 binlog 写成功为事务提交成功的标识，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。 处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计? binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 事务没提交的时候，redo log 会被持久化到磁盘吗？ 会的。 事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。 也就是说，事务没提交的时候，redo log 也是可能被持久化到磁盘的。 但不会造成数据不一致，mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。 redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。 两阶段提交有什么问题？ 磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。 锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。 MySQL日志具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下: 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录： 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新； 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样： 如果一样的话就不进行后续更新流程； 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作； 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。 InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。 至此，一条记录更新完了。 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）： prepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘； commit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）； 至此，一条更新语句执行完成。 性能调优慢查询的原因。怎么定位慢查询，怎么优化慢查询，思路是什么。✨原因 索引失效 连接查询没有用到索引，执行了笛卡尔查询 大数据量排序与分组 复杂的子查询 表结构设计问题 事务锁竞争 定位 慢查询日志。MySQL的慢查询日志会记录执行时间超过long_query_time 的SQL语句 show processlist。实时展示当前MySQL正在执行的线程,可以通过processlist来发现一些状态显示为Lock等的慢查询。 分析查询语句 使用EXPLAIN命令分析SQL执行计划，找出慢查询的原因，比如是否使用了全表扫描，是否存在索引未被利用的情况等，并根据相应情况对索引进行适当修改。 优化 创建或优化索引。 对于 where、order by、group by、join 连表查询的字典； 对于查询经常涉及多个字段，考虑创建联合索引。 避免索引失效。 左模糊匹配； 函数计算； 表达式计算； 类型转换； where 子句 or 都要有索引； 列与列比较； is null 判断 查询语句优化。 避免使用SELECT *，只查询真正需要的列； 使用覆盖索引，即索引包含所有查询的字段； 联表查询最好要以小表驱动大表，并且被驱动表的字段要有索引，当然最好通过冗余字段的设计，避免联表查询。 优化表结构。去除冗余字段,拆分过大的表,适当垂直拆分或者水平拆分表。 使用视图。将复杂查询封装为视图,以简化查询过程。 加缓存优化数据统计类的慢查询 部署读写分离架构,主库负责写,从库负责读,分散数据库压力。 使用 explain 优化你的 mysql 性能✨Explain 特点 explain 返回的结果是以表为粒度的，每个表输出一行，这里的表示广义上的表，可以是一个子查询，也可以是一个 UNION 后的结果。 explain 只能解析 Select 查询，对于 update，insert 等都不支持，我们可以使用 select 来模拟 update 操作近似获取 update 的执行过程 Explain 中的列explain 是查看 sql 的执行计划，主要用来分析 sql 语句的执行过程，比如有没有走索引，有没有外部排序，有没有索引覆盖等等。 type✨type 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是全表扫描还是索引扫描等，type 类型的性能比较，通常来说, 不同的 type 类型的性能关系如下: ALL &lt; index &lt; range &lt; ref &lt; eq_ref &lt; const const（结果只有一条的主键或唯一索引扫描）：const 是与常量进行比较，查询效率会更快。 eq_ref（唯一索引扫描）： eq_ref 通常用于多表联查中 ref（非唯一索引扫描） range（索引范围扫描）: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中。从这一级别开始，索引的作用会越来越明显，因此我们需要尽量让 SQL 查询可以使用到 range 这一级别及以上的 type 访问方式。 但是对于同样的 type = range 的查询，性能上还是有区别的： 1234# 虽然是都是范围查询，其实第二个查询时多个等值条件查询# 对于第一个查询，mysql 无法再使用该列后面的其它查询索引了，而第二个则可以继续使用索引select id from actor where id &gt; 45 and class_id = 3;select id from actor where id in (44, 47, 48) and class_id = 3; index（全索引扫描）: 表示通过索引进行全表扫描和 ALL 类型类似, 有点是避免了排序，确定是需要承担按照索引次序读取表的开销。 ALL（全表扫描）: 这个类型的查询是性能最差的查询之一。一般情况下都会从头到尾扫描所有行，除非使用了 Limit 或者 Extra 列中显示 “Using distinct/not exists”。 key此次查询中确切使用到的索引，如果在 possible_keys 中没有出现而在 key 中出现，说明优化器可能出于另外原因比如选择覆盖索引，所以 possiable_keys 揭示了哪一个索引有助于高效进行查找，而 key 显示了采用哪一个索引可以最小化查询成本。 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数。这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好。 通过把每个表的 rows 值相乘可以粗略的估算出整个查询要检查的行数 这个值只是一个估算的值，不是实际查出来的值 Extra✨EXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大。 但是 Explain 不会告诉你 Mysql 将使用文件排序还是内存排序： 1234-- 比如我们建立索引为：KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)，那么如下两个查询EXPLAIN SELECT * FROM order_info ORDER BY product_name； -- Using filesort，不能通过索引进行排序，需要优化EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name；-- 无 Using filesort，通过索引进行排序，优化成功 Using index “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using where 这意味着 Mysql 服务器在存储引擎检索行后再进行过滤，一般出现 “Using where” 会受益于不同的索引 Using temporary 查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 临时表可能是内存临时表或者文件临时表。效率低，要避免这种问题的出现。 内存Buffer PoolInnodb 存储引擎设计了一个缓冲池（*Buffer Pool*），来提高数据库的读写性能。 Buffer Pool 以页为单位缓冲数据，可以通过 innodb_buffer_pool_size 参数调整缓冲池的大小，默认是 128 M。 Innodb 通过三种链表来管理缓页： Free List （空闲页链表），管理空闲页； Flush List （脏页链表），管理脏页； LRU List，管理脏页+干净页，将最近且经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。； InnoDB 对 LRU 做了一些优化，我们熟悉的 LRU 算法通常是将最近查询的数据放到 LRU 链表的头部，而 InnoDB 做 2 点优化： 将 LRU 链表 分为young 和 old 两个区域，加入缓冲池的页，优先插入 old 区域；页被访问时，才进入 young 区域，目的是为了解决预读失效的问题。 当「页被访问」且「 old 区域停留时间超过 innodb_old_blocks_time 阈值（默认为1秒）」时，才会将页插入到 young 区域，否则还是插入到 old 区域，目的是为了解决批量数据访问，大量热数据淘汰的问题。 可以通过调整 innodb_old_blocks_pct 参数，设置 young 区域和 old 区域比例。 在开启了慢 SQL 监控后，如果你发现「偶尔」会出现一些用时稍长的 SQL，这可因为脏页在刷新到磁盘时导致数据库性能抖动。如果在很短的时间出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。 MyBatis mybatis 的执行流程 首先读取配置文件，然后加载映射文件，由SqlSessionFactory工厂对象去创建核心对象SqlSession，SqlSession对象会通过Executor执行器对象执行sql。 然后Executor执行器对象会调用StatementHandler对象去真正的访问数据库执行sql语句。在执行sql语句前MapperStatement会先对映射信息进行封装，存储要映射的SQL语句的id、参数等信息 然后调用ParameterHandler去设置编译参数【#{}，${}】 然后TypeHandler进行数据库类型和JavaBean类型映射处理。 然后调用JBDC原生API进行处理，获取执行结果，这个执行结果交给ResultSetHandler 来进行结果集封装，然后将结果返回给StatementHandler。","link":"/2024/01/12/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/"},{"title":"微服务基础-常见中间件","text":"注册中心-EurekaEureka的结构和作用 消费者该如何获取服务提供者具体信息？ 服务提供者启动时向eureka注册自己的信息 eureka保存这些信息 消费者根据服务名称向eureka拉取提供者信息 如果有多个服务提供者，消费者该如何选择？ 服务消费者利用负载均衡算法，从服务列表中挑选一个 消费者如何感知服务提供者健康状态？ 服务提供者会每隔30秒向EurekaServer发送心跳请求，报告健康状态 eureka会更新记录服务列表信息，心跳不正常会被剔除 消费者就可以拉取到最新的信息 搭建eureka-server引入eureka依赖引入SpringCloud为eureka提供的starter依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt;&lt;/dependency&gt; 编写启动类给eureka-server服务编写一个启动类，一定要添加一个@EnableEurekaServer注解，开启eureka的注册中心功能： 12345678910111213package cn.itcast.eureka;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class EurekaApplication { public static void main(String[] args) { SpringApplication.run(EurekaApplication.class, args); }} 编写配置文件编写一个application.yml文件，内容如下： 12345678910server: port: 10086 #服务端口spring: application: name: eurekaserver # 微服务注册名称eureka: client: service-url: #eureka地址信息 defaultZone: http://127.0.0.1:10086/eureka 服务注册下面，我们将user-service注册到eureka-server中去。 1）引入依赖在user-service的pom文件中，引入下面的eureka-client依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置文件在user-service中，修改application.yml文件，添加服务名称、eureka地址： 1234567spring: application: name: userserviceeureka: client: service-url: defaultZone: http://127.0.0.1:10086/eureka 服务发现服务拉取和负载均衡最后，我们要去eureka-server中拉取user-service服务的实例列表，并且实现负载均衡。 不过这些动作不用我们去做，只需要添加一些注解即可。 在order-service的OrderApplication中，给RestTemplate这个Bean添加一个@LoadBalanced注解： 123456789101112131415@MapperScan(\"cn.itcast.order.mapper\")@SpringBootApplicationpublic class OrderApplication { public static void main(String[] args) { SpringApplication.run(OrderApplication.class, args); } @Bean @LoadBalanced public RestTemplate restTemplate(){ return new RestTemplate(); }} 修改order-service服务中的cn.itcast.order.service包下的OrderService类中的queryOrderById方法。修改访问的url路径，用服务名代替ip、端口： 1234567891011121314151617181920212223@Servicepublic class OrderService { @Autowired private OrderMapper orderMapper; @Autowired private RestTemplate restTemplate; public Order queryOrderById(Long orderId) { // 1.查询订单 Order order = orderMapper.findById(orderId);// String url = \"http://localhost:8081/user/\" + order.getUserId(); String url = \"http://userservice/user/\" + order.getUserId(); User user = restTemplate.getForObject(url, User.class); order.setUser(user); // 4.返回 return order; }} 负载均衡-Ribbon负载均衡原理SpringCloud底层其实是利用了一个名为Ribbon的组件，来实现负载均衡功能的。 负载均衡策略负载均衡策略负载均衡的规则都定义在IRule接口中，而IRule有很多不同的实现类： 不同规则的含义如下： 内置负载均衡规则类 规则描述 RoundRobinRule 简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。 AvailabilityFilteringRule 对以下两种服务器进行忽略： （1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。 （2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的..ActiveConnectionsLimit属性进行配置。 WeightedResponseTimeRule 为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。 ZoneAvoidanceRule 以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。 BestAvailableRule 忽略那些短路的服务器，并选择并发数较低的服务器。 RandomRule 随机选择一个可用的服务器。 RetryRule 重试机制的选择逻辑 默认的实现就是ZoneAvoidanceRule，是一种轮询方案 自定义负载均衡策略通过定义IRule实现可以修改负载均衡规则，有两种方式： 代码方式：在order-service中的OrderApplication类中，定义一个新的IRule： 1234@Beanpublic IRule randomRule(){ return new RandomRule();} 配置文件方式：在order-service的application.yml文件中，添加新的配置也可以修改规则： 123userservice: # 给某个微服务配置负载均衡规则，这里是userservice服务 ribbon: NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule # 负载均衡规则 注意，一般用默认的负载均衡规则，不做修改。 饥饿加载Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。 而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载： 1234ribbon: eager-load: enabled: true clients: userservice 注册中心-Nacos安装nacosDocker启动安装nacosdocker部署nacos.md 启动安装nacosNacos安装指南.md 服务注册到nacos1）引入依赖父工程： pom文件中的&lt;dependencyManagement&gt;中引入SpringCloudAlibaba的依赖： 1234567&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-alibaba-dependencies&lt;/artifactId&gt; &lt;version&gt;2.2.6.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt;&lt;/dependency&gt; 客户端： pom文件中引入nacos-discovery依赖： 123456&lt;!-- nacos客户端依赖包 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置nacos地址在user-service和order-service的application.yml中添加nacos地址： 1234spring: cloud: nacos: server-addr: localhost:8848 3）重启重启微服务后，登录nacos管理页面，可以看到微服务信息： 服务分级存储模型Nacos就将同一机房内的实例 划分为一个集群。 也就是说，user-service是服务，一个服务可以包含多个集群，如杭州、上海，每个集群下可以有多个实例，形成分级模型，如图： 微服务互相访问时，应该尽可能访问同集群实例，因为本地访问速度更快。当本集群内不可用时，才访问其它集群。例如： 杭州机房内的order-service应该优先访问同机房的user-service。 给user-service配置集群修改user-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 同集群优先的负载均衡默认的ZoneAvoidanceRule并不能实现根据同集群优先来实现负载均衡。 因此Nacos中提供了一个NacosRule的实现，可以优先从同集群中挑选实例。 1）给order-service配置集群信息 修改order-service的application.yml文件，添加集群配置： 123456spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ # 集群名称 2）修改负载均衡规则 yml配置文件针对特定微服务 修改order-service的application.yml文件，修改负载均衡规则： 123userservice: ribbon: NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule # 负载均衡规则 在配置文件里面配置 1234@Bean public IRule nacosRule(){ return new NacosRule(); } 踩坑 要把OrderApplication.java的配置注释掉，不然会覆盖掉yml文件里面的轮询策略 1234// @Bean// public IRule randomRule() {// return new RandomRule();// } 假如本集群没有对应的节点，那么会报warn 然后找到其它集群里面的节点 NacosRule负载均衡策略 优先选择同集群服务实例列表 本地集群找不到提供者，才去其它集群寻找，并且会报警告 确定了可用实例列表后，再采用随机负载均衡挑选实例 权重配置 实例的权重控制 Nacos控制台可以设置实例的权重值，0~1之间 同集群内的多个实例，权重越高被访问的频率越高 权重设置为0则完全不会被访问 环境隔离Nacos提供了namespace来实现环境隔离功能。 nacos中可以有多个namespace namespace下可以有group、service等 不同namespace之间相互隔离，例如不同namespace的服务互相不可见 给微服务配置namespace给微服务配置namespace只能通过修改配置来实现。 例如，修改order-service的application.yml文件： 1234567spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: HZ namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID Nacos环境隔离 每个namespace都有唯一id 服务设置namespace时要写id而不是名称 不同namespace下的服务互相不可见 Nacos与Eureka的区别Nacos的服务实例分为两种l类型： 临时实例：如果实例宕机超过一定时间，会从服务列表剔除，默认的类型。 非临时实例：如果实例宕机，不会从服务列表剔除，也可以叫永久实例。 配置一个服务实例为永久实例： 12345spring: cloud: nacos: discovery: ephemeral: false # 设置为非临时实例 Nacos和Eureka整体结构类似，服务注册、服务拉取、心跳等待，但是也存在一些差异： Nacos与eureka的共同点 都支持服务注册和服务拉取 都支持服务提供者心跳方式做健康检测 Nacos与Eureka的区别 Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式 临时实例心跳不正常会被剔除，非临时实例则不会被剔除 Nacos支持服务列表变更的消息推送模式，服务列表更新更及时 Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式 配置管理-Nacos统一配置管理当微服务部署的实例越来越多，逐个修改微服务配置就会让人抓狂，而且很容易出错。我们需要一种统一配置管理方案，可以集中管理所有实例的配置。 Nacos一方面可以将配置集中管理，另一方可以在配置变更时，及时通知微服务，实现配置的热更新。 在nacos中添加配置文件 配置文件id：[服务名称]-[profile].[后缀名] 配置格式：yaml 注意：项目的核心配置，需要热更新的配置才有放到nacos管理的必要。基本不会变更的一些配置还是保存在微服务本地比较好。 从微服务拉取配置微服务要拉取nacos中管理的配置，并且与本地的application.yml配置合并，才能完成项目启动。 但如果尚未读取application.yml，又如何得知nacos地址呢？ 因此spring引入了一种新的配置文件：bootstrap.yaml文件，会在application.yml之前被读取，流程如下： 1）引入nacos-config依赖 首先，在user-service服务中，引入nacos-config的客户端依赖： 12345&lt;!--nacos配置管理依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-config&lt;/artifactId&gt;&lt;/dependency&gt; 2）添加bootstrap.yaml 然后，在user-service中添加一个bootstrap.yaml文件，内容如下： 12345678910spring: application: name: userservice # 服务名称 profiles: active: dev #开发环境，这里是dev cloud: nacos: server-addr: localhost:8848 # Nacos地址 config: file-extension: yaml # 文件后缀名 这里会根据spring.cloud.nacos.server-addr获取nacos地址，再根据 ${spring.application.name}-${spring.profiles.active}.${spring.cloud.nacos.config.file-extension}作为文件id，来读取配置。 本例中，就是去读取userservice-dev.yaml： 3）读取nacos配置 在user-service中的UserController中添加业务逻辑，读取pattern.dateformat配置： 配置热更新我们最终的目的，是修改nacos中的配置后，微服务中无需重启即可让配置生效，也就是配置热更新。 要实现配置热更新，可以使用两种方式： 方式一在@Value注入的变量所在类上添加注解@RefreshScope： 方式二使用@ConfigurationProperties注解代替@Value注解。 在user-service服务中，添加一个类，读取patterrn.dateformat属性： 123456789101112package cn.itcast.user.config;import lombok.Data;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.stereotype.Component;@Component@Data@ConfigurationProperties(prefix = \"pattern\")public class PatternProperties { private String dateformat;} 配置共享其实微服务启动时，会去nacos读取多个配置文件，例如： [spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml [spring.application.name].yaml，例如：userservice.yaml 而[spring.application.name].yaml不包含环境，因此可以被多个环境共享。 配置共享的优先级当nacos、服务本地同时出现相同属性时，优先级有高低之分： 搭建Nacos集群docker部署nacos集群.md 远程调用-Feign先来看我们以前利用RestTemplate发起远程调用的代码： 存在下面的问题： •代码可读性差，编程体验不统一 •参数复杂URL难以维护 Feign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign 其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。 Feign替代RestTemplate1）引入依赖我们在order-service服务的pom文件中引入feign的依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt;&lt;/dependency&gt; 2）添加注解在order-service的启动类添加注解开启Feign的功能： 3）编写Feign的客户端在order-service中新建一个接口，内容如下： 123456789101112package cn.itcast.order.client;import cn.itcast.order.pojo.User;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(\"userservice\")public interface UserClient { @GetMapping(\"/user/{id}\") User findById(@PathVariable(\"id\") Long id);} 这个客户端主要是基于SpringMVC的注解来声明远程调用的信息，比如： 服务名称：userservice 请求方式：GET 请求路径：/user/{id} 请求参数：Long id 返回值类型：User 这样，Feign就可以帮助我们发送http请求，无需自己使用RestTemplate来发送了。 4）测试修改order-service中的OrderService类中的queryOrderById方法，使用Feign客户端代替RestTemplate： 5）总结使用Feign的步骤： ① 引入依赖 ② 添加@EnableFeignClients注解 ③ 编写FeignClient接口 ④ 使用FeignClient中定义的方法代替RestTemplate 自定义配置Feign可以支持很多的自定义配置，如下表所示： 类型 作用 说明 feign.Logger.Level 修改日志级别 包含四种不同的级别：NONE、BASIC、HEADERS、FULL feign.codec.Decoder 响应结果的解析器 http远程调用的结果做解析，例如解析json字符串为java对象 feign.codec.Encoder 请求参数编码 将请求参数编码，便于通过http请求发送 feign. Contract 支持的注解格式 默认是SpringMVC的注解 feign. Retryer 失败重试机制 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试 一般情况下，默认值就能满足我们使用，如果要自定义时，只需要创建自定义的@Bean覆盖默认Bean即可。 下面以日志为例来演示如何自定义配置。 配置文件方式基于配置文件修改feign的日志级别可以针对单个服务： 12345feign: client: config: userservice: # 针对某个微服务的配置 loggerLevel: FULL # 日志级别 也可以针对所有服务： 12345feign: client: config: default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置 loggerLevel: FULL # 日志级别 而日志的级别分为四种： NONE：不记录任何日志信息，这是默认值。 BASIC：仅记录请求的方法，URL以及响应状态码和执行时间 HEADERS：在BASIC的基础上，额外记录了请求和响应的头信息 FULL：记录所有请求和响应的明细，包括头信息、请求体、元数据。 Java代码方式也可以基于Java代码来修改日志级别，先声明一个类，然后声明一个Logger.Level的对象： 123456public class DefaultFeignConfiguration { @Bean public Logger.Level feignLogLevel(){ return Logger.Level.BASIC; // 日志级别为BASIC }} 如果要全局生效，将其放到启动类的@EnableFeignClients这个注解中： 1@EnableFeignClients(defaultConfiguration = DefaultFeignConfiguration .class) 如果是局部生效，则把它放到对应的@FeignClient这个注解中： 1@FeignClient(value = \"userservice\", configuration = DefaultFeignConfiguration .class) Feign使用优化Feign底层发起http请求，依赖于其它的框架。其底层客户端实现包括： URLConnection：默认实现，不支持连接池 Apache HttpClient ：支持连接池 OKHttp：支持连接池 因此提高Feign的性能主要手段就是使用连接池代替默认的URLConnection。 这里我们用Apache的HttpClient来演示。 1）引入依赖 在order-service的pom文件中引入Apache的HttpClient依赖： 12345&lt;!--httpClient的依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;io.github.openfeign&lt;/groupId&gt; &lt;artifactId&gt;feign-httpclient&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置连接池 在order-service的application.yml中添加配置： 123456789feign: client: config: default: # default全局的配置 loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息 httpclient: enabled: true # 开启feign对HttpClient的支持 max-connections: 200 # 最大的连接数 max-connections-per-route: 50 # 每个路径的最大连接数 总结，Feign的优化： 1.日志级别尽量用basic 2.使用HttpClient或OKHttp代替URLConnection ① 引入feign-httpClient依赖 ② 配置文件开启httpClient功能，设置连接池参数 服务网关-GatewaySpring Cloud Gateway 是 Spring Cloud 的一个全新项目，该项目是基于 Spring 5.0，Spring Boot 2.0 和 Project Reactor 等响应式编程和事件流技术开发的网关，它旨在为微服务架构提供一种简单有效的统一的 API 路由管理方式。 Gateway网关是我们服务的守门神，所有微服务的统一入口。 网关的核心功能特性： 请求路由 权限控制 限流 架构图： 权限控制：网关作为微服务入口，需要校验用户是是否有请求资格，如果没有则进行拦截。 路由和负载均衡：一切请求都必须先经过gateway，但网关不处理业务，而是根据某种规则，把请求转发到某个微服务，这个过程叫做路由。当然路由的目标服务有多个时，还需要做负载均衡。 限流：当请求流量过高时，在网关中按照下流的微服务能够接受的速度来放行请求，避免服务压力过大。 在SpringCloud中网关的实现包括两种： gateway zuul Zuul是基于Servlet的实现，属于阻塞式编程。而SpringCloudGateway则是基于Spring5中提供的WebFlux，属于响应式编程的实现，具备更好的性能。 gateway快速入门基本步骤如下： 创建SpringBoot工程gateway，引入网关依赖 编写启动类 编写基础配置和路由规则 启动网关服务进行测试 1）创建gateway服务，引入依赖创建服务： 引入依赖： 12345678910&lt;!--网关--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-gateway&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--nacos服务发现依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-nacos-discovery&lt;/artifactId&gt;&lt;/dependency&gt; 2）编写启动类123456789101112package cn.itcast.gateway;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class GatewayApplication { public static void main(String[] args) { SpringApplication.run(GatewayApplication.class, args); }} 3）编写基础配置和路由规则创建application.yml文件，内容如下： 123456789101112131415server: port: 10010 # 网关端口spring: application: name: gateway # 服务名称 cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: # 网关路由配置 - id: user-service # 路由id，自定义，只要唯一即可 # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址 uri: lb://userservice # 路由的目标地址 lb就是负载均衡，后面跟服务名称 predicates: # 路由断言，也就是判断请求是否符合路由规则的条件 - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求 我们将符合Path 规则的一切请求，都代理到 uri参数指定的地址。 本例中，我们将 /user/**开头的请求，代理到lb://userservice，lb是负载均衡，根据服务名拉取服务列表，实现负载均衡。 4）重启测试重启网关，访问http://localhost:10010/user/1时，符合`/user/**`规则，请求转发到uri：http://userservice/user/1，得到了结果： 5）网关路由的流程图整个访问的流程如下： 总结： 网关搭建步骤： 创建项目，引入nacos服务发现和gateway依赖 配置application.yml，包括服务基本信息、nacos地址、路由 路由配置包括： 路由id：路由的唯一标示 路由目标（uri）：路由的目标地址，http代表固定地址，lb代表根据服务名负载均衡 路由断言（predicates）：判断路由的规则， 路由过滤器（filters）：对请求或响应做处理 断言工厂我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件 例如Path=/user/**是按照路径匹配，这个规则是由 org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来 处理的，像这样的断言工厂在SpringCloudGateway还有十几个: 名称 说明 示例 After 是某个时间点后的请求 - After=2037-01-20T17:42:47.789-07:00[America/Denver] Before 是某个时间点之前的请求 - Before=2031-04-13T15:14:47.433+08:00[Asia/Shanghai] Between 是某两个时间点之前的请求 - Between=2037-01-20T17:42:47.789-07:00[America/Denver], 2037-01-21T17:42:47.789-07:00[America/Denver] Cookie 请求必须包含某些cookie - Cookie=chocolate, ch.p Header 请求必须包含某些header - Header=X-Request-Id, \\d+ Host 请求必须是访问某个host（域名） - Host=.somehost.org,.anotherhost.org Method 请求方式必须是指定方式 - Method=GET,POST Path 请求路径必须符合指定规则 - Path=/red/{segment},/blue/** Query 请求参数必须包含指定参数 - Query=name, Jack或者- Query=name RemoteAddr 请求者的ip必须是指定范围 - RemoteAddr=192.168.1.1/24 Weight 权重处理 只需要掌握Path这种路由工程就可以了。 过滤器工厂GatewayFilter是网关中提供的一种过滤器，可以对进入网关的请求和微服务返回的响应做处理： 路由过滤器的种类Spring提供了31种不同的路由过滤器工厂。例如： 名称 说明 AddRequestHeader 给当前请求添加一个请求头 RemoveRequestHeader 移除请求中的一个请求头 AddResponseHeader 给响应结果中添加一个响应头 RemoveResponseHeader 从响应结果中移除有一个响应头 RequestRateLimiter 限制请求的流量 请求头过滤器下面我们以AddRequestHeader 为例来讲解。 需求：给所有进入userservice的请求添加一个请求头：Truth=itcast is freaking awesome! 只需要修改gateway服务的application.yml文件，添加路由过滤即可： 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** filters: # 过滤器 - AddRequestHeader=Truth, Itcast is freaking awesome! # 添加请求头 当前过滤器写在userservice路由下，因此仅仅对访问userservice的请求有效。 默认过滤器如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下： 12345678910spring: cloud: gateway: routes: - id: user-service uri: lb://userservice predicates: - Path=/user/** default-filters: # 默认过滤项 - AddRequestHeader=Truth, Itcast is freaking awesome! 总结过滤器的作用是什么？ ① 对路由的请求或响应做加工处理，比如添加请求头 ② 配置在路由下的过滤器只对当前路由的请求生效 defaultFilters的作用是什么？ ① 对所有路由都生效的过滤器 全局过滤器上一节学习的过滤器，网关提供了31种，但每一种过滤器的作用都是固定的。如果我们希望拦截请求，做自己的业务逻辑则没办法实现。 全局过滤器作用全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。区别在于GatewayFilter通过配置定义，处理逻辑是固定的；而GlobalFilter的逻辑需要自己写代码实现。 定义方式是实现GlobalFilter接口。 12345678910public interface GlobalFilter { /** * 处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理 * * @param exchange 请求上下文，里面可以获取Request、Response等信息 * @param chain 用来把请求委托给下一个过滤器 * @return {@code Mono&lt;Void&gt;} 返回标示当前过滤器业务结束 */ Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain);} 在filter中编写自定义逻辑，可以实现下列功能： 登录状态判断 权限校验 请求限流等 自定义全局过滤器需求：定义全局过滤器，拦截请求，判断请求的参数是否满足下面条件： 参数中是否有authorization， authorization参数值是否为admin 如果同时满足则放行，否则拦截 实现： 在gateway中定义一个过滤器： 12345678910111213141516171819202122232425262728293031package cn.itcast.gateway.filters;import org.springframework.cloud.gateway.filter.GatewayFilterChain;import org.springframework.cloud.gateway.filter.GlobalFilter;import org.springframework.core.annotation.Order;import org.springframework.http.HttpStatus;import org.springframework.stereotype.Component;import org.springframework.web.server.ServerWebExchange;import reactor.core.publisher.Mono;@Order(-1)@Componentpublic class AuthorizeFilter implements GlobalFilter { @Override public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { // 1.获取请求参数 MultiValueMap&lt;String, String&gt; params = exchange.getRequest().getQueryParams(); // 2.获取authorization参数 String auth = params.getFirst(\"authorization\"); // 3.校验 if (\"admin\".equals(auth)) { // 放行 return chain.filter(exchange); } // 4.拦截 // 4.1.禁止访问，设置状态码 exchange.getResponse().setStatusCode(HttpStatus.FORBIDDEN); // 4.2.结束处理 return exchange.getResponse().setComplete(); }} 过滤器执行顺序请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter 请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器： 排序的规则是什么呢？ 每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。 GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。 当过滤器的order值一样时，会按照 defaultFilter &gt; 路由过滤器 &gt; GlobalFilter的顺序执行。 详细内容，可以查看源码： org.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。 org.springframework.cloud.gateway.handler.FilteringWebHandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链 跨域问题什么是跨域问题跨域：域名不一致就是跨域，主要包括： 域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com 域名相同，端口不同：localhost:8080和localhost8081 跨域问题： CORS全称Cross-Origin Resource Sharing，意为跨域资源共享。当一个资源去访问另一个不同域名或者同域名不同端口的资源时，就会发出跨域请求。如果此时另一个资源不允许其进行跨域资源访问，那么访问就会遇到跨域问题。 跨域是指浏览器不能执行来自其它网站的脚本，是由浏览器的同源策略造成的，是浏览器对JavaScript 施加的安全限制。（需要注意的是，跨域并不是请求发不出去，请求能发出去，服务端能收到请求并正常返回结果，只是结果被浏览器拦截了） 引出同源策略 1.之所以会出现跨域现象，是因为受到了同源策略的限制，同源策略要求源相同才能正常进行通信，即协议、域名、端口号都完全一致。2.同源的存在，又可以保护用户隐私信息，防止身份伪造等。 同源策略限制内容 Cookie、LocalStorage等存储性内容 DOM 节点 AJAX 请求不能发送 解决方案：CORS，这个以前应该学习过，这里不再赘述了。不知道的小伙伴可以查看https://www.ruanyifeng.com/blog/2016/04/cors.html 跨源资源共享（CORS，或通俗地译为跨域资源共享）是一种基于 HTTP 头的机制，该机制通过允许服务器标示除了它自己以外的其他源（域、协议或端口），使得浏览器允许这些源访问加载自己的资源。 模拟跨域问题找到课前资料的页面文件： 放入tomcat或者nginx这样的web服务器中，启动并访问。 可以在浏览器控制台看到下面的错误： 12345&lt;script&gt; axios.get(\"http://localhost:10010/user/1?authorization=admin\") .then(resp =&gt; console.log(resp.data)) .catch(err =&gt; console.log(err))&lt;/script&gt; 从localhost:8090访问localhost:10010，端口不同，显然是跨域的请求。 解决跨域问题在gateway服务的application.yml文件中，添加下面的配置： 12345678910111213141516171819spring: cloud: gateway: # 。。。 globalcors: # 全局的跨域处理 add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题 corsConfigurations: '[/**]': allowedOrigins: # 允许哪些网站的跨域请求 - \"http://localhost:8090\" allowedMethods: # 允许的跨域ajax的请求方式 - \"GET\" - \"POST\" - \"DELETE\" - \"PUT\" - \"OPTIONS\" allowedHeaders: \"*\" # 允许在请求中携带的头信息 allowCredentials: true # 是否允许携带cookie maxAge: 360000 # 这次跨域检测的有效期 应用容器引擎-Dockerdocker介绍与安装docker介绍应用部署的环境问题大型项目组件较多，运行环境也较为复杂，部署时会碰到一些问题： 依赖关系复杂，容易出现兼容性问题 开发、测试、生产环境有差异 Docker解决依赖兼容问题Docker为了解决依赖的兼容问题的，采用了两个手段： 将应用的Libs（函数库）、Deps（依赖）、配置与应用一起打包 将每个应用放到一个隔离容器去运行，避免互相干扰 操作系统本身提供函数库——对内核指令封装 应用于计算机交互的流程如下： 1）应用调用操作系统应用（函数库），实现各种功能 2）系统函数库是对内核指令集的封装，会调用内核指令 3）内核指令操作计算机硬件 这样打包好的应用包中，既包含应用本身，也保护应用所需要的Libs、Deps，无需再操作系统上安装这些，自然就不存在不同应用之间的兼容问题了。 虽然解决了不同应用的兼容问题，但是开发、测试等环境会存在差异，操作系统版本也会有差异，怎么解决这些问题呢？ Docker解决操作系统环境差异Ubuntu和CentOSpringBoot都是基于Linux内核，无非是系统应用不同，提供的函数库有差异 此时，如果将一个Ubuntu版本的MySQL应用安装到CentOS系统，MySQL在调用Ubuntu函数库时，会发现找不到或者不匹配，就会报错了 Docker如何解决不同系统环境的问题？ Docker将用户程序与所需要调用的系统(比如Ubuntu)函数库一起打包 Docker运行到不同操作系统时，直接基于打包的函数库，借助于操作系统的Linux内核来运行 小结Docker如何解决大型项目依赖关系复杂，不同组件依赖的兼容性问题？ Docker允许开发中将应用、依赖、函数库、配置一起打包，形成可移植镜像 Docker应用运行在容器中，使用沙箱机制，相互隔离 Docker如何解决开发、测试、生产环境有差异的问题？ Docker镜像中包含完整运行环境，包括系统函数库，函数库仅依赖系统的Linux内核，因此可以在任意Linux操作系统上运行 Docker是一个快速交付应用、运行应用的技术，具备下列优势： 可以将程序及其依赖、运行环境一起打包为一个镜像，可以迁移到任意Linux操作系统 运行时利用沙箱机制形成隔离容器，各个应用互不干扰 启动、移除都可以通过一行命令完成，方便快捷 docker和虚拟机的区别虚拟机（virtual machine）是在操作系统中模拟硬件设备，然后运行另一个操作系统，比如在 Windows 系统里面运行 Ubuntu 系统，这样就可以运行任意的Ubuntu应用了。 Docker仅仅是封装函数库，并没有模拟完整的操作系统，如图： 小结： Docker和虚拟机的差异： docker是一个系统进程；虚拟机是在操作系统中的操作系统 docker体积小、启动速度快、性能好；虚拟机体积大、启动速度慢、性能一般 docker架构镜像和容器Docker中有几个重要的概念： 镜像（Image）：Docker将应用程序及其所需的依赖、函数库、环境、配置等文件打包在一起，称为镜像。 容器（Container）：镜像中的应用程序运行后形成的进程就是容器，只是Docker会给容器进程做隔离，对外不可见。 一切应用最终都是代码组成，都是硬盘中的一个个的字节形成的文件。只有运行时，才会加载到内存，形成进程。 而镜像，就是把一个应用在硬盘上的文件、及其运行环境、部分系统函数库文件一起打包形成的文件包。这个文件包是只读的。 容器呢，就是将这些文件中编写的程序、函数加载到内存中允许，形成进程，只不过要隔离起来。因此一个镜像可以启动多次，形成多个容器进程。 DockerHub开源应用程序非常多，打包这些应用往往是重复的劳动。为了避免这些重复劳动，人们就会将自己打包的应用镜像，例如Redis、MySQL镜像放到网络上，共享使用，就像GitHub的代码共享一样。 DockerHub：DockerHub是一个官方的Docker镜像的托管平台。这样的平台称为Docker Registry。 国内也有类似于DockerHub 的公开服务，比如 网易云镜像服务、阿里云镜像库等。 我们一方面可以将自己的镜像共享到DockerHub，另一方面也可以从DockerHub拉取镜像： Docker架构我们要使用Docker来操作镜像、容器，就必须要安装Docker。 Docker是一个CS架构的程序，由两部分组成： 服务端(server)：Docker守护进程，负责处理Docker指令，管理镜像、容器等 客户端(client)：通过命令或RestAPI向Docker服务端发送指令。可以在本地或远程向服务端发送指令。 如图： 小结镜像： 将应用程序及其依赖、环境、配置打包在一起 容器： 镜像运行起来就是容器，一个镜像可以运行多个容器 Docker结构： 服务端：接收命令或远程请求，操作镜像或容器 客户端：发送命令或者请求到Docker服务端 docker公有仓库和私有仓库的区别： 公有仓库：Docker官方提供一个公共仓库，称为Docker Hub。任何人都可以在Docker Hub上创建帐户，并在其中存储和共享Docker镜像。 私有仓库：私有仓库是指由用户自己搭建、管理和维护的Docker仓库，可以自己选择存储位置和数据安全性，也可以和其他项目无缝集成。类似的还有阿里云镜像服务，统称为DockerRegistry 安装docker部署docker.md Docker的基本操作镜像操作镜像名称首先来看下镜像的名称组成： 镜名称一般分两部分组成：[repository]:[tag]。 在没有指定tag时，默认是latest，代表最新版本的镜像 如图： 这里的mysql就是repository，5.7就是tag，合一起就是镜像名称，代表5.7版本的MySQL镜像。 镜像命令常见的镜像操作命令如图： 拉取、查看镜像需求：从DockerHub中拉取一个nginx镜像并查看 1）首先去镜像仓库搜索nginx镜像，比如DockerHub: 2）根据查看到的镜像名称，拉取自己需要的镜像，通过命令：docker pull nginx 3）通过命令：docker images 查看拉取到的镜像 保存、导入镜像需求：利用docker save将nginx镜像导出磁盘，然后再通过load加载回来 1）利用docker xx –help命令查看docker save和docker load的语法 例如，查看save命令用法，可以输入命令： 1docker save --help 命令格式： 1docker save -o [保存的目标文件名称] [镜像名称] 2）使用docker save导出镜像到磁盘 运行命令： 1docker save -o nginx.tar nginx:latest 3）使用docker load加载镜像 先删除本地的nginx镜像： 1docker rmi nginx:latest 然后运行命令，加载本地文件： 1docker load -i nginx.tar 容器操作容器相关命令容器操作的命令如图： 容器保护三个状态： 运行：进程正常运行 暂停：进程暂停，CPU不再运行，并不释放内存 停止：进程终止，回收进程占用的内存、CPU等资源 其中： docker run：创建并运行一个容器，处于运行状态 docker pause：让一个运行的容器暂停 docker unpause：让一个容器从暂停状态恢复运行 docker stop：停止一个运行的容器 docker start：让一个停止的容器再次运行 docker rm：删除一个容器 创建并运行一个容器创建并运行nginx容器的命令： 1docker run --name containerName -p 80:80 -d nginx 命令解读： docker run ：创建并运行一个容器 –name : 给容器起一个名字，比如叫做mn -p ：将宿主机端口与容器端口映射，冒号左侧是宿主机端口，右侧是容器端口 -d：后台运行容器 nginx：镜像名称，例如nginx 这里的-p参数，是将容器端口映射到宿主机端口。 默认情况下，容器是隔离环境，我们直接访问宿主机的80端口，肯定访问不到容器中的nginx。 现在，将容器的80与宿主机的80关联起来，当我们访问宿主机的80端口时，就会被映射到容器的80，这样就能访问到nginx了： 进入容器，修改文件需求：进入Nginx容器，修改HTML文件内容，添加“传智教育欢迎您” 提示：进入容器要用到docker exec命令。 步骤： 1）进入容器。进入我们刚刚创建的nginx容器的命令为： 1docker exec -it mn bash 命令解读： docker exec ：进入容器内部，执行一个命令 -it : 给当前进入的容器创建一个标准输入、输出终端，允许我们与容器交互 mn ：要进入的容器的名称 bash：进入容器后执行的命令，bash是一个linux终端交互命令 小结docker run命令的常见参数有哪些？ –name：指定容器名称 -p：指定端口映射 -d：让容器后台运行 查看容器日志的命令： docker logs 添加 -f 参数可以持续查看日志 查看容器状态： docker ps docker ps -a 查看所有容器，包括已经停止的 数据卷（容器数据管理）在之前的nginx案例中，修改nginx的html页面时，需要进入nginx内部。并且因为没有编辑器，修改文件也很麻烦。 这就是因为容器与数据（容器内文件）耦合带来的后果。 要解决这个问题，必须将数据与容器解耦，这就要用到数据卷了。 什么是数据卷数据卷（volume）是一个虚拟目录，指向宿主机文件系统中的某个目录。 一旦完成数据卷挂载，对容器的一切操作都会作用在数据卷对应的宿主机目录了。 数据集操作命令数据卷操作的基本语法如下： 1docker volume [COMMAND] docker volume命令是数据卷操作，根据命令后跟随的command来确定下一步的操作： create 创建一个volume inspect 显示一个或多个volume的信息 ls 列出所有的volume prune 删除未使用的volume rm 删除一个或多个指定的volume 创建和查看数据卷需求：创建一个数据卷，并查看数据卷在宿主机的目录位置 ① 创建数据卷 1docker volume create html ② 查看所有数据 1docker volume ls ③ 查看数据卷详细信息卷 1docker volume inspect html 小结： 数据卷的作用： 将容器与数据分离，解耦合，方便操作容器内数据，保证数据安全 数据卷操作： docker volume create：创建数据卷 docker volume ls：查看所有数据卷 docker volume inspect：查看数据卷详细信息，包括关联的宿主机目录位置 docker volume rm：删除指定数据卷 docker volume prune：删除所有未使用的数据卷 挂载数据卷我们在创建容器时，可以通过 -v 参数来挂载一个数据卷到某个容器内目录，命令格式如下： 12345docker run \\ --name mn \\ -v html:/root/html \\ -p 8080:80 nginx \\ 这里的-v就是挂载数据卷的命令： -v html:/root/htm ：把html数据卷挂载到容器内的/root/html这个目录中 给nginx挂载数据卷需求：创建一个nginx容器，修改容器内的html目录内的index.html内容 分析：上个案例中，我们进入nginx容器内部，已经知道nginx的html目录所在位置/usr/share/nginx/html ，我们需要把这个目录挂载到html这个数据卷上，方便操作其中的内容。 提示：运行容器时使用 -v 参数挂载数据卷 步骤： ① 创建容器并挂载数据卷到容器内的HTML目录 1docker run --name mn -v html:/usr/share/nginx/html -p 80:80 -d nginx ② 进入html数据卷所在位置，并修改HTML内容 123456# 查看html数据卷的位置docker volume inspect html# 进入该目录cd /var/lib/docker/volumes/html/_data# 修改文件vi index.html 给MySQL挂载本地目录容器不仅仅可以挂载数据卷，也可以直接挂载到宿主机目录上。关联关系如下： 带数据卷模式：宿主机目录 –&gt; 数据卷 —&gt; 容器内目录 直接挂载模式：宿主机目录 —&gt; 容器内目录 如图： 语法： 目录挂载与数据卷挂载的语法是类似的： -v [宿主机目录]:[容器内目录] -v [宿主机文件]:[容器内文件] 需求：创建并运行一个MySQL容器，将宿主机目录直接挂载到容器 实现思路如下： 1）在将课前资料中的mysql.tar文件上传到虚拟机，通过load命令加载为镜像 2）创建目录/tmp/mysql/data 3）创建目录/tmp/mysql/conf，将课前资料提供的hmy.cnf文件上传到/tmp/mysql/conf 4）去DockerHub查阅资料，创建并运行MySQL容器，要求： ① 挂载/tmp/mysql/data到mysql容器内数据存储目录 ② 挂载/tmp/mysql/conf/hmy.cnf到mysql容器的配置文件 ③ 设置MySQL密码 1docker run --name mysql -v /root/mysql/conf:/etc/mysql/conf.d -v /root/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d -p 3305:3306 mysql:5.7.25 小结docker run的命令中通过 -v 参数挂载文件或目录到容器中： -v volume名称:容器内目录 -v 宿主机文件:容器内文件 -v 宿主机目录:容器内目录 数据卷挂载与目录直接挂载的 数据卷挂载耦合度低，由docker来管理目录，但是目录较深，不好找 目录挂载耦合度高，需要我们自己管理目录，不过目录容易寻找查看 Dockerfile自定义镜像常见的镜像在DockerHub就能找到，但是我们自己写的项目就必须自己构建镜像了。 而要自定义镜像，就必须先了解镜像的结构才行。 镜像结构镜像是将应用程序及其需要的系统函数库、环境、配置、依赖打包而成。 我们以MySQL为例，来看看镜像的组成结构： 简单来说，镜像就是在系统函数库、运行环境基础上，添加应用程序文件、配置文件、依赖文件等组合，然后编写好启动脚本打包在一起形成的文件。 我们要构建镜像，其实就是实现上述打包的过程。 Dockerfile语法构建自定义的镜像时，并不需要一个个文件去拷贝，打包。 我们只需要告诉Docker，我们的镜像的组成，需要哪些BaseImage、需要拷贝什么文件、需要安装什么依赖、启动脚本是什么，将来Docker会帮助我们构建镜像。 而描述上述信息的文件就是Dockerfile文件。 Dockerfile就是一个文本文件，其中包含一个个的**指令(Instruction)**，用指令来说明要执行什么操作来构建镜像。每一个指令都会形成一层Layer。 更新详细语法说明，请参考官网文档： https://docs.docker.com/engine/reference/builder 构建Java项目基于Ubuntu构建Java项目需求：基于Ubuntu镜像构建一个新镜像，运行一个java项目 步骤1：新建一个空文件夹docker-demo 步骤2：拷贝课前资料中的docker-demo.jar文件到docker-demo这个目录 步骤3：拷贝课前资料中的jdk8.tar.gz文件到docker-demo这个目录 步骤4：拷贝课前资料提供的Dockerfile到docker-demo这个目录 其中的内容如下： 12345678910111213141516171819202122# 指定基础镜像FROM ubuntu:16.04# 配置环境变量，JDK的安装目录ENV JAVA_DIR=/usr/local# 拷贝jdk和java项目的包COPY ./jdk8.tar.gz $JAVA_DIR/COPY ./docker-demo.jar /tmp/app.jar# 安装JDKRUN cd $JAVA_DIR \\ &amp;&amp; tar -xf ./jdk8.tar.gz \\ &amp;&amp; mv ./jdk1.8.0_144 ./java8# 配置环境变量ENV JAVA_HOME=$JAVA_DIR/java8ENV PATH=$PATH:$JAVA_HOME/bin# 暴露端口EXPOSE 8090# 入口，java项目的启动命令ENTRYPOINT java -jar /tmp/app.jar 步骤5：进入docker-demo 将准备好的docker-demo上传到虚拟机任意目录，然后进入docker-demo目录下 步骤6：运行命令： 1docker build -t javaweb:1.0 . 最后访问 http://192.168.150.101:8090/hello/count，其中的ip改成你的虚拟机ip 基于java8构建Java项目虽然我们可以基于Ubuntu基础镜像，添加任意自己需要的安装包，构建镜像，但是却比较麻烦。所以大多数情况下，我们都可以在一些安装了部分软件的基础镜像上做改造。 例如，构建java项目的镜像，可以在已经准备了JDK的基础镜像基础上构建。 需求：基于java:8-alpine镜像，将一个Java项目构建为镜像 实现思路如下： ① 新建一个空的目录，然后在目录中新建一个文件，命名为Dockerfile ② 拷贝课前资料提供的docker-demo.jar到这个目录中 ③ 编写Dockerfile文件： a ）基于java:8-alpine作为基础镜像 b ）将app.jar拷贝到镜像中 c ）暴露端口 d ）编写入口ENTRYPOINT 内容如下： 1234FROM java:8-alpineCOPY ./app.jar /tmp/app.jarEXPOSE 8090ENTRYPOINT java -jar /tmp/app.jar ④ 使用docker build命令构建镜像 ⑤ 使用docker run创建容器并运行 小结 Dockerfile的本质是一个文件，通过指令描述镜像的构建过程 Dockerfile的第一行必须是FROM，从一个基础镜像来构建 基础镜像可以是基本操作系统，如Ubuntu。也可以是其他人制作好的镜像，例如：java:8-alpine Docker-ComposeDocker Compose可以基于Compose文件帮我们快速的部署分布式应用，而无需手动一个个创建和运行容器！ 初识DockerComposeCompose文件是一个文本文件，通过指令定义集群中的每个容器如何运行。格式如下： 1234567891011121314version:&nbsp;\"3.8\" services:&nbsp;&nbsp;mysql:&nbsp;&nbsp;&nbsp;&nbsp;image:&nbsp;mysql:5.7.25 environment: MYSQL_ROOT_PASSWORD: 123 &nbsp;&nbsp;&nbsp;&nbsp;volumes:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;\"/tmp/mysql/data:/var/lib/mysql\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;\"/tmp/mysql/conf/hmy.cnf:/etc/mysql/conf.d/hmy.cnf\"&nbsp;&nbsp;web:&nbsp;&nbsp;&nbsp;&nbsp;build:&nbsp;.&nbsp;&nbsp;&nbsp;&nbsp;ports:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- \"8090:8090\" 上面的Compose文件就描述一个项目，其中包含两个容器： mysql：一个基于mysql:5.7.25镜像构建的容器，并且挂载了两个目录 web：一个基于docker build临时构建的镜像容器，映射端口时8090 DockerCompose的详细语法参考官网：https://docs.docker.com/compose/compose-file/ 其实DockerCompose文件可以看做是将多个docker run命令写到一个文件，只是语法稍有差异。 安装DockerCompose部署docker.md 部署微服务集群需求：将之前学习的cloud-demo微服务集群利用DockerCompose部署 实现思路： ① 查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件 ② 修改自己的cloud-demo项目，将数据库、nacos地址都命名为docker-compose中的服务名 ③ 使用maven打包工具，将项目中的每个微服务都打包为app.jar ④ 将打包好的app.jar拷贝到cloud-demo中的每一个对应的子目录中 ⑤ 将cloud-demo上传至虚拟机，利用 docker-compose up -d 来部署 compose文件查看课前资料提供的cloud-demo文件夹，里面已经编写好了docker-compose文件，而且每个微服务都准备了一个独立的目录： 内容如下： 123456789101112131415161718192021222324version: \"3.2\"services: nacos: image: nacos/nacos-server environment: MODE: standalone ports: - \"8848:8848\" mysql: image: mysql:5.7.25 environment: MYSQL_ROOT_PASSWORD: 123 volumes: - \"$PWD/mysql/data:/var/lib/mysql\" - \"$PWD/mysql/conf:/etc/mysql/conf.d/\" userservice: build: ./user-service orderservice: build: ./order-service gateway: build: ./gateway ports: - \"10010:10010\" 可以看到，其中包含5个service服务： nacos：作为注册中心和配置中心 image: nacos/nacos-server： 基于nacos/nacos-server镜像构建 environment：环境变量 MODE: standalone：单点模式启动 ports：端口映射，这里暴露了8848端口 mysql：数据库 image: mysql:5.7.25：镜像版本是mysql:5.7.25 environment：环境变量 MYSQL_ROOT_PASSWORD: 123：设置数据库root账户的密码为123 volumes：数据卷挂载，这里挂载了mysql的data、conf目录，其中有我提前准备好的数据 userservice、orderservice、gateway：都是基于Dockerfile临时构建的 查看mysql目录，可以看到其中已经准备好了cloud_order、cloud_user表： 查看微服务目录，可以看到都包含Dockerfile文件： 内容如下： 123FROM java:8-alpineCOPY ./app.jar /tmp/app.jarENTRYPOINT java -jar /tmp/app.jar 修改微服务配置因为微服务将来要部署为docker容器，而容器之间互联不是通过IP地址，而是通过容器名。这里我们将order-service、user-service、gateway服务的mysql、nacos地址都修改为基于容器名的访问。 如下所示： 1234567891011spring: datasource: url: jdbc:mysql://mysql:3306/cloud_order?useSSL=false username: root password: 123 driver-class-name: com.mysql.jdbc.Driver application: name: orderservice cloud: nacos: server-addr: nacos:8848 # nacos服务地址 打包接下来需要将我们的每个微服务都打包。因为之前查看到Dockerfile中的jar包名称都是app.jar，因此我们的每个微服务都需要用这个名称。 可以通过修改pom.xml中的打包名称来实现，每个微服务都需要修改： 12345678910&lt;build&gt; &lt;!-- 服务打包的最终名称 --&gt; &lt;finalName&gt;app&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 打包后： 拷贝jar包到部署目录编译打包好的app.jar文件，需要放到Dockerfile的同级目录中。注意：每个微服务的app.jar放到与服务名称对应的目录，别搞错了。 user-service： order-service： gateway： 部署最后，我们需要将文件整个cloud-demo文件夹上传到虚拟机中，理由DockerCompose部署。 上传到任意目录： 部署： 进入cloud-demo目录，然后运行下面的命令： 1docker-compose up -d Docker镜像仓库搭建私有镜像仓库部署docker.md 推送、拉取镜像推送镜像到私有镜像服务必须先tag，步骤如下： ① 重新tag本地镜像，名称前缀为私有仓库的地址：192.168.150.101:8080/ 1docker tag nginx:latest 192.168.150.101:8080/nginx:1.0 ② 推送镜像 1docker push 192.168.150.101:8080/nginx:1.0 ③ 拉取镜像 1docker pull 192.168.150.101:8080/nginx:1.0 服务异步通信-RabbitMQ1.MQ基础1.1. 同步和异步通信1.1.1.同步通讯例如feign调用 总结： 优点 时效性较强，可以立即得到结果 问题 耦合度高 性能和吞吐能力下降 有额外的资源消耗 有级联失败问题 1.1.2.异步通讯为了解除事件发布者与订阅者之间的耦合，两者并不是直接通信，而是有一个中间人（Broker）。发布者发布事件到Broker，不关心谁来订阅事件。订阅者从Broker订阅事件，不关心谁发来的消息。 Broker 是一个像数据总线一样的东西，所有的服务要接收数据和发送数据都发到这个总线上，这个总线就像协议一样，让服务间的通讯变得标准和可控。 好处： 吞吐量提升：无需等待订阅者处理完成，响应更快速 故障隔离：服务没有直接调用，不存在级联失败问题 调用间没有阻塞，不会造成无效的资源占用 耦合度极低，每个服务都可以灵活插拔，可替换 流量削峰：不管发布事件的流量波动多大，都由Broker接收，订阅者可以按照自己的速度去处理事件 缺点： 架构复杂了，业务没有明显的流程线，不好管理 需要依赖于Broker的可靠、安全、性能 MQ技术就是一种Broker的软件 1.2.技术对比：MQ，中文是消息队列（MessageQueue），字面来看就是存放消息的队列。也就是事件驱动架构中的Broker。 比较常见的MQ实现： ActiveMQ RabbitMQ RocketMQ Kafka 几种常见MQ的对比： RabbitMQ ActiveMQ RocketMQ Kafka 公司/社区 Rabbit Apache 阿里 Apache 开发语言 Erlang Java Java Scala&amp;Java 协议支持 AMQP，XMPP，SMTP，STOMP OpenWire,STOMP，REST,XMPP,AMQP 自定义协议 自定义协议 可用性 高 一般 高 高 单机吞吐量 一般 差 高 非常高 消息延迟 微秒级 毫秒级 毫秒级 毫秒以内 消息可靠性 高 一般 高 一般 追求可用性：Kafka、 RocketMQ 、RabbitMQ 追求可靠性：RabbitMQ、RocketMQ 追求吞吐能力：RocketMQ、Kafka 追求消息低延迟：RabbitMQ、Kafka 2. RabbitMQ2.1. 部署RabbitMQpdf 2.2.RabbitMQ基本结构MQ的基本结构： RabbitMQ中的一些角色： publisher：生产者 consumer：消费者 exchange个：交换机，负责消息路由 queue：队列，存储消息 virtualHost：虚拟主机，隔离不同租户的exchange、queue、消息的隔离 2.3.RabbitMQ消息模型RabbitMQ官方提供了5个不同的Demo示例，对应了不同的消息模型： 3.SpringAMQPSpringAMQP是基于RabbitMQ封装的一套模板，并且还利用SpringBoot对其实现了自动装配，使用起来非常方便。 SpringAmqp的官方地址：https://spring.io/projects/spring-amqp SpringAMQP提供了三个功能： 自动声明队列、交换机及其绑定关系 基于注解的监听器模式，异步接收消息 封装了RabbitTemplate工具，用于发送消息 3.1.Basic Queue 简单队列模型在父工程mq-demo中引入依赖 12345&lt;!--AMQP依赖，包含RabbitMQ--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 3.1.1.消息发送首先配置MQ地址，在publisher服务的application.yml中添加配置： 1234567spring: rabbitmq: host: 192.168.150.101 # 主机名 port: 5672 # 端口 virtual-host: / # 虚拟主机 username: itcast # 用户名 password: 123321 # 密码 然后在publisher服务中编写测试类SpringAmqpTest，并利用RabbitTemplate实现消息发送： 1234567891011121314151617181920212223242526package cn.itcast.mq.spring;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;@RunWith(SpringRunner.class)@SpringBootTestpublic class SpringAmqpTest { @Autowired private RabbitTemplate rabbitTemplate; @Test public void testSimpleQueue() { // 队列名称 String queueName = \"simple.queue\"; // 消息 String message = \"hello, spring amqp!\"; // 发送消息 rabbitTemplate.convertAndSend(queueName, message); }} 3.1.2.消息接收首先配置MQ地址，在consumer服务的application.yml中添加配置： 1234567spring: rabbitmq: host: 192.168.125.130 # 主机名 port: 5672 # 端口 virtual-host: / # 虚拟主机 username: itcast # 用户名 password: 123321 # 密码 然后在consumer服务的cn.itcast.mq.listener包中新建一个类SpringRabbitListener，代码如下： 12345678910111213package cn.itcast.mq.listener;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;@Componentpublic class SpringRabbitListener { @RabbitListener(queues = \"simple.queue\") public void listenSimpleQueueMessage(String msg) throws InterruptedException { System.out.println(\"spring 消费者接收到消息：【\" + msg + \"】\"); }} 3.1.3.测试启动consumer服务，然后在publisher服务中运行测试代码，发送MQ消息 3.2.WorkQueueWork queues，也被称为（Task queues），任务模型。简单来说就是让多个消费者绑定到一个队列，共同消费队列中的消息。 当消息处理比较耗时的时候，可能生产消息的速度会远远大于消息的消费速度。长此以往，消息就会堆积越来越多，无法及时处理。 此时就可以使用work 模型，多个消费者共同处理消息处理，速度就能大大提高了。 3.2.1.消息发送这次我们循环发送，模拟大量消息堆积现象。 在publisher服务中的SpringAmqpTest类中添加一个测试方法： 12345678910111213141516/** * workQueue * 向队列中不停发送消息，模拟消息堆积。 */@Testpublic void testWorkQueue() throws InterruptedException { // 队列名称 String queueName = \"simple.queue\"; // 消息 String message = \"hello, message_\"; for (int i = 0; i &lt; 50; i++) { // 发送消息 rabbitTemplate.convertAndSend(queueName, message + i); Thread.sleep(20); }} 3.2.2.消息接收要模拟多个消费者绑定同一个队列，我们在consumer服务的SpringRabbitListener中添加2个新的方法： 1234567891011@RabbitListener(queues = \"simple.queue\")public void listenWorkQueue1(String msg) throws InterruptedException { System.out.println(\"消费者1接收到消息：【\" + msg + \"】\" + LocalTime.now()); Thread.sleep(20);}@RabbitListener(queues = \"simple.queue\")public void listenWorkQueue2(String msg) throws InterruptedException { System.err.println(\"消费者2........接收到消息：【\" + msg + \"】\" + LocalTime.now()); Thread.sleep(200);} 注意到这个消费者sleep了1000秒，模拟任务耗时。 3.2.3.测试启动ConsumerApplication后，在执行publisher服务中刚刚编写的发送测试方法testWorkQueue。 可以看到消费者1很快完成了自己的25条消息。消费者2却在缓慢的处理自己的25条消息。 也就是说消息是平均分配给每个消费者，并没有考虑到消费者的处理能力。这样显然是有问题的。 3.2.4.能者多劳在spring中有一个简单的配置，可以解决这个问题。我们修改consumer服务的application.yml文件，添加配置： 12345spring: rabbitmq: listener: simple: prefetch: 1 # 每次只能获取一条消息，处理完成才能获取下一个消息 3.2.5.总结Work模型的使用： 多个消费者绑定到一个队列，同一条消息只会被一个消费者处理 通过设置prefetch来控制消费者预取的消息数量 3.3.发布/订阅发布订阅的模型如图： 可以看到，在订阅模型中，多了一个exchange角色，而且过程略有变化： Publisher：生产者，也就是要发送消息的程序，但是不再发送到队列中，而是发给X（交换机） Exchange：交换机，图中的X。一方面，接收生产者发送的消息。另一方面，知道如何处理消息，例如递交给某个特别队列、递交给所有队列、或是将消息丢弃。到底如何操作，取决于Exchange的类型。Exchange有以下3种类型： Fanout：广播，将消息交给所有绑定到交换机的队列 Direct：定向，把消息交给符合指定routing key 的队列 Topic：通配符，把消息交给符合routing pattern（路由模式） 的队列 Consumer：消费者，与以前一样，订阅队列，没有变化 Queue：消息队列也与以前一样，接收消息、缓存消息。 Exchange（交换机）只负责转发消息，不具备存储消息的能力，因此如果没有任何队列与Exchange绑定，或者没有符合路由规则的队列，那么消息会丢失！ 3.4.FanoutFanout，英文翻译是扇出，我觉得在MQ中叫广播更合适。 在广播模式下，消息发送流程是这样的： 1） 可以有多个队列 2） 每个队列都要绑定到Exchange（交换机） 3） 生产者发送的消息，只能发送到交换机，交换机来决定要发给哪个队列，生产者无法决定 4） 交换机把消息发送给绑定过的所有队列 5） 订阅队列的消费者都能拿到消息 我们的计划是这样的： 创建一个交换机 itcast.fanout，类型是Fanout 创建两个队列fanout.queue1和fanout.queue2，绑定到交换机itcast.fanout 3.4.1.声明队列和交换机Spring提供了一个接口Exchange，来表示所有不同类型的交换机： 在consumer中创建一个类，声明队列和交换机： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package cn.itcast.mq.config;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.FanoutExchange;import org.springframework.amqp.core.Queue;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class FanoutConfig { /** * 声明交换机 * @return Fanout类型交换机 */ @Bean public FanoutExchange fanoutExchange(){ return new FanoutExchange(\"itcast.fanout\"); } /** * 第1个队列 */ @Bean public Queue fanoutQueue1(){ return new Queue(\"fanout.queue1\"); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue1(Queue fanoutQueue1, FanoutExchange fanoutExchange){ return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange); } /** * 第2个队列 */ @Bean public Queue fanoutQueue2(){ return new Queue(\"fanout.queue2\"); } /** * 绑定队列和交换机 */ @Bean public Binding bindingQueue2(Queue fanoutQueue2, FanoutExchange fanoutExchange){ return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange); }} 3.4.2.消息发送在publisher服务的SpringAmqpTest类中添加测试方法： 12345678@Testpublic void testFanoutExchange() { // 队列名称 String exchangeName = \"itcast.fanout\"; // 消息 String message = \"hello, everyone!\"; rabbitTemplate.convertAndSend(exchangeName, \"\", message);} 3.4.3.消息接收在consumer服务的SpringRabbitListener中添加两个方法，作为消费者： 123456789@RabbitListener(queues = \"fanout.queue1\")public void listenFanoutQueue1(String msg) { System.out.println(\"消费者1接收到Fanout消息：【\" + msg + \"】\");}@RabbitListener(queues = \"fanout.queue2\")public void listenFanoutQueue2(String msg) { System.out.println(\"消费者2接收到Fanout消息：【\" + msg + \"】\");} 3.4.4.总结交换机的作用是什么？ 接收publisher发送的消息 将消息按照规则路由到与之绑定的队列 不能缓存消息，路由失败，消息丢失 FanoutExchange的会将消息路由到每个绑定的队列 声明队列、交换机、绑定关系的Bean是什么？ Queue FanoutExchange Binding 3.5.Direct在Fanout模式中，一条消息，会被所有订阅的队列都消费。但是，在某些场景下，我们希望不同的消息被不同的队列消费。这时就要用到Direct类型的Exchange。 在Direct模型下： 队列与交换机的绑定，不能是任意绑定了，而是要指定一个RoutingKey（路由key） 消息的发送方在 向 Exchange发送消息时，也必须指定消息的 RoutingKey。 Exchange不再把消息交给每一个绑定的队列，而是根据消息的Routing Key进行判断，只有队列的Routingkey与消息的 Routing key完全一致，才会接收到消息 案例需求如下： 利用@RabbitListener声明Exchange、Queue、RoutingKey 在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2 在publisher中编写测试方法，向itcast. direct发送消息 3.5.1.基于注解声明队列和交换机基于@Bean的方式声明队列和交换机比较麻烦，Spring还提供了基于注解方式来声明。 在consumer的SpringRabbitListener中添加两个消费者，同时基于注解来声明队列和交换机： 1234567891011121314151617@RabbitListener(bindings = @QueueBinding( value = @Queue(name = \"direct.queue1\"), exchange = @Exchange(name = \"itcast.direct\", type = ExchangeTypes.DIRECT), key = {\"red\", \"blue\"}))public void listenDirectQueue1(String msg){ System.out.println(\"消费者接收到direct.queue1的消息：【\" + msg + \"】\");}@RabbitListener(bindings = @QueueBinding( value = @Queue(name = \"direct.queue2\"), exchange = @Exchange(name = \"itcast.direct\", type = ExchangeTypes.DIRECT), key = {\"red\", \"yellow\"}))public void listenDirectQueue2(String msg){ System.out.println(\"消费者接收到direct.queue2的消息：【\" + msg + \"】\");} 3.5.2.消息发送在publisher服务的SpringAmqpTest类中添加测试方法： 123456789@Testpublic void testSendDirectExchange() { // 交换机名称 String exchangeName = \"itcast.direct\"; // 消息 String message = \"红色警报！日本乱排核废水，导致海洋生物变异，惊现哥斯拉！\"; // 发送消息 rabbitTemplate.convertAndSend(exchangeName, \"red\", message);} 3.5.3.总结描述下Direct交换机与Fanout交换机的差异？ Fanout交换机将消息路由给每一个与之绑定的队列 Direct交换机根据RoutingKey判断路由给哪个队列 如果多个队列具有相同的RoutingKey，则与Fanout功能类似 基于@RabbitListener注解声明队列和交换机有哪些常见注解？ @Queue @Exchange 3.6.Topic3.6.1.说明Topic类型的Exchange与Direct相比，都是可以根据RoutingKey把消息路由到不同的队列。只不过Topic类型Exchange可以让队列在绑定Routing key 的时候使用通配符！ Routingkey 一般都是有一个或多个单词组成，多个单词之间以”.”分割，例如： item.insert 通配符规则： #：匹配一个或多个词 *：匹配不多不少恰好1个词 举例： item.#：能够匹配item.spu.insert 或者 item.spu item.*：只能匹配item.spu ​ 图示： 解释： Queue1：绑定的是china.# ，因此凡是以 china.开头的routing key 都会被匹配到。包括china.news和china.weather Queue2：绑定的是#.news ，因此凡是以 .news结尾的 routing key 都会被匹配。包括china.news和japan.news 案例需求： 实现思路如下： 并利用@RabbitListener声明Exchange、Queue、RoutingKey 在consumer服务中，编写两个消费者方法，分别监听topic.queue1和topic.queue2 在publisher中编写测试方法，向itcast. topic发送消息 3.6.2.消息发送在publisher服务的SpringAmqpTest类中添加测试方法： 123456789101112/** * topicExchange */@Testpublic void testSendTopicExchange() { // 交换机名称 String exchangeName = \"itcast.topic\"; // 消息 String message = \"喜报！孙悟空大战哥斯拉，胜!\"; // 发送消息 rabbitTemplate.convertAndSend(exchangeName, \"china.news\", message);} 3.6.3.消息接收在consumer 服务的SpringRabbitListener中添加方法： 1234567891011121314151617@RabbitListener(bindings = @QueueBinding( value = @Queue(name = \"topic.queue1\"), exchange = @Exchange(name = \"itcast.topic\", type = ExchangeTypes.TOPIC), key = \"china.#\"))public void listenTopicQueue1(String msg){ System.out.println(\"消费者接收到topic.queue1的消息：【\" + msg + \"】\");}@RabbitListener(bindings = @QueueBinding( value = @Queue(name = \"topic.queue2\"), exchange = @Exchange(name = \"itcast.topic\", type = ExchangeTypes.TOPIC), key = \"#.news\"))public void listenTopicQueue2(String msg){ System.out.println(\"消费者接收到topic.queue2的消息：【\" + msg + \"】\");} 3.6.4.总结描述下Direct交换机与Topic交换机的差异？ Topic交换机接收的消息RoutingKey必须是多个单词，以 **.** 分割 Topic交换机与队列绑定时的bindingKey可以指定通配符 #：代表0个或多个词 *：代表1个词 3.7.消息转换器之前说过，Spring会把你发送的消息序列化为字节发送给MQ，接收消息的时候，还会把字节反序列化为Java对象。 只不过，默认情况下Spring采用的序列化方式是JDK序列化。众所周知，JDK序列化存在下列问题： 数据体积过大 有安全漏洞 可读性差 我们来测试一下。 3.7.1.测试默认转换器我们修改消息发送的代码，发送一个Map对象： 123456789@Testpublic void testSendMap() throws InterruptedException { // 准备消息 Map&lt;String,Object&gt; msg = new HashMap&lt;&gt;(); msg.put(\"name\", \"Jack\"); msg.put(\"age\", 21); // 发送消息 rabbitTemplate.convertAndSend(\"simple.queue\",\"\", msg);} 停止consumer服务 发送消息后查看控制台： 3.7.2.配置JSON转换器显然，JDK序列化方式并不合适。我们希望消息体的体积更小、可读性更高，因此可以使用JSON方式来做序列化和反序列化。 在publisher和consumer两个服务中都引入依赖： 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.dataformat&lt;/groupId&gt; &lt;artifactId&gt;jackson-dataformat-xml&lt;/artifactId&gt; &lt;version&gt;2.9.10&lt;/version&gt;&lt;/dependency&gt; 配置消息转换器。 在启动类中添加一个Bean即可： 1234@Beanpublic MessageConverter jsonMessageConverter(){ return new Jackson2JsonMessageConverter();} 分布式搜索引擎-elasticsearch1.ES基础1.1.elasticsearch&amp;ELK技术栈ELK：是以elasticsearch为核心的技术栈，包括beats、Logstash、kibana、elasticsearch elasticsearch：一个开源的分布式搜索引擎，可以用来实现搜索、日志统计、分析、系统监控等功能。是elastic stack的核心，负责存储、搜索、分析数据。 Lucene：是Apache的开源搜索引擎类库，提供了搜索引擎的核心API 1.2.倒排索引倒排索引的概念是基于MySQL这样的正向索引而言的。 1.2.1.正向索引根据id查询，直接走索引查询 基于数据内容做模糊查询，只能逐行扫描数据，判断是否符合条件，若符合放入结果集，不符合则丢弃。 那么什么是正向索引呢？例如给下表（tb_goods）中的id创建索引： 1.2.2.倒排索引倒排索引中有两个非常重要的概念： 文档（Document）：用来搜索的数据，其中的每一条数据就是一个文档。例如一个网页、一个商品信息 词条（Term）：对文档数据或用户搜索数据，利用某种算法分词，得到的具备含义的词语就是词条。例如：我是中国人，就可以分为：我、是、中国人、中国、国人这样的几个词条 创建倒排索引是对正向索引的一种特殊处理，流程如下： 将每一个文档的数据利用算法分词，得到一个个词条 创建表，每行数据包括词条、词条所在文档id、位置等信息 因为词条唯一性，可以给词条创建索引，例如hash表结构索引 如图： 倒排索引的搜索流程如下（以搜索”华为手机”为例）： 1）用户输入条件\"华为手机\"进行搜索。 2）对用户输入内容分词，得到词条：华为、手机。 3）拿着词条在倒排索引中查找，可以得到包含词条的文档id：1、2、3。 4）拿着文档id到正向索引中查找具体文档。 如图： 虽然要先查询倒排索引，再查询正向索引，但是无论是词条、还是文档id都建立了索引，查询速度非常快！无需全表扫描。 1.2.3.正向和倒排那么为什么一个叫做正向索引，一个叫做倒排索引呢？ 正向索引是最传统的，根据id索引的方式。但根据词条查询时，必须先逐条获取每个文档，然后判断文档中是否包含所需要的词条，是根据文档找词条的过程。 而倒排索引则相反，是先找到用户要搜索的词条，根据词条得到保护词条的文档的id，然后根据id获取文档。是根据词条找文档的过程。 是不是恰好反过来了？ 那么两者方式的优缺点是什么呢？ 正向索引： 优点： 可以给多个字段创建索引 根据索引字段搜索、排序速度非常快 缺点： 根据非索引字段，或者索引字段中的部分词条查找时，只能全表扫描。 倒排索引： 优点： 根据词条搜索、模糊搜索时，速度非常快 缺点： 只能给词条创建索引，而不是字段 无法根据字段做排序 1.3.es的一些概念elasticsearch中有很多独有的概念，与mysql中略有差别，但也有相似之处。 1.3.1.文档和字段elasticsearch是面向文档（Document）存储的，可以是数据库中的一条商品数据，一个订单信息。文档数据会被序列化为json格式后存储在elasticsearch中： 而Json文档中往往包含很多的字段（Field），类似于数据库中的列。 1.3.2.索引和映射索引（Index），就是相同类型的文档的集合。 例如： 所有用户文档，就可以组织在一起，称为用户的索引； 所有商品的文档，可以组织在一起，称为商品的索引； 所有订单的文档，可以组织在一起，称为订单的索引； 因此，我们可以把索引当做是数据库中的表。 数据库的表会有约束信息，用来定义表的结构、字段的名称、类型等信息。因此，索引库中就有映射（mapping），是索引中文档的字段约束信息，类似表的结构约束。 1.3.3.mysql与elasticsearch我们统一的把mysql与elasticsearch的概念做一下对比： MySQL Elasticsearch 说明 Table Index 索引(index)，就是文档的集合，类似数据库的表(table) Row Document 文档（Document），就是一条条的数据，类似数据库中的行（Row），文档都是JSON格式 Column Field 字段（Field），就是JSON文档中的字段，类似数据库中的列（Column） Schema Mapping Mapping（映射）是索引中文档的约束，例如字段类型约束。类似数据库的表结构（Schema） SQL DSL DSL是elasticsearch提供的JSON风格的请求语句，用来操作elasticsearch，实现CRUD mysql和es的优缺点 Mysql：擅长事务类型操作，可以确保数据的安全和一致性 Elasticsearch：擅长海量数据的搜索、分析、计算 因此在企业中，往往是两者结合使用： 对安全性要求较高的写操作，使用mysql实现 对查询性能要求较高的搜索需求，使用elasticsearch实现 两者再基于某种方式，实现数据的同步，保证一致性 1.4.安装es、kibana1.4.1.安装安装elasticsearch.md 1.4.2.分词器1.4.3.总结分词器的作用是什么？ 创建倒排索引时对文档分词 用户搜索时，对输入的内容分词 IK分词器有几种模式？ ik_smart：智能切分，粗粒度 ik_max_word：最细切分，细粒度 IK分词器如何拓展词条？如何停用词条？ 利用config目录的IkAnalyzer.cfg.xml文件添加拓展词典和停用词典 在词典中添加拓展词条或者停用词条 2.索引库操作索引库就类似数据库表，mapping映射就类似表的结构。 我们要向es中存储数据，必须先创建“库”和“表”。 2.1.mapping映射属性mapping是对索引库中文档的约束，常见的mapping属性包括： type：字段数据类型，常见的简单类型有： 字符串：text（可分词的文本）、keyword（精确值，例如：品牌、国家、ip地址） 数值：long、integer、short、byte、double、float、 布尔：boolean 日期：date 对象：object index：是否创建索引，默认为true analyzer：使用哪种分词器 properties：该字段的子字段 例如下面的json文档： 123456789101112{&nbsp;&nbsp;&nbsp;&nbsp;\"age\":&nbsp;21,&nbsp;&nbsp;&nbsp;&nbsp;\"weight\":&nbsp;52.1,&nbsp;&nbsp;&nbsp;&nbsp;\"isMarried\":&nbsp;false,&nbsp;&nbsp;&nbsp;&nbsp;\"info\":&nbsp;\"黑马程序员Java讲师\", \"email\":&nbsp;\"zy@itcast.cn\", \"score\":&nbsp;[99.1, 99.5, 98.9],&nbsp;&nbsp;&nbsp;&nbsp;\"name\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"firstName\":&nbsp;\"云\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lastName\":&nbsp;\"赵\"&nbsp;&nbsp;&nbsp;&nbsp;}} 对应的每个字段映射（mapping）： age：类型为 integer；参与搜索，因此需要index为true；无需分词器 weight：类型为float；参与搜索，因此需要index为true；无需分词器 isMarried：类型为boolean；参与搜索，因此需要index为true；无需分词器 info：类型为字符串，需要分词，因此是text；参与搜索，因此需要index为true；分词器可以用ik_smart email：类型为字符串，但是不需要分词，因此是keyword；不参与搜索，因此需要index为false；无需分词器 score：虽然是数组，但是我们只看元素的类型，类型为float；参与搜索，因此需要index为true；无需分词器 name：类型为object，需要定义多个子属性 name.firstName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器 name.lastName；类型为字符串，但是不需要分词，因此是keyword；参与搜索，因此需要index为true；无需分词器 2.2.索引库的CRUD这里我们统一使用Kibana编写DSL的方式来演示。 2.2.1.创建索引库和映射基本语法： 请求方式：PUT 请求路径：/索引库名，可以自定义 请求参数：mapping映射 格式： 1234567891011121314151617181920212223PUT&nbsp;/索引库名称{&nbsp;&nbsp;\"mappings\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"properties\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"字段名\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"text\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"analyzer\":&nbsp;\"ik_smart\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"字段名2\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"keyword\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"index\":&nbsp;\"false\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"字段名3\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"properties\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"子字段\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"keyword\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}, // ...略&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 示例：1234567891011121314151617181920212223PUT&nbsp;/heima{&nbsp;&nbsp;\"mappings\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"properties\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"info\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"text\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"analyzer\":&nbsp;\"ik_smart\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"email\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"keyword\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"index\":&nbsp;\"falsae\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"properties\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"firstName\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"keyword\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}, // ... 略&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 2.2.2.查询索引库基本语法： 请求方式：GET 请求路径：/索引库名 请求参数：无 格式： 1GET /索引库名 2.2.3.修改索引库倒排索引结构虽然不复杂，但是一旦数据结构改变（比如改变了分词器），就需要重新创建倒排索引，这简直是灾难。因此索引库一旦创建，无法修改mapping。 虽然无法修改mapping中已有的字段，但是却允许添加新的字段到mapping中，因为不会对倒排索引产生影响。 语法说明： 12345678PUT&nbsp;/索引库名/_mapping{&nbsp;&nbsp;\"properties\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"新字段名\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"integer\"&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 2.2.4.删除索引库语法： 请求方式：DELETE 请求路径：/索引库名 请求参数：无 格式： 1DELETE /索引库名 2.2.5.总结索引库操作有哪些？ 创建索引库：PUT /索引库名 查询索引库：GET /索引库名 删除索引库：DELETE /索引库名 添加字段：PUT /索引库名/_mapping 3.文档操作3.1.新增文档语法： 12345678910POST&nbsp;/索引库名/_doc/文档id{&nbsp;&nbsp;&nbsp;&nbsp;\"字段1\":&nbsp;\"值1\",&nbsp;&nbsp;&nbsp;&nbsp;\"字段2\":&nbsp;\"值2\",&nbsp;&nbsp;&nbsp;&nbsp;\"字段3\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"子属性1\":&nbsp;\"值3\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"子属性2\":&nbsp;\"值4\"&nbsp;&nbsp;&nbsp;&nbsp;}, // ...} 示例： 123456789POST&nbsp;/heima/_doc/1{&nbsp;&nbsp;&nbsp;&nbsp;\"info\":&nbsp;\"黑马程序员Java讲师\",&nbsp;&nbsp;&nbsp;&nbsp;\"email\":&nbsp;\"zy@itcast.cn\",&nbsp;&nbsp;&nbsp;&nbsp;\"name\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"firstName\":&nbsp;\"云\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lastName\":&nbsp;\"赵\"&nbsp;&nbsp;&nbsp;&nbsp;}} 响应： 3.2.查询文档根据rest风格，新增是post，查询应该是get，不过查询一般都需要条件，这里我们把文档id带上。 语法： 1GET /{索引库名称}/_doc/{id} 通过kibana查看数据： 1GET /heima/_doc/1 查看结果： 3.3.删除文档删除使用DELETE请求，同样，需要根据id进行删除： 语法： 1DELETE /{索引库名}/_doc/id值 示例： 12# 根据id删除数据DELETE /heima/_doc/1 结果： 3.4.修改文档修改有两种方式： 全量修改：直接覆盖原来的文档 增量修改：修改文档中的部分字段 3.4.1.全量修改全量修改是覆盖原来的文档，其本质是： 根据指定的id删除文档 新增一个相同id的文档 注意：如果根据id删除时，id不存在，第二步的新增也会执行，也就从修改变成了新增操作了。 语法： 1234567PUT&nbsp;/{索引库名}/_doc/文档id{&nbsp;&nbsp;&nbsp;&nbsp;\"字段1\":&nbsp;\"值1\",&nbsp;&nbsp;&nbsp;&nbsp;\"字段2\":&nbsp;\"值2\", // ... 略} 示例： 123456789PUT&nbsp;/heima/_doc/1{&nbsp;&nbsp;&nbsp;&nbsp;\"info\":&nbsp;\"黑马程序员高级Java讲师\",&nbsp;&nbsp;&nbsp;&nbsp;\"email\":&nbsp;\"zy@itcast.cn\",&nbsp;&nbsp;&nbsp;&nbsp;\"name\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"firstName\":&nbsp;\"云\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lastName\":&nbsp;\"赵\"&nbsp;&nbsp;&nbsp;&nbsp;}} 3.4.2.增量修改增量修改是只修改指定id匹配的文档中的部分字段。 语法： 123456POST&nbsp;/{索引库名}/_update/文档id{&nbsp;&nbsp;&nbsp;&nbsp;\"doc\": { \"字段名\":&nbsp;\"新的值\", }} 示例： 123456POST&nbsp;/heima/_update/1{&nbsp;&nbsp;\"doc\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"email\":&nbsp;\"ZhaoYun@itcast.cn\"&nbsp;&nbsp;}} 3.5.总结文档操作有哪些？ 创建文档：POST /{索引库名}/_doc/文档id { json文档 } 查询文档：GET /{索引库名}/_doc/文档id 删除文档：DELETE /{索引库名}/_doc/文档id 修改文档： 全量修改：PUT /{索引库名}/_doc/文档id { json文档 } 增量修改：POST /{索引库名}/_update/文档id { “doc”: {字段}} 4.RestAPIES官方提供了各种不同语言的客户端，用来操作ES。这些客户端的本质就是组装DSL语句，通过http请求发送给ES。官方文档地址：https://www.elastic.co/guide/en/elasticsearch/client/index.html 其中的Java Rest Client又包括两种： Java Low Level Rest Client Java High Level Rest Client 我们学习的是Java HighLevel Rest Client客户端API 4.0.导入Demo工程4.0.1.导入数据首先导入课前资料提供的数据库数据： 数据结构如下： 123456789101112131415CREATE&nbsp;TABLE&nbsp;`tb_hotel`&nbsp;(&nbsp;&nbsp;`id`&nbsp;bigint(20)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店id',&nbsp;&nbsp;`name`&nbsp;varchar(255)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店名称；例：7天酒店',&nbsp;&nbsp;`address`&nbsp;varchar(255)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店地址；例：航头路',&nbsp;&nbsp;`price`&nbsp;int(10)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店价格；例：329',&nbsp;&nbsp;`score`&nbsp;int(2)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店评分；例：45，就是4.5分',&nbsp;&nbsp;`brand`&nbsp;varchar(32)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店品牌；例：如家',&nbsp;&nbsp;`city`&nbsp;varchar(32)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'所在城市；例：上海',&nbsp;&nbsp;`star_name`&nbsp;varchar(16)&nbsp;DEFAULT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店星级，从低到高分别是：1星到5星，1钻到5钻',&nbsp;&nbsp;`business`&nbsp;varchar(255)&nbsp;DEFAULT&nbsp;NULL&nbsp;COMMENT&nbsp;'商圈；例：虹桥',&nbsp;&nbsp;`latitude`&nbsp;varchar(32)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'纬度；例：31.2497',&nbsp;&nbsp;`longitude`&nbsp;varchar(32)&nbsp;NOT&nbsp;NULL&nbsp;COMMENT&nbsp;'经度；例：120.3925',&nbsp;&nbsp;`pic`&nbsp;varchar(255)&nbsp;DEFAULT&nbsp;NULL&nbsp;COMMENT&nbsp;'酒店图片；例:/img/1.jpg',&nbsp;&nbsp;PRIMARY&nbsp;KEY&nbsp;(`id`))&nbsp;ENGINE=InnoDB&nbsp;DEFAULT&nbsp;CHARSET=utf8mb4; 4.0.2.导入项目然后导入课前资料提供的项目: 项目结构如图： 4.0.3.mapping映射分析创建索引库，最关键的是mapping映射，而mapping映射要考虑的信息包括： 字段名 字段数据类型 是否参与搜索 是否需要分词 如果分词，分词器是什么？ 其中： 字段名、字段数据类型，可以参考数据表结构的名称和类型 是否参与搜索要分析业务来判断，例如图片地址，就无需参与搜索 是否分词呢要看内容，内容如果是一个整体就无需分词，反之则要分词 分词器，我们可以统一使用ik_max_word 来看下酒店数据的索引库结构: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950PUT /hotel{ \"mappings\": { \"properties\": { \"id\": { \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\", \"analyzer\": \"ik_max_word\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\", \"index\": false }, \"price\": { \"type\": \"integer\" }, \"score\": { \"type\": \"integer\" }, \"brand\": { \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\": { \"type\": \"keyword\", \"copy_to\": \"all\" }, \"starName\": { \"type\": \"keyword\" }, \"business\": { \"type\": \"keyword\" }, \"location\": { \"type\": \"geo_point\" }, \"pic\": { \"type\": \"keyword\", \"index\": false }, \"all\": { \"type\": \"text\", \"analyzer\": \"ik_max_word\" } } }} 几个特殊字段说明： location：地理坐标，里面包含精度、纬度 all：一个组合字段，其目的是将多字段的值 利用copy_to合并，提供给用户搜索 地理坐标说明： copy_to说明： 4.0.4.初始化RestClient在elasticsearch提供的API中，与elasticsearch一切交互都封装在一个名为RestHighLevelClient的类中，必须先完成这个对象的初始化，建立与elasticsearch的连接。 分为三步： 1）引入es的RestHighLevelClient依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;&lt;/dependency&gt; 2）因为SpringBoot默认的ES版本是7.6.2，所以我们需要覆盖默认的ES版本： 1234&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;elasticsearch.version&gt;7.12.1&lt;/elasticsearch.version&gt;&lt;/properties&gt; 3）初始化RestHighLevelClient： 初始化的代码如下： 123RestHighLevelClient client = new RestHighLevelClient(RestClient.builder( HttpHost.create(\"http://192.168.150.101:9200\"))); 这里为了单元测试方便，我们创建一个测试类HotelIndexTest，然后将初始化的代码编写在@BeforeEach方法中： 12345678910111213141516171819202122232425package cn.itcast.hotel;import org.apache.http.HttpHost;import org.elasticsearch.client.RestHighLevelClient;import org.junit.jupiter.api.AfterEach;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import java.io.IOException;public class HotelIndexTest { private RestHighLevelClient client; @BeforeEach void setUp() { this.client = new RestHighLevelClient(RestClient.builder( HttpHost.create(\"http://192.168.150.101:9200\") )); } @AfterEach void tearDown() throws IOException { this.client.close(); }} 4.1.创建索引库4.1.1.代码解读创建索引库的API如下： 代码分为三步： 1）创建Request对象。因为是创建索引库的操作，因此Request是CreateIndexRequest。 2）添加请求参数，其实就是DSL的JSON参数部分。因为json字符串很长，这里是定义了静态字符串常量MAPPING_TEMPLATE，让代码看起来更加优雅。 3）发送请求，client.indices()方法的返回值是IndicesClient类型，封装了所有与索引库操作有关的方法。 4.1.2.完整示例在hotel-demo的cn.itcast.hotel.constants包下，创建一个类，定义mapping映射的JSON字符串常量： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package cn.itcast.hotel.constants;public class HotelConstants { public static final String MAPPING_TEMPLATE = \"{\\n\" + \" \\\"mappings\\\": {\\n\" + \" \\\"properties\\\": {\\n\" + \" \\\"id\\\": {\\n\" + \" \\\"type\\\": \\\"keyword\\\"\\n\" + \" },\\n\" + \" \\\"name\\\":{\\n\" + \" \\\"type\\\": \\\"text\\\",\\n\" + \" \\\"analyzer\\\": \\\"ik_max_word\\\",\\n\" + \" \\\"copy_to\\\": \\\"all\\\"\\n\" + \" },\\n\" + \" \\\"address\\\":{\\n\" + \" \\\"type\\\": \\\"keyword\\\",\\n\" + \" \\\"index\\\": false\\n\" + \" },\\n\" + \" \\\"price\\\":{\\n\" + \" \\\"type\\\": \\\"integer\\\"\\n\" + \" },\\n\" + \" \\\"score\\\":{\\n\" + \" \\\"type\\\": \\\"integer\\\"\\n\" + \" },\\n\" + \" \\\"brand\\\":{\\n\" + \" \\\"type\\\": \\\"keyword\\\",\\n\" + \" \\\"copy_to\\\": \\\"all\\\"\\n\" + \" },\\n\" + \" \\\"city\\\":{\\n\" + \" \\\"type\\\": \\\"keyword\\\",\\n\" + \" \\\"copy_to\\\": \\\"all\\\"\\n\" + \" },\\n\" + \" \\\"starName\\\":{\\n\" + \" \\\"type\\\": \\\"keyword\\\"\\n\" + \" },\\n\" + \" \\\"business\\\":{\\n\" + \" \\\"type\\\": \\\"keyword\\\"\\n\" + \" },\\n\" + \" \\\"location\\\":{\\n\" + \" \\\"type\\\": \\\"geo_point\\\"\\n\" + \" },\\n\" + \" \\\"pic\\\":{\\n\" + \" \\\"type\\\": \\\"keyword\\\",\\n\" + \" \\\"index\\\": false\\n\" + \" },\\n\" + \" \\\"all\\\":{\\n\" + \" \\\"type\\\": \\\"text\\\",\\n\" + \" \\\"analyzer\\\": \\\"ik_max_word\\\"\\n\" + \" }\\n\" + \" }\\n\" + \" }\\n\" + \"}\";} 在hotel-demo中的HotelIndexTest测试类中，编写单元测试，实现创建索引： 123456789@Testvoid createHotelIndex() throws IOException { // 1.创建Request对象 CreateIndexRequest request = new CreateIndexRequest(\"hotel\"); // 2.准备请求的参数：DSL语句 request.source(MAPPING_TEMPLATE, XContentType.JSON); // 3.发送请求 client.indices().create(request, RequestOptions.DEFAULT);} 4.2.删除索引库删除索引库的DSL语句非常简单： 1DELETE /hotel 与创建索引库相比： 请求方式从PUT变为DELTE 请求路径不变 无请求参数 所以代码的差异，注意体现在Request对象上。依然是三步走： 1）创建Request对象。这次是DeleteIndexRequest对象 2）准备参数。这里是无参 3）发送请求。改用delete方法 在hotel-demo中的HotelIndexTest测试类中，编写单元测试，实现删除索引： 1234567@Testvoid testDeleteHotelIndex() throws IOException { // 1.创建Request对象 DeleteIndexRequest request = new DeleteIndexRequest(\"hotel\"); // 2.发送请求 client.indices().delete(request, RequestOptions.DEFAULT);} 4.3.判断索引库是否存在判断索引库是否存在，本质就是查询，对应的DSL是： 1GET /hotel 因此与删除的Java代码流程是类似的。依然是三步走： 1）创建Request对象。这次是GetIndexRequest对象 2）准备参数。这里是无参 3）发送请求。改用exists方法 123456789@Testvoid testExistsHotelIndex() throws IOException { // 1.创建Request对象 GetIndexRequest request = new GetIndexRequest(\"hotel\"); // 2.发送请求 boolean exists = client.indices().exists(request, RequestOptions.DEFAULT); // 3.输出 System.err.println(exists ? \"索引库已经存在！\" : \"索引库不存在！\");} 4.4.总结JavaRestClient操作elasticsearch的流程基本类似。核心是client.indices()方法来获取索引库的操作对象。 索引库操作的基本步骤： 初始化RestHighLevelClient 创建XxxIndexRequest。XXX是Create、Get、Delete 准备DSL（ Create时需要，其它是无参） 发送请求。调用RestHighLevelClient#indices().xxx()方法，xxx是create、exists、delete 5.RestClient操作文档为了与索引库操作分离，我们再次参加一个测试类，做两件事情： 初始化RestHighLevelClient 我们的酒店数据在数据库，需要利用IHotelService去查询，所以注入这个接口 123456789101112131415161718192021222324252627282930313233package cn.itcast.hotel;import cn.itcast.hotel.pojo.Hotel;import cn.itcast.hotel.service.IHotelService;import org.junit.jupiter.api.AfterEach;import org.junit.jupiter.api.BeforeEach;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import java.io.IOException;import java.util.List;@SpringBootTestpublic class HotelDocumentTest { @Autowired private IHotelService hotelService; private RestHighLevelClient client; @BeforeEach void setUp() { this.client = new RestHighLevelClient(RestClient.builder( HttpHost.create(\"http://192.168.150.101:9200\") )); } @AfterEach void tearDown() throws IOException { this.client.close(); }} 5.1.新增文档我们要将数据库的酒店数据查询出来，写入elasticsearch中。 5.1.1.索引库实体类数据库查询后的结果是一个Hotel类型的对象。结构如下： 1234567891011121314151617@Data@TableName(\"tb_hotel\")public class Hotel { @TableId(type = IdType.INPUT) private Long id; private String name; private String address; private Integer price; private Integer score; private String brand; private String city; private String starName; private String business; private String longitude; private String latitude; private String pic;} 与我们的索引库结构存在差异： longitude和latitude需要合并为location 因此，我们需要定义一个新的类型，与索引库结构吻合： 1234567891011121314151617181920212223242526272829303132333435package cn.itcast.hotel.pojo;import lombok.Data;import lombok.NoArgsConstructor;@Data@NoArgsConstructorpublic class HotelDoc { private Long id; private String name; private String address; private Integer price; private Integer score; private String brand; private String city; private String starName; private String business; private String location; private String pic; public HotelDoc(Hotel hotel) { this.id = hotel.getId(); this.name = hotel.getName(); this.address = hotel.getAddress(); this.price = hotel.getPrice(); this.score = hotel.getScore(); this.brand = hotel.getBrand(); this.city = hotel.getCity(); this.starName = hotel.getStarName(); this.business = hotel.getBusiness(); this.location = hotel.getLatitude() + \", \" + hotel.getLongitude(); this.pic = hotel.getPic(); }} 5.1.2.语法说明新增文档的DSL语句如下： 12345POST /{索引库名}/_doc/1{ \"name\": \"Jack\", \"age\": 21} 对应的java代码如图： 可以看到与创建索引库类似，同样是三步走： 1）创建Request对象 2）准备请求参数，也就是DSL中的JSON文档 3）发送请求 变化的地方在于，这里直接使用client.xxx()的API，不再需要client.indices()了。 5.1.3.完整代码我们导入酒店数据，基本流程一致，但是需要考虑几点变化： 酒店数据来自于数据库，我们需要先查询出来，得到hotel对象 hotel对象需要转为HotelDoc对象 HotelDoc需要序列化为json格式 因此，代码整体步骤如下： 1）根据id查询酒店数据Hotel 2）将Hotel封装为HotelDoc 3）将HotelDoc序列化为JSON 4）创建IndexRequest，指定索引库名和id 5）准备请求参数，也就是JSON文档 6）发送请求 在hotel-demo的HotelDocumentTest测试类中，编写单元测试： 12345678910111213141516@Testvoid testAddDocument() throws IOException { // 1.根据id查询酒店数据 Hotel hotel = hotelService.getById(61083L); // 2.转换为文档类型 HotelDoc hotelDoc = new HotelDoc(hotel); // 3.将HotelDoc转json String json = JSON.toJSONString(hotelDoc); // 1.准备Request对象 IndexRequest request = new IndexRequest(\"hotel\").id(hotelDoc.getId().toString()); // 2.准备Json文档 request.source(json, XContentType.JSON); // 3.发送请求 client.index(request, RequestOptions.DEFAULT);} 5.2.查询文档5.2.1.语法说明查询的DSL语句如下： 1GET /hotel/_doc/{id} 非常简单，因此代码大概分两步： 准备Request对象 发送请求 不过查询的目的是得到结果，解析为HotelDoc，因此难点是结果的解析。完整代码如下： 可以看到，结果是一个JSON，其中文档放在一个_source属性中，因此解析就是拿到_source，反序列化为Java对象即可。 与之前类似，也是三步走： 1）准备Request对象。这次是查询，所以是GetRequest 2）发送请求，得到结果。因为是查询，这里调用client.get()方法 3）解析结果，就是对JSON做反序列化 5.2.2.完整代码在hotel-demo的HotelDocumentTest测试类中，编写单元测试： 123456789101112@Testvoid testGetDocumentById() throws IOException { // 1.准备Request GetRequest request = new GetRequest(\"hotel\", \"61082\"); // 2.发送请求，得到响应 GetResponse response = client.get(request, RequestOptions.DEFAULT); // 3.解析响应结果 String json = response.getSourceAsString(); HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class); System.out.println(hotelDoc);} 5.3.删除文档删除的DSL为是这样的： 1DELETE /hotel/_doc/{id} 与查询相比，仅仅是请求方式从DELETE变成GET，可以想象Java代码应该依然是三步走： 1）准备Request对象，因为是删除，这次是DeleteRequest对象。要指定索引库名和id 2）准备参数，无参 3）发送请求。因为是删除，所以是client.delete()方法 在hotel-demo的HotelDocumentTest测试类中，编写单元测试： 1234567@Testvoid testDeleteDocument() throws IOException { // 1.准备Request DeleteRequest request = new DeleteRequest(\"hotel\", \"61083\"); // 2.发送请求 client.delete(request, RequestOptions.DEFAULT);} 5.4.修改文档5.4.1.语法说明修改我们讲过两种方式： 全量修改：本质是先根据id删除，再新增 增量修改：修改文档中的指定字段值 在RestClient的API中，全量修改与新增的API完全一致，判断依据是ID： 如果新增时，ID已经存在，则修改 如果新增时，ID不存在，则新增 这里不再赘述，我们主要关注增量修改。 代码示例如图： 与之前类似，也是三步走： 1）准备Request对象。这次是修改，所以是UpdateRequest 2）准备参数。也就是JSON文档，里面包含要修改的字段 3）更新文档。这里调用client.update()方法 5.4.2.完整代码在hotel-demo的HotelDocumentTest测试类中，编写单元测试： 123456789101112@Testvoid testUpdateDocument() throws IOException { // 1.准备Request UpdateRequest request = new UpdateRequest(\"hotel\", \"61083\"); // 2.准备请求参数 request.doc( \"price\", \"952\", \"starName\", \"四钻\" ); // 3.发送请求 client.update(request, RequestOptions.DEFAULT);} 5.5.批量导入文档案例需求：利用BulkRequest批量将数据库数据导入到索引库中。 步骤如下： 利用mybatis-plus查询酒店数据 将查询到的酒店数据（Hotel）转换为文档类型数据（HotelDoc） 利用JavaRestClient中的BulkRequest批处理，实现批量新增文档 5.5.1.语法说明批量处理BulkRequest，其本质就是将多个普通的CRUD请求组合在一起发送。 其中提供了一个add方法，用来添加其他请求： 可以看到，能添加的请求包括： IndexRequest，也就是新增 UpdateRequest，也就是修改 DeleteRequest，也就是删除 因此Bulk中添加了多个IndexRequest，就是批量新增功能了。示例： 其实还是三步走： 1）创建Request对象。这里是BulkRequest 2）准备参数。批处理的参数，就是其它Request对象，这里就是多个IndexRequest 3）发起请求。这里是批处理，调用的方法为client.bulk()方法 我们在导入酒店数据时，将上述代码改造成for循环处理即可。 5.5.2.完整代码在hotel-demo的HotelDocumentTest测试类中，编写单元测试： 12345678910111213141516171819@Testvoid testBulkRequest() throws IOException { // 批量查询酒店数据 List&lt;Hotel&gt; hotels = hotelService.list(); // 1.创建Request BulkRequest request = new BulkRequest(); // 2.准备参数，添加多个新增的Request for (Hotel hotel : hotels) { // 2.1.转换为文档类型HotelDoc HotelDoc hotelDoc = new HotelDoc(hotel); // 2.2.创建新增文档的Request对象 request.add(new IndexRequest(\"hotel\") .id(hotelDoc.getId().toString()) .source(JSON.toJSONString(hotelDoc), XContentType.JSON)); } // 3.发送请求 client.bulk(request, RequestOptions.DEFAULT);} 5.6.小结文档操作的基本步骤： 初始化RestHighLevelClient 创建XxxRequest。XXX是Index、Get、Update、Delete、Bulk 准备参数（Index、Update、Bulk时需要） 发送请求。调用RestHighLevelClient#.xxx()方法，xxx是index、get、update、delete、bulk 解析结果（Get时需要） 1.DSL查询文档elasticsearch的查询依然是基于JSON风格的DSL来实现的。 1.1.DSL查询分类Elasticsearch提供了基于JSON的DSL（Domain Specific Language）来定义查询。常见的查询类型包括： 查询所有：查询出所有数据，一般测试用。例如：match_all 全文检索（full text）查询：利用分词器对用户输入内容分词，然后去倒排索引库中匹配。例如： match_query multi_match_query 精确查询：根据精确词条值查找数据，一般是查找keyword、数值、日期、boolean等类型字段。例如： ids range term 地理（geo）查询：根据经纬度查询。例如： geo_distance geo_bounding_box 复合（compound）查询：复合查询可以将上述各种查询条件组合起来，合并查询条件。例如： bool function_score 查询的语法基本一致： 12345678GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"查询类型\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"查询条件\":&nbsp;\"条件值\"&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 我们以查询所有为例，其中： 查询类型为match_all 没有查询条件 12345678// 查询所有GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"match_all\":&nbsp;{ }&nbsp;&nbsp;}} 其它查询无非就是查询类型、查询条件的变化。 1.2.全文检索查询1.2.1.使用场景全文检索查询的基本流程如下： 对用户搜索的内容做分词，得到词条 根据词条去倒排索引库中匹配，得到文档id 根据文档id找到文档，返回给用户 比较常用的场景包括： 商城的输入框搜索 百度输入框搜索 例如京东： 因为是拿着词条去匹配，因此参与搜索的字段也必须是可分词的text类型的字段。 1.2.2.基本语法常见的全文检索查询包括： match查询：单字段查询 multi_match查询：多字段查询，任意一个字段符合条件就算符合查询条件 match查询语法如下： 12345678GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"match\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;\"TEXT\"&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} mulit_match语法如下： 123456789GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"multi_match\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"query\":&nbsp;\"TEXT\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"fields\":&nbsp;[\"FIELD1\",&nbsp;\" FIELD12\"]&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 1.2.3.示例match查询示例： multi_match查询示例： 可以看到，两种查询结果是一样的，为什么？ 因为我们将brand、name、business值都利用copy_to复制到了all字段中。因此你根据三个字段搜索，和根据all字段搜索效果当然一样了。 但是，搜索字段越多，对查询性能影响越大，因此建议采用copy_to，然后单字段查询的方式。 1.2.4.总结match和multi_match的区别是什么？ match：根据一个字段查询 multi_match：根据多个字段查询，参与查询字段越多，查询性能越差 1.3.精准查询精确查询一般是查找keyword、数值、日期、boolean等类型字段。所以不会对搜索条件分词。常见的有： term：根据词条精确值查询 range：根据值的范围查询 1.3.1.term查询因为精确查询的字段搜是不分词的字段，因此查询的条件也必须是不分词的词条。查询时，用户输入的内容跟自动值完全匹配时才认为符合条件。如果用户输入的内容过多，反而搜索不到数据。 语法说明： 1234567891011//&nbsp;term查询GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"term\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"value\":&nbsp;\"VALUE\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 示例： 当我搜索的是精确词条时，能正确查询出结果： 但是，当我搜索的内容不是词条，而是多个词语形成的短语时，反而搜索不到： 1.3.2.range查询范围查询，一般应用在对数值类型做范围过滤的时候。比如做价格范围过滤。 基本语法： 123456789101112//&nbsp;range查询GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"range\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"gte\":&nbsp;10, // 这里的gte代表大于等于，gt则代表大于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lte\":&nbsp;20 // lte代表小于等于，lt则代表小于&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 示例： 1.3.3.总结精确查询常见的有哪些？ term查询：根据词条精确匹配，一般搜索keyword类型、数值类型、布尔类型、日期类型字段 range查询：根据数值范围查询，可以是数值、日期的范围 1.4.地理坐标查询所谓的地理坐标查询，其实就是根据经纬度查询，官方文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/geo-queries.html 常见的使用场景包括： 携程：搜索我附近的酒店 滴滴：搜索我附近的出租车 微信：搜索我附近的人 附近的酒店： 附近的车： 1.4.1.矩形范围查询矩形范围查询，也就是geo_bounding_box查询，查询坐标落在某个矩形范围的所有文档： 查询时，需要指定矩形的左上、右下两个点的坐标，然后画出一个矩形，落在该矩形内的都是符合条件的点。 语法如下： 123456789101112131415161718//&nbsp;geo_bounding_box查询GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"geo_bounding_box\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"top_left\":&nbsp;{ // 左上点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lat\":&nbsp;31.1,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lon\":&nbsp;121.5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"bottom_right\":&nbsp;{ // 右下点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lat\":&nbsp;30.9,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lon\":&nbsp;121.7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 这种并不符合“附近的人”这样的需求，所以我们就不做了。 1.4.2.附近查询附近查询，也叫做距离查询（geo_distance）：查询到指定中心点小于某个距离值的所有文档。 换句话来说，在地图上找一个点作为圆心，以指定距离为半径，画一个圆，落在圆内的坐标都算符合条件： 语法说明： 12345678910//&nbsp;geo_distance 查询GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"geo_distance\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"distance\":&nbsp;\"15km\", // 半径&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;\"31.21,121.5\" // 圆心&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 示例： 我们先搜索陆家嘴附近15km的酒店： 发现共有47家酒店。 然后把半径缩短到3公里： 可以发现，搜索到的酒店数量减少到了5家。 1.5.复合查询复合（compound）查询：复合查询可以将其它简单查询组合起来，实现更复杂的搜索逻辑。常见的有两种： fuction score：算分函数查询，可以控制文档相关性算分，控制文档排名 bool query：布尔查询，利用逻辑关系组合多个其它的查询，实现复杂搜索 1.5.1.相关性算分当我们利用match查询时，文档结果会根据与搜索词条的关联度打分（_score），返回结果时按照分值降序排列。 例如，我们搜索 “虹桥如家”，结果如下： 1234567891011121314151617181920[&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"_score\"&nbsp;:&nbsp;17.850193,&nbsp;&nbsp;&nbsp;&nbsp;\"_source\"&nbsp;:&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\"&nbsp;:&nbsp;\"虹桥如家酒店真不错\",&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;},&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"_score\"&nbsp;:&nbsp;12.259849,&nbsp;&nbsp;&nbsp;&nbsp;\"_source\"&nbsp;:&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\"&nbsp;:&nbsp;\"外滩如家酒店真不错\",&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;},&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"_score\"&nbsp;:&nbsp;11.91091,&nbsp;&nbsp;&nbsp;&nbsp;\"_source\"&nbsp;:&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\"&nbsp;:&nbsp;\"迪士尼如家酒店真不错\",&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}] 在elasticsearch中，早期使用的打分算法是TF-IDF算法，公式如下： TF-IDF算法步骤 第一步，计算词频： 考虑到文章有长短之分，为了便于不同文章的比较，进行”词频”标准化。 第二步，计算逆文档频率： 这时，需要一个语料库（corpus），用来模拟语言的使用环境。 如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。 第三步，计算TF-IDF： 可以看到，TF-IDF与一个词在文档中的出现次数成正比，与该词在整个语言中的出现次数成反比。所以，自动提取关键词的算法就很清楚了，就是计算出文档的每个词的TF-IDF值，然后按降序排列，取排在最前面的几个词。 优缺点 TF-IDF的优点是简单快速，而且容易理解。缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。如果要体现词的上下文结构，那么你可能需要使用word2vec算法来支持。 在后来的5.1版本升级中，elasticsearch将算法改进为BM25算法，公式如下： TF-IDF算法有一各缺陷，就是词条频率越高，文档得分也会越高，单个词条对文档影响较大。而BM25则会让单个词条的算分有一个上限，曲线更加平滑： 小结：elasticsearch会根据词条和文档的相关度做打分，算法由两种： TF-IDF算法 BM25算法，elasticsearch5.1版本后采用的算法 1.5.2.算分函数查询根据相关度打分是比较合理的需求，但合理的不一定是产品经理需要的。 以百度为例，你搜索的结果中，并不是相关度越高排名越靠前，而是谁掏的钱多排名就越靠前。如图： 要想认为控制相关性算分，就需要利用elasticsearch中的function score 查询了。 1）语法说明 function score 查询中包含四部分内容： 原始查询条件：query部分，基于这个条件搜索文档，并且基于BM25算法给文档打分，原始算分（query score) 过滤条件：filter部分，符合该条件的文档才会重新算分 算分函数：符合filter条件的文档要根据这个函数做运算，得到的函数算分（function score），有四种函数 weight：函数结果是常量 field_value_factor：以文档中的某个字段值作为函数结果 random_score：以随机数作为函数结果 script_score：自定义算分函数算法 运算模式：算分函数的结果、原始查询的相关性算分，两者之间的运算方式，包括： multiply：相乘 replace：用function score替换query score 其它，例如：sum、avg、max、min function score的运行流程如下： 1）根据原始条件查询搜索文档，并且计算相关性算分，称为原始算分（query score） 2）根据过滤条件，过滤文档 3）符合过滤条件的文档，基于算分函数运算，得到函数算分（function score） 4）将原始算分（query score）和函数算分（function score）基于运算模式做运算，得到最终结果，作为相关性算分。 因此，其中的关键点是： 过滤条件：决定哪些文档的算分被修改 算分函数：决定函数算分的算法 运算模式：决定最终算分结果 2）示例需求：给“如家”这个品牌的酒店排名靠前一些 翻译一下这个需求，转换为之前说的四个要点： 原始条件：不确定，可以任意变化 过滤条件：brand = “如家” 算分函数：可以简单粗暴，直接给固定的算分结果，weight 运算模式：比如求和 因此最终的DSL语句如下： 12345678910111213141516171819GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"function_score\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"query\":&nbsp;{ .... }, // 原始查询，可以是任意条件&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"functions\":&nbsp;[&nbsp;//&nbsp;算分函数&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"filter\":&nbsp;{&nbsp;//&nbsp;满足的条件，品牌必须是如家&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"term\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"brand\":&nbsp;\"如家\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"weight\":&nbsp;2&nbsp;//&nbsp;算分权重为2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;], \"boost_mode\": \"sum\" // 加权模式，求和&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 测试，在未添加算分函数时，如家得分如下： 添加了算分函数后，如家得分就提升了： 3）小结function score query定义的三要素是什么？ 过滤条件：哪些文档要加分 算分函数：如何计算function score 加权方式：function score 与 query score如何运算 1.5.3.布尔查询布尔查询是一个或多个查询子句的组合，每一个子句就是一个子查询。子查询的组合方式有： must：必须匹配每个子查询，类似“与” should：选择性匹配子查询，类似“或” must_not：必须不匹配，不参与算分，类似“非” filter：必须匹配，不参与算分 比如在搜索酒店时，除了关键字搜索外，我们还可能根据品牌、价格、城市等字段做过滤： 每一个不同的字段，其查询的条件、方式都不一样，必须是多个不同的查询，而要组合这些查询，就必须用bool查询了。 需要注意的是，搜索时，参与打分的字段越多，查询的性能也越差。因此这种多条件查询时，建议这样做： 搜索框的关键字搜索，是全文检索查询，使用must查询，参与算分 其它过滤条件，采用filter查询。不参与算分 1）语法示例：1234567891011121314151617181920GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"bool\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"must\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{\"term\":&nbsp;{\"city\":&nbsp;\"上海\"&nbsp;}}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"should\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{\"term\":&nbsp;{\"brand\":&nbsp;\"皇冠假日\"&nbsp;}}, {\"term\":&nbsp;{\"brand\":&nbsp;\"华美达\"&nbsp;}}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"must_not\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;\"range\":&nbsp;{&nbsp;\"price\":&nbsp;{&nbsp;\"lte\":&nbsp;500&nbsp;}&nbsp;}}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;],&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"filter\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;\"range\":&nbsp;{\"score\":&nbsp;{&nbsp;\"gte\":&nbsp;45&nbsp;}&nbsp;}}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;]&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 2）示例需求：搜索名字包含“如家”，价格不高于400，在坐标31.21,121.5周围10km范围内的酒店。 分析： 名称搜索，属于全文检索查询，应该参与算分。放到must中 价格不高于400，用range查询，属于过滤条件，不参与算分。放到must_not中 周围10km范围内，用geo_distance查询，属于过滤条件，不参与算分。放到filter中 3）小结bool查询有几种逻辑关系？ must：必须匹配的条件，可以理解为“与” should：选择性匹配的条件，可以理解为“或” must_not：必须不匹配的条件，不参与打分 filter：必须匹配的条件，不参与打分 2.搜索结果处理搜索的结果可以按照用户指定的方式去处理或展示。 2.1.排序elasticsearch默认是根据相关度算分（_score）来排序，但是也支持自定义方式对搜索结果排序。可以排序字段类型有：keyword类型、数值类型、地理坐标类型、日期类型等。 2.1.1.普通字段排序keyword、数值、日期类型排序的语法基本一致。 语法： 1234567891011GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"match_all\":&nbsp;{}&nbsp;&nbsp;},&nbsp;&nbsp;\"sort\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;\"desc\"&nbsp;&nbsp;//&nbsp;排序字段、排序方式ASC、DESC&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;]} 排序条件是一个数组，也就是可以写多个排序条件。按照声明的顺序，当第一个条件相等时，再按照第二个条件排序，以此类推 示例： 需求描述：酒店数据按照用户评价（score)降序排序，评价相同的按照价格(price)升序排序 2.1.2.地理坐标排序地理坐标排序略有不同。 语法说明： 123456789101112131415GET&nbsp;/indexName/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"match_all\":&nbsp;{}&nbsp;&nbsp;},&nbsp;&nbsp;\"sort\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"_geo_distance\"&nbsp;:&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\"&nbsp;:&nbsp;\"纬度，经度\", // 文档中geo_point类型的字段名、目标坐标点&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"order\"&nbsp;:&nbsp;\"asc\", // 排序方式&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"unit\"&nbsp;:&nbsp;\"km\" // 排序的距离单位&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;]} 这个查询的含义是： 指定一个坐标，作为目标点 计算每一个文档中，指定字段（必须是geo_point类型）的坐标 到目标点的距离是多少 根据距离排序 示例： 需求描述：实现对酒店数据按照到你的位置坐标的距离升序排序 提示：获取你的位置的经纬度的方式：https://lbs.amap.com/demo/jsapi-v2/example/map/click-to-get-lnglat/ 假设我的位置是：31.034661，121.612282，寻找我周围距离最近的酒店。 2.2.分页elasticsearch 默认情况下只返回top10的数据。而如果要查询更多数据就需要修改分页参数了。elasticsearch中通过修改from、size参数来控制要返回的分页结果： from：从第几个文档开始 size：总共查询几个文档 类似于mysql中的limit ?, ? 2.2.1.基本的分页分页的基本语法如下： 1234567891011GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"match_all\":&nbsp;{}&nbsp;&nbsp;},&nbsp;&nbsp;\"from\":&nbsp;0,&nbsp;//&nbsp;分页开始的位置，默认为0&nbsp;&nbsp;\"size\":&nbsp;10,&nbsp;//&nbsp;期望获取的文档总数&nbsp;&nbsp;\"sort\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;{\"price\":&nbsp;\"asc\"}&nbsp;&nbsp;]} 2.2.2.深度分页问题现在，我要查询990~1000的数据，查询逻辑要这么写： 1234567891011GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"match_all\":&nbsp;{}&nbsp;&nbsp;},&nbsp;&nbsp;\"from\":&nbsp;990,&nbsp;//&nbsp;分页开始的位置，默认为0&nbsp;&nbsp;\"size\":&nbsp;10,&nbsp;//&nbsp;期望获取的文档总数&nbsp;&nbsp;\"sort\":&nbsp;[&nbsp;&nbsp;&nbsp;&nbsp;{\"price\":&nbsp;\"asc\"}&nbsp;&nbsp;]} 这里是查询990开始的数据，也就是 第990~第1000条 数据。 不过，elasticsearch内部分页时，必须先查询 0~1000条，然后截取其中的990 ~ 1000的这10条： 查询TOP1000，如果es是单点模式，这并无太大影响。 但是elasticsearch将来一定是集群，例如我集群有5个节点，我要查询TOP1000的数据，并不是每个节点查询200条就可以了。 因为节点A的TOP200，在另一个节点可能排到10000名以外了。 因此要想获取整个集群的TOP1000，必须先查询出每个节点的TOP1000，汇总结果后，重新排名，重新截取TOP1000。 那如果我要查询9900~10000的数据呢？是不是要先查询TOP10000呢？那每个节点都要查询10000条？汇总到内存中？ 当查询分页深度较大时，汇总数据过多，对内存和CPU会产生非常大的压力，因此elasticsearch会禁止from+ size 超过10000的请求。 针对深度分页，ES提供了两种解决方案，官方文档： search after：分页时需要排序，原理是从上一次的排序值开始，查询下一页数据。官方推荐使用的方式。 scroll：原理将排序后的文档id形成快照，保存在内存。官方已经不推荐使用。 2.2.3.小结分页查询的常见实现方案以及优缺点： from + size： 优点：支持随机翻页 缺点：深度分页问题，默认查询上限（from + size）是10000 场景：百度、京东、谷歌、淘宝这样的随机翻页搜索 after search： 优点：没有查询上限（单次查询的size不超过10000） 缺点：只能向后逐页查询，不支持随机翻页 场景：没有随机翻页需求的搜索，例如手机向下滚动翻页 scroll： 优点：没有查询上限（单次查询的size不超过10000） 缺点：会有额外内存消耗，并且搜索结果是非实时的 场景：海量数据的获取和迁移。从ES7.1开始不推荐，建议用 after search方案。 2.3.高亮2.3.1.高亮原理什么是高亮显示呢？ 我们在百度，京东搜索时，关键字会变成红色，比较醒目，这叫高亮显示： 高亮显示的实现分为两步： 1）给文档中的所有关键字都添加一个标签，例如&lt;em&gt;标签 2）页面给&lt;em&gt;标签编写CSS样式 2.3.2.实现高亮高亮的语法： 12345678910111213141516GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"match\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;\"TEXT\" // 查询条件，高亮一定要使用全文检索查询&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;},&nbsp;&nbsp;\"highlight\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"fields\":&nbsp;{&nbsp;//&nbsp;指定要高亮的字段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"FIELD\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"pre_tags\":&nbsp;\"&lt;em&gt;\",&nbsp;&nbsp;//&nbsp;用来标记高亮字段的前置标签&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"post_tags\":&nbsp;\"&lt;/em&gt;\"&nbsp;//&nbsp;用来标记高亮字段的后置标签&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 注意： 高亮是对关键字高亮，因此搜索条件必须带有关键字，而不能是范围这样的查询。 默认情况下，高亮的字段，必须与搜索指定的字段一致，否则无法高亮 如果要对非搜索字段高亮，则需要添加一个属性：required_field_match=false 示例： 2.4.总结查询的DSL是一个大的JSON对象，包含下列属性： query：查询条件 from和size：分页条件 sort：排序条件 highlight：高亮条件 示例： 3.RestClient查询文档文档的查询同样适用昨天学习的 RestHighLevelClient对象，基本步骤包括： 1）准备Request对象 2）准备请求参数 3）发起请求 4）解析响应 3.1.快速入门我们以match_all查询为例 3.1.1.发起查询请求 代码解读： 第一步，创建SearchRequest对象，指定索引库名 第二步，利用request.source()构建DSL，DSL中可以包含查询、分页、排序、高亮等 query()：代表查询条件，利用QueryBuilders.matchAllQuery()构建一个match_all查询的DSL 第三步，利用client.search()发送请求，得到响应 这里关键的API有两个，一个是request.source()，其中包含了查询、排序、分页、高亮等所有功能： 另一个是QueryBuilders，其中包含match、term、function_score、bool等各种查询： 3.1.2.解析响应响应结果的解析： elasticsearch返回的结果是一个JSON字符串，结构包含： hits：命中的结果 total：总条数，其中的value是具体的总条数值 max_score：所有结果中得分最高的文档的相关性算分 hits：搜索结果的文档数组，其中的每个文档都是一个json对象 _source：文档中的原始数据，也是json对象 因此，我们解析响应结果，就是逐层解析JSON字符串，流程如下： SearchHits：通过response.getHits()获取，就是JSON中的最外层的hits，代表命中的结果 SearchHits#getTotalHits().value：获取总条数信息 SearchHits#getHits()：获取SearchHit数组，也就是文档数组 SearchHit#getSourceAsString()：获取文档结果中的_source，也就是原始的json文档数据 3.1.3.完整代码完整代码如下： 12345678910111213141516171819202122232425262728293031@Testvoid testMatchAll() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL request.source() .query(QueryBuilders.matchAllQuery()); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);}private void handleResponse(SearchResponse response) { // 4.解析响应 SearchHits searchHits = response.getHits(); // 4.1.获取总条数 long total = searchHits.getTotalHits().value; System.out.println(\"共搜索到\" + total + \"条数据\"); // 4.2.文档数组 SearchHit[] hits = searchHits.getHits(); // 4.3.遍历 for (SearchHit hit : hits) { // 获取文档source String json = hit.getSourceAsString(); // 反序列化 HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class); System.out.println(\"hotelDoc = \" + hotelDoc); }} 3.1.4.小结查询的基本步骤是： 创建SearchRequest对象 准备Request.source()，也就是DSL。 ① QueryBuilders来构建查询条件 ② 传入Request.source() 的 query() 方法 发送请求，得到结果 解析结果（参考JSON结果，从外到内，逐层解析） 3.2.match查询全文检索的match和multi_match查询与match_all的API基本一致。差别是查询条件，也就是query的部分。 因此，Java代码上的差异主要是request.source().query()中的参数了。同样是利用QueryBuilders提供的方法： 而结果解析代码则完全一致，可以抽取并共享。 完整代码如下： 12345678910111213@Testvoid testMatch() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL request.source() .query(QueryBuilders.matchQuery(\"all\", \"如家\")); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);} 3.3.精确查询精确查询主要是两者： term：词条精确匹配 range：范围查询 与之前的查询相比，差异同样在查询条件，其它都一样。 查询条件构造的API如下： 3.4.布尔查询布尔查询是用must、must_not、filter等方式组合其它查询，代码示例如下： 可以看到，API与其它查询的差别同样是在查询条件的构建，QueryBuilders，结果解析等其他代码完全不变。 完整代码如下： 12345678910111213141516171819@Testvoid testBool() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.准备BooleanQuery BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); // 2.2.添加term boolQuery.must(QueryBuilders.termQuery(\"city\", \"杭州\")); // 2.3.添加range boolQuery.filter(QueryBuilders.rangeQuery(\"price\").lte(250)); request.source().query(boolQuery); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);} 3.5.排序、分页搜索结果的排序和分页是与query同级的参数，因此同样是使用request.source()来设置。 对应的API如下： 完整代码示例： 1234567891011121314151617181920@Testvoid testPageAndSort() throws IOException { // 页码，每页大小 int page = 1, size = 5; // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query request.source().query(QueryBuilders.matchAllQuery()); // 2.2.排序 sort request.source().sort(\"price\", SortOrder.ASC); // 2.3.分页 from、size request.source().from((page - 1) * size).size(5); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);} 3.6.高亮高亮的代码与之前代码差异较大，有两点： 查询的DSL：其中除了查询条件，还需要添加高亮条件，同样是与query同级。 结果解析：结果除了要解析_source文档数据，还要解析高亮结果 3.6.1.高亮请求构建高亮请求的构建API如下： 上述代码省略了查询条件部分，但是大家不要忘了：高亮查询必须使用全文检索查询，并且要有搜索关键字，将来才可以对关键字高亮。 完整代码如下： 123456789101112131415@Testvoid testHighlight() throws IOException { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query request.source().query(QueryBuilders.matchQuery(\"all\", \"如家\")); // 2.2.高亮 request.source().highlighter(new HighlightBuilder().field(\"name\").requireFieldMatch(false)); // 3.发送请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析响应 handleResponse(response);} 3.6.2.高亮结果解析高亮的结果与查询的文档结果默认是分离的，并不在一起。 因此解析高亮的代码需要额外处理： 代码解读： 第一步：从结果中获取source。hit.getSourceAsString()，这部分是非高亮结果，json字符串。还需要反序列为HotelDoc对象 第二步：获取高亮结果。hit.getHighlightFields()，返回值是一个Map，key是高亮字段名称，值是HighlightField对象，代表高亮值 第三步：从map中根据高亮字段名称，获取高亮字段值对象HighlightField 第四步：从HighlightField中获取Fragments，并且转为字符串。这部分就是真正的高亮字符串了 第五步：用高亮的结果替换HotelDoc中的非高亮结果 完整代码如下： 1234567891011121314151617181920212223242526272829private void handleResponse(SearchResponse response) { // 4.解析响应 SearchHits searchHits = response.getHits(); // 4.1.获取总条数 long total = searchHits.getTotalHits().value; System.out.println(\"共搜索到\" + total + \"条数据\"); // 4.2.文档数组 SearchHit[] hits = searchHits.getHits(); // 4.3.遍历 for (SearchHit hit : hits) { // 获取文档source String json = hit.getSourceAsString(); // 反序列化 HotelDoc hotelDoc = JSON.parseObject(json, HotelDoc.class); // 获取高亮结果 Map&lt;String, HighlightField&gt; highlightFields = hit.getHighlightFields(); if (!CollectionUtils.isEmpty(highlightFields)) { // 根据字段名获取高亮结果 HighlightField highlightField = highlightFields.get(\"name\"); if (highlightField != null) { // 获取高亮值 String name = highlightField.getFragments()[0].string(); // 覆盖非高亮结果 hotelDoc.setName(name); } } System.out.println(\"hotelDoc = \" + hotelDoc); }} 1.数据聚合**聚合（aggregations）**可以让我们极其方便的实现对数据的统计、分析、运算。例如： 什么品牌的手机最受欢迎？ 这些手机的平均价格、最高价格、最低价格？ 这些手机每月的销售情况如何？ 实现这些统计功能的比数据库的sql要方便的多，而且查询速度非常快，可以实现近实时搜索效果。 1.1.聚合的种类聚合常见的有三类： 桶（Bucket）聚合：用来对文档做分组 TermAggregation：按照文档字段值分组，例如按照品牌值分组、按照国家分组 Date Histogram：按照日期阶梯分组，例如一周为一组，或者一月为一组 度量（Metric）聚合：用以计算一些值，比如：最大值、最小值、平均值等 Avg：求平均值 Max：求最大值 Min：求最小值 Stats：同时求max、min、avg、sum等 管道（pipeline）聚合：其它聚合的结果为基础做聚合 注意：参加聚合的字段必须是keyword、日期、数值、布尔类型 1.2.DSL实现聚合现在，我们要统计所有数据中的酒店品牌有几种，其实就是按照品牌对数据分组。此时可以根据酒店品牌的名称做聚合，也就是Bucket聚合。 1.2.1.Bucket聚合语法语法如下： 123456789101112GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"size\":&nbsp;0,&nbsp;&nbsp;//&nbsp;设置size为0，结果中不包含文档，只包含聚合结果&nbsp;&nbsp;\"aggs\":&nbsp;{&nbsp;//&nbsp;定义聚合&nbsp;&nbsp;&nbsp;&nbsp;\"brandAgg\":&nbsp;{&nbsp;//给聚合起个名字&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"terms\":&nbsp;{&nbsp;//&nbsp;聚合的类型，按照品牌值聚合，所以选择term&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"field\":&nbsp;\"brand\",&nbsp;//&nbsp;参与聚合的字段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"size\":&nbsp;20&nbsp;//&nbsp;希望获取的聚合结果数量&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 结果如图： 1.2.2.聚合结果排序默认情况下，Bucket聚合会统计Bucket内的文档数量，记为_count，并且按照_count降序排序。 我们可以指定order属性，自定义聚合的排序方式： 123456789101112131415GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"size\":&nbsp;0,&nbsp;&nbsp;&nbsp;\"aggs\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"brandAgg\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"terms\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"field\":&nbsp;\"brand\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"order\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"_count\":&nbsp;\"asc\" //&nbsp;按照_count升序排列&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"size\":&nbsp;20&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 1.2.3.限定聚合范围默认情况下，Bucket聚合是对索引库的所有文档做聚合，但真实场景下，用户会输入搜索条件，因此聚合必须是对搜索结果聚合。那么聚合必须添加限定条件。 我们可以限定要聚合的文档范围，只要添加query条件即可： 12345678910111213141516171819GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"query\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"range\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"price\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"lte\":&nbsp;200 // 只对200元以下的文档聚合&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;\"size\":&nbsp;0,&nbsp;&nbsp;&nbsp;\"aggs\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"brandAgg\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"terms\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"field\":&nbsp;\"brand\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"size\":&nbsp;20&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 这次，聚合得到的品牌明显变少了： 1.2.4.Metric聚合语法上节课，我们对酒店按照品牌分组，形成了一个个桶。现在我们需要对桶内的酒店做运算，获取每个品牌的用户评分的min、max、avg等值。 这就要用到Metric聚合了，例如stat聚合：就可以获取min、max、avg等结果。 语法如下： 12345678910111213141516171819GET&nbsp;/hotel/_search{&nbsp;&nbsp;\"size\":&nbsp;0,&nbsp;&nbsp;&nbsp;\"aggs\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"brandAgg\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"terms\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"field\":&nbsp;\"brand\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"size\":&nbsp;20&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"aggs\":&nbsp;{&nbsp;//&nbsp;是brands聚合的子聚合，也就是分组后对每组分别计算&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"score_stats\":&nbsp;{&nbsp;//&nbsp;聚合名称&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"stats\":&nbsp;{&nbsp;//&nbsp;聚合类型，这里stats可以计算min、max、avg等&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"field\":&nbsp;\"score\"&nbsp;//&nbsp;聚合字段，这里是score&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 这次的score_stats聚合是在brandAgg的聚合内部嵌套的子聚合。因为我们需要在每个桶分别计算。 另外，我们还可以给聚合结果做个排序，例如按照每个桶的酒店平均分做排序： 1.2.5.小结aggs代表聚合，与query同级，此时query的作用是？ 限定聚合的的文档范围 聚合必须的三要素： 聚合名称 聚合类型 聚合字段 聚合可配置属性有： size：指定聚合结果数量 order：指定聚合结果排序方式 field：指定聚合字段 1.3.RestAPI实现聚合1.3.1.API语法聚合条件与query条件同级别，因此需要使用request.source()来指定聚合条件。 聚合条件的语法： 聚合的结果也与查询结果不同，API也比较特殊。不过同样是JSON逐层解析： 1.3.2.业务需求需求：搜索页面的品牌、城市等信息不应该是在页面写死，而是通过聚合索引库中的酒店数据得来的： 分析： 目前，页面的城市列表、星级列表、品牌列表都是写死的，并不会随着搜索结果的变化而变化。但是用户搜索条件改变时，搜索结果会跟着变化。 例如：用户搜索“东方明珠”，那搜索的酒店肯定是在上海东方明珠附近，因此，城市只能是上海，此时城市列表中就不应该显示北京、深圳、杭州这些信息了。 也就是说，搜索结果中包含哪些城市，页面就应该列出哪些城市；搜索结果中包含哪些品牌，页面就应该列出哪些品牌。 如何得知搜索结果中包含哪些品牌？如何得知搜索结果中包含哪些城市？ 使用聚合功能，利用Bucket聚合，对搜索结果中的文档基于品牌分组、基于城市分组，就能得知包含哪些品牌、哪些城市了。 因为是对搜索结果聚合，因此聚合是限定范围的聚合，也就是说聚合的限定条件跟搜索文档的条件一致。 查看浏览器可以发现，前端其实已经发出了这样的一个请求： 请求参数与搜索文档的参数完全一致。 返回值类型就是页面要展示的最终结果： 结果是一个Map结构： key是字符串，城市、星级、品牌、价格 value是集合，例如多个城市的名称 1.3.3.业务实现在cn.itcast.hotel.web包的HotelController中添加一个方法，遵循下面的要求： 请求方式：POST 请求路径：/hotel/filters 请求参数：RequestParams，与搜索文档的参数一致 返回值类型：Map&lt;String, List&lt;String&gt;&gt; 代码： 1234@PostMapping(\"filters\")public Map&lt;String, List&lt;String&gt;&gt; getFilters(@RequestBody RequestParams params){ return hotelService.getFilters(params);} 这里调用了IHotelService中的getFilters方法，尚未实现。 在cn.itcast.hotel.service.IHotelService中定义新方法： 1Map&lt;String, List&lt;String&gt;&gt; filters(RequestParams params); 在cn.itcast.hotel.service.impl.HotelService中实现该方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Overridepublic Map&lt;String, List&lt;String&gt;&gt; filters(RequestParams params) { try { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL // 2.1.query buildBasicQuery(params, request); // 2.2.设置size request.source().size(0); // 2.3.聚合 buildAggregation(request); // 3.发出请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析结果 Map&lt;String, List&lt;String&gt;&gt; result = new HashMap&lt;&gt;(); Aggregations aggregations = response.getAggregations(); // 4.1.根据品牌名称，获取品牌结果 List&lt;String&gt; brandList = getAggByName(aggregations, \"brandAgg\"); result.put(\"品牌\", brandList); // 4.2.根据品牌名称，获取品牌结果 List&lt;String&gt; cityList = getAggByName(aggregations, \"cityAgg\"); result.put(\"城市\", cityList); // 4.3.根据品牌名称，获取品牌结果 List&lt;String&gt; starList = getAggByName(aggregations, \"starAgg\"); result.put(\"星级\", starList); return result; } catch (IOException e) { throw new RuntimeException(e); }}private void buildAggregation(SearchRequest request) { request.source().aggregation(AggregationBuilders .terms(\"brandAgg\") .field(\"brand\") .size(100) ); request.source().aggregation(AggregationBuilders .terms(\"cityAgg\") .field(\"city\") .size(100) ); request.source().aggregation(AggregationBuilders .terms(\"starAgg\") .field(\"starName\") .size(100) );}private List&lt;String&gt; getAggByName(Aggregations aggregations, String aggName) { // 4.1.根据聚合名称获取聚合结果 Terms brandTerms = aggregations.get(aggName); // 4.2.获取buckets List&lt;? extends Terms.Bucket&gt; buckets = brandTerms.getBuckets(); // 4.3.遍历 List&lt;String&gt; brandList = new ArrayList&lt;&gt;(); for (Terms.Bucket bucket : buckets) { // 4.4.获取key String key = bucket.getKeyAsString(); brandList.add(key); } return brandList;} 2.自动补全当用户在搜索框输入字符时，我们应该提示出与该字符有关的搜索项，如图： 这种根据用户输入的字母，提示完整词条的功能，就是自动补全了。 因为需要根据拼音字母来推断，因此要用到拼音分词功能。 2.1.拼音分词器要实现根据字母做补全，就必须对文档按照拼音分词。在GitHub上恰好有elasticsearch的拼音分词插件。地址：https://github.com/medcl/elasticsearch-analysis-pinyin 课前资料中也提供了拼音分词器的安装包： 安装方式与IK分词器一样，分三步： ​ ①解压 ​ ②上传到虚拟机中，elasticsearch的plugin目录 ​ ③重启elasticsearch ​ ④测试 详细安装步骤可以参考IK分词器的安装过程。 测试用法如下： 12345POST&nbsp;/_analyze{&nbsp;&nbsp;\"text\":&nbsp;\"如家酒店还不错\",&nbsp;&nbsp;\"analyzer\":&nbsp;\"pinyin\"} 结果： 2.2.自定义分词器默认的拼音分词器会将每个汉字单独分为拼音，而我们希望的是每个词条形成一组拼音，需要对拼音分词器做个性化定制，形成自定义分词器。 elasticsearch中分词器（analyzer）的组成包含三部分： character filters：在tokenizer之前对文本进行处理。例如删除字符、替换字符 tokenizer：将文本按照一定的规则切割成词条（term）。例如keyword，就是不分词；还有ik_smart tokenizer filter：将tokenizer输出的词条做进一步处理。例如大小写转换、同义词处理、拼音处理等 文档分词时会依次由这三部分来处理文档： 声明自定义分词器的语法如下： 123456789101112131415161718192021222324252627282930313233PUT&nbsp;/test{&nbsp;&nbsp;\"settings\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"analysis\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"analyzer\":&nbsp;{&nbsp;//&nbsp;自定义分词器&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"my_analyzer\":&nbsp;{&nbsp;&nbsp;//&nbsp;分词器名称&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"tokenizer\":&nbsp;\"ik_max_word\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"filter\":&nbsp;\"py\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"filter\":&nbsp;{&nbsp;//&nbsp;自定义tokenizer&nbsp;filter&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"py\":&nbsp;{&nbsp;//&nbsp;过滤器名称&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"pinyin\",&nbsp;//&nbsp;过滤器类型，这里是pinyin \"keep_full_pinyin\":&nbsp;false,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"keep_joined_full_pinyin\":&nbsp;true,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"keep_original\":&nbsp;true,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"limit_first_letter_length\":&nbsp;16,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"remove_duplicated_term\":&nbsp;true,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"none_chinese_pinyin_tokenize\":&nbsp;false&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;},&nbsp;&nbsp;\"mappings\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"properties\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"name\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"text\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"analyzer\":&nbsp;\"my_analyzer\",&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"search_analyzer\":&nbsp;\"ik_smart\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 测试： 总结： 如何使用拼音分词器？ ①下载pinyin分词器 ②解压并放到elasticsearch的plugin目录 ③重启即可 如何自定义分词器？ ①创建索引库时，在settings中配置，可以包含三部分 ②character filter ③tokenizer ④filter 拼音分词器注意事项？ 为了避免搜索到同音字，搜索时不要使用拼音分词器 2.3.自动补全查询elasticsearch提供了Completion Suggester查询来实现自动补全功能。这个查询会匹配以用户输入内容开头的词条并返回。为了提高补全查询的效率，对于文档中字段的类型有一些约束： 参与补全查询的字段必须是completion类型。 字段的内容一般是用来补全的多个词条形成的数组。 比如，一个这样的索引库： 1234567891011//&nbsp;创建索引库PUT&nbsp;test{&nbsp;&nbsp;\"mappings\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"properties\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"title\":{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\":&nbsp;\"completion\"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 然后插入下面的数据： 12345678910111213// 示例数据POST&nbsp;test/_doc{&nbsp;&nbsp;\"title\":&nbsp;[\"Sony\",&nbsp;\"WH-1000XM3\"]}POST&nbsp;test/_doc{&nbsp;&nbsp;\"title\":&nbsp;[\"SK-II\",&nbsp;\"PITERA\"]}POST&nbsp;test/_doc{&nbsp;&nbsp;\"title\":&nbsp;[\"Nintendo\",&nbsp;\"switch\"]} 查询的DSL语句如下： 1234567891011121314//&nbsp;自动补全查询GET&nbsp;/test/_search{&nbsp;&nbsp;\"suggest\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;\"title_suggest\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"text\":&nbsp;\"s\",&nbsp;//&nbsp;关键字&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"completion\":&nbsp;{&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"field\":&nbsp;\"title\",&nbsp;//&nbsp;补全查询的字段&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"skip_duplicates\":&nbsp;true,&nbsp;//&nbsp;跳过重复的&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"size\":&nbsp;10&nbsp;//&nbsp;获取前10条结果&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;&nbsp;&nbsp;}&nbsp;&nbsp;}} 2.4.实现酒店搜索框自动补全现在，我们的hotel索引库还没有设置拼音分词器，需要修改索引库中的配置。但是我们知道索引库是无法修改的，只能删除然后重新创建。 另外，我们需要添加一个字段，用来做自动补全，将brand、suggestion、city等都放进去，作为自动补全的提示。 因此，总结一下，我们需要做的事情包括： 修改hotel索引库结构，设置自定义拼音分词器 修改索引库的name、all字段，使用自定义分词器 索引库添加一个新字段suggestion，类型为completion类型，使用自定义的分词器 给HotelDoc类添加suggestion字段，内容包含brand、business 重新导入数据到hotel库 2.4.1.修改酒店映射结构代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// 酒店数据索引库PUT /hotel{ \"settings\": { \"analysis\": { \"analyzer\": { \"text_anlyzer\": { \"tokenizer\": \"ik_max_word\", \"filter\": \"py\" }, \"completion_analyzer\": { \"tokenizer\": \"keyword\", \"filter\": \"py\" } }, \"filter\": { \"py\": { \"type\": \"pinyin\", \"keep_full_pinyin\": false, \"keep_joined_full_pinyin\": true, \"keep_original\": true, \"limit_first_letter_length\": 16, \"remove_duplicated_term\": true, \"none_chinese_pinyin_tokenize\": false } } } }, \"mappings\": { \"properties\": { \"id\":{ \"type\": \"keyword\" }, \"name\":{ \"type\": \"text\", \"analyzer\": \"text_anlyzer\", \"search_analyzer\": \"ik_smart\", \"copy_to\": \"all\" }, \"address\":{ \"type\": \"keyword\", \"index\": false }, \"price\":{ \"type\": \"integer\" }, \"score\":{ \"type\": \"integer\" }, \"brand\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"city\":{ \"type\": \"keyword\" }, \"starName\":{ \"type\": \"keyword\" }, \"business\":{ \"type\": \"keyword\", \"copy_to\": \"all\" }, \"location\":{ \"type\": \"geo_point\" }, \"pic\":{ \"type\": \"keyword\", \"index\": false }, \"all\":{ \"type\": \"text\", \"analyzer\": \"text_anlyzer\", \"search_analyzer\": \"ik_smart\" }, \"suggestion\":{ \"type\": \"completion\", \"analyzer\": \"completion_analyzer\" } } }} 2.4.2.修改HotelDoc实体HotelDoc中要添加一个字段，用来做自动补全，内容可以是酒店品牌、城市、商圈等信息。按照自动补全字段的要求，最好是这些字段的数组。 因此我们在HotelDoc中添加一个suggestion字段，类型为List&lt;String&gt;，然后将brand、city、business等信息放到里面。 代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package cn.itcast.hotel.pojo;import lombok.Data;import lombok.NoArgsConstructor;import java.util.ArrayList;import java.util.Arrays;import java.util.Collections;import java.util.List;@Data@NoArgsConstructorpublic class HotelDoc { private Long id; private String name; private String address; private Integer price; private Integer score; private String brand; private String city; private String starName; private String business; private String location; private String pic; private Object distance; private Boolean isAD; private List&lt;String&gt; suggestion; public HotelDoc(Hotel hotel) { this.id = hotel.getId(); this.name = hotel.getName(); this.address = hotel.getAddress(); this.price = hotel.getPrice(); this.score = hotel.getScore(); this.brand = hotel.getBrand(); this.city = hotel.getCity(); this.starName = hotel.getStarName(); this.business = hotel.getBusiness(); this.location = hotel.getLatitude() + \", \" + hotel.getLongitude(); this.pic = hotel.getPic(); // 组装suggestion if(this.business.contains(\"/\")){ // business有多个值，需要切割 String[] arr = this.business.split(\"/\"); // 添加元素 this.suggestion = new ArrayList&lt;&gt;(); this.suggestion.add(this.brand); Collections.addAll(this.suggestion, arr); }else { this.suggestion = Arrays.asList(this.brand, this.business); } }} 2.4.3.重新导入重新执行之前编写的导入数据功能，可以看到新的酒店数据中包含了suggestion： 2.4.4.自动补全查询的JavaAPI之前我们学习了自动补全查询的DSL，而没有学习对应的JavaAPI，这里给出一个示例： 而自动补全的结果也比较特殊，解析的代码如下： 2.4.5.实现搜索框自动补全查看前端页面，可以发现当我们在输入框键入时，前端会发起ajax请求： 返回值是补全词条的集合，类型为List&lt;String&gt; 1）在cn.itcast.hotel.web包下的HotelController中添加新接口，接收新的请求： 1234@GetMapping(\"suggestion\")public List&lt;String&gt; getSuggestions(@RequestParam(\"key\") String prefix) { return hotelService.getSuggestions(prefix);} 2）在cn.itcast.hotel.service包下的IhotelService中添加方法： 1List&lt;String&gt; getSuggestions(String prefix); 3）在cn.itcast.hotel.service.impl.HotelService中实现该方法： 1234567891011121314151617181920212223242526272829303132@Overridepublic List&lt;String&gt; getSuggestions(String prefix) { try { // 1.准备Request SearchRequest request = new SearchRequest(\"hotel\"); // 2.准备DSL request.source().suggest(new SuggestBuilder().addSuggestion( \"suggestions\", SuggestBuilders.completionSuggestion(\"suggestion\") .prefix(prefix) .skipDuplicates(true) .size(10) )); // 3.发起请求 SearchResponse response = client.search(request, RequestOptions.DEFAULT); // 4.解析结果 Suggest suggest = response.getSuggest(); // 4.1.根据补全查询名称，获取补全结果 CompletionSuggestion suggestions = suggest.getSuggestion(\"suggestions\"); // 4.2.获取options List&lt;CompletionSuggestion.Entry.Option&gt; options = suggestions.getOptions(); // 4.3.遍历 List&lt;String&gt; list = new ArrayList&lt;&gt;(options.size()); for (CompletionSuggestion.Entry.Option option : options) { String text = option.getText().toString(); list.add(text); } return list; } catch (IOException e) { throw new RuntimeException(e); }} 3.数据同步elasticsearch中的酒店数据来自于mysql数据库，因此mysql数据发生改变时，elasticsearch也必须跟着改变，这个就是elasticsearch与mysql之间的数据同步。 3.1.思路分析常见的数据同步方案有三种： 同步调用 异步通知 监听binlog 3.1.1.同步调用方案一：同步调用 基本步骤如下： hotel-demo对外提供接口，用来修改elasticsearch中的数据 酒店管理服务在完成数据库操作后，直接调用hotel-demo提供的接口， 3.1.2.异步通知方案二：异步通知 流程如下： hotel-admin对mysql数据库数据完成增、删、改后，发送MQ消息 hotel-demo监听MQ，接收到消息后完成elasticsearch数据修改 3.1.3.监听binlog方案三：监听binlog 流程如下： 给mysql开启binlog功能 mysql完成增、删、改操作都会记录在binlog中 hotel-demo基于canal监听binlog变化，实时更新elasticsearch中的内容 3.1.4.选择方式一：同步调用 优点：实现简单，粗暴 缺点：业务耦合度高 方式二：异步通知 优点：低耦合，实现难度一般 缺点：依赖mq的可靠性 方式三：监听binlog 优点：完全解除服务间耦合 缺点：开启binlog增加数据库负担、实现复杂度高 3.2.实现数据同步3.2.1.思路利用课前资料提供的hotel-admin项目作为酒店管理的微服务。当酒店数据发生增、删、改时，要求对elasticsearch中数据也要完成相同操作。 步骤： 导入课前资料提供的hotel-admin项目，启动并测试酒店数据的CRUD 声明exchange、queue、RoutingKey 在hotel-admin中的增、删、改业务中完成消息发送 在hotel-demo中完成消息监听，并更新elasticsearch中数据 启动并测试数据同步功能 3.2.2.导入demo导入课前资料提供的hotel-admin项目： 运行后，访问 http://localhost:8099 其中包含了酒店的CRUD功能： 3.2.3.声明交换机、队列MQ结构如图： 1）引入依赖在hotel-admin、hotel-demo中引入rabbitmq的依赖： 12345&lt;!--amqp--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt;&lt;/dependency&gt; 2）声明队列交换机名称在hotel-admin和hotel-demo中的cn.itcast.hotel.constatnts包下新建一个类MqConstants： 123456789101112131415161718192021222324package cn.itcast.hotel.constatnts; public class MqConstants { /** * 交换机 */ public final static String HOTEL_EXCHANGE = \"hotel.topic\"; /** * 监听新增和修改的队列 */ public final static String HOTEL_INSERT_QUEUE = \"hotel.insert.queue\"; /** * 监听删除的队列 */ public final static String HOTEL_DELETE_QUEUE = \"hotel.delete.queue\"; /** * 新增或修改的RoutingKey */ public final static String HOTEL_INSERT_KEY = \"hotel.insert\"; /** * 删除的RoutingKey */ public final static String HOTEL_DELETE_KEY = \"hotel.delete\";} 3）声明队列交换机在hotel-demo中，定义配置类，声明队列、交换机： 12345678910111213141516171819202122232425262728293031323334353637package cn.itcast.hotel.config;import cn.itcast.hotel.constants.MqConstants;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.Queue;import org.springframework.amqp.core.TopicExchange;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;@Configurationpublic class MqConfig { @Bean public TopicExchange topicExchange(){ return new TopicExchange(MqConstants.HOTEL_EXCHANGE, true, false); } @Bean public Queue insertQueue(){ return new Queue(MqConstants.HOTEL_INSERT_QUEUE, true); } @Bean public Queue deleteQueue(){ return new Queue(MqConstants.HOTEL_DELETE_QUEUE, true); } @Bean public Binding insertQueueBinding(){ return BindingBuilder.bind(insertQueue()).to(topicExchange()).with(MqConstants.HOTEL_INSERT_KEY); } @Bean public Binding deleteQueueBinding(){ return BindingBuilder.bind(deleteQueue()).to(topicExchange()).with(MqConstants.HOTEL_DELETE_KEY); }} 3.2.4.发送MQ消息在hotel-admin中的增、删、改业务中分别发送MQ消息： 3.2.5.接收MQ消息hotel-demo接收到MQ消息要做的事情包括： 新增消息：根据传递的hotel的id查询hotel信息，然后新增一条数据到索引库 删除消息：根据传递的hotel的id删除索引库中的一条数据 1）首先在hotel-demo的cn.itcast.hotel.service包下的IHotelService中新增新增、删除业务 123void deleteById(Long id);void insertById(Long id); 2）给hotel-demo中的cn.itcast.hotel.service.impl包下的HotelService中实现业务： 123456789101112131415161718192021222324252627282930@Overridepublic void deleteById(Long id) { try { // 1.准备Request DeleteRequest request = new DeleteRequest(\"hotel\", id.toString()); // 2.发送请求 client.delete(request, RequestOptions.DEFAULT); } catch (IOException e) { throw new RuntimeException(e); }}@Overridepublic void insertById(Long id) { try { // 0.根据id查询酒店数据 Hotel hotel = getById(id); // 转换为文档类型 HotelDoc hotelDoc = new HotelDoc(hotel); // 1.准备Request对象 IndexRequest request = new IndexRequest(\"hotel\").id(hotel.getId().toString()); // 2.准备Json文档 request.source(JSON.toJSONString(hotelDoc), XContentType.JSON); // 3.发送请求 client.index(request, RequestOptions.DEFAULT); } catch (IOException e) { throw new RuntimeException(e); }} 3）编写监听器 在hotel-demo中的cn.itcast.hotel.mq包新增一个类： 1234567891011121314151617181920212223242526272829303132package cn.itcast.hotel.mq;import cn.itcast.hotel.constants.MqConstants;import cn.itcast.hotel.service.IHotelService;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;@Componentpublic class HotelListener { @Autowired private IHotelService hotelService; /** * 监听酒店新增或修改的业务 * @param id 酒店id */ @RabbitListener(queues = MqConstants.HOTEL_INSERT_QUEUE) public void listenHotelInsertOrUpdate(Long id){ hotelService.insertById(id); } /** * 监听酒店删除的业务 * @param id 酒店id */ @RabbitListener(queues = MqConstants.HOTEL_DELETE_QUEUE) public void listenHotelDelete(Long id){ hotelService.deleteById(id); }} 4.集群单机的elasticsearch做数据存储，必然面临两个问题：海量数据存储问题、单点故障问题。 海量数据存储问题：将索引库从逻辑上拆分为N个分片（shard），存储到多个节点 单点故障问题：将分片数据在不同节点备份（replica ） ES集群相关概念: 集群（cluster）：一组拥有共同的 cluster name 的 节点。 节点（node) ：集群中的一个 Elasticearch 实例 分片（shard）：索引可以被拆分为不同的部分进行存储，称为分片。在集群环境下，一个索引的不同分片可以拆分到不同的节点中 解决问题：数据量太大，单点存储量有限的问题。 此处，我们把数据分成3片：shard0、shard1、shard2 主分片（Primary shard）：相对于副本分片的定义。 副本分片（Replica shard）每个主分片可以有一个或者多个副本，数据和主分片一样。 ​ 数据备份可以保证高可用，但是每个分片备份一份，所需要的节点数量就会翻一倍，成本实在是太高了！ 为了在高可用和成本间寻求平衡，我们可以这样做： 首先对数据分片，存储到不同节点 然后对每个分片进行备份，放到对方节点，完成互相备份 这样可以大大减少所需要的服务节点数量，如图，我们以3分片，每个分片备份一份为例： 现在，每个分片都有1个备份，存储在3个节点： node0：保存了分片0和1 node1：保存了分片0和2 node2：保存了分片1和2 4.1.搭建ES集群参考课前资料的文档： 其中的第四章节： 4.2.集群脑裂问题4.2.1.集群职责划分elasticsearch中集群节点有不同的职责划分： 默认情况下，集群中的任何一个节点都同时具备上述四种角色。 但是真实的集群一定要将集群职责分离： master节点：对CPU要求高，但是内存要求第 data节点：对CPU和内存要求都高 coordinating节点：对网络带宽、CPU要求高 职责分离可以让我们根据不同节点的需求分配不同的硬件去部署。而且避免业务之间的互相干扰。 一个典型的es集群职责划分如图： 4.2.2.脑裂问题脑裂是因为集群中的节点失联导致的。 例如一个集群中，主节点与其它节点失联： 此时，node2和node3认为node1宕机，就会重新选主： 当node3当选后，集群继续对外提供服务，node2和node3自成集群，node1自成集群，两个集群数据不同步，出现数据差异。 当网络恢复后，因为集群中有两个master节点，集群状态的不一致，出现脑裂的情况： 解决脑裂的方案是，要求选票超过 ( eligible节点数量 + 1 ）/ 2 才能当选为主，因此eligible节点数量最好是奇数。对应配置项是discovery.zen.minimum_master_nodes，在es7.0以后，已经成为默认配置，因此一般不会发生脑裂问题 例如：3个节点形成的集群，选票必须超过 （3 + 1） / 2 ，也就是2票。node3得到node2和node3的选票，当选为主。node1只有自己1票，没有当选。集群中依然只有1个主节点，没有出现脑裂。 4.2.3.小结master eligible节点的作用是什么？ 参与集群选主 主节点可以管理集群状态、管理分片信息、处理创建和删除索引库的请求 data节点的作用是什么？ 数据的CRUD coordinator节点的作用是什么？ 路由请求到其它节点 合并查询到的结果，返回给用户 4.3.集群分布式存储当新增文档时，应该保存到不同分片，保证数据均衡，那么coordinating node如何确定数据该存储到哪个分片呢？ 4.3.1.分片存储测试插入三条数据： 测试可以看到，三条数据分别在不同分片： 结果： 4.3.2.分片存储原理elasticsearch会通过hash算法来计算文档应该存储到哪个分片： 说明： _routing默认是文档的id 算法与分片数量有关，因此索引库一旦创建，分片数量不能修改！ 新增文档的流程如下： 解读： 1）新增一个id=1的文档 2）对id做hash运算，假如得到的是2，则应该存储到shard-2 3）shard-2的主分片在node3节点，将数据路由到node3 4）保存文档 5）同步给shard-2的副本replica-2，在node2节点 6）返回结果给coordinating-node节点 4.4.集群分布式查询elasticsearch的查询分成两个阶段： scatter phase：分散阶段，coordinating node会把请求分发到每一个分片 gather phase：聚集阶段，coordinating node汇总data node的搜索结果，并处理为最终结果集返回给用户 4.5.集群故障转移集群的master节点会监控集群中的节点状态，如果发现有节点宕机，会立即将宕机节点的分片数据迁移到其它节点，确保数据安全，这个叫做故障转移。 1）例如一个集群结构如图： 现在，node1是主节点，其它两个节点是从节点。 2）突然，node1发生了故障： 宕机后的第一件事，需要重新选主，例如选中了node2： node2成为主节点后，会检测集群监控状态，发现：shard-1、shard-0没有副本节点。因此需要将node1上的数据迁移到node2、node3： 微服务保护-Sentinel1.初识Sentinel1.1.雪崩问题及解决方案1.1.1.雪崩问题微服务中，服务间调用关系错综复杂，一个微服务往往依赖于多个其它微服务。 如图，如果服务提供者I发生了故障，当前的应用的部分业务因为依赖于服务I，因此也会被阻塞。此时，其它不依赖于服务I的业务似乎不受影响。 但是，依赖服务I的业务请求被阻塞，用户不会得到响应，则tomcat的这个线程不会释放，于是越来越多的用户请求到来，越来越多的线程会阻塞： 服务器支持的线程和并发数有限，请求一直阻塞，会导致服务器资源耗尽，从而导致所有其它服务都不可用，那么当前服务也就不可用了。 那么，依赖于当前服务的其它服务随着时间的推移，最终也都会变的不可用，形成级联失败，雪崩就发生了： 1.1.2.超时处理解决雪崩问题的常见方式有四种： •超时处理：设定超时时间，请求超过一定时间没有响应就返回错误信息，不会无休止等待 1.1.3.仓壁模式方案2：仓壁模式 仓壁模式来源于船舱的设计： 船舱都会被隔板分离为多个独立空间，当船体破损时，只会导致部分空间进入，将故障控制在一定范围内，避免整个船体都被淹没。 于此类似，我们可以限定每个业务能使用的线程数，避免耗尽整个tomcat的资源，因此也叫线程隔离。 1.1.4.断路器断路器模式：由断路器统计业务执行的异常比例，如果超出阈值则会熔断该业务，拦截访问该业务的一切请求。 断路器会统计访问某个服务的请求数量，异常比例： 当发现访问服务D的请求异常比例过高时，认为服务D有导致雪崩的风险，会拦截访问服务D的一切请求，形成熔断： 1.1.5.限流流量控制：限制业务访问的QPS，避免服务因流量的突增而故障。 1.1.6.总结什么是雪崩问题？ 微服务之间相互调用，因为调用链中的一个服务故障，引起整个链路都无法访问的情况。 可以认为： 限流是对服务的保护，避免因瞬间高并发流量而导致服务故障，进而避免雪崩。是一种预防措施。 超时处理、线程隔离、降级熔断是在部分服务故障时，将故障控制在一定范围，避免雪崩。是一种补救措施。 1.2.服务保护技术对比在SpringCloud当中支持多种服务保护技术： Netfix Hystrix Sentinel Resilience4J 早期比较流行的是Hystrix框架，但目前国内实用最广泛的还是阿里巴巴的Sentinel框架，这里我们做下对比： Sentinel Hystrix 隔离策略 信号量隔离 线程池隔离/信号量隔离 熔断降级策略 基于慢调用比例或异常比例 基于失败比率 实时指标实现 滑动窗口 滑动窗口（基于 RxJava） 规则配置 支持多种数据源 支持多种数据源 扩展性 多个扩展点 插件的形式 基于注解的支持 支持 支持 限流 基于 QPS，支持基于调用关系的限流 有限的支持 流量整形 支持慢启动、匀速排队模式 不支持 系统自适应保护 支持 不支持 控制台 开箱即用，可配置规则、查看秒级监控、机器发现等 不完善 常见框架的适配 Servlet、Spring Cloud、Dubbo、gRPC 等 Servlet、Spring Cloud Netflix 1.3.Sentinel介绍和安装1.3.1.初识SentinelSentinel是阿里巴巴开源的一款微服务流量控制组件。官网地址：https://sentinelguard.io/zh-cn/index.html Sentinel 具有以下特征: •丰富的应用场景：Sentinel 承接了阿里巴巴近 10 年的双十一大促流量的核心场景，例如秒杀（即突发流量控制在系统容量可以承受的范围）、消息削峰填谷、集群流量控制、实时熔断下游不可用应用等。 •完备的实时监控：Sentinel 同时提供实时的监控功能。您可以在控制台中看到接入应用的单台机器秒级数据，甚至 500 台以下规模的集群的汇总运行情况。 •广泛的开源生态：Sentinel 提供开箱即用的与其它开源框架/库的整合模块，例如与 Spring Cloud、Dubbo、gRPC 的整合。您只需要引入相应的依赖并进行简单的配置即可快速地接入 Sentinel。 •完善的 SPI 扩展点：Sentinel 提供简单易用、完善的 SPI 扩展接口。您可以通过实现扩展接口来快速地定制逻辑。例如定制规则管理、适配动态数据源等。 1.3.2.安装Sentinel1）下载 sentinel官方提供了UI控制台，方便我们对系统做限流设置。大家可以在GitHub下载。 课前资料也提供了下载好的jar包： 2）运行 将jar包放到任意非中文目录，执行命令： 1java -jar sentinel-dashboard-1.8.1.jar 如果要修改Sentinel的默认端口、账户、密码，可以通过下列配置： 配置项 默认值 说明 server.port 8080 服务端口 sentinel.dashboard.auth.username sentinel 默认用户名 sentinel.dashboard.auth.password sentinel 默认密码 例如，修改端口： 1java -Dserver.port=8090 -jar sentinel-dashboard-1.8.1.jar 3）访问 访问http://localhost:8080页面，就可以看到sentinel的控制台了： 需要输入账号和密码，默认都是：sentinel 登录后，发现一片空白，什么都没有： 这是因为我们还没有与微服务整合。 1.4.微服务整合Sentinel我们在order-service中整合sentinel，并连接sentinel的控制台，步骤如下： 1）引入sentinel依赖 12345&lt;!--sentinel--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-sentinel&lt;/artifactId&gt;&lt;/dependency&gt; 2）配置控制台 修改application.yaml文件，添加下面内容： 1234567server: port: 8088spring: cloud: sentinel: transport: dashboard: localhost:8080 3）访问order-service的任意端点 打开浏览器，访问http://localhost:8088/order/101，这样才能触发sentinel的监控。 然后再访问sentinel的控制台，查看效果： 2.流量控制雪崩问题虽然有四种方案，但是限流是避免服务因突发的流量而发生故障，是对微服务雪崩问题的预防。我们先学习这种模式。 2.1.簇点链路当请求进入微服务时，首先会访问DispatcherServlet，然后进入Controller、Service、Mapper，这样的一个调用链就叫做簇点链路。簇点链路中被监控的每一个接口就是一个资源。 默认情况下sentinel会监控SpringMVC的每一个端点（Endpoint，也就是controller中的方法），因此SpringMVC的每一个端点（Endpoint）就是调用链路中的一个资源。 例如，我们刚才访问的order-service中的OrderController中的端点：/order/{orderId} 流控、熔断等都是针对簇点链路中的资源来设置的，因此我们可以点击对应资源后面的按钮来设置规则： 流控：流量控制 降级：降级熔断 热点：热点参数限流，是限流的一种 授权：请求的权限控制 2.1.快速入门2.1.1.示例点击资源/order/{orderId}后面的流控按钮，就可以弹出表单。 表单中可以填写限流规则，如下： 其含义是限制 /order/{orderId}这个资源的单机QPS为1，即每秒只允许1次请求，超出的请求会被拦截并报错。 2.1.2.练习：需求：给 /order/{orderId}这个资源设置流控规则，QPS不能超过 5，然后测试。 1）首先在sentinel控制台添加限流规则 2）利用jmeter测试 如果没有用过jmeter，可以参考课前资料提供的文档《Jmeter快速入门.md》 课前资料提供了编写好的Jmeter测试样例： 打开jmeter，导入课前资料提供的测试样例： 选择： 20个用户，2秒内运行完，QPS是10，超过了5. 选中流控入门，QPS&lt;5右键运行： 注意，不要点击菜单中的执行按钮来运行。 结果： 可以看到，成功的请求每次只有5个 2.2.流控模式在添加限流规则时，点击高级选项，可以选择三种流控模式： 直接：统计当前资源的请求，触发阈值时对当前资源直接限流，也是默认的模式 关联：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流 链路：统计从指定链路访问到本资源的请求，触发阈值时，对指定链路限流 快速入门测试的就是直接模式。 2.2.1.关联模式关联模式：统计与当前资源相关的另一个资源，触发阈值时，对当前资源限流 配置规则： 语法说明：当/write资源访问量触发阈值时，就会对/read资源限流，避免影响/write资源。 使用场景：比如用户支付时需要修改订单状态，同时用户要查询订单。查询和修改操作会争抢数据库锁，产生竞争。业务需求是优先支付和更新订单的业务，因此当修改订单业务触发阈值时，需要对查询订单业务限流。 需求说明： 在OrderController新建两个端点：/order/query和/order/update，无需实现业务 配置流控规则，当/order/ update资源被访问的QPS超过5时，对/order/query请求限流 1）定义/order/query端点，模拟订单查询 1234@GetMapping(\"/query\")public String queryOrder() { return \"查询订单成功\";} 2）定义/order/update端点，模拟订单更新 1234@GetMapping(\"/update\")public String updateOrder() { return \"更新订单成功\";} 重启服务，查看sentinel控制台的簇点链路： 3）配置流控规则 对哪个端点限流，就点击哪个端点后面的按钮。我们是对订单查询/order/query限流，因此点击它后面的按钮： 在表单中填写流控规则： 4）在Jmeter测试 选择《流控模式-关联》： 可以看到1000个用户，100秒，因此QPS为10，超过了我们设定的阈值：5 查看http请求： 请求的目标是/order/update，这样这个断点就会触发阈值。 但限流的目标是/order/query，我们在浏览器访问，可以发现： 确实被限流了。 5）总结 2.2.2.链路模式链路模式：只针对从指定链路访问到本资源的请求做统计，判断是否超过阈值。 配置示例： 例如有两条请求链路： /test1 –&gt; /common /test2 –&gt; /common 如果只希望统计从/test2进入到/common的请求，则可以这样配置： 实战案例 需求：有查询订单和创建订单业务，两者都需要查询商品。针对从查询订单进入到查询商品的请求统计，并设置限流。 步骤： 在OrderService中添加一个queryGoods方法，不用实现业务 在OrderController中，改造/order/query端点，调用OrderService中的queryGoods方法 在OrderController中添加一个/order/save的端点，调用OrderService的queryGoods方法 给queryGoods设置限流规则，从/order/query进入queryGoods的方法限制QPS必须小于2 实现： 1）添加查询商品方法在order-service服务中，给OrderService类添加一个queryGoods方法： 123public void queryGoods(){ System.err.println(\"查询商品\");} 2）查询订单时，查询商品在order-service的OrderController中，修改/order/query端点的业务逻辑： 12345678@GetMapping(\"/query\")public String queryOrder() { // 查询商品 orderService.queryGoods(); // 查询订单 System.out.println(\"查询订单\"); return \"查询订单成功\";} 3）新增订单，查询商品在order-service的OrderController中，修改/order/save端点，模拟新增订单： 12345678@GetMapping(\"/save\")public String saveOrder() { // 查询商品 orderService.queryGoods(); // 查询订单 System.err.println(\"新增订单\"); return \"新增订单成功\";} 4）给查询商品添加资源标记默认情况下，OrderService中的方法是不被Sentinel监控的，需要我们自己通过注解来标记要监控的方法。 给OrderService的queryGoods方法添加@SentinelResource注解： 1234@SentinelResource(\"goods\")public void queryGoods(){ System.err.println(\"查询商品\");} 链路模式中，是对不同来源的两个链路做监控。但是sentinel默认会给进入SpringMVC的所有请求设置同一个root资源，会导致链路模式失效。 我们需要关闭这种对SpringMVC的资源聚合，修改order-service服务的application.yml文件： 1234spring: cloud: sentinel: web-context-unify: false # 关闭context整合 重启服务，访问/order/query和/order/save，可以查看到sentinel的簇点链路规则中，出现了新的资源： 5）添加流控规则点击goods资源后面的流控按钮，在弹出的表单中填写下面信息： 只统计从/order/query进入/goods的资源，QPS阈值为2，超出则被限流。 6）Jmeter测试选择《流控模式-链路》： 可以看到这里200个用户，50秒内发完，QPS为4，超过了我们设定的阈值2 一个http请求是访问/order/save： 运行的结果： 完全不受影响。 另一个是访问/order/query： 运行结果： 每次只有2个通过。 2.2.3.总结流控模式有哪些？ •直接：对当前资源限流 •关联：高优先级资源触发阈值，对低优先级资源限流。 •链路：阈值统计时，只统计从指定资源进入当前资源的请求，是对请求来源的限流 2.3.流控效果在流控的高级选项中，还有一个流控效果选项： 流控效果是指请求达到流控阈值时应该采取的措施，包括三种： 快速失败：达到阈值后，新的请求会被立即拒绝并抛出FlowException异常。是默认的处理方式。 warm up：预热模式，对超出阈值的请求同样是拒绝并抛出异常。但这种模式阈值会动态变化，从一个较小值逐渐增加到最大阈值。 排队等待：让所有的请求按照先后次序排队执行，两个请求的间隔不能小于指定时长 2.3.1.warm up阈值一般是一个微服务能承担的最大QPS，但是一个服务刚刚启动时，一切资源尚未初始化（冷启动），如果直接将QPS跑到最大值，可能导致服务瞬间宕机。 warm up也叫预热模式，是应对服务冷启动的一种方案。请求阈值初始值是 maxThreshold / coldFactor，持续指定时长后，逐渐提高到maxThreshold值。而coldFactor的默认值是3. 例如，我设置QPS的maxThreshold为10，预热时间为5秒，那么初始阈值就是 10 / 3 ，也就是3，然后在5秒后逐渐增长到10. 案例 需求：给/order/{orderId}这个资源设置限流，最大QPS为10，利用warm up效果，预热时长为5秒 1）配置流控规则： 2）Jmeter测试选择《流控效果，warm up》： QPS为10. 刚刚启动时，大部分请求失败，成功的只有3个，说明QPS被限定在3： 随着时间推移，成功比例越来越高： 到Sentinel控制台查看实时监控： 一段时间后： 2.3.2.排队等待当请求超过QPS阈值时，快速失败和warm up 会拒绝新的请求并抛出异常。 而排队等待则是让所有请求进入一个队列中，然后按照阈值允许的时间间隔依次执行。后来的请求必须等待前面执行完成，如果请求预期的等待时间超出最大时长，则会被拒绝。 工作原理 例如：QPS = 5，意味着每200ms处理一个队列中的请求；timeout = 2000，意味着预期等待时长超过2000ms的请求会被拒绝并抛出异常。 那什么叫做预期等待时长呢？ 比如现在一下子来了12 个请求，因为每200ms执行一个请求，那么： 第6个请求的预期等待时长 = 200 * （6 - 1） = 1000ms 第12个请求的预期等待时长 = 200 * （12-1） = 2200ms 现在，第1秒同时接收到10个请求，但第2秒只有1个请求，此时QPS的曲线这样的： 如果使用队列模式做流控，所有进入的请求都要排队，以固定的200ms的间隔执行，QPS会变的很平滑： 平滑的QPS曲线，对于服务器来说是更友好的。 案例 需求：给/order/{orderId}这个资源设置限流，最大QPS为10，利用排队的流控效果，超时时长设置为5s 1）添加流控规则 2）Jmeter测试选择《流控效果，队列》： QPS为15，已经超过了我们设定的10。 如果是之前的 快速失败、warmup模式，超出的请求应该会直接报错。 但是我们看看队列模式的运行结果： 全部都通过了。 再去sentinel查看实时监控的QPS曲线： QPS非常平滑，一致保持在10，但是超出的请求没有被拒绝，而是放入队列。因此响应时间（等待时间）会越来越长。 当队列满了以后，才会有部分请求失败： 2.3.3.总结流控效果有哪些？ 快速失败：QPS超过阈值时，拒绝新的请求 warm up： QPS超过阈值时，拒绝新的请求；QPS阈值是逐渐提升的，可以避免冷启动时高并发导致服务宕机。 排队等待：请求会进入队列，按照阈值允许的时间间隔依次执行请求；如果请求预期等待时长大于超时时间，直接拒绝 2.4.热点参数限流之前的限流是统计访问某个资源的所有请求，判断是否超过QPS阈值。而热点参数限流是分别统计参数值相同的请求，判断是否超过QPS阈值。 2.4.1.全局参数限流例如，一个根据id查询商品的接口： 访问/goods/{id}的请求中，id参数值会有变化，热点参数限流会根据参数值分别统计QPS，统计结果： 当id=1的请求触发阈值被限流时，id值不为1的请求不受影响。 配置示例： 代表的含义是：对hot这个资源的0号参数（第一个参数）做统计，每1秒相同参数值的请求数不能超过5 2.4.2.热点参数限流刚才的配置中，对查询商品这个接口的所有商品一视同仁，QPS都限定为5. 而在实际开发中，可能部分商品是热点商品，例如秒杀商品，我们希望这部分商品的QPS限制与其它商品不一样，高一些。那就需要配置热点参数限流的高级选项了： 结合上一个配置，这里的含义是对0号的long类型参数限流，每1秒相同参数的QPS不能超过5，有两个例外： •如果参数值是100，则每1秒允许的QPS为10 •如果参数值是101，则每1秒允许的QPS为15 2.4.4.案例案例需求：给/order/{orderId}这个资源添加热点参数限流，规则如下： •默认的热点参数规则是每1秒请求量不超过2 •给102这个参数设置例外：每1秒请求量不超过4 •给103这个参数设置例外：每1秒请求量不超过10 注意事项：热点参数限流对默认的SpringMVC资源无效，需要利用@SentinelResource注解标记资源 1）标记资源给order-service中的OrderController中的/order/{orderId}资源添加注解： 2）热点参数限流规则访问该接口，可以看到我们标记的hot资源出现了： 这里不要点击hot后面的按钮，页面有BUG 点击左侧菜单中热点规则菜单： 点击新增，填写表单： 3）Jmeter测试选择《热点参数限流 QPS1》： 这里发起请求的QPS为5. 包含3个http请求： 普通参数，QPS阈值为2 运行结果： 例外项，QPS阈值为4 运行结果： 例外项，QPS阈值为10 运行结果： 3.隔离和降级限流是一种预防措施，虽然限流可以尽量避免因高并发而引起的服务故障，但服务还会因为其它原因而故障。 而要将这些故障控制在一定范围，避免雪崩，就要靠线程隔离（舱壁模式）和熔断降级手段了。 线程隔离之前讲到过：调用者在调用服务提供者时，给每个调用的请求分配独立线程池，出现故障时，最多消耗这个线程池内资源，避免把调用者的所有资源耗尽。 熔断降级：是在调用方这边加入断路器，统计对服务提供者的调用，如果调用的失败比例过高，则熔断该业务，不允许访问该服务的提供者了。 可以看到，不管是线程隔离还是熔断降级，都是对客户端（调用方）的保护。需要在调用方 发起远程调用时做线程隔离、或者服务熔断。 而我们的微服务远程调用都是基于Feign来完成的，因此我们需要将Feign与Sentinel整合，在Feign里面实现线程隔离和服务熔断。 3.1.FeignClient整合SentinelSpringCloud中，微服务调用都是通过Feign来实现的，因此做客户端保护必须整合Feign和Sentinel。 3.1.1.修改配置，开启sentinel功能修改OrderService的application.yml文件，开启Feign的Sentinel功能： 123feign: sentinel: enabled: true # 开启feign对sentinel的支持 3.1.2.编写失败降级逻辑业务失败后，不能直接报错，而应该返回用户一个友好提示或者默认结果，这个就是失败降级逻辑。 给FeignClient编写失败后的降级逻辑 ①方式一：FallbackClass，无法对远程调用的异常做处理 ②方式二：FallbackFactory，可以对远程调用的异常做处理，我们选择这种 这里我们演示方式二的失败降级处理。 步骤一：在feing-api项目中定义类，实现FallbackFactory： 代码： 123456789101112131415161718192021package cn.itcast.feign.clients.fallback;import cn.itcast.feign.clients.UserClient;import cn.itcast.feign.pojo.User;import feign.hystrix.FallbackFactory;import lombok.extern.slf4j.Slf4j;@Slf4jpublic class UserClientFallbackFactory implements FallbackFactory&lt;UserClient&gt; { @Override public UserClient create(Throwable throwable) { return new UserClient() { @Override public User findById(Long id) { log.error(\"查询用户异常\", throwable); return new User(); } }; }} 步骤二：在feing-api项目中的DefaultFeignConfiguration类中将UserClientFallbackFactory注册为一个Bean： 1234@Beanpublic UserClientFallbackFactory userClientFallbackFactory(){ return new UserClientFallbackFactory();} 步骤三：在feing-api项目中的UserClient接口中使用UserClientFallbackFactory： 123456789101112import cn.itcast.feign.clients.fallback.UserClientFallbackFactory;import cn.itcast.feign.pojo.User;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.PathVariable;@FeignClient(value = \"userservice\", fallbackFactory = UserClientFallbackFactory.class)public interface UserClient { @GetMapping(\"/user/{id}\") User findById(@PathVariable(\"id\") Long id);} 重启后，访问一次订单查询业务，然后查看sentinel控制台，可以看到新的簇点链路： 3.1.3.总结Sentinel支持的雪崩解决方案： 线程隔离（仓壁模式） 降级熔断 Feign整合Sentinel的步骤： 在application.yml中配置：feign.sentienl.enable=true 给FeignClient编写FallbackFactory并注册为Bean 将FallbackFactory配置到FeignClient 3.2.线程隔离（舱壁模式）3.2.1.线程隔离的实现方式线程隔离有两种方式实现： 线程池隔离 信号量隔离（Sentinel默认采用） 如图： 线程池隔离：给每个服务调用业务分配一个线程池，利用线程池本身实现隔离效果 信号量隔离：不创建线程池，而是计数器模式，记录业务使用的线程数量，达到信号量上限时，禁止新的请求。 两者的优缺点： 3.2.2.sentinel的线程隔离用法说明： 在添加限流规则时，可以选择两种阈值类型： QPS：就是每秒的请求数，在快速入门中已经演示过 线程数：是该资源能使用用的tomcat线程数的最大值。也就是通过限制线程数量，实现线程隔离（舱壁模式）。 案例需求：给 order-service服务中的UserClient的查询用户接口设置流控规则，线程数不能超过 2。然后利用jemeter测试。 1）配置隔离规则选择feign接口后面的流控按钮： 填写表单： 2）Jmeter测试选择《阈值类型-线程数&lt;2》： 一次发生10个请求，有较大概率并发线程数超过2，而超出的请求会走之前定义的失败降级逻辑。 查看运行结果： 发现虽然结果都是通过了，不过部分请求得到的响应是降级返回的null信息。 3.2.3.总结线程隔离的两种手段是？ 信号量隔离 线程池隔离 信号量隔离的特点是？ 基于计数器模式，简单，开销小 线程池隔离的特点是？ 基于线程池模式，有额外开销，但隔离控制更强 3.3.熔断降级熔断降级是解决雪崩问题的重要手段。其思路是由断路器统计服务调用的异常比例、慢请求比例，如果超出阈值则会熔断该服务。即拦截访问该服务的一切请求；而当服务恢复时，断路器会放行访问该服务的请求。 断路器控制熔断和放行是通过状态机来完成的： 状态机包括三个状态： closed：关闭状态，断路器放行所有请求，并开始统计异常比例、慢请求比例。超过阈值则切换到open状态 open：打开状态，服务调用被熔断，访问被熔断服务的请求会被拒绝，快速失败，直接走降级逻辑。Open状态5秒后会进入half-open状态 half-open：半开状态，放行一次请求，根据执行结果来判断接下来的操作。 请求成功：则切换到closed状态 请求失败：则切换到open状态 断路器熔断策略有三种：慢调用、异常比例、异常数 3.3.1.慢调用慢调用：业务的响应时长（RT）大于指定时长的请求认定为慢调用请求。在指定时间内，如果请求数量超过设定的最小数量，慢调用比例大于设定的阈值，则触发熔断。 例如： 解读：RT超过500ms的调用是慢调用，统计最近10000ms内的请求，如果请求量超过10次，并且慢调用比例不低于0.5，则触发熔断，熔断时长为5秒。然后进入half-open状态，放行一次请求做测试。 案例 需求：给 UserClient的查询用户接口设置降级规则，慢调用的RT阈值为50ms，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5 1）设置慢调用修改user-service中的/user/{id}这个接口的业务。通过休眠模拟一个延迟时间： 此时，orderId=101的订单，关联的是id为1的用户，调用时长为60ms： orderId=102的订单，关联的是id为2的用户，调用时长为非常短； 2）设置熔断规则下面，给feign接口设置降级规则： 规则： 超过50ms的请求都会被认为是慢请求 3）测试在浏览器访问：http://localhost:8088/order/101，快速刷新5次，可以发现： 触发了熔断，请求时长缩短至5ms，快速失败了，并且走降级逻辑，返回的null 在浏览器访问：http://localhost:8088/order/102，竟然也被熔断了： 3.3.2.异常比例、异常数异常比例或异常数：统计指定时间内的调用，如果调用次数超过指定请求数，并且出现异常的比例达到设定的比例阈值（或超过指定异常数），则触发熔断。 例如，一个异常比例设置： 解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于0.4，则触发熔断。 一个异常数设置： 解读：统计最近1000ms内的请求，如果请求量超过10次，并且异常比例不低于2次，则触发熔断。 案例 需求：给 UserClient的查询用户接口设置降级规则，统计时间为1秒，最小请求数量为5，失败阈值比例为0.4，熔断时长为5s 1）设置异常请求首先，修改user-service中的/user/{id}这个接口的业务。手动抛出异常，以触发异常比例的熔断： 也就是说，id 为 2时，就会触发异常 2）设置熔断规则下面，给feign接口设置降级规则： 规则： 在5次请求中，只要异常比例超过0.4，也就是有2次以上的异常，就会触发熔断。 3）测试在浏览器快速访问：http://localhost:8088/order/102，快速刷新5次，触发熔断： 此时，我们去访问本来应该正常的103： 4.授权规则授权规则可以对请求方来源做判断和控制。 4.1.授权规则4.1.1.基本规则授权规则可以对调用方的来源做控制，有白名单和黑名单两种方式。 白名单：来源（origin）在白名单内的调用者允许访问 黑名单：来源（origin）在黑名单内的调用者不允许访问 点击左侧菜单的授权，可以看到授权规则： 资源名：就是受保护的资源，例如/order/{orderId} 流控应用：是来源者的名单， 如果是勾选白名单，则名单中的来源被许可访问。 如果是勾选黑名单，则名单中的来源被禁止访问。 比如： 我们允许请求从gateway到order-service，不允许浏览器访问order-service，那么白名单中就要填写网关的来源名称（origin）。 4.1.2.如何获取originSentinel是通过RequestOriginParser这个接口的parseOrigin来获取请求的来源的。 123456public interface RequestOriginParser { /** * 从请求request对象中获取origin，获取方式自定义 */ String parseOrigin(HttpServletRequest request);} 这个方法的作用就是从request对象中，获取请求者的origin值并返回。 默认情况下，sentinel不管请求者从哪里来，返回值永远是default，也就是说一切请求的来源都被认为是一样的值default。 因此，我们需要自定义这个接口的实现，让不同的请求，返回不同的origin。 例如order-service服务中，我们定义一个RequestOriginParser的实现类： 123456789101112131415161718192021package cn.itcast.order.sentinel;import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.RequestOriginParser;import org.springframework.stereotype.Component;import org.springframework.util.StringUtils;import javax.servlet.http.HttpServletRequest;@Componentpublic class HeaderOriginParser implements RequestOriginParser { @Override public String parseOrigin(HttpServletRequest request) { // 1.获取请求头 String origin = request.getHeader(\"origin\"); // 2.非空判断 if (StringUtils.isEmpty(origin)) { origin = \"blank\"; } return origin; }} 我们会尝试从request-header中获取origin值。 4.1.3.给网关添加请求头既然获取请求origin的方式是从reques-header中获取origin值，我们必须让所有从gateway路由到微服务的请求都带上origin头。 这个需要利用之前学习的一个GatewayFilter来实现，AddRequestHeaderGatewayFilter。 修改gateway服务中的application.yml，添加一个defaultFilter： 1234567spring: cloud: gateway: default-filters: - AddRequestHeader=origin,gateway routes: # ...略 这样，从gateway路由的所有请求都会带上origin头，值为gateway。而从其它地方到达微服务的请求则没有这个头。 4.1.4.配置授权规则接下来，我们添加一个授权规则，放行origin值为gateway的请求。 配置如下： 现在，我们直接跳过网关，访问order-service服务： 通过网关访问： 4.2.自定义异常结果默认情况下，发生限流、降级、授权拦截时，都会抛出异常到调用方。异常结果都是flow limmiting（限流）。这样不够友好，无法得知是限流还是降级还是授权拦截。 4.2.1.异常类型而如果要自定义异常时的返回结果，需要实现BlockExceptionHandler接口： 123456public interface BlockExceptionHandler { /** * 处理请求被限流、降级、授权拦截时抛出的异常：BlockException */ void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception;} 这个方法有三个参数： HttpServletRequest request：request对象 HttpServletResponse response：response对象 BlockException e：被sentinel拦截时抛出的异常 这里的BlockException包含多个不同的子类： 异常 说明 FlowException 限流异常 ParamFlowException 热点参数限流的异常 DegradeException 降级异常 AuthorityException 授权规则异常 SystemBlockException 系统规则异常 4.2.2.自定义异常处理下面，我们就在order-service定义一个自定义异常处理类： 123456789101112131415161718192021222324252627282930313233343536package cn.itcast.order.sentinel;import com.alibaba.csp.sentinel.adapter.spring.webmvc.callback.BlockExceptionHandler;import com.alibaba.csp.sentinel.slots.block.BlockException;import com.alibaba.csp.sentinel.slots.block.authority.AuthorityException;import com.alibaba.csp.sentinel.slots.block.degrade.DegradeException;import com.alibaba.csp.sentinel.slots.block.flow.FlowException;import com.alibaba.csp.sentinel.slots.block.flow.param.ParamFlowException;import org.springframework.stereotype.Component;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;@Componentpublic class SentinelExceptionHandler implements BlockExceptionHandler { @Override public void handle(HttpServletRequest request, HttpServletResponse response, BlockException e) throws Exception { String msg = \"未知异常\"; int status = 429; if (e instanceof FlowException) { msg = \"请求被限流了\"; } else if (e instanceof ParamFlowException) { msg = \"请求被热点参数限流\"; } else if (e instanceof DegradeException) { msg = \"请求被降级了\"; } else if (e instanceof AuthorityException) { msg = \"没有权限访问\"; status = 401; } response.setContentType(\"application/json;charset=utf-8\"); response.setStatus(status); response.getWriter().println(\"{\\\"msg\\\": \" + msg + \", \\\"status\\\": \" + status + \"}\"); }} 重启测试，在不同场景下，会返回不同的异常消息. 限流： 授权拦截时： 5.规则持久化现在，sentinel的所有规则都是内存存储，重启后所有规则都会丢失。在生产环境下，我们必须确保这些规则的持久化，避免丢失。 5.1.规则管理模式规则是否能持久化，取决于规则管理模式，sentinel支持三种规则管理模式： 原始模式：Sentinel的默认模式，将规则保存在内存，重启服务会丢失。 pull模式 push模式 5.1.1.pull模式pull模式：控制台将配置的规则推送到Sentinel客户端，而客户端会将配置规则保存在本地文件或数据库中。以后会定时去本地文件或数据库中查询，更新本地规则。 5.1.2.push模式push模式：控制台将配置规则推送到远程配置中心，例如Nacos。Sentinel客户端监听Nacos，获取配置变更的推送消息，完成本地配置更新。 5.2.实现push模式详细步骤可以参考课前资料的《sentinel规则持久化》： 分布式事务-Seata1.分布式事务问题1.1.本地事务本地事务，也就是传统的单机事务。在传统数据库事务中，必须要满足四个原则： 1.2.分布式事务分布式事务，就是指不是在单个服务或单个数据库架构下，产生的事务，例如： 跨数据源的分布式事务 跨服务的分布式事务 综合情况 在数据库水平拆分、服务垂直拆分之后，一个业务操作通常要跨多个数据库、服务才能完成。例如电商行业中比较常见的下单付款案例，包括下面几个行为： 创建新订单 扣减商品库存 从用户账户余额扣除金额 完成上面的操作需要访问三个不同的微服务和三个不同的数据库。 订单的创建、库存的扣减、账户扣款在每一个服务和数据库内是一个本地事务，可以保证ACID原则。 但是当我们把三件事情看做一个”业务”，要满足保证“业务”的原子性，要么所有操作全部成功，要么全部失败，不允许出现部分成功部分失败的现象，这就是分布式系统下的事务了。 此时ACID难以满足，这是分布式事务要解决的问题 1.3.演示分布式事务问题我们通过一个案例来演示分布式事务的问题： 1）创建数据库，名为seata_demo，然后导入课前资料提供的SQL文件： 2）导入课前资料提供的微服务： 微服务结构如下： 其中： seata-demo：父工程，负责管理项目依赖 account-service：账户服务，负责管理用户的资金账户。提供扣减余额的接口 storage-service：库存服务，负责管理商品库存。提供扣减库存的接口 order-service：订单服务，负责管理订单。创建订单时，需要调用account-service和storage-service 3）启动nacos、所有微服务 4）测试下单功能，发出Post请求： 请求如下： 1curl --location --request POST 'http://localhost:8082/order?userId=user202103032042012&amp;commodityCode=100202003032041&amp;count=20&amp;money=200' 如图： 测试发现，当库存不足时，如果余额已经扣减，并不会回滚，出现了分布式事务问题。 2.理论基础解决分布式事务问题，需要一些分布式系统的基础知识作为理论指导。 2.1.CAP定理1998年，加州大学的计算机科学家 Eric Brewer 提出，分布式系统有三个指标。 Consistency（一致性） Availability（可用性） Partition tolerance （分区容错性） 它们的第一个字母分别是 C、A、P。 Eric Brewer 说，这三个指标不可能同时做到。这个结论就叫做 CAP 定理。 2.1.1.一致性Consistency（一致性）：用户访问分布式系统中的任意节点，得到的数据必须一致。 比如现在包含两个节点，其中的初始数据是一致的： 当我们修改其中一个节点的数据时，两者的数据产生了差异： 要想保住一致性，就必须实现node01 到 node02的数据 同步： 并发的两个线程，一个写一个读，为了保持一致性，需要在写的时候对存储节点上锁，读线程就要等待。 2.1.2.可用性Availability （可用性）：用户访问集群中的任意健康节点，必须能得到响应，而不是超时或拒绝。 如图，有三个节点的集群，访问任何一个都可以及时得到响应： 当有部分节点因为网络故障或其它原因无法访问时，代表节点不可用： 所有请求都有响应，且不会出现响应超时或响应错误 为了满足这个要求，需要采取异步请求同步数据 2.1.3.分区容错分区容错性是分布式系统具备的基本能力 Partition（分区）：因为网络故障或其它原因导致分布式系统中的部分节点与其它节点失去连接，形成独立分区。 Tolerance（容错）：在集群出现分区时，整个系统也要持续对外提供服务。即一个节点挂掉不影响另一个节点对外提供服务。 2.1.4.矛盾在分布式系统中，系统间的网络不能100%保证健康，一定会有故障的时候，而服务有必须对外保证服务。因此Partition Tolerance不可避免。 当节点接收到新的数据变更时，就会出现问题了： 如果此时要保证一致性，就必须等待网络恢复，完成数据同步后，整个集群才对外提供服务，服务处于阻塞状态，不可用。 如果此时要保证可用性，就不能等待网络恢复，那node01、node02与node03之间就会出现数据不一致。 对于一个分布式系统，分区容错性是必须要有的，在P一定要有的情况下就会涉及网络之间的通信，也就会出现网络的不稳定，也就会出现同步失败，是否枷锁保证数据一致性决定A、C只能取舍一个。 也就是说，在P一定会出现的情况下，A和C之间只能实现一个。 2.2.BASE理论BASE理论是对CAP的一种解决思路，包含三个思想： Basically Available （基本可用）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用。 Soft State（软状态）：在一定时间内，允许出现中间状态，比如临时的不一致状态。 Eventually Consistent（最终一致性）：虽然无法保证强一致性，但是在软状态结束后，最终达到数据一致。 2.3.解决分布式事务的思路分布式事务最大的问题是各个子事务的一致性问题，因此可以借鉴CAP定理和BASE理论，有两种解决思路： AP模式：各子事务分别执行和提交，允许出现结果不一致，然后采用弥补措施恢复数据即可，实现最终一致。 CP模式：各个子事务执行后互相等待，同时提交，同时回滚，达成强一致。但事务等待过程中，处于弱可用状态。 但不管是哪一种模式，都需要在子系统事务之间互相通讯，协调事务状态，也就是需要一个**事务协调者(TC)**： 这里的子系统事务，称为分支事务；有关联的各个分支事务在一起称为全局事务。 3.初识SeataSeata是 2019 年 1 月份蚂蚁金服和阿里巴巴共同开源的分布式事务解决方案。致力于提供高性能和简单易用的分布式事务服务，为用户打造一站式的分布式解决方案。 官网地址：http://seata.io/，其中的文档、播客中提供了大量的使用说明、源码分析。 3.1.Seata的架构Seata事务管理中有三个重要的角色： TC (Transaction Coordinator) - 事务协调者：维护全局和分支事务的状态，协调全局事务提交或回滚。 TM (Transaction Manager) - 事务管理器：定义全局事务的范围、开始全局事务、提交或回滚全局事务。 RM (Resource Manager) - 资源管理器：管理分支事务处理的资源，与TC交谈以注册分支事务和报告分支事务的状态，并驱动分支事务提交或回滚。 整体的架构如图： Seata基于上述架构提供了四种不同的分布式事务解决方案： XA模式：强一致性分阶段事务模式，牺牲了一定的可用性，无业务侵入 TCC模式：最终一致的分阶段事务模式，有业务侵入 AT模式：最终一致的分阶段事务模式，无业务侵入，也是Seata的默认模式 SAGA模式：长事务模式，有业务侵入 无论哪种方案，都离不开TC，也就是事务的协调者。 3.2.部署TC服务参考课前资料提供的文档《 seata的部署和集成.md 》： 3.3.微服务集成Seata我们以order-service为例来演示。 3.3.1.引入依赖首先，在order-service中引入依赖： 123456789101112131415161718&lt;!--seata--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-alibaba-seata&lt;/artifactId&gt; &lt;exclusions&gt; &lt;!--版本较低，1.3.0，因此排除--&gt; &lt;exclusion&gt; &lt;artifactId&gt;seata-spring-boot-starter&lt;/artifactId&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.seata&lt;/groupId&gt; &lt;artifactId&gt;seata-spring-boot-starter&lt;/artifactId&gt; &lt;!--seata starter 采用1.4.2版本--&gt; &lt;version&gt;${seata.version}&lt;/version&gt;&lt;/dependency&gt; 3.3.2.配置TC地址在order-service中的application.yml中，配置TC服务信息，通过注册中心nacos，结合服务名称获取TC地址： 1234567891011121314seata: registry: # TC服务注册中心的配置，微服务根据这些信息去注册中心获取tc服务地址 type: nacos # 注册中心类型 nacos nacos: server-addr: 127.0.0.1:8848 # nacos地址 namespace: \"\" # namespace，默认为空 group: DEFAULT_GROUP # 分组，默认是DEFAULT_GROUP application: seata-tc-server # seata服务名称 username: nacos password: nacos tx-service-group: seata-demo # 事务组名称 service: vgroup-mapping: # 事务组与cluster的映射关系 seata-demo: SH 微服务如何根据这些配置寻找TC的地址呢？ 我们知道注册到Nacos中的微服务，确定一个具体实例需要四个信息： namespace：命名空间 group：分组 application：服务名 cluster：集群名 以上四个信息，在刚才的yaml文件中都能找到： namespace为空，就是默认的public 结合起来，TC服务的信息就是：public@DEFAULT_GROUP@seata-tc-server@SH，这样就能确定TC服务集群了。然后就可以去Nacos拉取对应的实例信息了。 3.3.3.其它服务其它两个微服务也都参考order-service的步骤来做，完全一样。 4.动手实践下面我们就一起学习下Seata中的四种不同的事务模式。 4.1.XA模式XA 规范 是 X/Open 组织定义的分布式事务处理（DTP，Distributed Transaction Processing）标准，XA 规范 描述了全局的TM与局部的RM之间的接口，几乎所有主流的数据库都对 XA 规范 提供了支持。 4.1.1.两阶段提交XA是规范，目前主流数据库都实现了这种规范，实现的原理都是基于两阶段提交。 正常情况： 异常情况： 一阶段： 事务协调者通知每个事物参与者执行本地事务 本地事务执行完成后报告事务执行状态给事务协调者，此时事务不提交，继续持有数据库锁 二阶段： 事务协调者基于一阶段的报告来判断下一步操作 如果一阶段都成功，则通知所有事务参与者，提交事务 如果一阶段任意一个参与者失败，则通知所有事务参与者回滚事务 4.1.2.Seata的XA模型Seata对原始的XA模式做了简单的封装和改造，以适应自己的事务模型，基本架构如图： RM一阶段的工作： ​ ① 注册分支事务到TC ​ ② 执行分支业务sql但不提交 ​ ③ 报告执行状态到TC TC二阶段的工作： TC检测各分支事务执行状态 a.如果都成功，通知所有RM提交事务 b.如果有失败，通知所有RM回滚事务 RM二阶段的工作： 接收TC指令，提交或回滚事务 4.1.3.优缺点XA模式的优点是什么？ 事务的强一致性，满足ACID原则。 常用数据库都支持，实现简单，并且没有代码侵入 XA模式的缺点是什么？ 因为一阶段需要锁定数据库资源，等待二阶段结束才释放，性能较差 依赖关系型数据库实现事务 4.1.4.实现XA模式Seata的starter已经完成了XA模式的自动装配，实现非常简单，步骤如下： 1）修改application.yml文件（每个参与事务的微服务），开启XA模式： 12seata: data-source-proxy-mode: XA 2）给发起全局事务的入口方法添加@GlobalTransactional注解: 本例中是OrderServiceImpl中的create方法. 3）重启服务并测试 重启order-service，再次测试，发现无论怎样，三个微服务都能成功回滚。 4.2.AT模式AT模式同样是分阶段提交的事务模型，不过缺弥补了XA模型中资源锁定周期过长的缺陷。 4.2.1.Seata的AT模型基本流程图： 阶段一RM的工作： 注册分支事务 记录undo-log（数据快照） 执行业务sql并提交 报告事务状态 阶段二提交时RM的工作： 删除undo-log即可 阶段二回滚时RM的工作： 根据undo-log恢复数据到更新前 4.2.2.流程梳理我们用一个真实的业务来梳理下AT模式的原理。 比如，现在又一个数据库表，记录用户余额： id money 1 100 其中一个分支业务要执行的SQL为： 1update tb_account set money = money - 10 where id = 1 AT模式下，当前分支事务执行流程如下： 一阶段： 1）TM发起并注册全局事务到TC 2）TM调用分支事务 3）分支事务准备执行业务SQL 4）RM拦截业务SQL，根据where条件查询原始数据，形成快照。 123{ \"id\": 1, \"money\": 100} 5）RM执行业务SQL，提交本地事务，释放数据库锁。此时 money = 90 6）RM报告本地事务状态给TC 二阶段： 1）TM通知TC事务结束 2）TC检查分支事务状态 ​ a）如果都成功，则立即删除快照 ​ b）如果有分支事务失败，需要回滚。读取快照数据（{\"id\": 1, \"money\": 100}），将快照恢复到数据库。此时数据库再次恢复为100 流程图： 4.2.3.AT与XA的区别简述AT模式与XA模式最大的区别是什么？ XA模式一阶段不提交事务，锁定资源；AT模式一阶段直接提交，不锁定资源。 XA模式依赖数据库机制实现回滚；AT模式利用数据快照实现数据回滚。 XA模式强一致；AT模式最终一致 4.2.4.脏写问题在多线程并发访问AT模式的分布式事务时，有可能出现脏写问题，如图： 解决思路就是引入了全局锁的概念。在释放DB锁之前，先拿到全局锁。避免同一时刻有另外一个事务来操作当前数据。 4.2.5.优缺点AT模式的优点： 一阶段完成直接提交事务，释放数据库资源，性能比较好 利用全局锁实现读写隔离 没有代码侵入，框架自动完成回滚和提交 AT模式的缺点： 两阶段之间属于软状态，属于最终一致 框架的快照功能会影响性能，但比XA模式要好很多 4.2.6.实现AT模式AT模式中的快照生成、回滚等动作都是由框架自动完成，没有任何代码侵入，因此实现非常简单。 只不过，AT模式需要一个表来记录全局锁、另一张表来记录数据快照undo_log。 1）导入数据库表，记录全局锁 导入课前资料提供的Sql文件：seata-at.sql，其中lock_table导入到TC服务关联的数据库，undo_log表导入到微服务关联的数据库： 2）修改application.yml文件，将事务模式修改为AT模式即可： 12seata: data-source-proxy-mode: AT # 默认就是AT 3）重启服务并测试 4.3.TCC模式TCC模式与AT模式非常相似，每阶段都是独立事务，不同的是TCC通过人工编码来实现数据恢复。需要实现三个方法： Try：资源的检测和预留； Confirm：完成资源操作业务；要求 Try 成功 Confirm 一定要能成功。 Cancel：预留资源释放，可以理解为try的反向操作。 4.3.1.流程分析举例，一个扣减用户余额的业务。假设账户A原来余额是100，需要余额扣减30元。 阶段一（ Try ）：检查余额是否充足，如果充足则冻结金额增加30元，可用余额扣除30 初识余额： 余额充足，可以冻结： 此时，总金额 = 冻结金额 + 可用金额，数量依然是100不变。事务直接提交无需等待其它事务。 **阶段二（Confirm)**：假如要提交（Confirm），则冻结金额扣减30 确认可以提交，不过之前可用金额已经扣减过了，这里只要清除冻结金额就好了： 此时，总金额 = 冻结金额 + 可用金额 = 0 + 70 = 70元 **阶段二(Canncel)**：如果要回滚（Cancel），则冻结金额扣减30，可用余额增加30 需要回滚，那么就要释放冻结金额，恢复可用金额： 4.3.2.Seata的TCC模型Seata中的TCC模型依然延续之前的事务架构，如图： 4.3.3.优缺点TCC模式的每个阶段是做什么的？ Try：资源检查和预留 Confirm：业务执行和提交 Cancel：预留资源的释放 TCC的优点是什么？ 一阶段完成直接提交事务，释放数据库资源，性能好 相比AT模型，无需生成快照，无需使用全局锁，性能最强 不依赖数据库事务，而是依赖补偿操作，可以用于非事务型数据库 TCC的缺点是什么？ 有代码侵入，需要人为编写try、Confirm和Cancel接口，太麻烦 软状态，事务是最终一致 需要考虑Confirm和Cancel的失败情况，做好幂等处理 4.3.4.事务悬挂和空回滚1）空回滚 当某分支事务的try阶段阻塞时，可能导致全局事务超时而触发二阶段的cancel操作。在未执行try操作时先执行了cancel操作，这时cancel不能做回滚，就是空回滚。 如图： 执行cancel操作时，应当判断try是否已经执行，如果尚未执行，则应该空回滚。 2）业务悬挂 对于已经空回滚的业务，之前被阻塞的try操作恢复，继续执行try，就永远不可能confirm或cancel ，事务一直处于中间状态，这就是业务悬挂。 执行try操作时，应当判断cancel是否已经执行过了，如果已经执行，应当阻止空回滚后的try操作，避免悬挂 4.3.5.实现TCC模式解决空回滚和业务悬挂问题，必须要记录当前事务状态，是在try、还是cancel？ 1）思路分析 这里我们定义一张表： 12345678CREATE&nbsp;TABLE&nbsp;`account_freeze_tbl`&nbsp;(&nbsp;&nbsp;`xid`&nbsp;varchar(128)&nbsp;NOT&nbsp;NULL,&nbsp;&nbsp;`user_id`&nbsp;varchar(255)&nbsp;DEFAULT&nbsp;NULL&nbsp;COMMENT&nbsp;'用户id',&nbsp;&nbsp;`freeze_money`&nbsp;int(11)&nbsp;unsigned&nbsp;DEFAULT&nbsp;'0'&nbsp;COMMENT&nbsp;'冻结金额',&nbsp;&nbsp;`state`&nbsp;int(1)&nbsp;DEFAULT&nbsp;NULL&nbsp;COMMENT&nbsp;'事务状态，0:try，1:confirm，2:cancel',&nbsp;&nbsp;PRIMARY&nbsp;KEY&nbsp;(`xid`)&nbsp;USING&nbsp;BTREE)&nbsp;ENGINE=InnoDB&nbsp;DEFAULT&nbsp;CHARSET=utf8&nbsp;ROW_FORMAT=COMPACT; 其中： xid：是全局事务id freeze_money：用来记录用户冻结金额 state：用来记录事务状态 那此时，我们的业务开怎么做呢？ Try业务： 记录冻结金额和事务状态到account_freeze表 扣减account表可用金额 Confirm业务 根据xid删除account_freeze表的冻结记录 Cancel业务 修改account_freeze表，冻结金额为0，state为2 修改account表，恢复可用金额 如何判断是否空回滚？ cancel业务中，根据xid查询account_freeze，如果为null则说明try还没做，需要空回滚 如何避免业务悬挂？ try业务中，根据xid查询account_freeze ，如果已经存在则证明Cancel已经执行，拒绝执行try业务 接下来，我们改造account-service，利用TCC实现余额扣减功能。 2）声明TCC接口 TCC的Try、Confirm、Cancel方法都需要在接口中基于注解来声明， 我们在account-service项目中的cn.itcast.account.service包中新建一个接口，声明TCC三个接口： 123456789101112131415161718package cn.itcast.account.service;import io.seata.rm.tcc.api.BusinessActionContext;import io.seata.rm.tcc.api.BusinessActionContextParameter;import io.seata.rm.tcc.api.LocalTCC;import io.seata.rm.tcc.api.TwoPhaseBusinessAction;@LocalTCCpublic interface AccountTCCService { @TwoPhaseBusinessAction(name = \"deduct\", commitMethod = \"confirm\", rollbackMethod = \"cancel\") void deduct(@BusinessActionContextParameter(paramName = \"userId\") String userId, @BusinessActionContextParameter(paramName = \"money\")int money); boolean confirm(BusinessActionContext ctx); boolean cancel(BusinessActionContext ctx);} 3）编写实现类 在account-service服务中的cn.itcast.account.service.impl包下新建一个类，实现TCC业务： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package cn.itcast.account.service.impl;import cn.itcast.account.entity.AccountFreeze;import cn.itcast.account.mapper.AccountFreezeMapper;import cn.itcast.account.mapper.AccountMapper;import cn.itcast.account.service.AccountTCCService;import io.seata.core.context.RootContext;import io.seata.rm.tcc.api.BusinessActionContext;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import org.springframework.transaction.annotation.Transactional;@Service@Slf4jpublic class AccountTCCServiceImpl implements AccountTCCService { @Autowired private AccountMapper accountMapper; @Autowired private AccountFreezeMapper freezeMapper; @Override @Transactional public void deduct(String userId, int money) { // 0.获取事务id String xid = RootContext.getXID(); // 1.扣减可用余额 accountMapper.deduct(userId, money); // 2.记录冻结金额，事务状态 AccountFreeze freeze = new AccountFreeze(); freeze.setUserId(userId); freeze.setFreezeMoney(money); freeze.setState(AccountFreeze.State.TRY); freeze.setXid(xid); freezeMapper.insert(freeze); } @Override public boolean confirm(BusinessActionContext ctx) { // 1.获取事务id String xid = ctx.getXid(); // 2.根据id删除冻结记录 int count = freezeMapper.deleteById(xid); return count == 1; } @Override public boolean cancel(BusinessActionContext ctx) { // 0.查询冻结记录 String xid = ctx.getXid(); AccountFreeze freeze = freezeMapper.selectById(xid); // 1.恢复可用余额 accountMapper.refund(freeze.getUserId(), freeze.getFreezeMoney()); // 2.将冻结金额清零，状态改为CANCEL freeze.setFreezeMoney(0); freeze.setState(AccountFreeze.State.CANCEL); int count = freezeMapper.updateById(freeze); return count == 1; }} 4.4.SAGA模式Saga 模式是 Seata 即将开源的长事务解决方案，将由蚂蚁金服主要贡献。 其理论基础是Hector &amp; Kenneth 在1987年发表的论文Sagas。 Seata官网对于Saga的指南：https://seata.io/zh-cn/docs/user/saga.html 4.4.1.原理在 Saga 模式下，分布式事务内有多个参与者，每一个参与者都是一个冲正补偿服务，需要用户根据业务场景实现其正向操作和逆向回滚操作。 分布式事务执行过程中，依次执行各参与者的正向操作，如果所有正向操作均执行成功，那么分布式事务提交。如果任何一个正向操作执行失败，那么分布式事务会去退回去执行前面各参与者的逆向回滚操作，回滚已提交的参与者，使分布式事务回到初始状态。 Saga也分为两个阶段： 一阶段：直接提交本地事务 二阶段：成功则什么都不做；失败则通过编写补偿业务来回滚 4.4.2.优缺点优点： 事务参与者可以基于事件驱动实现异步调用，吞吐高 一阶段直接提交事务，无锁，性能好 不用编写TCC中的三个阶段，实现简单 缺点： 软状态持续时间不确定，时效性差 没有锁，没有事务隔离，会有脏写 4.5.四种模式对比我们从以下几个方面来对比四种实现： 一致性：能否保证事务的一致性？强一致还是最终一致？ 隔离性：事务之间的隔离性如何？ 代码侵入：是否需要对业务代码改造？ 性能：有无性能损耗？ 场景：常见的业务场景 如图： 5.高可用Seata的TC服务作为分布式事务核心，一定要保证集群的高可用性。 5.1.高可用架构模型搭建TC服务集群非常简单，启动多个TC服务，注册到nacos即可。 但集群并不能确保100%安全，万一集群所在机房故障怎么办？所以如果要求较高，一般都会做异地多机房容灾。 比如一个TC集群在上海，另一个TC集群在杭州： 微服务基于事务组（tx-service-group)与TC集群的映射关系，来查找当前应该使用哪个TC集群。当SH集群故障时，只需要将vgroup-mapping中的映射关系改成HZ。则所有微服务就会切换到HZ的TC集群了。 5.2.实现高可用具体实现请参考课前资料提供的文档《seata的部署和集成.md》： 分布式缓存-Redis集群– 基于Redis集群解决单机Redis存在的问题 单机的Redis存在四大问题： 1.Redis持久化Redis有两种持久化方案： RDB持久化 AOF持久化 1.1.RDB持久化RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。快照文件称为RDB文件，默认是保存在当前运行目录。 1.1.1.执行时机RDB持久化在四种情况下会执行： 执行save命令 执行bgsave命令 Redis停机时 触发RDB条件时 1）save命令 执行下面的命令，可以立即执行一次RDB： save命令会导致主进程执行RDB，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。 2）bgsave命令 下面的命令可以异步执行RDB： 这个命令执行后会开启独立进程完成RDB，主进程可以持续处理用户请求，不受影响。 3）停机时 Redis停机时会执行一次save命令，实现RDB持久化。 4）触发RDB条件 Redis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下： 1234# 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save \"\" 则表示禁用RDBsave 900 1 save 300 10 save 60 10000 RDB的其它配置也可以在redis.conf文件中设置： 12345678# 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱rdbcompression yes# RDB文件名称dbfilename dump.rdb # 文件保存的路径目录dir ./ 1.1.2.RDB原理bgsave开始时会fork主进程得到子进程，子进程共享主进程的内存数据。完成fork后读取内存数据并写入 RDB 文件。 fork采用的是copy-on-write技术： 当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作。 1.1.3.小结RDB方式bgsave的基本流程？ fork主进程得到一个子进程，共享内存空间 子进程读取内存数据并写入新的RDB文件 用新RDB文件替换旧的RDB文件 RDB会在什么时候执行？save 60 1000代表什么含义？ 默认是服务停止时 代表60秒内至少执行1000次修改则触发RDB RDB的缺点？ RDB执行间隔时间长，两次RDB之间写入数据有丢失的风险 fork子进程、压缩、写出RDB文件都比较耗时 1.2.AOF持久化1.2.1.AOF原理AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 1.2.2.AOF配置AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF： 1234# 是否开启AOF功能，默认是noappendonly yes# AOF文件的名称appendfilename \"appendonly.aof\" AOF的命令记录的频率也可以通过redis.conf文件来配： 123456# 表示每执行一次写命令，立即记录到AOF文件appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘appendfsync no 三种策略对比： 1.2.3.AOF文件重写因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。 如图，AOF原本有三个命令，但是set num 123 和 set num 666都是对num的操作，第二次会覆盖第一次的值，因此第一个命令记录下来没有意义。 所以重写命令后，AOF文件内容就是：mset name jack num 666 Redis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置： 1234# AOF文件比上次文件 增长超过多少百分比则触发重写auto-aof-rewrite-percentage 100# AOF文件体积最小多大以上才触发重写 auto-aof-rewrite-min-size 64mb 1.3.RDB与AOF对比RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。 2.Redis主从2.1.搭建主从架构单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。 具体搭建流程参考课前资料《Redis集群.md》： 2.2.主从数据同步原理2.2.1.全量同步主从第一次建立连接时，会执行全量同步，将master节点的所有数据都拷贝给slave节点，流程： 这里有一个问题，master如何得知salve是第一次来连接呢？？ 有几个概念，可以作为判断依据： Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。 因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。 master判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。 master会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。 因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。 如图： 完整流程描述： slave节点请求增量同步 master节点判断replid，发现不一致，拒绝增量同步 master将完整内存数据生成RDB，发送RDB到slave slave清空本地数据，加载master的RDB master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave slave执行接收到的命令，保持与master之间的同步 2.2.2.增量同步全量同步需要先做RDB，然后将RDB文件通过网络传输个slave，成本太高了。因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步。 什么是增量同步？就是只更新slave与master存在差异的部分数据。如图： 那么master怎么知道slave与自己的数据差异在哪里呢? 2.2.3.repl_backlog原理master怎么知道slave与自己的数据差异在哪里呢? 这就要说到全量同步时的repl_baklog文件了。 这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。 repl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset： slave与master的offset之间的差异，就是salve需要增量拷贝的数据了。 随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset： 直到数组被填满： 此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。 但是，如果slave出现网络阻塞，导致master的offset远远超过了slave的offset： 如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖： 棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。 2.3.主从同步优化主从同步可以保证主从数据的一致性，非常重要。 可以从以下几个方面来优化Redis主从就集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。 Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 主从从架构图： 2.4.小结简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave。 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 3.Redis哨兵Redis提供了哨兵（Sentinel）机制来实现主从集群的自动故障恢复。 3.1.哨兵原理3.1.1.集群结构和作用哨兵的结构如图： 哨兵的作用如下： 监控：Sentinel 会不断检查您的master和slave是否按预期工作 自动故障恢复：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主 通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端 3.1.2.集群监控原理Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令： •主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。 •客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。 3.1.3.集群故障恢复原理一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的： 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 最后是判断slave节点的运行id大小，越小优先级越高。 当选出一个新的master后，该如何实现切换呢？ 流程如下： sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master sentinel给所有其它slave发送slaveof 192.168.150.101 7002 命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点 3.1.4.小结Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no one 然后让所有节点都执行slaveof 新master 修改故障节点配置，添加slaveof 新master 3.2.搭建哨兵集群具体搭建流程参考课前资料《Redis集群.md》： 3.3.RedisTemplate在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化，及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。 下面，我们通过一个测试来实现RedisTemplate集成哨兵机制。 3.3.1.导入Demo工程首先，我们引入课前资料提供的Demo工程： 3.3.2.引入依赖在项目的pom文件中引入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 3.3.3.配置Redis地址然后在配置文件application.yml中指定redis的sentinel相关信息： 12345678spring: redis: sentinel: master: mymaster nodes: - 192.168.150.101:27001 - 192.168.150.101:27002 - 192.168.150.101:27003 3.3.4.配置读写分离在项目的启动类中，添加一个新的bean： 1234@Beanpublic LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer(){ return clientConfigurationBuilder -&gt; clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED);} 这个bean中配置的就是读写策略，包括四种： MASTER：从主节点读取 MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica REPLICA：从slave（replica）节点读取 REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master 4.Redis分片集群4.1.搭建分片集群主从和哨兵可以解决高可用、高并发读的问题。但是依然有两个问题没有解决： 海量数据存储问题 高并发写的问题 使用分片集群可以解决上述问题，如图: 分片集群特征： 集群中有多个master，每个master保存不同数据 每个master都可以有多个slave节点 master之间通过ping监测彼此健康状态 客户端请求可以访问集群任意节点，最终都会被转发到正确节点 具体搭建流程参考课前资料《Redis集群.md》： 4.2.散列插槽4.2.1.插槽原理Redis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到： 数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况： key中包含”{}”，且“{}”中至少包含1个字符，“{}”中的部分是有效部分 key中不包含“{}”，整个key都是有效部分 例如：key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。 如图，在7001这个节点执行set a 1时，对a做hash运算，对16384取余，得到的结果是15495，因此要存储到103节点。 到了7003后，执行get num时，对num做hash运算，对16384取余，得到的结果是2765，因此需要切换到7001节点 4.2.1.小结Redis如何判断某个key应该在哪个实例？ 将16384个插槽分配到不同的实例 根据key的有效部分计算哈希值，对16384取余 余数作为插槽，寻找插槽所在实例即可 如何将同一类数据固定的保存在同一个Redis实例？ 这一类数据使用相同的有效部分，例如key都以{typeId}为前缀 4.3.集群伸缩redis-cli –cluster提供了很多操作集群的命令，可以通过下面方式查看： 比如，添加节点的命令： 4.3.1.需求分析需求：向集群中添加一个新的master节点，并向其中存储 num = 10 启动一个新的redis实例，端口为7004 添加7004到之前的集群，并作为一个master节点 给7004节点分配插槽，使得num这个key可以存储到7004实例 这里需要两个新的功能： 添加一个节点到集群中 将部分插槽分配到新插槽 4.3.2.创建新的redis实例创建一个文件夹： 1mkdir 7004 拷贝配置文件： 1cp redis.conf /7004 修改配置文件： 1sed /s/6379/7004/g 7004/redis.conf 启动 1redis-server 7004/redis.conf 4.3.3.添加新节点到redis添加节点的语法如下： 执行命令： 1redis-cli --cluster add-node 192.168.150.101:7004 192.168.150.101:7001 通过命令查看集群状态： 1redis-cli -p 7001 cluster nodes 如图，7004加入了集群，并且默认是一个master节点： 但是，可以看到7004节点的插槽数量为0，因此没有任何数据可以存储到7004上 4.3.4.转移插槽我们要将num存储到7004节点，因此需要先看看num的插槽是多少： 如上图所示，num的插槽为2765. 我们可以将0~3000的插槽从7001转移到7004，命令格式如下： 具体命令如下： 建立连接： 得到下面的反馈： 询问要移动多少个插槽，我们计划是3000个： 新的问题来了： 那个node来接收这些插槽？？ 显然是7004，那么7004节点的id是多少呢？ 复制这个id，然后拷贝到刚才的控制台后： 这里询问，你的插槽是从哪里移动过来的？ all：代表全部，也就是三个节点各转移一部分 具体的id：目标节点的id done：没有了 这里我们要从7001获取，因此填写7001的id： 填完后，点击done，这样插槽转移就准备好了： 确认要转移吗？输入yes： 然后，通过命令查看结果： 可以看到： 目的达成。 4.4.故障转移集群初识状态是这样的： 其中7001、7002、7003都是master，我们计划让7002宕机。 4.4.1.自动故障转移当集群中有一个master宕机会发生什么呢？ 直接停止一个redis实例，例如7002： 1redis-cli -p 7002 shutdown 1）首先是该实例与其它实例失去连接 2）然后是疑似宕机： 3）最后是确定下线，自动提升一个slave为新的master： 4）当7002再次启动，就会变为一个slave节点了： 4.4.2.手动故障转移利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。其流程如下： 这种failover命令可以指定三种模式： 缺省：默认的流程，如图1~6歩 force：省略了对offset的一致性校验 takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见 案例需求：在7002这个slave节点执行手动故障转移，重新夺回master地位 步骤如下： 1）利用redis-cli连接7002这个节点 2）执行cluster failover命令 如图： 效果： 4.5.RedisTemplate访问分片集群RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致： 1）引入redis的starter依赖 2）配置分片集群地址 3）配置读写分离 与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下： 12345678910spring: redis: cluster: nodes: - 192.168.150.101:7001 - 192.168.150.101:7002 - 192.168.150.101:7003 - 192.168.150.101:8001 - 192.168.150.101:8002 - 192.168.150.101:8003","link":"/2024/01/04/%E4%B8%AD%E9%97%B4%E4%BB%B6/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%9F%BA%E7%A1%80/"},{"title":"MySQL实践","text":"建表规约索引规约DQL语句count[强制]禁止使用 count(列名)或 count(常量)来替代 count(*) 因为COUNT(*)是SQL92定义的标准统计行数的语法，所以MySQL对他进行了很多优化，MyISAM中会直接把表的总行数单独记录下来供COUNT(*)查询，而InnoDB则会在扫表的时候选择最小的索引来降低成本。当然，这些优化的前提都是没有进行where和group的条件查询。 在InnoDB中COUNT(*)和COUNT(1)实现上没有区别，而且效率一样，但是COUNT(字段)需要进行字段的非NULL判断，所以效率会低一些。 总结: 因为COUNT(*)是SQL92定义的标准统计行数的语法，并且效率高，所以请直接使用COUNT(*)查询表的行数！ count(distinct col)count(distinct col) 计算该列除NULL之外的不重复行数, 注意 count(distinct col1, col2) 如果其中一列全为NULL，那么即使另一列有不同的值，也返回为0。 sum使用sum()时需注意NPE(空指针)问题当某一列的值全是NULL时，count(col)的返回结果为0，但sum(col)的返回结果为NULL，因此使用sum()时需注意NPE问题。 正例：可以使用如下方式来避免sum的NPE问题： 1select if(isnull(sum(g)), 0, sum(g)) from table 使用ISNULL()来判断是否为NULL值说明：NULL与任何值的直接比较都为NULL。 1） NULL&lt;&gt;NULL的返回结果是NULL，而不是false。2） NULL=NULL的返回结果是NULL，而不是true。3） NULL&lt;&gt;1的返回结果是NULL，而不是true。 select在代码中写分页查询逻辑时，若count为0应直接返回，避免执行后面的分页语句。一般做分页查询需要从前端获取 page 和limit, 根据offset = (page-1)*limit得到偏移量, 然后执行sql计算符合条件的个数并进行判断. 快速判断数据库中某条数据是否存在实际项目中经常会遇到判断数据是否存在的情况。一般都是直接查询数据库，查到认为存在，否则就是不存在。 1SELECT * FROM `t_user` where username='tw' 这种实现方式没有问题，但是如果只是单纯的判断数据是否存在，似乎有点小题大做。 1SELECT count(*) FROM `t_user` where username='tw' limit 1 加上LIMIT 1，只要找到了对应的一条记录，就不会继续向下扫描了，效率会大大提高。 LIMIT 1适用于查询结果为1条（也可能为0）会导致全表扫描的的SQL语句。 如果username是索引的话，就不需要加上LIMIT 1，如果是根据主键查询一条记录也不需要LIMIT 1，主键也是索引。 【推荐】利用延迟关联或者子查询优化超多分页场景。前端界面通常会展示第N页，每次点击显示当页的数据，这个分页能力由后端提供，前端提供页号[pageNum]和每页的数据量[pageSize]到达后端转换为 limit （pageNum-1）*PageSize,pageSize MySQL并不是跳过offset行，而是取offset+N行，然后返回放弃前offset行，返回N行，那当offset特别大的时候，效率就非常的低下，要么控制返回的总页数，要么对超过特定阈值的页数进行SQL改写。 针对分页优化有两种方法: 自查询利用覆盖索引先得到查询的id,然后利用连接查询得到最终结果 123SELECT a.* FROM 表1 as a, (select id from 表1 where 条件LIMIT 100000,20 ) as b where a.id=b.idSELECT a.* FROM 表1 as a inner join (select id from 表1 where 条件LIMIT 100000,20) as b on a.id=b.id 由前端传上一页的最后一个id 1234select *from tablewhere id&gt;xxxlimit pageSize 但不适合跨页的查询 约束whereis null、and、or的顺序优先级: is null＞and ＞or DDL语句索引修改表一次添加多个列(字段)和索引1ALTER TABLE table_name ADD func varchar(50), ADD gene varchar(50), ADD genedetail varchar(50); 【推荐】建组合索引的时候，区分度最高的在最左边正例：如果where a=? and b=? ，a列的几乎接近于唯一值，那么只需要单建idx_a索引即可。说明：存在非等号和等号混合判断条件时，在建索引时，请把等号条件的列前置。如：where a&gt;? and b=? 那么即使a的区分度更高，也必须把b放在索引的最前列。 【推荐】 SQL性能优化的目标：至少要达到 range 级别，要求是ref级别，如果可以是consts最好。说明：1）consts 单表中最多只有一个匹配行（主键或者唯一索引），在优化阶段即可读取到数据。2）ref 指的是使用普通的索引（normal index）。3）range 对索引进行范围检索。反例：explain表的结果，type=index，索引物理文件全扫描，速度非常慢，这个index级别比较range还低，与全表扫描[ALL]是小巫见大巫。 建表DEFAULT CHARSETmysql中有utf8和utf8mb4两种编码，在mysql中请大家忘记utf8，永远使用utf8mb4[most byte 4]。这是mysql的一个遗留问题，mysql中的utf8最多只能支持3bytes长度的字符编码，对于一些需要占据4bytes的文字，mysql的utf8就不支持了，要使用utf8mb4才行。 具体体现: MySQL utf8无法显示生僻字与Emoji表情符号，utf8mb4可以。 COLLATECOLLATE会影响到ORDER BY语句的顺序，会影响到WHERE条件中大于小于号筛选出来的结果，会影响DISTINCT、GROUP BY、HAVING语句的查询结果。另外，mysql建索引的时候，如果索引列是字符类型，也会影响索引创建，只不过这种影响我们感知不到。总之，凡是涉及到字符类型比较或排序的地方，都会和COLLATE有关。 具体体现: utf8mb4_unicode_ci 准确更符合西方国家的字母但比较慢utf8mb4_general_ci 速度快但相对不准确。如，德语字母“ß”，在utf8mb4_unicode_ci中是等价于\"ss\"两个字母的（这是符合德国人习惯的做法），而在utf8mb4_general_ci中，它却和字母“s”等价。 设置级别及其优先级设置COLLATE可以在示例级别、库级别、表级别、列级别、以及SQL指定。如果全都显示设置了，那么优先级顺序是 SQL语句 &gt; 列级别设置 &gt; 表级别设置 &gt; 库级别设置 &gt; 实例级别设置。同时也是依次继承,若没有列级别设置就会继承表级别设置 库级别设置 1CREATE DATABASE &lt;db_name&gt; DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; 表级别设置 1234567CREATE TABLE (……) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci; 列级别设置 1234567CREATE TABLE (`field1` VARCHAR（64） CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci NOT NULL DEFAULT '',……) …… SQL查询的时候显示声明 1SELECT field1, field2 FROM table1 ORDER BY field1 COLLATE utf8mb4_unicode_ci; 【强制】表达是与否概念的字段，必须使用is_xxx的方式命名，数据类型是unsigned tinyint（ 1表示是，0表示否）。说明：任何字段如果为非负数，必须是unsigned。正例：表达逻辑删除的字段名is_deleted，1表示删除，0表示未删除。 【强制】表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。 说明：MySQL在Windows下不区分大小写，但在Linux下默认是区分大小写。因此，数据库名、表名、字段名，都不允许出现任何大写字母，避免节外生枝。正例：aliyun_admin，rdc_config，level3_name反例：AliyunAdmin，rdcConfig，level_3_name 【强制】主键索引名为pk字段名；唯一索引名为uk字段名；普通索引名则为idx字段名。说明：pk 即primary key；uk 即 unique key；idx 即index的简称。 【强制】varchar是可变长字符串，不预先分配存储空间，长度不要超过5000，如果存储长度大于此值，定义字段类型为text，独立出来一张表，用主键来对应，避免影响其它字段索引效率。1、因为mysql 是行存储模式，所以会把整行读取出来。text 储存了大量的数据。读取时，占了大量的io。所以会十分的慢。 2、每行的数据过大或发生行溢出 InnoDB 会将一些大对象数据存放在数据页之外的 BLOB 页中，然后在查询时根据指针去对应的 BLOB 页中查询 【推荐】单表行数超过500万行或者单表容量超过2GB，才推荐进行分库分表。说明：如果预计三年后的数据量根本达不到这个级别，请不要在创建表时就分库分表","link":"/2024/01/12/%E6%95%B0%E6%8D%AE%E5%BA%93/MySQL%E5%AE%9E%E8%B7%B5/"},{"title":"mysql部署","text":"实现思路如下： 1）拉取MySQL镜像: docker pull mysql:5.7 2）创建目录/tmp/mysql/data 3）创建目录/tmp/mysql/conf，新建文件hmy.cnf, 内容如下: 12345[mysqld]skip-name-resolvecharacter_set_server=utf8datadir=/var/lib/mysqlserver-id=1000 4）去DockerHub查阅资料，创建并运行MySQL容器，要求： ① 挂载/tmp/mysql/data到mysql容器内数据存储目录 ② 挂载/tmp/mysql/conf/hmy.cnf到mysql容器的配置文件 ③ 设置MySQL密码 1docker run --name mysql5.7 -v /root/mysql5.7/conf:/etc/mysql/conf.d -v /root/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d -p 3305:3306 mysql:5.7.25","link":"/2023/11/05/%E6%95%B0%E6%8D%AE%E5%BA%93/docker%E9%83%A8%E7%BD%B2mysql/"},{"title":"mysql中FIND_IN_SET()函数","text":"语法FIND_INSET(str, strlist) str: 要查询的字符串 strlist: 字段名 【数据以,分割】 查询字段strlist中包含str的结果，返回结果为null或记录 使用代替in代替like背景在进行策略配置时，假如配置多个策略，利用,进行分割，如a,b,c，以string格式存放在config字段中。 当我们需要查询a命中哪几个策略时，可以用FIND_IN_SET来简化like like模糊查询需要考虑只有a、a在第一个、a在中间和a在最后这四种情况。 1234select * from table where config=a;select * from table where config like concat(a, ',%');select * from table where config like concat('%,', a);select * from table where config like concat('%,', a, ',%'); FIND_IN_SET1select * from table where FIND_IN_SET(a, config); 在字段config中看哪些记录有a 注意：mysql字符串函数 find_in_set(str1,str2)函数是返回str2中str1所在的位置索引，str2必须以”,”分割开。 总结：like是广泛的模糊匹配，字符串中没有分隔符，Find_IN_SET 是精确匹配，字段值以英文”,”分隔，Find_IN_SET查询的结果要小于like查询的结果。","link":"/2024/05/31/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql%E4%B8%ADFindInSet%E5%87%BD%E6%95%B0/"},{"title":"子查询中使用order by失效问题","text":"背景利用子查询优化超多分页场景: 如下所示 自查询利用覆盖索引先得到查询的id,然后利用连接查询得到最终结果 123SELECT a.* FROM 表1 as a, (select id from 表1 where 条件LIMIT 100000,20 ) as b where a.id=b.idSELECT a.* FROM 表1 as a inner join (select id from 表1 where 条件LIMIT 100000,20) as b on a.id=b.id 问题当在自查询中使用order by对id排序, 会发生order by失效的情况 高版本的MySQL（5.6以上）在子查询中使用order by 语句后查询结果并不会显示排序后的结果。 原因是: 当子查询的order by语句后面没有limit关键字时，数据库会自动优化，即忽略order by语句。 但是我在分页场景下发现, 假如pagelimit是10, 数据量在[1, 4]时, 即使加上limit, order by也会失效, 但是在[5,9]时, order by不会失效. 1SELECT a.* FROM 表1 as a, (select id from 表1 where 条件 order by id desc LIMIT 100000,20 ) as b where a.id=b.id 解决我的解决方案是直接把order by id从自查询中拿到外面, 效果其实也应该是一样的. 1SELECT a.* FROM 表1 as a, (select id from 表1 where 条件 LIMIT 100000,20 ) as b where a.id=b.id order by id desc","link":"/2024/05/21/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%AD%90%E6%9F%A5%E8%AF%A2%E4%B8%AD%E4%BD%BF%E7%94%A8order%20by%E5%A4%B1%E6%95%88%E9%97%AE%E9%A2%98/"},{"title":"二分法","text":"一、查找一个数1. 左闭右闭1234567891011121314151617class Solution { public int search(int[] nums, int target) { int left = 0, right = nums.length-1;//1 while(left&lt;=right){//2 int mid = left + (right-left)/2; if(nums[mid] == target){ return mid;//3 }else if(nums[mid] &lt; target){ left = mid + 1; }else if(nums[mid] &gt; target){ right = mid - 1; } } return -1; }} 2. 左闭右开1234567891011121314151617class Solution { public int search(int[] nums, int target) { int left = 0, right = nums.length; //1 while(left&lt;right){ //2 int mid = left + (right-left)/2; if(nums[mid] == target){ return mid; }else if(nums[mid] &lt; target){ left = mid + 1; }else if(nums[mid] &gt; target){ right = mid; //3 } } return -1; }} 总结：细节有三处 右边界的初始化 while 终止条件 右边界赋值 盯着区间考虑即可 while 循环终止条件：搜索区间为空的时候应该终止，意味着没找到 算法缺陷： 有多个匹配的值，但是该算法只能找到中间那个匹配值 二、 寻找左侧边界的二分搜索思路：在找到 target 的时候缩小「搜索区间」的上界，即不断向左收缩，达到锁定左侧边界的目的。 123456789101112131415int findleft(int[] nums, int target){ int left = 0, right = nums.length-1; while(left&lt;=right){ int mid = left + (right - left)/2; if(nums[mid] == target){ right = mid-1; }else if(nums[mid] &gt; target){ right = mid-1; }else if(nums[mid] &lt; target){ left = mid+1; } } if(left&lt;0 || left&gt;=nums.length) return -1; return nums[left]==target? left : -1;} 最后找到匹配值后 right=mid-1所以最后要用left 来定位 三、 寻找右侧边界的二分搜索思路：在找到 target 的时候缩小「搜索区间」的下界，即不断向右收缩，达到锁定右侧边界的目的。 123456789101112131415int findright(int[] nums, int target){ int left = 0, right = nums.length-1; while(left&lt;=right){ int mid = left + (right - left)/2; if(nums[mid] == target){ left = mid+1; }else if(nums[mid] &gt; target){ right = mid-1; }else if(nums[mid] &lt; target){ left = mid+1; } } if(right&lt;0 || right&gt;=nums.length) return -1; return nums[right]==target? right : -1;} 最后找到匹配值后 left=mid+1所以最后要用right 来定位 总结1、分析二分查找代码时，不要出现 else，全部展开成 else if 方便理解。 2、注意「搜索区间」和 while 的终止条件，如果存在漏掉的元素，记得在最后检查。 3、如需定义左闭右开的「搜索区间」搜索左右边界，只要在 nums[mid] == target 时做修改即可，搜索右侧时需要减一。 4、如果将「搜索区间」全都统一成两端都闭，好记，只要稍改 nums[mid] == target 条件处的代码和返回的逻辑即可，推荐拿小本本记下，作为二分搜索模板。 最后我想说，以上二分搜索的框架属于「术」的范畴，如果上升到「道」的层面，二分思维的精髓就是：通过已知信息尽可能多地收缩（折半）搜索空间，从而增加穷举效率，快速找到目标 相关题目69.x 的平方根123456789101112131415161718192021222324252627282930313233343536373839404142//解法1//直接套二分查找模板，最后 mid*mid &gt; x right-1 然后退出循环则 right 为所需值class Solution { public int mySqrt(int x) { if(x == 0) return 0; if(x == 1) return 1; int left = 0, right = x; while(left&lt;=right){ int mid = left + (right-left)/2; int s = x / mid; //防止溢出 if(mid == s) return mid; else if(mid &lt; s){ left = mid + 1; }else if(mid &gt; s){ right = mid - 1; } } return right; }}//解法 2//二分查找的下界为 0,上界可以粗略地设定为 x。在二分查找的每一步中，我们只需要比较中间元素 mid 的平方与x 的大小关系，并通过比较的结果调整上下界的范围。由于我们所有的运算都是整数运算，不会存在误差，因此在得到最终的答案 ans 后，也就不需要再去尝试 ans+1class Solution { public int mySqrt(int x) { int ans = -1; if(x == 0) return 0; if(x == 1) return 1; int left = 0, right = x; while(left&lt;=right){ int mid = left + (right-left)/2; int s = x / mid; //防止溢出 if(mid &lt;= s){ ans = mid; left = mid + 1; }else if(mid &gt; s){ right = mid - 1; } } return ans; }} 367. 有效的完全平方数1234567891011121314151617181920class Solution { public boolean isPerfectSquare(int x) { if(x == 1) return true; int left = 1, right = x; while(left&lt;=right){ int mid = left + (right-left)/2; int s = x / mid; //防止溢出 if(mid == s){ if(x % mid == 0) return true; left = mid + 1; // 说明有/把小数抹掉了 mid * mid &gt; x } else if(mid &lt; s){ left = mid + 1; }else if(mid &gt; s){ right = mid - 1; } } return false; }} 240. 搜索二维矩阵 II 在一个二维数组array中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 [ [1,2,8,9],[2,4,9,12],[4,7,10,13],[6,8,11,15] ] 给定 target = 7，返回 true。 给定 target = 3，返回 false。 数据范围：矩阵的长宽满足 0≤n,m≤500 ， 矩阵中的值满足 0≤val≤109进阶：空间复杂度 O(1) ，时间复杂度 O(n+m) 二分 思路与算法 由于矩阵 matrix中每一行的元素都是升序排列的，因此我们可以对每一行都使用一次二分查找，判断 target是否在该行中，从而判断 target 是否出现。 123456789101112131415161718192021222324252627class Solution { public boolean searchMatrix(int[][] matrix, int target) { for (int[] row : matrix) { int index = search(row, target); if (index &gt;= 0) { return true; } } return false; } public int search(int[] nums, int target) { int low = 0, high = nums.length - 1; while (low &lt;= high) { int mid = (high - low) / 2 + low; int num = nums[mid]; if (num == target) { return mid; } else if (num &gt; target) { high = mid - 1; } else { low = mid + 1; } } return -1; }} z字型 解题思路：利用二维数组行列递增特性主要思路： 由于行列递增，可以得出：a.在一列中的某个数字，其上的数字都比它小b.在一行中的某个数字，其右的数字都比它大 搜索流程：a.首先从数组左下角搜索.b.如果当前数字大于target,那么查找往上移一位,如果当前数字小于target,那么查找往右移一位。c.查找到target,返回true; 如果越界，返回false; 123456789101112131415161718192021222324252627public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param target int整型 * @param array int整型二维数组 * @return bool布尔型 */ public boolean Find (int target, int[][] array) { // write code here int row = array.length; int col = array[0].length; for(int i = row-1, j = 0; i&gt;=0 &amp;&amp; j&lt;col; ){ if(array[i][j] == target){ return true; }else if(array[i][j] &lt; target){ j++; }else if(array[i][j] &gt; target){ i--; } } return false; }} 162. 寻找峰值 给定一个长度为n的数组nums，请你找到峰值并返回其索引。数组可能包含多个峰值，在这种情况下，返回任何一个所在位置即可。 1.峰值元素是指其值严格大于左右相邻值的元素。严格大于即不能有等于 2.假设 nums[-1] = nums[n] = −∞−∞ 3.对于所有有效的 i 都有 nums[i] != nums[i + 1] 4.你可以使用O(logN)的时间复杂度实现此问题吗？ 思路： 因为题目将数组边界看成最小值，而我们只需要找到其中一个波峰，因此只要不断地往高处走，一定会有波峰。那我们可以每次找一个标杆元素，将数组分成两个区间，每次就较高的一边走，因此也可以用分治来解决，而标杆元素可以选择区间中点。 1234567891011121314151617181920212223public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param nums int整型一维数组 * @return int整型 */ public int findPeakElement (int[] nums) { // write code here //往高处走 int l = 0, r = nums.length-1; while(l&lt;r){ int mid = l + ((r-l)&gt;&gt;1)/2; if(nums[mid] &gt; nums[mid+1]){ r = mid; }else{ l = mid+1; } } return r; }} 215. 数组中的第K个最大元素TopK，得到答案并不难，但不断优化的过程，挺艰难。 1题目描述:``有一个整数数组，请你根据快速排序的思路，找出数组中第K大的数。``给定一个整数数组a,同时给定它的大小n和要找的K(K在``1``到n之间)，请返回第K大的数，保证答案存在。 1.全局排序，时间复杂度取决于排序算法，一般是 O(n*lgn)。 相信大多数朋友看到这题的思路就是排序，返回第k大的值，甚至还有小机灵鬼直接调用内置方法 1234public int findKth(int[] a, int n, int K) { Arrays.sort(a); return a[n-K];} 2.局部排序，只排序TopK个数，O(n*k)，冒泡、直接选择、直接插入都可以，但k的取值趋近n时，时间复杂度又趋近与O(n^2)。 12345678910111213public int findKth(int[] a, int n, int K){ // 冒泡k次 for (int i = 0; i &lt; K; i++) { for (int j = 0; j &lt; n - 1 - i; j++) { if (arr[j] &gt; arr[j + 1]) { int temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; } } } return arr[n - K];} 3.堆，TopK个数也不排序了，O(n*lg(k)),想练手的可以模仿堆排序手动维护小根堆。也可以使用最小优先队列。 4.快速排序的思想–随机选择法，时间复杂度 O(n) 需要理解两个思想，快排的分治，二分查找的剪枝 分治法，每个分支“都要”递归，例如：快速排序，O(n*lg(n)) 减治法，“只要”递归一个分支，例如：二分查找O(lg(n))，随机选择O(n) TopK的另一个解法：随机选择 + partition 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param a int整型一维数组 * @param n int整型 * @param K int整型 * @return int整型 */ public int findKth (int[] a, int n, int K) { // write code here //二分查找全局 // quickSort(a, 0, n-1); //partition+二分 return binaryFind(a, 0, a.length-1, K); } int binaryFind(int[] a, int h, int t,int K){ //找到 i 小的 idx int i = partition(a, h, t); if(i &lt; a.length-K){ return binaryFind(a, i+1, t, K); }else if(i &gt; a.length-K){ return binaryFind(a, h, i-1, K); }else{ return a[i]; } } void quickSort(int[] a, int h, int t){ if(h&gt;=t) return; int i = partition(a, h, t); // if(i&lt;) quickSort(a, h, i-1); quickSort(a, i+1, t); } int partition(int[] a, int h, int t){ /** * Math.random()得到[0,1) * Math.random()*(t-h+1)得到[0, t-h] */ int partationIdx = (int)(Math.random()*(t-h+1)+h); int partation = a[partationIdx]; swap(a, h, partationIdx); int i=h, j=t; while(i&lt;j){ while(i&lt;j &amp;&amp; a[j]&gt;=partation) j--; swap(a, i, j); while(i&lt;j &amp;&amp; a[i]&lt;=partation) i++; swap(a, i, j); } return i; } void swap(int[] num, int i, int j){ int temp = num[i]; num[i] = num[j]; num[j] = temp; }} Math.random()得到随机范围值 1234567891011121314151617// 生成 [0, 1) 的随机数double random = Math.random();// 生成 [0, max) 的随机数Math.random() * n;// 例如：生成 [0, 6) 的随机数double t1 = Math.random() * 6;// 生成 [min, max) 的随机数Math.random() * (max - min) + min;// 例如：生成 [6, 23) 的随机数double t2 = Math.random() * (23 - 6) + 6;// 生成 [min, max] 的随机数Math.random() * (max - min + 1) + min;// 例如：生成 [8, 24] 的随机数double t3 = Math.random() * (24 - 8 + 1) + 8; 表达式求值算法：栈 1.用栈保存各部分计算的和 2.遍历表达式，使用 sign 变量记录运算符，初始化是 ‘+’；使用 number 变量记录字符串中的数字部分的数字值是多少 2.0 遇到空格时跳过 2.1 遇到数字时继续遍历求这个完整的数字的值，保存到 number 中 2.2 遇到左括号时递归求这个括号里面的表达式的值 先遍历找到对应的右括号，因为可能里面还嵌有多对括号，使用一个变量 counterPartition 统计括号对数直到变量为 0 2.3 遇到运算符时或者到表达式末尾时，就去计算上一个运算符并把计算结果 push 进栈，然后保存新的运算符到 sign 如果是 + ，不要计算，push 进去 如果是 - ，push 进去负的当前数 如果是 ×、÷ ，pop 出一个运算数和当前数作计算 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * 返回表达式的值 * @param s string字符串 待计算的表达式 * @return int整型 */ public int solve (String s) { // write code here //数据格式转换 char[] charArray = s.toCharArray(); Stack&lt;Integer&gt; stack = new Stack&lt;&gt;(); char sign = '+'; int number = 0; for(int i=0; i&lt;charArray.length; ++i){ char c = charArray[i]; if(c ==' '){ continue; } //若为数字 if(Character.isDigit(c)){ number = number*10 + c - '0'; } //若为括号 if(c == '('){ int j = i+1; int khNum = 1; while(khNum != 0){ if(charArray[j] == '('){ khNum++; }else if(charArray[j] == ')'){ khNum--; } j++; } number = solve(s.substring(i+1, j-1)); i = j - 1; } //若为加减乘除符号 if(!Character.isDigit(c) || i == charArray.length-1){ if(sign == '+'){ stack.push(number); }else if(sign == '-'){ stack.push(-1 * number); }else if(sign == '*'){ int last = stack.pop(); stack.push(last*number); }else if(sign == '/'){ int last = stack.pop(); stack.push(last/number); } sign = c; number = 0; } } int ans = 0; while(!stack.isEmpty()){ ans += stack.pop(); } return ans; }}","link":"/2023/02/04/%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%88%86%E6%B3%95/"},{"title":"二叉树","text":"二叉树解题的思维模式1、是否可以通过遍历一遍二叉树得到答案？如果可以，用一个 traverse 函数配合外部变量来实现，这叫「遍历」的思维模式。 2、是否可以定义一个递归函数，通过子问题（子树）的答案推导出原问题的答案？如果可以，写出这个递归函数的定义，并充分利用这个函数的返回值，这叫「分解问题」的思维模式。 无论使用哪种思维模式，你都需要思考： 如果单独抽出一个二叉树节点，它需要做什么事情？需要在什么时候（前/中/后序位置）做？其他的节点不用你操心，递归函数会帮你在所有节点上执行相同的操作。 116.填充节点的右侧指针传统的 traverse 函数是遍历二叉树的所有节点，但现在我们想遍历的其实是两个相邻节点之间的「空隙」。 所以我们可以在二叉树的基础上进行抽象，你把图中的每一个方框看做一个节点： 这样，一棵二叉树被抽象成了一棵三叉树，三叉树上的每个节点就是原先二叉树的两个相邻节点。 遍历144. 二叉树的前序遍历方法 1：递归 方法 2：栈 12345678910111213141516171819202122232425262728293031323334public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param root TreeNode类 * @return int整型一维数组 */ int[] ans; List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); public int[] preorderTraversal (TreeNode root) { // write code here if(root == null){ return new int[]{}; } stack.push(root); while(!stack.isEmpty()){ TreeNode node = stack.pop(); list.add(node.val); if(node.right != null){ stack.push(node.right); } if(node.left != null){ stack.push(node.left); } } ans = new int[list.size()]; for(int i=0; i&lt;list.size(); ++i){ ans[i] = list.get(i); } return ans; }} 145. 二叉树的后序遍历具体做法： step 1：开辟一个辅助栈，用于记录要访问的子节点，开辟一个前序指针pre。 step 2：从根节点开始，每次优先进入每棵的子树的最左边一个节点，我们将其不断加入栈中，用来保存父问题。 step 3：弹出一个栈元素，看成该子树的根，判断这个根的右边有没有节点或是有没有被访问过，如果没有右节点或是被访问过了，可以访问这个根，并将前序节点标记为这个根。 step 4：如果没有被访问，那这个根必须入栈，进入右子树继续访问，只有右子树结束了回到这里才能继续访问根。 图示： 1234567891011121314151617181920212223242526272829303132333435363738394041public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param root TreeNode类 * @return int整型一维数组 */ List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); TreeNode pre = null; public int[] postorderTraversal (TreeNode root) { // write code here if(root == null){ return new int[]{}; } while(root != null || !stack.isEmpty()){ //每次先找到最左边的节点 while(root != null){ stack.push(root); root = root.left; } TreeNode node = stack.pop(); //如果当前节点的右孩子不空且没有访问 if(node.right != null &amp;&amp; node.right != pre){ //保存根节点 stack.push(node); root = node.right; }else{ pre = node; list.add(node.val); } } int[] ans = new int[list.size()]; for(int i=0; i&lt;list.size(); ++i){ ans[i] = list.get(i); } return ans; }} 94. 二叉树的中序遍历方法：栈 具体做法： step 1：优先判断树是否为空，空树不遍历。 step 2：准备辅助栈，当二叉树节点为空了且栈中没有节点了，我们就停止访问。 step 3：从根节点开始，每次优先进入每棵的子树的最左边一个节点，我们将其不断加入栈中，用来保存父问题。 step 4：到达最左后，可以开始访问，如果它还有右节点，则将右边也加入栈中，之后右子树的访问也是优先到最左。 图示： 123456789101112131415161718192021222324252627282930313233public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param root TreeNode类 * @return int整型一维数组 */ List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); public int[] inorderTraversal (TreeNode root) { // write code here if(root == null){ return new int[]{}; } while(root != null || !stack.isEmpty()){ while(root!=null){ stack.push(root); root = root.left; } TreeNode node = stack.pop(); list.add(node.val); if(node.right!=null){ root = node.right; } } int[] ans = new int[list.size()]; for(int i=0; i&lt;list.size(); ++i){ ans[i] = list.get(i); } return ans; }} 105. 从前序与中序遍历序列构造二叉树分解子问题 可以用 map 来存中序遍历值的对应下标 1234567891011121314151617181920class Solution { Map&lt;Integer, Integer&gt; val2idx = new HashMap&lt;&gt;(); public TreeNode buildTree(int[] preorder, int[] inorder) { for(int i=0; i&lt;inorder.length; ++i){ val2idx.put(inorder[i],i); } return build(preorder, 0, preorder.length-1, inorder, 0, inorder.length-1); //左闭右闭 } TreeNode build(int[] preorder, int preh, int pret, int[]inorder, int inorh, int inort){ if(preh&gt;pret) return null; //左闭右闭 TreeNode node = new TreeNode(preorder[preh]); int idx = val2idx.get(preorder[preh]); int size = idx-inorh; node.left = build(preorder, preh+1, size+preh, inorder, inorh, idx-1); node.right = build(preorder, preh+1+size, pret, inorder, idx+1, inort); return node; }} 106 从中序与后序遍历序列构造二叉树1234567891011121314151617181920class Solution { Map&lt;Integer, Integer&gt; val2idx = new HashMap&lt;&gt;(); public TreeNode buildTree(int[] inorder, int[] postorder) { for(int i=0; i&lt;inorder.length; ++i){ val2idx.put(inorder[i], i); } return build(inorder, 0, inorder.length-1, postorder, 0, postorder.length-1); } TreeNode build(int[] inorder, int h1, int t1, int[] postorder, int h2, int t2){ if(h1 &gt; t1) return null; int val = postorder[t2]; int idx = val2idx.get(postorder[t2]); int leftSize = idx-h1; TreeNode node = new TreeNode(val); node.left = build(inorder, h1, idx-1, postorder, h2, h2+leftSize-1); node.right = build(inorder, idx+1, t1, postorder, h2+leftSize, t2-1); return node; }} 889. 根据前序和后序遍历构造二叉树123456789101112131415161718192021222324class Solution { Map&lt;Integer, Integer&gt; val2idx = new HashMap&lt;&gt;(); public TreeNode constructFromPrePost(int[] preorder, int[] postorder) { for(int i=0; i&lt;postorder.length; ++i){ val2idx.put(postorder[i], i); } return build(preorder, 0, preorder.length-1, postorder, 0, postorder.length-1); } TreeNode build(int[] preorder, int h1, int t1, int[] postorder, int h2, int t2){ if(h1&gt;t1) return null; int val = preorder[h1]; if(h1 == t1) return new TreeNode(val); //下面要用h1+1所以避免越界这里要先判断是不是子树只有一个节点 int leftVal = preorder[h1+1]; //造成答案不唯一 可能是左子树也可能是右子树 例如123 321 int idx = val2idx.get(leftVal); int leftSize = idx-h2; TreeNode node = new TreeNode(val); node.left = build(preorder, h1+1, h1+1+leftSize, postorder, h2, h2+leftSize); node.right = build(preorder, h1+1+leftSize+1, t1, postorder, h2+leftSize+1, t2-1); return node; }} 652. 寻找重复的子树 前序位置的代码只能从函数参数中获取父节点传递来的数据，而后序位置的代码不仅可以获取参数数据，还可以获取到子树通过函数返回值传递回来的数据。 那么换句话说，一旦你发现题目和子树有关，那大概率要给函数设置合理的定义和返回值，在后序位置写代码了。 思路：就是将子树序列化成唯一的表示 123456789101112131415161718192021class Solution { Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;(); List&lt;TreeNode&gt; list = new ArrayList&lt;&gt;(); public List&lt;TreeNode&gt; findDuplicateSubtrees(TreeNode root) { dfs(root); return list; } String dfs(TreeNode root){ if(root == null) return \" \"; // 节点空 则用“ ”表示 StringBuilder sb = new StringBuilder(); //序列化的形式是：root_left_right sb.append(root.val).append(\"_\"); sb.append(dfs(root.left)).append(\"_\"); sb.append(dfs(root.right)); String s = sb.toString(); map.put(s,map.getOrDefault(s, 0)+1); if(map.get(s) == 2) list.add(root); return s; }} 前/中/后序和二叉树的唯一性问题： ​ 什么样的序列化的数据可以反序列化出唯一的一棵二叉树？ 回答： ​ 若包含空指针信息，前序和后序遍历能唯一确定一颗二叉树；但中序遍历不可以 愿意： ​ 前序、后序遍历可以确定根节点位置；中序遍历根节点位置无法确定 总结： 如果你的序列化结果中不包含空指针的信息，且你只给出一种遍历顺序，那么你无法还原出唯一的一棵二叉树。 如果你的序列化结果中不包含空指针的信息，且你会给出两种遍历顺序，分两种情况： 如果你给出的是前序和中序，或者后序和中序，那么你可以还原出唯一的一棵二叉树。 如果你给出前序和后序，那么你无法还原出唯一的一棵二叉树。 如果你的序列化结果中包含空指针的信息，且你只给出一种遍历顺序，也要分两种情况： 如果你给出的是前序或者后序，那么你可以还原出唯一的一棵二叉树。 如果你给出的是中序，那么你无法还原出唯一的一棵二叉树。 617. 合并二叉树具体做法：（不需要遍历所有 加入一个子树没有就返回另一个子树） step 1：首先判断t1与t2是否为空，若为则用另一个代替，若都为空，返回的值也是空。 step 2：然后依据前序遍历的特点，优先访问根节点，将两个根点的值相加创建到新树中。 step 3：两棵树再依次同步进入左子树和右子树。 12345678910111213141516import java.util.*;public class Solution { public TreeNode mergeTrees (TreeNode t1, TreeNode t2) { //若只有一个节点返回另一个，两个都为null自然返回null if (t1 == null) return t2; if (t2 == null) return t1; //根左右的方式递归 TreeNode head = new TreeNode(t1.val + t2.val); head.left = mergeTrees(t1.left, t2.left); head.right = mergeTrees(t1.right, t2.right); return head; }} 判断是不是完全二叉树思路： 对完全二叉树最重要的定义就是叶子节点只能出现在最下层和次下层，所以我们想到可以使用队列辅助进行层次遍历——从上到下遍历所有层，每层从左到右，只有次下层和最下层才有叶子节点，其他层出现叶子节点就意味着不是完全二叉树。 具体做法： step 1：先判断空树一定是完全二叉树。 step 2：初始化一个队列辅助层次遍历，将根节点加入。 step 3：逐渐从队列中弹出元素访问节点，如果遇到某个节点为空，进行标记，代表到了完全二叉树的最下层，若是后续还有访问，则说明提前出现了叶子节点，不符合完全二叉树的性质。 step 4：否则，继续加入左右子节点进入队列排队，等待访问。 图示： 1234567891011121314151617181920212223242526272829303132333435public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param root TreeNode类 * @return bool布尔型 */ public boolean isCompleteTree (TreeNode root) { // write code here if(root == null){ return true; } Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); boolean visitLeaf = false; while(!queue.isEmpty()){ int len = queue.size(); for(int i=0; i&lt;len; ++i){ TreeNode node = queue.poll(); if(node == null){ visitLeaf = true; continue; } if(visitLeaf){ return false; } queue.add(node.left); queue.add(node.right); } } return true; }} 235. 二叉搜索树的最近公共祖先思路： 我们也可以利用二叉搜索树的性质：对于某一个节点若是p与q都小于等于这个这个节点值，说明p、q都在这个节点的左子树，而最近的公共祖先也一定在这个节点的左子树；若是p与q都大于等于这个节点，说明p、q都在这个节点的右子树，而最近的公共祖先也一定在这个节点的右子树。而若是对于某个节点，p与q的值一个大于等于节点值，一个小于等于节点值，说明它们分布在该节点的两边，而这个节点就是最近的公共祖先，因此从上到下的其他祖先都将这个两个节点放到同一子树，只有最近公共祖先会将它们放入不同的子树，每次进入一个子树又回到刚刚的问题，因此可以使用递归。 具体做法： step 1：首先检查空节点，空树没有公共祖先。 step 2：对于某个节点，比较与p、q的大小，若p、q在该节点两边说明这就是最近公共祖先。 step 3：如果p、q都在该节点的左边，则递归进入左子树。 step 4：如果p、q都在该节点的右边，则递归进入右子树。 12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root.val &gt; Math.min(p.val,q.val) &amp;&amp; root.val &gt; Math.max(p.val,q.val)){ return lowestCommonAncestor(root.left, p, q); } else if(root.val &lt; Math.min(p.val,q.val) &amp;&amp; root.val &lt; Math.max(p.val,q.val)){ return lowestCommonAncestor(root.right, p, q); } return root; }} 236. 二叉树的最近公共祖先递归情况：1.当到达空节点（既叶子节点的子节点）时，直接返回空2.当root等于 o1 或 o2 时，返回root3.若不为1， 2中情况，说明需要继续处理：对左子树进行递归，返回值记为 t1对右子树进行递归，返回值记位 t2t1 ，t2 存在以下几种情况：①. 当t1, t2都为空时，说明root的左右子树中都不存在o1, o2， 返回空②. 当t1为空且t2不为空时，说明左子树找不到 o1, o2,所以返回 t2③. 当t2为空且t1不为空时，说明右子树找不到 o1, o2,所以返回 t1④. 当t1, t2都不为空时,说明o1, o2分别位于root的左右子树中，既root为答案，返回root 1234567891011121314151617181920212223/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if(root == null) return null; // 遍历到空结点 if(root == p || root == q) return root; // 找到结点p 或者 q TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); if(left != null &amp;&amp; right != null) return root; // 找到最深公共结点 if(left != null) return left; // 将找到的结点信息传导到上一层 if(right != null) return right; return null; }} 二叉搜索树 BSF98. 验证二叉搜索树解法 1：在中序遍历的时候判断是不是遍历的值是不是递增 1234567891011121314class Solution { long min = Long.MIN_VALUE; // 判断中序为升序 public boolean isValidBST(TreeNode root) { if(root == null) return true; boolean f = true; f &amp;= isValidBST(root.left); if(min &lt; root.val) min = root.val; else return false; f &amp;= isValidBST(root.right); return f; }} 解法 2：遍历的时候控制范围 12345678910111213141516class Solution { // 控制 node.val 值的范围 public boolean isValidBST(TreeNode root) { return dfs(root, null, null); } boolean dfs(TreeNode root, Integer min, Integer max){ if(root == null) return true; if(min!=null &amp;&amp; min &gt;= root.val) return false; if(max!=null &amp;&amp; max &lt;= root.val) return false; return dfs(root.left, min, root.val) &amp;&amp; dfs(root.right, root.val, max); }} 700. 二叉搜索树中的搜索123456789class Solution { public TreeNode searchBST(TreeNode root, int val) { if(root == null) return null; if(val == root.val) return root; if(val &lt; root.val) return searchBST(root.left, val); if(val &gt; root.val) return searchBST(root.right, val); return null; }} 若是在一颗普通的二叉树中寻找 123456789class Solution { public TreeNode searchBST(TreeNode root, int val) { if(root == null) return null; if(val == root.val) return root; TreeNode left = searchBST(root.left, val); TreeNode right = searchBST(root.right, val); return left != null ? left : right; }} 701. 二叉搜索树中的插入操作上一个问题，我们总结了 BST 中的遍历框架，就是「找」的问题。直接套框架，加上「改」的操作即可。一旦涉及「改」，就类似二叉树的构造问题，函数要返回 TreeNode 类型，并且要对递归调用的返回值进行接收。 1234567891011class Solution { public TreeNode insertIntoBST(TreeNode root, int val) { if(root == null) return new TreeNode(val); if(root.val &lt; val){ root.right =insertIntoBST(root.right, val); }else if(root.val &gt; val){ root.left = insertIntoBST(root.left, val); } return root; }} 450. 删除二叉搜索树中的节点1234567891011121314151617181920212223242526272829303132333435363738class Solution { public TreeNode deleteNode(TreeNode root, int key) { if(root == null) return null; if(root.val == key){ //找到了 有三种情况 //左节点空 if(root.left == null){ return root.right; } //右节点空 if(root.right == null){ return root.left; } //左右节点都不空 if(root.left != null &amp;&amp; root.right != null){ //找左子树的最大值 TreeNode l = root.left; while(l.right != null){ l = l.right; } // 删除左子树最大的节点 root.left = deleteNode(root.left, l.val); // 用左子树最大的节点替换 root 节点 l.left = root.left; l.right = root.right; root = l; } }else if(root.val &lt; key){ root.right = deleteNode(root.right, key); }else if(root.val &gt; key){ root.left = deleteNode(root.left, key); } return root; }} 96. 不同的搜索树 标签：动态规划 假设 n 个节点存在二叉排序树的个数是 G (n)，令 f(i) 为以 i 为根的二叉搜索树的个数，则 $$G(n)=f(1)+f(2)+f(3)+f(4)+…+f(n)$$ 当 i 为根节点时，其左子树节点个数为 i-1 个，右子树节点为 n-i，则 $$f(i)=G(i−1)∗G(n−i)$$ 综合两个公式可以得到 卡特兰数 公式 $$G(n)=G(0)∗G(n−1)+G(1)∗(n−2)+…+G(n−1)∗G(0)$$ 那么就是要用两层 for 循环： 外层 i 循环 2～n 内层 j 循环 0～i-1 12345678910111213class Solution { public int numTrees(int n) { int[] dp = new int[n+1]; dp[0] = 1; dp[1] = 1; for(int i=2; i&lt;=n; ++i){ for(int j=0; j&lt;=i-1; ++j){ dp[i] += dp[j] * dp[i-1-j]; } } return dp[n]; }} 96. 不同的搜索树II首先要想到这是问题分解。 遍历应该是后序，因为要用到子节点返回的子树来新建父亲节点 不同的是每一层递归都需要一个局部的 list 来存当前根节点 递归结束的条件是[start,end]为空，那么就给 list 加入 null，以便回溯的时候根节点可以用到无子树的情况，若不加 null 那么就会一直进入不了内层的两个循环体 123456789101112131415161718192021222324252627282930class Solution { List&lt;TreeNode&gt; ans = new ArrayList&lt;&gt;(); public List&lt;TreeNode&gt; generateTrees(int n) { if(n == 0) return new ArrayList&lt;TreeNode&gt;(); return build(1, n); } List&lt;TreeNode&gt; build(int start, int end){ List&lt;TreeNode&gt; list = new ArrayList&lt;&gt;(); if(start &gt; end){ list.add(null);//关键 相当于[start, end]没有要新建的点 return list; } for(int i=start; i&lt;=end; ++i){ List&lt;TreeNode&gt; leftList = build(start, i-1); List&lt;TreeNode&gt; rightList = build(i+1, end); for(TreeNode left : leftList){ for(TreeNode right : rightList){ TreeNode node = new TreeNode(i); node.left = left; node.right = right; list.add(node); } } } return list; }}","link":"/2023/03/04/%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"title":"位运算","text":"268. 丢失的数字给定一个包含 [0, n] 中 n 个数的数组 nums ，找出 [0, n] 这个范围内没有出现在数组中的那个数。 思路： 找缺省值、找出现一次数都是异或和的经典应用 先对[0，n]求异或和，然后对nums求异或和 123456789class Solution { public int missingNumber(int[] nums) { int yh = 0; int n = nums.length; for(int i=0; i&lt;=n; ++i) yh ^=i; for(var num : nums) yh ^=num; return yh; }} 136. 只出现一次的数字给你一个 非空 整数数组 nums ，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。 你必须设计并实现线性时间复杂度的算法来解决此问题，且该算法只使用常量额外空间。 思路： 使用异或^的特性，两个相同的数做异或操作，会被消掉 123456789class Solution { public int singleNumber(int[] nums) { int t = 0; for(var i : nums){ t ^= i; } return t; }} *137. 只出现一次的数字 II给你一个整数数组 nums ，除某个元素仅出现 一次 外，其余每个元素都恰出现 三次 。请你找出并返回那个只出现了一次的元素。 你必须设计并实现线性时间复杂度的算法且使用常数级空间来解决此问题。 思路： 按位运算，因为有三个数相同，那么一个位置上的数累加之后一定是0或者3的倍数，那么累加数组中每个位置上的值然后对3取余，结果就是目标元素在该位置上的值。依次计算结果的每个位置是0还是1。 12345678910111213class Solution { public int singleNumber(int[] nums) { int ans = 0; for(int i=0; i&lt;32; ++i){ int sum = 0; for(var n : nums){ sum += (n&gt;&gt;i)&amp;1; } ans |= (sum % 3)&lt;&lt;i; } return ans; }} 扩展： 那么就可以套用在n个数相同上 *260. 只出现一次的数字 III给你一个整数数组 nums，其中恰好有两个元素只出现一次，其余所有元素均出现两次。 找出只出现一次的那两个元素。你可以按 任意顺序 返回答案。 你必须设计并实现线性时间复杂度的算法且仅使用常量额外空间来解决此问题。 思路： 如何拆分该问题变成两个子问题——即恰好只有一个元素只出现一次，其余所有元素均出现两次 对所有数取异或，因为有两个元素不同，那么结果一定有一个位置上是1，就说明这两个不同的元素在该位置上不同的，那么就可以利用该点将该问题分解 技巧： 求lowbit：即求最低异或和中 的某个值为1的比特位 1234 s = 101100 ~s = 010011(~s)+1 = 010100 // 根据补码的定义，这就是 -s 最低 1 左侧取反，右侧不变s &amp; -s = 000100 // lowbit s &amp; (-s+1) 1234567891011121314class Solution { public int[] singleNumber(int[] nums) { int yh = 0; for(var i:nums){ yh ^= i; } int lowbit = yh &amp; (-yh); // 等价于 yh &amp; (~yh + 1) int[] ans = new int[2]; for(var i:nums){ ans[(i&amp;lowbit)==0?0:1] ^= i; } return ans; }} 190. 颠倒二进制位颠倒给定的 32 位无符号整数的二进制位。 提示： 请注意，在某些语言（如 Java）中，没有无符号整数类型。在这种情况下，输入和输出都将被指定为有符号整数类型，并且不应影响您的实现，因为无论整数是有符号的还是无符号的，其内部的二进制表示形式都是相同的。 在 Java 中，编译器使用二进制补码记法来表示有符号整数。因此，在 示例 2 中，输入表示有符号整数 -3，输出表示有符号整数 -1073741825。 1234567891011121314public class Solution { // you need treat n as an unsigned value public int reverseBits(int n) { int ans = 0; for(int i=0; i&lt;32; ++i){ int x = (n &amp; 1); n = n&gt;&gt;&gt;1; ans = ans&lt;&lt;1; ans |= x; // System.out.println(Integer.toBinaryString(n)); } return ans; }} java 要用无符号右移&gt;&gt;&gt; 而不是符号右移&gt;&gt; 231. 2 的幂给你一个整数 n，请你判断该整数是否是 2 的幂次方。如果是，返回 true ；否则，返回 false 。 如果存在一个整数 x 使得 n == 2x ，则认为 n 是 2 的幂次方。 思路： n &amp; (n - 1) 其中 &amp; 表示按位与运算。该位运算技巧可以直接将 n二进制表示的最低位 1 移除。原理： 假设 n的二进制表示为 (a10⋯0)，其中 a 表示若干个高位，1 表示最低位的那个 1，0⋯0表示后面的若干个 0，那么 n−1 的二进制表示为：(a01⋯1) 我们将 (a10⋯0)与 (a01⋯1) 进行按位与运算，高位 a 不变，在这之后的所有位都会变为 0，这样我们就将最低位的那个 1 移除了。 n &amp; (-n)其中 −n 是 n的相反数，是一个负数。该位运算技巧可以直接获取 n 二进制表示的最低位的 1。 由于负数是按照补码规则在计算机中存储的，−n 的二进制表示为 n的二进制表示的每一位取反再加上 1，因此它的原理如下： 假设 n的二进制表示为 (a10⋯0)，其中 a表示若干个高位，1表示最低位的那个 1，0⋯0表示后面的若干个 0，那么 −n 的二进制表示为： (aˉ01⋯1)+(1)=(aˉ10⋯0) 其中 aˉ 表示将 a 每一位取反。我们将 (a10⋯0)与 (aˉ10⋯0)进行按位与运算，高位全部变为 0，最低位的 1 以及之后的所有 0 不变，这样我们就获取了 n二进制表示的最低位的 1。 12345678class Solution { public boolean isPowerOfTwo(int n) { int lowbit = n &amp; -n; if(lowbit == 0 || lowbit == -2147483648) return false; // System.out.print(Integer.toBinaryString(lowbit)); return (n - lowbit) == 0 ? true : false; }} 二进制、八进制、十六进制表示法12345678910111213System.out.println(0b101);//二进制:5 （0b开头的）System.out.println(0e1011);//0.0System.out.println(011);//八进制:9 (0开头的)System.out.println(11);//十进制:11System.out.println(0x11C);//十六进制:284 （0x开头的）System.out.printf(\"%010x\\n\",7);//0000000007 按10位十六进制输出，向右靠齐，左边用0补齐System.out.printf(\"%010o\\n\",13);//0000000015 按10位八进制输出，向右靠齐，左边用0补齐System.out.printf(\"%x\\n\",7);//7 按16进制输出System.out.printf(\"%o\\n\",13);//15 按8进制输出System.out.println(Integer.toBinaryString(11));//1011 二进制 342. 4的幂给定一个整数，写一个函数来判断它是否是 4 的幂次方。如果是，返回 true ；否则，返回 false 。 整数 n 是 4 的幂次方需满足：存在整数 x 使得 n == 4x 由于题目保证了 n 是一个 32 位的有符号整数，因此我们可以构造一个整数 mask，它的所有偶数二进制位都是 0，所有奇数二进制位都是 1。这样一来，我们将 n 和 mask 进行按位与运算，如果结果为 0，说明 n 二进制表示中的 1 出现在偶数的位置，否则说明其出现在奇数的位置。 123456class Solution { public boolean isPowerOfFour(int n) { int isOne = n &amp; (n-1); return isOne == 0 &amp;&amp; (n &amp; 0b10101010101010101010101010101010) == 0; }} 123456class Solution { public boolean isPowerOfFour(int n) { int isOne = n &amp; (n-1); return isOne == 0 &amp;&amp; (n &amp; 0xAAAAAAAA) == 0; }} 数组中出现次数超过一半的数字描述 给一个长度为 n 的数组，数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。 例如输入一个长度为9的数组[1,2,3,2,2,2,5,4,2]。由于数字2在数组中出现了5次，超过数组长度的一半，因此输出2。 数据范围：n≤5000，数组中元素的值 0≤val≤10000 要求：空间复杂度：O(1)，时间复杂度 O(n) 思路： 哈希但是空间复杂度就是 O(n) 候选人思想：是一种用于在一个数组中寻找出现次数超过一半的元素的算法，即找出主要元素。 核心思想是假设数组中的第一个元素为候选元素，并初始化一个计数count为1。然后从数组的第二个元素开始遍历，如果当前元素和候选元素相同，则计数器count加1；如果不同，则计数器count减1。当计数器count减到0时，表示当前候选元素的出现次数不足以超过数组长度的一半，需要重新选择一个新的候选元素，并将计数器count重置为1。最后所选的候选元素即为出现次数超过数组长度一半的主要元素。 该算法的时间复杂度为O(n)，空间复杂度为O(1)，效率较高。 使用摩尔投票算法需要注意的是，输入的数组必须满足存在出现次数超过一半的主要元素，否则算法可能返回不正确的结果。 2961. 双模幂运算 给你一个下标从 0 开始的二维数组 variables ，其中 variables[i] = [ai, bi, ci, mi]，以及一个整数 target 。 如果满足以下公式，则下标 i 是 好下标： 0 &lt;= i &lt; variables.length ((aibi % 10)ci) % mi == target 返回一个由 好下标 组成的数组，顺序不限 。 思路快速幂： 取模运算：https://leetcode.cn/circle/discuss/mDfnkW/ 12345678910111213141516171819202122232425class Solution { public List&lt;Integer&gt; getGoodIndices(int[][] variables, int target) { List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); int i = 0; for (int[] nums : variables) { if (pow(pow(nums[0], nums[1], 10), nums[2], nums[3]) == target) { ans.add(i); } i++; } return ans; } private int pow (int a, int b, int c) { int ans = 1; while (b &gt; 0){ if ((b &amp; 1) == 1) { ans = ans * a % c; } a = a * a % c; b = b &gt;&gt; 1; } return ans; }}","link":"/2023/02/04/%E7%AE%97%E6%B3%95/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"title":"动态规划","text":"一、动态规划基础70. 爬楼梯假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 注意：给定 n 是一个正整数。 123456789101112class Solution { public int climbStairs(int n) { if(n==1 || n==2) return n; int[] dp = new int[n+1]; dp[1] = 1; dp[2] = 2; for(int i=3; i&lt;=n; ++i){ dp[i] = dp[i-1] + dp[i-2]; } return dp[n]; }} 总结：一开始会先着如何定义dp[0]也就是起点位置，然后会根据结果来反推dp[0]=1，其实n的范围是[1,…]，所以完全不用考虑dp[0]。 746. 使用最小花费爬楼梯给你一个整数数组 cost ，其中 cost[i] 是从楼梯第 i 个台阶向上爬需要支付的费用。一旦你支付此费用，即可选择向上爬一个或者两个台阶。 你可以选择从下标为 0 或下标为 1 的台阶开始爬楼梯。 请你计算并返回达到楼梯顶部的最低花费。 思路 台阶：[0,n] cost：[0,n-1] 确定dp[i]含义：爬到第i个台阶费用 12345678910111213class Solution { public int minCostClimbingStairs(int[] cost) { int n = cost.length+1; int[] dp = new int[n]; dp[0] = 0; dp[1] = 0; for(int i=2; i&lt;n; ++i){ dp[i] = Math.min(dp[i-2]+cost[i-2], dp[i-1]+cost[i-1]); } return dp[n-1]; }} 62. 不同路径一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。 问总共有多少条不同的路径？ 思路 使用二维dp数组，dp[i][j]数组含义：机器人走到[i][j]处时一共有多少条不同路径 初始化：第一行和第一列都只有一条路径到达 状态转移方程：dp[i][j] = dp[i-1][j] + dp[i][j-1]; 1234567891011121314151617class Solution { public int uniquePaths(int m, int n) { int[][] dp = new int[m][n]; for(int i=0; i&lt;m; ++i) { dp[i][0] = 1; } for(int j=0; j&lt;n; ++j) { dp[0][j] = 1; } for(int i=1; i&lt;m; ++i) { for(int j=1; j&lt;n; ++j) { dp[i][j] = dp[i-1][j] + dp[i][j-1]; } } return dp[m-1][n-1]; }} 压缩成一维数组 1234567891011121314class Solution { public int uniquePaths(int m, int n) { int[] dp = new int[n]; for(int i=0; i&lt;n; ++i) { dp[i] = 1; } for(int i=1; i&lt;m; ++i) { for(int j=1; j&lt;n; ++j) { dp[j] += dp[j-1]; } } return dp[n-1]; }} 63. 不同路径 II一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。 现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？ 思路 只有没有障碍的时候才加上 12345678910111213141516171819202122class Solution { public int uniquePathsWithObstacles(int[][] obstacleGrid) { int m = obstacleGrid.length, n = obstacleGrid[0].length; if(obstacleGrid[m-1][n-1] == 1) return 0; int[][] dp = new int[m][n]; for(int i=0; i&lt;m; ++i) { if(obstacleGrid[i][0] == 1) break; dp[i][0] = 1; } for(int j=0; j&lt;n; ++j) { if(obstacleGrid[0][j] == 1) break; dp[0][j] = 1; } for(int i=1; i&lt;m; ++i) { for(int j=1; j&lt;n; ++j) { if(obstacleGrid[i-1][j] == 0) dp[i][j] += dp[i-1][j]; if(obstacleGrid[i][j-1] == 0) dp[i][j] += dp[i][j-1]; } } return dp[m-1][n-1]; }} 压缩为一维数组 1234567891011121314151617181920class Solution { public int uniquePathsWithObstacles(int[][] obstacleGrid) { int m = obstacleGrid.length, n = obstacleGrid[0].length; if(obstacleGrid[m-1][n-1] == 1) return 0; int[] dp = new int[n]; for(int j=0; j&lt;n; ++j) { if(obstacleGrid[0][j] == 1) break; dp[j] = 1; } for(int i=1; i&lt;m; ++i) { if(obstacleGrid[i][0] == 1) dp[0] = 0 ; for(int j=1; j&lt;n; ++j) { if(obstacleGrid[i][j] == 0) dp[j] += dp[j-1]; else dp[j] = 0; } } return dp[n-1]; }} *343. 整数拆分给定一个正整数 n ，将其拆分为 k 个 正整数 的和（ k &gt;= 2 ），并使这些整数的乘积最大化。 返回 你可以获得的最大乘积 。 示例 1: 输入: 2 输出: 1 解释: 2 = 1 + 1, 1 × 1 = 1。 示例 2: 输入: 10 输出: 36 解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36。 说明: 你可以假设 n 不小于 2 且不大于 58。 思路 确定dp[i]：拆分数字i，可以得到的最大乘积 确定递推公式： dp[i]有两种渠道获得—— j*(i-j) ：代表i分成两部分 j*dp[i-j] ：代表i分成三部分及三部以上 j不用拆分，可以理解为固定一个部分，让另一部分去自适应，就能把所有可能囊括 dp初始化：dp[0],dp[1] 都无意义直接不用管，dp[2] = 1 确定遍历顺序 dp[i] 依靠 dp[i-j]所以i从前向后遍历 i —— [3, n] j —— [1, i） 优化 [1, i/2] 因为可以理解为一个数平均拆分乘积最大 1234567891011121314class Solution { public int integerBreak(int n) { int[] dp = new int[n+1]; //拆分i 乘积最大值 dp[0] = 0; dp[1] = 0; dp[2] = 1; for(int i=3; i&lt;=n; ++i){ for(int j=1; j&lt;=i/2; ++j){ dp[i] = Math.max(Math.max(dp[i], j * (i-j)), j * dp[i-j]); } } return dp[n]; }} *96. 不同的二叉搜索树给定一个整数 n，求以 1 … n 为节点组成的二叉搜索树有多少种？ 思路 dp[3]，就是 元素1为头结点搜索树的数量 + 元素2为头结点搜索树的数量 + 元素3为头结点搜索树的数量 元素1为头结点搜索树的数量 = 右子树有2个元素的搜索树数量 * 左子树有0个元素的搜索树数量 元素2为头结点搜索树的数量 = 右子树有1个元素的搜索树数量 * 左子树有1个元素的搜索树数量 元素3为头结点搜索树的数量 = 右子树有0个元素的搜索树数量 * 左子树有2个元素的搜索树数量 总结：$$dp[i] = \\sum_{j=1}^{i-1}{dp[j]*dp[i-j-1]}$$ 12345678910111213141516class Solution { public int numTrees(int n) { int[] dp = new int[n+1]; if(n==1) return 1; if(n==2) return 2; dp[0] = 1; dp[1] = 1; dp[2] = 2; for(int i=3; i&lt;=n; ++i){ for(int j=0; j&lt;=i-1; ++j){ dp[i] += dp[j]*dp[i-j-1]; } } return dp[n]; }} 二、0-1背包问题理论基础背包问题最基础的是用二维dp数组来做：dp[i][j] 表示从下标为[0-i]的物品里任意取，放进容量为j的背包，价值总和最大是多少。 这里两次循环可以互相交换顺序 但是若压缩dp数组的话，只能是先遍历物品，然后遍历背包容量，且背包容量要从后往前遍历。 原因：一维dp数组等于是复用了上一层循环的结果，假如遍历背包的时候从前往后，那么会重复加入物品。倒序遍历的原因是，本质上还是一个对二维数组的遍历，并且右下角的值依赖上一层左上角的值，因此需要保证左边的值仍然是上一层的，从右向左覆盖。 同时使用一维dp不需要初始化 12345678910111213141516171819202122232425262728293031// 创建dp数组 int goods = weight.length; // 获取物品的数量 int[][] dp = new int[goods][bagSize + 1]; // 初始化dp数组 // 创建数组后，其中默认的值就是0 for (int j = weight[0]; j &lt;= bagSize; j++) { dp[0][j] = value[0]; } // 填充dp数组 for (int i = 1; i &lt; weight.length; i++) { for (int j = 1; j &lt;= bagSize; j++) { if (j &lt; weight[i]) { /** * 当前背包的容量都没有当前物品i大的时候，是不放物品i的 * 那么前i-1个物品能放下的最大价值就是当前情况的最大价值 */ dp[i][j] = dp[i-1][j]; } else { /** * 当前背包的容量可以放下物品i * 那么此时分两种情况： * 1、不放物品i * 2、放物品i * 比较这两种情况下，哪种背包中物品的最大价值最大 */ dp[i][j] = Math.max(dp[i-1][j] , dp[i-1][j-weight[i]] + value[i]); } } } 123456789int wLen = weight.length; //定义dp数组：dp[j]表示背包容量为j时，能获得的最大价值 int[] dp = new int[bagWeight + 1]; //遍历顺序：先遍历物品，再遍历背包容量 for (int i = 0; i &lt; wLen; i++){ for (int j = bagWeight; j &gt;= weight[i]; j--){ dp[j] = Math.max(dp[j], dp[j - weight[i]] + value[i]); } } 416. 分割等和子集给你一个 只包含正整数 的 非空 数组 nums 。请你判断是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 01背包相对于本题，主要要理解，题目中物品是nums[i]，重量是nums[i]，价值也是nums[i]，背包体积是sum/2。 1234567891011121314151617class Solution { public boolean canPartition(int[] nums) { int sum = 0; for(var n : nums){ sum += n; } if(sum % 2 == 1) return false; sum /= 2; int[] dp = new int[sum + 1]; for(int i=0; i&lt;nums.length; ++i){ for(int j=sum; j&gt;=nums[i]; j--){ dp[j] = Math.max(dp[j], dp[j-nums[i]] + nums[i]); } } return dp[sum] == sum ? true : false; }} 1049. 最后一块石头的重量 II有一堆石头，用整数数组 stones 表示。其中 stones[i] 表示第 i 块石头的重量。 每一回合，从中选出任意两块石头，然后将它们一起粉碎。假设石头的重量分别为 x 和 y，且 x &lt;= y。那么粉碎的可能结果如下： 如果 x == y，那么两块石头都会被完全粉碎； 如果 x != y，那么重量为 x 的石头将会完全粉碎，而重量为 y 的石头新重量为 y-x。 最后，最多只会剩下一块 石头。返回此石头 最小的可能重量 。如果没有石头剩下，就返回 0。 本题其实和416. 分割等和子集 (opens new window)几乎是一样的，只是最后对dp[target]的处理方式不同。 416. 分割等和子集 (opens new window)相当于是求背包是否正好装满，而本题是求背包最多能装多少。 12345678910111213141516171819class Solution { public int lastStoneWeightII(int[] stones) { int sum = 0; for(var i:stones){ sum+=i; } int half = sum / 2; int[] dp = new int[half+1]; for(int i=0; i&lt;stones.length; ++i){ for(int j=half; j&gt;=stones[i]; j--){ dp[j] = Math.max(dp[j], dp[j-stones[i]] + stones[i]); } } return sum - dp[half] - dp[half]; }} *494. 目标和给你一个非负整数数组 nums 和一个整数 target 。 向数组中的每个整数前添加 '+' 或 '-' ，然后串联起所有整数，可以构造一个 表达式 ： 例如，nums = [2, 1] ，可以在 2 之前添加 '+' ，在 1 之前添加 '-' ，然后串联起来得到表达式 \"+2-1\" 。 返回可以通过上述方法构造的、运算结果等于 target 的不同 表达式 的数目。 这次和之前遇到的背包问题不一样了，之前都是求容量为j的背包，最多能装多少。 本题则是装满有几种方法。其实这就是一个组合问题了。 dp[j] 表示：填满j（包括j）这么大容积的包，有dp[j]种方法 只要搞到nums[i]，凑成dp[j]就有dp[j - nums[i]] 种方法。 例如：dp[j]，j 为5， 已经有一个1（nums[i]） 的话，有 dp[4]种方法 凑成 容量为5的背包。 已经有一个2（nums[i]） 的话，有 dp[3]种方法 凑成 容量为5的背包。 已经有一个3（nums[i]） 的话，有 dp[2]中方法 凑成 容量为5的背包 已经有一个4（nums[i]） 的话，有 dp[1]中方法 凑成 容量为5的背包 已经有一个5 （nums[i]）的话，有 dp[0]中方法 凑成 容量为5的背包 那么凑整dp[5]有多少方法呢，也就是把 所有的 dp[j - nums[i]] 累加起来。 所以求组合类问题的公式，都是类似这种： 1dp[j] += dp[j - nums[i]] 1234567891011121314151617181920class Solution { public int findTargetSumWays(int[] nums, int target) { int sum = 0; for(var i:nums){ sum+=i; } int half = (target + sum) / 2; if((target+sum) % 2 == 1) return 0; if(Math.abs(target) &gt; sum) return 0; int[] dp = new int[half+1]; dp[0] = 1; for(int i=0; i&lt;nums.length; ++i){ for(int j=half; j&gt;=nums[i]; j--){ dp[j] += dp[j-nums[i]]; } } return dp[half]; }} 474. 一和零给你一个二进制字符串数组 strs 和两个整数 m 和 n 。 请你找出并返回 strs 的最大子集的长度，该子集中 最多 有 m 个 0 和 n 个 1 。 如果 x 的所有元素也是 y 的元素，集合 x 是集合 y 的 子集 。 本题中strs 数组里的元素就是物品，每个物品都是一个！ 而m 和 n相当于是一个背包，两个维度的背包。只不过这个背包有两个维度，一个是m 一个是n，而不同长度的字符串就是不同大小的待装物品。 **dp[i][j]：最多有i个0和j个1的strs的最大子集的大小为dp[i][j]**。 字符串的zeroNum和oneNum相当于物品的重量（weight[i]），字符串本身的个数相当于物品的价值（value[i]）。 12345678910111213141516171819202122232425262728class Solution { public int findMaxForm(String[] strs, int m, int n) { int num = strs.length; int[] m_nums = new int[num]; int[] n_nums = new int[num]; for(int i=0; i&lt;strs.length; ++i){ char[] c_array = strs[i].toCharArray(); int z_num = 0, o_num = 0; for(var c : c_array){ if(c == '0') z_num++; else o_num++; } m_nums[i] = z_num; n_nums[i] = o_num; } int[][] dp = new int[m+1][n+1]; for(int k = 0; k &lt; num; ++k){ for(int i = m; i &gt;= m_nums[k]; i--){ for(int j = n; j &gt;= n_nums[k]; j--){ dp[i][j] = Math.max(dp[i][j], dp[i-m_nums[k]][j-n_nums[k]]+1); } } } return dp[m][n]; }} 总结 纯0-1背包：求给定背包容量 装满背包的最大价值是多少 分割等和子集：是求 给定背包容量，能不能装满这个背包 最后一块石头的种类：求给定背包容量，尽可能装，最多能装多少 目标和：求给定背包容量，装满背包有多少种方法 一和零：给定背包容量，装满背包最多有多少个物品 三、完全背包有N件物品和一个最多能背重量为W的背包。第i件物品的重量是weight[i]，得到的价值是value[i] 。每件物品都有无限个（也就是可以放入背包多次），求解将哪些物品装入背包里物品价值总和最大。 完全背包和01背包问题唯一不同的地方就是，每种物品有无限件。 首先再回顾一下01背包的核心代码 12345for(int i = 0; i &lt; weight.size(); i++) { // 遍历物品 for(int j = bagWeight; j &gt;= weight[i]; j--) { // 遍历背包容量 dp[j] = max(dp[j], dp[j - weight[i]] + value[i]); }} 我们知道01背包内嵌的循环是从大到小遍历，为了保证每个物品仅被添加一次。 而完全背包的物品是可以添加多次的，所以要从小到大去遍历，即： 1234567// 先遍历物品，再遍历背包for(int i = 0; i &lt; weight.size(); i++) { // 遍历物品 for(int j = weight[i]; j &lt;= bagWeight ; j++) { // 遍历背包容量 dp[j] = max(dp[j], dp[j - weight[i]] + value[i]); }} 在完全背包中，对于一维dp数组来说，其实两个for循环嵌套顺序是无所谓的！ 因为dp[j] 是根据 下标j之前所对应的dp[j]计算出来的。 只要保证下标j之前的dp[j]都是经过计算的就可以了。 先遍历背包在遍历物品，代码如下： 1234567// 先遍历背包，再遍历物品for(int j = 0; j &lt;= bagWeight; j++) { // 遍历背包容量 for(int i = 0; i &lt; weight.size(); i++) { // 遍历物品 if (j - weight[i] &gt;= 0) dp[j] = max(dp[j], dp[j - weight[i]] + value[i]); } cout &lt;&lt; endl;} 518. 零钱兑换 II给你一个整数数组 coins 表示不同面额的硬币，另给一个整数 amount 表示总金额。 请你计算并返回可以凑成总金额的硬币组合数。如果任何硬币组合都无法凑出总金额，返回 0 。 假设每一种面额的硬币有无限个。 题目数据保证结果符合 32 位带符号整数。 思路： 同494.目标和一样是求装满背包有多少种组合 注意： 标准的完全背包问题，两次循环的顺序是可以交换的，因为纯完全背包问题球装满背包的最大价值是多少，和凑成总和的元素有没有顺序没有关系 本题求凑成综合的组合数，元素之间明确要求没有顺序。那么： 先遍历物品，再遍历背包容量——求组合数 先遍历背包容量，再遍历物品——求排列数 123456789101112class Solution { public int change(int amount, int[] coins) { int[] dp = new int[amount+1]; dp[0]=1; for(int i=0; i&lt;coins.length; ++i){ for(int j=coins[i]; j&lt;=amount; ++j){ dp[j] += dp[j-coins[i]]; } } return dp[amount]; }} 377. 组合总和 Ⅳ给你一个由 不同 整数组成的数组 nums ，和一个目标整数 target 。请你从 nums 中找出并返回总和为 target 的元素组合的个数。 题目数据保证答案符合 32 位整数范围。 示例 1： 输入：nums = [1,2,3], target = 4输出：7解释：所有可能的组合为：(1, 1, 1, 1)(1, 1, 2)(1, 2, 1)(1, 3)(2, 1, 1)(2, 2)(3, 1)请注意，顺序不同的序列被视作不同的组合。 1234567891011121314class Solution { public int combinationSum4(int[] nums, int target) { int n = nums.length; int[] dp = new int[target+1]; //总和为i的元素组合的个数 dp[0]=1; for(int j=0; j&lt;=target; ++j){ for(int i=0; i&lt;n; ++i){ if(nums[i]&lt;=j) dp[j] += dp[j-nums[i]]; } } return dp[target]; }} 个人觉得这道题不是背包问题，也没必要套用背包问题的代码框架，避免纠结于物品遍历内外层循环的问题。 其实这道题换个问法就会觉得很简单： 楼梯的长度为target,每次爬楼梯可选的层数从nums数组中挑选，问有几种爬法？ 70. 爬楼梯（进阶）改为：一步一个台阶，两个台阶，三个台阶，…….，直到 m个台阶。问有多少种不同的方法可以爬到楼顶呢？ 本题看起来是一道简单题目，稍稍进阶一下其实就是一个完全背包！ 如果我来面试的话，我就会先给候选人出一个 本题原题，看其表现，如果顺利写出来，进而在要求每次可以爬[1 - m]个台阶应该怎么写。 顺便再考察一下两个for循环的嵌套顺序，为什么target放外面，nums放里面。 322. 零钱兑换给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。 计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。 你可以认为每种硬币的数量是无限的。 示例 1： 123输入：coins = [1, 2, 5], amount = 11输出：3 解释：11 = 5 + 5 + 1 示例 2： 12输入：coins = [2], amount = 3输出：-1 示例 3： 12输入：coins = [1], amount = 0输出：0 12345678910111213class Solution { public int coinChange(int[] coins, int amount) { int[] dp = new int[amount+1];//凑成i的最少硬币个数 Arrays.setAll(dp, i -&gt; Integer.MAX_VALUE); dp[0] = 0; for(int i = 0; i &lt; coins.length; ++i){ for(int j = coins[i]; j&lt;=amount; ++j){ if(dp[j-coins[i]] != Integer.MAX_VALUE) dp[j] = Math.min(dp[j], dp[j-coins[i]]+1); } } return dp[amount] == Integer.MAX_VALUE ? -1 : dp[amount]; }} 需要注意只有当dp[j-coins[i]]之前被改变 后面才能用，不然会溢出 本题求钱币最小个数，那么钱币有顺序和没有顺序都可以，都不影响钱币的最小个数。 所以本题并不强调集合是组合还是排列。 如果求组合数就是外层for循环遍历物品，内层for遍历背包。 如果求排列数就是外层for遍历背包，内层for循环遍历物品。 279. 完全平方数给你一个整数 n ，返回 和为 n 的完全平方数的最少数量 。 完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。 示例 1： 123输入：n = 12输出：3 解释：12 = 4 + 4 + 4 示例 2： 123输入：n = 13输出：2解释：13 = 4 + 9 12345678910111213class Solution { public int numSquares(int n) { int[] dp = new int[n+1]; Arrays.setAll(dp, i -&gt; Integer.MAX_VALUE); dp[0] = 0; for(int i=1; i&lt;=n ; ++i){ for(int j=i*i; j&lt;=n; ++j){ if(dp[j-i*i] != Integer.MAX_VALUE) dp[j] = Math.min(dp[j], dp[j-i*i]+1); } } return dp[n] == Integer.MAX_VALUE ? -1 : dp[n]; }} *139. 单词拆分给你一个字符串 s 和一个字符串列表 wordDict 作为字典。请你判断是否可以利用字典中出现的单词拼接出 s 。 注意：不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。 示例 1： 123输入: s = \"leetcode\", wordDict = [\"leet\", \"code\"]输出: true解释: 返回 true 因为 \"leetcode\" 可以由 \"leet\" 和 \"code\" 拼接成。 示例 2： 1234输入: s = \"applepenapple\", wordDict = [\"apple\", \"pen\"]输出: true解释: 返回 true 因为 \"applepenapple\" 可以由 \"apple\" \"pen\" \"apple\" 拼接成。注意，你可以重复使用字典中的单词。 示例 3： 12输入: s = \"catsandog\", wordDict = [\"cats\", \"dog\", \"sand\", \"and\", \"cat\"]输出: false 一刷： 123456789101112131415class Solution { public boolean wordBreak(String s, List&lt;String&gt; wordDict) { boolean[] dp = new boolean[s.length()+1]; // [0,i)能否用wordDict中的单词拼接 dp[0] = true; for(int i = 1; i &lt;= s.length(); ++i){ for(int j = 0; j &lt; wordDict.size(); ++j){ int wordSize = wordDict.get(j).length(); if(i-wordSize&gt;=0 &amp;&amp; dp[i-wordSize] &amp;&amp; wordDict.get(j).equals(s.substring(i-wordSize,i))) dp[i] = dp[i - wordSize]; } } return dp[s.length()]; }} 出现问题： 出现已经dp被打上1但是在内层循环会被其他情况覆盖为0 解决： 判断dp[i-wordsize] == 1 判断当前dp[i]是否为1 如果是1就终止内层循环 思路： 考虑遍历顺序时，可以理解该问题为排列问题，所以本体是先遍历背包，再遍历物品 140. 单词拆分 II给定一个字符串 s 和一个字符串字典 wordDict ，在字符串 s 中增加空格来构建一个句子，使得句子中所有的单词都在词典中。以任意顺序 返回所有这些可能的句子。 注意：词典中的同一个单词可能在分段中被重复使用多次。 示例 1： 12输入:s = \"catsanddog\", wordDict = [\"cat\",\"cats\",\"and\",\"sand\",\"dog\"]输出:[\"cats and dog\",\"cat sand dog\"] 示例 2： 123输入:s = \"pineapplepenapple\", wordDict = [\"apple\",\"pen\",\"applepen\",\"pine\",\"pineapple\"]输出:[\"pine apple pen apple\",\"pineapple pen apple\",\"pine applepen apple\"]解释: 注意你可以重复使用字典中的单词。 示例 3： 12输入:s = \"catsandog\", wordDict = [\"cats\",\"dog\",\"sand\",\"and\",\"cat\"]输出:[] 思路：除了定义一个dp数组判断[0,i]能否用字典里面的word拼凑，还需要一个sentenseList来存放当前[0,i]的所有分词结果 123456789101112131415161718192021222324252627class Solution { public List&lt;String&gt; wordBreak(String s, List&lt;String&gt; wordDict) { int sLength = s.length(); boolean[] dp = new boolean[sLength+1]; List&lt;String&gt;[] sentenseList = new ArrayList[sLength+1]; Arrays.setAll(sentenseList, e -&gt; new ArrayList&lt;String&gt;()); sentenseList[0].add(\"\"); List&lt;String&gt; ans = new ArrayList(); dp[0] = true; for(int i = 1; i &lt;= sLength; ++i){ for(int j = 0; j &lt; wordDict.size(); ++j){ String word = wordDict.get(j); if(i-word.length()&gt;=0 &amp;&amp; dp[i-word.length()] &amp;&amp; word.equals(s.substring(i-word.length(),i))){ dp[i] = dp[i-word.length()]; for(String sentense : sentenseList[i-word.length()]){ String stmp = sentense+\" \"+word; sentenseList[i].add(stmp.trim()); } } } } return sentenseList[sLength]; }} 四、背包问题总结背包递推公式 问能否能装满背包（或者最多装多少）：$$dp[j] = max(dp[j], dp[j-weight[i]] + weight[i])$$ ​ 对应题目： 416、1049 问装满背包有几种方法：$$dp[j] += dp[j-weight[i]]$$对应题目：494、518、377、70 问背包装满最大价值：$$dp[j]=max(dp[j],dp[j-weight[i]]+value[i])$$对应题目：474 问装满背包所有物品的最小个数$$dp[j] = min(dp[j],dp[j-weight[i]]+1)$$对应题目：322、279 遍历顺序01背包 二维dp数组01背包先遍历物品还是先遍历背包都是可以的，且两层for循环是从小到大遍历。 一维dp数组01背包只能先遍历物品再遍历背包容量，且第二层for循环是从大到小遍历。 完全背包 纯完全背包的一维dp数组实现，先遍历物品还是先遍历背包都是可以的，且第二层for循环是从小到大遍历。 如果求组合数就是外层for循环遍历物品，内层for遍历背包。 如果求排列数就是外层for遍历背包，内层for循环遍历物品。 五、打家劫舍198. 打家劫舍你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。 示例 1： 1234输入：[1,2,3,1]输出：4解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2： 1234输入：[2,7,9,3,1]输出：12解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 123456789101112131415class Solution { public int rob(int[] nums) { //dp[i] [0,i] 能偷窃到的最高金额 int[] dp = new int[nums.length]; if(nums.length == 1) return nums[0]; // 根据dp数组的定义 初始化 dp[0] = nums[0]; dp[1] = Math.max(dp[0], nums[1]); for(int i=2; i&lt;nums.length; ++i){ // dp[i] 取决于偷不偷这个房子 dp[i] = Math.max(dp[i-1], dp[i-2]+nums[i]); } return dp[nums.length-1]; }} 213. 打家劫舍 II你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。 给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额。 示例 1： 123输入：nums = [2,3,2]输出：3解释：你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 示例 2： 1234输入：nums = [1,2,3,1]输出：4解释：你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 3： 12输入：nums = [1,2,3]输出：3 123456789101112131415161718192021class Solution { public int rob(int[] nums) { int[] dp = new int[nums.length]; if(nums.length == 1) return nums[0]; // 根据dp数组的定义 初始化 dp[0] = nums[0]; dp[1] = Math.max(dp[0], nums[1]); for(int i=2; i&lt;nums.length-1; ++i){ // dp[i] 取决于偷不偷这个房子 dp[i] = Math.max(dp[i-1], dp[i-2]+nums[i]); } int ans = dp[nums.length-2]; dp[0] = 0; dp[1] = nums[1]; for(int i=2; i&lt;nums.length; ++i){ // dp[i] 取决于偷不偷这个房子 dp[i] = Math.max(dp[i-1], dp[i-2]+nums[i]); } return Math.max(ans, dp[nums.length-1]); }} 这题拆成两列可能跳的有点大 我的想法是：基于打家劫舍第一个题目，选第一个打劫不会影响选最后一个打劫。那么换成这道题，选第一个打劫会影响选最后一个打劫，那么我就把题目换成固定先打劫第一个不打劫最后一个，看能打劫多少；然后再去看固定打劫第二个可以打劫最后一个，看能打劫多少；最后再看固定前两个都不打劫，看能打劫多少。这样就能得到最后的结果，其实时间复杂度都是O(n) 337. 打家劫舍 III小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为 root 。 除了 root 之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果 两个直接相连的房子在同一天晚上被打劫 ，房屋将自动报警。 给定二叉树的 root 。返回 在不触动警报的情况下 ，小偷能够盗取的最高金额 。 树形dp入门题 12// 不偷：Max(左孩子不偷，左孩子偷) + Max(又孩子不偷，右孩子偷)// 偷：左孩子不偷+ 右孩子不偷 + 当前节点偷 123456789101112131415161718class Solution { public int rob(TreeNode root) { int[] ans = postorder(root); return Math.max(ans[0],ans[1]); } int[] postorder(TreeNode root){ if(root == null) return new int[]{0,0}; int[] left = postorder(root.left); int[] right = postorder(root.right); // 偷cur，那么就不能偷左右节点。 int nsteal = left[0]+right[0]+root.val; // 不偷cur，那么可以偷也可以不偷左右节点，则取较大的情况 int steal = Math.max(left[0],left[1])+Math.max(right[0],right[1]); return new int[]{steal, nsteal}; }} 1186. 删除一次得到子数组最大和1234567891011121314class Solution { public int maximumSum(int[] arr) { int ans = arr[0]; int dp0 = arr[0]; int dp1 = 0; for(int i=1; i&lt;arr.length; ++i){ //dp1 和 dp0 顺序还不能反 dp1 = Math.max(dp0, dp1 + arr[i]); dp0 = Math.max(dp0 , 0) + arr[i]; ans = Math.max(ans,Math.max(dp0, dp1)); } return ans; }} 本题是典型的动态规划应用题，我们可以将问题拆分成多个子问题，即求解以 arr[i] 结尾的最多删除一次的非空子数组的最大和。 dp[ i ][ 0 ]表示以arr[i] 为结尾最多删除0次的非空子数组的最大和 dp[ i ][ 1 ]表示以arr[i] 为结尾最多删除1次的非空子数组的最大和 第一个转移方程表示在不删除的情况下， 以arr[i] 为结尾的非空子数组的最大和dp[i][0]与dp[i-1][0]有关，当dp[i-1][0] &gt;0时，直接将arr[i]与i-1时的最大非空子数组连接时，取得最大和，非则只选arr[i]时，取得最大和。 第二个转移方程式表示在删除的情况下， 以arr[i] 为结尾的非空子数组有两种情况： 不删除arr[i] 删除arr[i] 六、买卖股票系列121. 买卖股票的最佳时机给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 示例 1： 1234输入：[7,1,5,3,6,4]输出：5解释：在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 示例 2： 123输入：prices = [7,6,4,3,1]输出：0解释：在这种情况下, 没有交易完成, 所以最大利润为 0。 贪心：思路： 遍历数组的时候记录[0,i)可以买到的最小股票 1234567891011class Solution { public int maxProfit(int[] prices) { int dp0 = prices[0]; //[0,i)可以买的最小 int ans = 0; for(int i=1; i&lt;prices.length; ++i){ ans = Math.max(ans, prices[i]-dp0); dp0 = Math.min(dp0, prices[i]); } return ans; }} 动态规划： 思路： 状态有两种： 天数 1&lt;=i&lt;=n （第0天要作为base） 持有状态 {0,1} dp数组定义： dp[i][0] 第i天不持有股票所得最大现金 (包括之前不持有和当前不持有两种情况) dp[i][1] 第i天持有股票所得最大现金 (包括之前持有和当前持有两种情况) 根据该图，写出状态转移方程 12dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1] + price[i-1]) = Math.max(今天选择rest， 今天选择sell ) 12dp[i][1] = Math.max(dp[i-1][1], dp[i-1][0] - price[i-1]) = Math.max(今天选择rest， 今天选择buy ) basic case 第0天为basic case，即还没有开始的时候，状态0的时候利润为0；状态为1的时候，相当于第0天买入股票即-prices[0] code 12345678910111213141516171819202122class Solution { // dp 做法 /** dp[i][0] 第i天不持有股票所得最大现金 dp[i][1] 第i天持有股票所得最大现金 天数 0 ~ prices.length 交易数 0 ~ 1 持有 1 未持有 0 */ public int maxProfit(int[] prices) { int[][] dp = new int[prices.length][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for(int i=1; i&lt;prices.length; ++i){ dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1] + prices[i]); dp[i][1] = Math.max(dp[i-1][1], - prices[i]); } return dp[prices.length-1][0]; }} 122. 买卖股票的最佳时机 II给你一个整数数组 prices ，其中 prices[i] 表示某支股票第 i 天的价格。 在每一天，你可以决定是否购买和/或出售股票。你在任何时候 最多 只能持有 一股 股票。你也可以先购买，然后在 同一天 出售。 返回 你能获得的 最大 利润 。 示例 1： 12345输入：prices = [7,1,5,3,6,4]输出：7解释：在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5 - 1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6 - 3 = 3 。 总利润为 4 + 3 = 7 。 示例 2： 1234输入：prices = [1,2,3,4,5]输出：4解释：在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5 - 1 = 4 。 总利润为 4 。 示例 3： 123输入：prices = [7,6,4,3,1]输出：0解释：在这种情况下, 交易无法获得正利润，所以不参与交易可以获得最大利润，最大利润为 0 。 贪心：1234567891011121314class Solution { public int maxProfit(int[] prices) { // 贪心策略就是用一个cur_min记录[0,i）最小值 一旦发现prices[i]&gt;cur_min 那么就加到ans里，然后修改cur_min为当前值 int ans = 0; int cur_min = Integer.MAX_VALUE; for(int i : prices){ if(i &gt;= cur_min){ ans += i-cur_min; } cur_min = i; } return ans; }} 动态规划：思路： ​ 这里需要想与121题的区别。121题是只能买一次，而本题是可以多次购买。那么我们思考一下dp[i][1]和dp[i][0]的含义，前者是指持有股票下的债务情况，在121题情况下一般是负值，期望寻找[0,i]的最小买入价格；后者是指未持有股票下的盈利情况，在121题情况下是想在[0,i]区间找到一个最佳的卖出时机。 ​ 那么返回本题，是不是要考虑累计盈利呢——等价与要在中间过程中把卖出的盈利额也加入进去。那么就可以考虑将i-1时未持有股票时手上的金额加入到i时持有股票的判断中 1234567891011121314class Solution { public int maxProfit(int[] prices) { int[][] dp = new int[prices.length][2]; dp[0][0] = 0; dp[0][1] = -prices[0]; for(int i=1; i&lt;prices.length; ++i){ dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1] + prices[i]); dp[i][1] = Math.max(dp[i-1][1], dp[i-1][0] - prices[i]); } return dp[prices.length-1][0]; }} 123. 买卖股票的最佳时机 III给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。 注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 思路：多出一个状态即最多交易笔数 1&lt;=k&lt;=Kbasic case ： 我们需要观察状态转移方程中，有i-1以及j-1那么就要考虑dp[0][…][] 1234567891011121314151617181920class Solution { public int maxProfit(int[] prices) { // 天数 0 ~ prices.length // 交易数 0 ~ k // 持有 1 未持有 0 int k = 2; int[][][] dp = new int[prices.length][k+1][2]; for(int i=1; i&lt;=k; ++i){ dp[0][i][1] = -prices[0]; } for(int i=1; i&lt;prices.length; ++i){ for(int j=1; j&lt;=k; ++j){ dp[i][j][0] = Math.max(dp[i-1][j][0], dp[i-1][j][1] + prices[i]); dp[i][j][1] = Math.max(dp[i-1][j][1], dp[i-1][j-1][0] - prices[i]); } } return dp[prices.length-1][k][0]; }} 188. 买卖股票的最佳时机 IV思路：同123题，开一个列长为2*k+1大小的dp数组 123456789101112131415161718class Solution { public int maxProfit(int k, int[] prices) { int[][] dp = new int[prices.length][2*k+1]; for(int i=0; i&lt;k; ++i){ dp[0][i*2+1] -= prices[0]; } for(int i=1; i&lt;prices.length; ++i){ for(int j=0; j&lt;k; ++j){ dp[i][j*2+1] = Math.max(dp[i-1][j*2+1], dp[i-1][j*2]- prices[i]); dp[i][j*2+2] = Math.max(dp[i-1][j*2+2], dp[i-1][j*2+1] + prices[i]); } } return dp[prices.length-1][2*k]; }} 309. 最佳买卖股票时机含冷冻期123456789101112131415class Solution { public int maxProfit(int[] prices) { int[][] dp = new int[prices.length][2]; dp[0][0] = -prices[0]; dp[0][1] = 0; for(int i=1; i&lt;prices.length; ++i){ // 因为有冷冻期 所以dp[i][0]买股票的时候累加的收益应该是i-2处的 if(i==1) dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1]-prices[i]); else dp[i][0] = Math.max(dp[i-1][0], dp[i-2][1]-prices[i]); dp[i][1] = Math.max(dp[i-1][1], dp[i-1][0]+prices[i]); } return dp[prices.length-1][1]; }} 14. 买卖股票的最佳时机含手续费1234567891011121314class Solution { public int maxProfit(int[] prices, int fee) { int ans = 0; int[][] dp = new int[prices.length][2]; dp[0][0] = -prices[0]; dp[0][1] = 0; for(int i=1; i&lt;prices.length; ++i){ dp[i][0] = Math.max(dp[i-1][0], dp[i-1][1] - prices[i]); // 把累计的收益加入当前收益中 dp[i][1] = Math.max(dp[i-1][1], dp[i-1][0] + prices[i] - fee); } return dp[prices.length-1][1]; }} 718. 最长重复子数组12345678910111213141516class Solution { public int findLength(int[] nums1, int[] nums2) { int ans = 0; // dp[i][j] 以nums1[i-1]和以nums2[j-1]为结尾的最长公共子数组长度 int[][] dp = new int[nums1.length+1][nums2.length+1]; for(int i=1; i&lt;=nums1.length; i++){ // i指向nums1 j指向nums2 for(int j=1; j&lt;=nums2.length; j++){ if(nums1[i-1] == nums2[j-1]) dp[i][j] = dp[i-1][j-1] + 1; ans = Math.max(ans, dp[i][j]); } } return ans; }} 思路：其实就是遇到俩个字符串要摒弃惯性思维以为一维dp就能解决 392. 判断子序列给定字符串 s 和 t ，判断 s 是否为 t 的子序列。 这道题目 其实是可以用双指针或者贪心的的，但是我在开篇的时候就说了这是编辑距离的入门题目，因为从题意中我们也可以发现，只需要计算删除的情况，不用考虑增加和替换的情况。 if (s[i - 1] == t[j - 1]) t中找到了一个字符在s中也出现了 if (s[i - 1] != t[j - 1]) 相当于t要删除元素，继续匹配 状态转移方程： 12if (s[i - 1] == t[j - 1]) dp[i][j] = dp[i - 1][j - 1] + 1;else dp[i][j] = dp[i][j - 1]; 115. 不同的子序列给定一个字符串 s 和一个字符串 t ，计算在 s 的子序列中 t 出现的个数。 当s[i - 1] 与 t[j - 1]相等时，dp[i][j]可以有两部分组成。 一部分是用s[i - 1]来匹配，那么个数为dp[i - 1][j - 1]。 一部分是不用s[i - 1]来匹配，个数为dp[i - 1][j]。 这里可能有同学不明白了，为什么还要考虑 不用s[i - 1]来匹配，都相同了指定要匹配啊。 例如： s：bagg 和 t：bag ，s[3] 和 t[2]是相同的，但是字符串s也可以不用s[3]来匹配，即用s[0]s[1]s[2]组成的bag。 当然也可以用s[3]来匹配，即：s[0]s[1]s[3]组成的bag。 所以当s[i - 1] 与 t[j - 1]相等时，dp[i][j] = dp[i - 1][j - 1] + dp[i - 1][j]; 当s[i - 1] 与 t[j - 1]不相等时，dp[i][j]只有一部分组成，不用s[i - 1]来匹配，即：dp[i - 1][j] 所以递推公式为：dp[i][j] = dp[i - 1][j]; 状态转移方程： 12345if (s[i - 1] == t[j - 1]) { dp[i][j] = dp[i - 1][j - 1] + dp[i - 1][j];} else { dp[i][j] = dp[i - 1][j];} 583. 两个字符串的删除操作动态规划：583.两个字符串的删除操作 (opens new window)给定两个单词 word1 和 word2，找到使得 word1 和 word2 相同所需的最小步数，每步可以删除任意一个字符串中的一个字符。 本题和动态规划：115.不同的子序列 (opens new window)相比，其实就是两个字符串可以都可以删除了，情况虽说复杂一些，但整体思路是不变的。 当word1[i - 1] 与 word2[j - 1]相同的时候 当word1[i - 1] 与 word2[j - 1]不相同的时候 当word1[i - 1] 与 word2[j - 1]相同的时候，dp[i][j] = dp[i - 1][j - 1]; 当word1[i - 1] 与 word2[j - 1]不相同的时候，有三种情况： 情况一：删word1[i - 1]，最少操作次数为dp[i - 1][j] + 1 情况二：删word2[j - 1]，最少操作次数为dp[i][j - 1] + 1 情况三：同时删word1[i - 1]和word2[j - 1]，操作的最少次数为dp[i - 1][j - 1] + 2 那最后当然是取最小值，所以当word1[i - 1] 与 word2[j - 1]不相同的时候，递推公式：dp[i][j] = min({dp[i - 1][j - 1] + 2, dp[i - 1][j] + 1, dp[i][j - 1] + 1}); 状态转移方程： 12345if (word1[i - 1] == word2[j - 1]) { dp[i][j] = dp[i - 1][j - 1];} else { dp[i][j] = min({dp[i - 1][j - 1] + 2, dp[i - 1][j] + 1, dp[i][j - 1] + 1});} 72. 编辑距离动态规划：72.编辑距离 (opens new window)给你两个单词 word1 和 word2，请你计算出将 word1 转换成 word2 所使用的最少操作数 。 编辑距离终于来了，有了前面三道题目的铺垫，应该有思路了，本题是两个字符串可以增删改，比 动态规划：判断子序列 (opens new window)，动态规划：不同的子序列 (opens new window)，动态规划：两个字符串的删除操作 (opens new window)都要复杂的多。 在确定递推公式的时候，首先要考虑清楚编辑的几种操作，整理如下： if (word1[i - 1] == word2[j - 1]) 不操作 if (word1[i - 1] != word2[j - 1]) 增 删 换 也就是如上四种情况。 if (word1[i - 1] == word2[j - 1]) 那么说明不用任何编辑，dp[i][j] 就应该是 dp[i - 1][j - 1]，即dp[i][j] = dp[i - 1][j - 1]; 此时可能有同学有点不明白，为啥要即dp[i][j] = dp[i - 1][j - 1]呢？ 那么就在回顾上面讲过的dp[i][j]的定义，word1[i - 1] 与 word2[j - 1]相等了，那么就不用编辑了，以下标i-2为结尾的字符串word1和以下标j-2为结尾的字符串word2的最近编辑距离dp[i - 1][j - 1] 就是 dp[i][j]了。 在下面的讲解中，如果哪里看不懂，就回想一下dp[i][j]的定义，就明白了。 在整个动规的过程中，最为关键就是正确理解dp[i][j]的定义！ if (word1[i - 1] != word2[j - 1])，此时就需要编辑了，如何编辑呢？ 操作一：word1增加一个元素，使其word1[i - 1]与word2[j - 1]相同，那么就是以下标i-2为结尾的word1 与 i-1为结尾的word2的最近编辑距离 加上一个增加元素的操作。 即 dp[i][j] = dp[i - 1][j] + 1; 操作二：word2添加一个元素，使其word1[i - 1]与word2[j - 1]相同，那么就是以下标i-1为结尾的word1 与 j-2为结尾的word2的最近编辑距离 加上一个增加元素的操作。 即 dp[i][j] = dp[i][j - 1] + 1; 这里有同学发现了，怎么都是添加元素，删除元素去哪了。 word2添加一个元素，相当于word1删除一个元素，例如 word1 = “ad” ，word2 = “a”，word2添加一个元素d，也就是相当于word1删除一个元素d，操作数是一样！ 操作三：替换元素，word1替换word1[i - 1]，使其与word2[j - 1]相同，此时不用增加元素，那么以下标i-2为结尾的word1 与 j-2为结尾的word2的最近编辑距离 加上一个替换元素的操作。 即 dp[i][j] = dp[i - 1][j - 1] + 1; 综上，当 if (word1[i - 1] != word2[j - 1]) 时取最小的，即：dp[i][j] = min({dp[i - 1][j - 1], dp[i - 1][j], dp[i][j - 1]}) + 1; 递归公式代码如下： 123456if (word1[i - 1] == word2[j - 1]) { dp[i][j] = dp[i - 1][j - 1];}else { dp[i][j] = min({dp[i - 1][j - 1], dp[i - 1][j], dp[i][j - 1]}) + 1;}","link":"/2023/02/04/%E7%AE%97%E6%B3%95/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"title":"模拟","text":"54. 螺旋矩阵给你一个 m 行 n 列的矩阵 matrix ，请按照 顺时针螺旋顺序 ，返回矩阵中的所有元素。 示例 1： 12输入：matrix = [[1,2,3],[4,5,6],[7,8,9]]输出：[1,2,3,6,9,8,7,4,5] 示例 2： 12输入：matrix = [[1,2,3,4],[5,6,7,8],[9,10,11,12]]输出：[1,2,3,4,8,12,11,10,9,5,6,7] 思路： 初始化： 矩阵 左、右、上、下 四个边界 l , r , t , b ，用于打印的结果列表 res 。 循环打印： “从左向右、从上向下、从右向左、从下向上” 四个方向循环打印。 根据边界打印，即将元素按顺序添加至列表 res 尾部。 边界向内收缩 1 （代表已被打印）。 判断边界是否相遇（是否打印完毕），若打印完毕则跳出。 1234567891011121314151617181920212223242526272829class Solution { public List&lt;Integer&gt; spiralOrder(int[][] matrix) { if (matrix == null || matrix.length == 0 || matrix[0].length == 0) { return new ArrayList&lt;&gt;(); } List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); int l = 0, r = matrix[0].length-1, t = 0, b = matrix.length-1; while (true) { for (int j=l; j&lt;=r; ++j) { ans.add(matrix[t][j]); } if (++t &gt; b) break; for (int i=t; i&lt;=b; ++i) { ans.add(matrix[i][r]); } if (l &gt; --r) break; for (int j=r; j&gt;=l; --j) { ans.add(matrix[b][j]); } if (t &gt; --b) break; for (int i=b; i&gt;=t; --i) { ans.add(matrix[i][l]); } if (++l &gt; r) break; } return ans; }}","link":"/2023/02/02/%E7%AE%97%E6%B3%95/%E6%A8%A1%E6%8B%9F/"},{"title":"手撕","text":"题库目录 题目 难度 类型 掌握程度 438. 找到字符串中所有字母异位词 中等 滑动窗口+哈希表 ⭐ 189. 轮转数组 中等 翻转前要mod ⭐三种方法 238. 除自身以外数组的乘积 中等 前缀和 ⭐ 41. 缺失的第一个正数 困难 原地哈希 ⭐ 73. 矩阵置零 中等 ⭐空间复杂度O(mn)O(m+n)O(1) 54. 螺旋矩阵 中等 ⭐上下左右四个分界线 206. 反转链表 简单 迭代、递归 234. 回文链表 简单 双指针找中间节点，翻转链表 142. 环形链表 II 中等 双指针 2. 两数相加 中等 双指针 19. 删除链表的倒数第 N 个结点 中等 双指针 24. 两两交换链表中的节点 中等 递归⭐、迭代 25. K 个一组翻转链表 困难 递归 138. 随机链表的复制 中等 哈希表 148. 排序链表 中等 归并（找中点直到一个元素，然后合并） ⭐注意当只剩下一个元素就要跳出循环 23. 合并 K 个升序链表 困难 归并、优先队列 ⭐ 146. LRU 缓存 中等 哈希+双向链表 ⭐⭐ 94. 二叉树的中序遍历 简单 递归、栈迭代（颜色法）⭐ ⭐⭐前中后的迭代法 101. 对称二叉树 简单 dfs-递归、bfs-迭代⭐ 543. 二叉树的直径 简单 dfs ⭐ 108. 将有序数组转换为二叉搜索树 简单 分治 98. 验证二叉搜索树 中等 114. 二叉树展开为链表 中等 后续遍历 ⭐ 45. 跳跃游戏 II 中等 1190. 反转每对括号间的子串 中等 781. 森林中的兔子 中等 739. 每日温度 中等 3. 无重复字符的最长子串 中等 46. 全排列 中等 475. 供暖器 中等 20. 有效的括号 简单 394. 字符串解码 中等 179. 最大数 中等 LCP 09. 最小跳跃次数 困难 135. 分发糖果 困难 面试题 17.24. 最大子矩阵 困难 1. 两数之和 简单 哈希 49. 字母异位词分组 中等 哈希 ⭐注意char[]如何转换String 128.最长连续序列 中等 哈希 ⭐ 42. 接雨水 困难 单调栈 22. 括号生成 中等 554. 砖墙 中等 547. 省份数量 中等 55. 跳跃游戏 中等 621. 任务调度器 中等 1047. 删除字符串中的所有相邻重复项 简单 514. 自由之路 困难 5. 最长回文子串 中等 14. 最长公共前缀 简单 300. 最长递增子序列 中等 56. 合并区间 中等 排序+双指针 ⭐⭐ 200. 岛屿数量 中等 11. 盛最多水的容器 中等 双指针 799. 香槟塔 中等 316. 去除重复字母 中等 ⭐ 392. 判断子序列 简单 85. 最大矩形 困难 简单 221. 最大正方形 中等 ⭐ 32. 最长有效括号 困难 62. 不同路径 中等 中等 ⭐ 130. 被围绕的区域 中等 679. 24 点游戏 困难 ⭐ 70. 爬楼梯 简单 17. 电话号码的字母组合 中等 中等 15. 三数之和 中等 双指针 ⭐ 151. 翻转字符串里的单词 中等 957. N 天后的牢房 中等 ⭐ 518. 零钱兑换 II 中等 139. 单词拆分 中等 ⭐ 179. 最大数 中等 ⭐ 410. 分割数组的最大值 困难 ⭐ 47. 全排列 II 中等 ⭐ 860. 柠檬水找零 简单 90. 子集 II 中等 ⭐ 980. 不同路径 III 困难 ⭐ 471. 编码最短长度的字符串 困难 93. 复原 IP 地址 中等 224. 基本计算器 困难 4. 寻找两个正序数组的中位数 困难 ⭐ 88. 合并两个有序数组 简单 692. 前K个高频单词 中等 ⭐ 38. 外观数列 中等 64. 最小路径和 中等 30. 串联所有单词的子串 困难 ⭐ 735. 行星碰撞 中等 165. 比较版本号 中等 354. 俄罗斯套娃信封问题 困难 1160. 拼写单词 简单 678. 有效的括号字符串 中等 ⭐ 102. 二叉树的层序遍历 中等 287. 寻找重复数 中等 二分法、快慢指针 ⭐⭐ 16. 最接近的三数之和 中等 264. 丑数 II 中等 ⭐ 1293. 网格中的最短路径 困难 ⭐ 剑指 Offer 38. 字符串的排列 中等 40. 组合总和 II 中等 746. 使用最小花费爬楼梯 简单 974. 和可被 K 整除的子数组 中等 ⭐ 685. 冗余连接 II 困难 ⭐ 260. 只出现一次的数字 III 中等 37. 解数独 困难 72. 编辑距离 困难 695. 岛屿的最大面积 中等 36. 有效的数独 中等 84. 柱状图中最大的矩形 困难 ⭐ 148. 排序链表 中等 199. 二叉树的右视图 中等 121. 买卖股票的最佳时机 简单 dp 322. 零钱兑换 中等 dp 741. 摘樱桃 困难 [记忆化搜索](741. 摘樱桃 - 力扣（LeetCode）) ⭐ 214. 最短回文串 困难 kmp ⭐ 406. 根据身高重建队列 中等 贪心 ⭐ 1162. 地图分析 中等 bfs ⭐ 402. 移掉 K 位数字 中等 单调栈 343. 整数拆分 中等 dp ⭐ 617. 合并二叉树 简单 dfs 387. 字符串中的第一个唯一字符 简单 哈希表、数组 76. 最小覆盖子串 困难 滑动窗口 ⭐ 34. 在排序数组中查找元素的第一个和最后一个位置 中等 二分 29. 两数相除 中等 位运算 ⭐ 105. 从前序与中序遍历序列构造二叉树 中等 分治 451. 根据字符出现频率排序 中等 哈希表+优先队列 1109. 航班预订统计 中等 差分 ⭐ 166. 分数到小数 中等 模拟 ⭐ 278. 第一个错误的版本 简单 二分 207. 课程表 中等 bfs ⭐ 875. 爱吃香蕉的珂珂 中等 二分 134. 加油站 中等 后缀和 剑指 Offer 22. 链表中倒数第k个节点 简单 双指针 611. 有效三角形的个数 中等 双指针 ⭐ LCR 182. 动态口令 简单 ⭐ 面试题 02.05. 链表求和 中等 正向——栈 面试题 01.06. 字符串压缩 简单 239. 滑动窗口的最大值 困难 优先队列 ⭐⭐ 155. 最小栈 中等 栈 ⭐ LCR 120. 寻找文件副本 简单 初级：哈希表 进阶：原地交换 ⭐ 面试题 08.11. 硬币 中等 完全0-1背包问题 ⭐ 剑指 Offer 50. 第一个只出现一次的字符 简单 哈希 994. 腐烂的橘子 中等 bfs 70. 爬楼梯 简单 dp 剑指 Offer 13. 机器人的运动范围 中等 dfs、bfs 1004. 最大连续1的个数 III 中等 滑动窗口 ⭐ \\1494. 并行课程 II 1386. 安排电影院座位 中等 状态压缩 ⭐ 349. 两个数组的交集 简单 哈希表 397. 整数替换 中等 记忆化搜索 ⭐ 63. 不同路径 II 中等 dp 371. 两整数之和 中等 位运算 373. 查找和最小的K对数字 中等 优先队列 ⭐ 面试题 17.21. 直方图的水量（接雨水） 困难 单调栈、双指针 ⭐⭐ 847. 访问所有节点的最短路径 困难 状态压缩、bfs ⭐ 面试题 04.01. 节点间通路 中等 bfs、dfs 剑指 Offer 05. 替换空格 简单 面试题 16.19. 水域大小 中等 bfs、dfs \\1723. 完成所有工作的最短时间 困难 874. 模拟行走机器人 中等 模拟 263. 丑数 简单 934. 最短的桥 中等 dfs+bfs 1497. 检查数组对是否可以被 k 整除 中等 哈希表 ⭐ \\1755. 最接近目标值的子序列和 7. 整数反转 中等 位运算 ⭐ 43. 字符串相乘 中等 模拟 203. 移除链表元素 简单 242. 有效的字母异位词 简单 13. 罗马数字转整数 简单 哈希 58. 最后一个单词的长度 简单 198. 打家劫舍 中等 dp 235. 二叉搜索树的最近公共祖先 简单 二叉搜索树 ⭐ 279. 完全平方数 中等 dp、完全背包问题 ⭐ 403. 青蛙过河 415. 字符串相加 中等 442. 数组中重复的数据 中等 原地哈希 同41. 缺失的第一个正数 593. 有效的正方形 中等 数学 ⭐ 227. 基本计算器 II 中等 栈 ⭐ 647. 回文子串 中等 中心 扩展 204. 计数质数 中等 埃氏筛 ⭐ 48. 旋转图像 中等 模拟原地，水平翻转+对角线翻转 171. Excel 表列序号 简单 进制 301. 删除无效的括号 208. 实现 Trie (前缀树) 中等 字典树 ⭐ 1013. 将数组分成和相等的三个部分 简单 前缀和 445. 两数相加 II 中等 350. 两个数组的交集 II 简单 哈希表 870. 优势洗牌 中等 贪心+双指针 77. 组合 中等 dfs+回溯剪枝 539. 最小时间差 中等 881. 救生艇 中等 双指针 ⭐ 98. 验证二叉搜索树 中等 中序遍历 75. 颜色分类 中等 双指针 ⭐ 210. 课程表 II 中等 拓扑排序（bfs+queue+map） 976. 三角形的最大周长 简单 850. 矩形面积 II 605. 种花问题 简单 152. 乘积最大子数组 中等 dp ⭐ 面经 78. 子集 中等 560. 和为 K 的子数组 中等 前缀和+哈希 ⭐ 234. 回文链表 简单 6. Z 字形变换 中等 模拟 44. 通配符匹配 233. 数字 1 的个数 1044. 最长重复子串 820. 单词的压缩编码 中等 字典树 ⭐ 315. 计算右侧小于当前元素的个数 172. 阶乘后的零 中等 39. 组合总和 中等 dfs、dp 79. 单词搜索 中等 dfs 160. 相交链表 简单 哈希、双指针 223. 矩形面积 中等 数学 ⭐ 338. 比特位计数 简单 dp ⭐ 468. 验证IP地址 中等 模拟 ⭐ split toLowerCase 136. 只出现一次的数字 简单 中等 169. 多数元素 简单 188. 买卖股票的最佳时机 IV 困难 ⭐ 231. 2 的幂 笔试复盘题目：老鼠串门现有一个狭小的老鼠洞，每次仅能一只老鼠进或者出（类似于栈的特性），如果通道里有多只老鼠，那么先进洞的老鼠会比晚进洞的老鼠出来更晚，假如有一窝老鼠来串门，我们给每只老鼠单独编个数字号码，1、2、3…允许老鼠进洞后，又出洞，再次进洞，且若众多老鼠都挤满到洞口了，则不再会有老鼠进洞，最后出洞的顺序就按洞口到洞底的老鼠编号输出。假如老鼠进洞的顺序是1、2、3，那么可能的出洞顺序是3、2、1， 考虑到洞未满的情况下，老鼠进洞后又出洞了，也可能是1、2、3等，但不可能是3、1、2。现给定一个进洞序列，序列里数字可能重复，重复表示出洞后再次进洞，假定序列最后洞是满的，序列长度小于10000。 即老鼠编号范围是[1,10000]请给出老鼠出洞的顺序？ 输入描述输入一行数字数列，每个数字之间用英文空格分隔。如 1 2 3 输出描述3 2 1 样例输入一 1 2 3 2 3 4 5 样例输出一 3 2 5 4 3 2 1 说明 123后又出现2，说明2号老鼠是之前已经出洞了，再重新进洞，2号老鼠要出洞，需要3号老鼠先出洞，因而最先出洞的是3号老鼠，接着是2号老鼠。2号重新进洞后，接着3号又进洞，再是4号和5号，5号后面没其它的说明洞口满了。那么出洞顺序 就是 3 2 5 4 3 2 1。 解决思路： 问题可以使用栈+哈希表来解决。老鼠进洞可以抽象成入栈，老鼠出洞可以抽象成出栈。那么题目就能转换成根据入栈的序列求出出栈的序列。 根据栈的先入后出的特性，假如一个元素出现在入栈序列，那么就需要确保该元素及其后入栈的元素都先出栈。 实现方式可以采用一个栈以及一个set，set用来标识编号为i的老鼠是否入栈，假如入栈了就需要将其加入到set中。 整体的流程就可以简单如下描述：首先遍历入洞序列，判断该序列是否在set中，假如不在，那么就直接将编号push到栈中并add到set中；假如在，那么就需要用一个while循环出栈直到该编号也出栈为止，在出栈的过程中用一个list记录出栈的顺序。 1234567891011121314151617181920212223242526272829303132333435363738import java.util.*;public class Main { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); String[] input = scanner.nextLine().split(\" \"); List&lt;Integer&gt; seq = new ArrayList&lt;&gt;(); for (String s : input) { seq.add(Integer.parseInt(s)); } Stack&lt;Integer&gt; sk = new Stack&lt;&gt;(); Set&lt;Integer&gt; occ = new HashSet&lt;&gt;(); List&lt;Integer&gt; res = new ArrayList&lt;&gt;(); for (int num : seq) { if (occ.contains(num)) { while (!sk.isEmpty() &amp;&amp; sk.peek() != num) { int tp = sk.pop(); res.add(tp); occ.remove(tp); } if (!sk.isEmpty() &amp;&amp; sk.peek() == num) { res.add(num); } } else { occ.add(num); sk.push(num); } } while (!sk.isEmpty()) { res.add(sk.pop()); } System.out.println(res.stream().map(String::valueOf).reduce((a, b) -&gt; a + \" \" + b).orElse(\"\")); }} 题目：元素消除给定一个整数数组nums，同时给定一个整数interval。指定数组nums中的某个元素作为起点，然后以interval为间隔递增，如果递增的数（包含起点）等于nums中的元素，则将数组nums中对应的元素消除，返回消除元素最多的起点元素。如果消除的元素同样多，则返回最小的起点元素。 输入描述第一行输入整数数组的长度n 第二行输入长度为n的整数数组nums 第三行输入整数interval1 &lt;= n &lt;= 10^5 0 &lt;= nums[i] &lt;= 10^8 0 &lt;= interval &lt;= 10^5 解题思路： 由于给地你个间隔是固定的，就想到间隔相同的数之间是同余关系。 处理输入，对序列中的每个数同interval取模得到t，然后使用hashMap记录t的数值（即余数同为t的数的个数），并在遍历的过程中，不断更新最大的余数个数max。 然后再用一次遍历，如果某个数的余数的哈希值为max且数值比ans小，那么就更新ans 1234567891011121314151617181920212223public class Main { public static int work(int n, int[] nums, int k) { HashMap&lt;Integer, Integer&gt; hashmap = new HashMap&lt;&gt;(); int anscnt = 0; int ans = Integer.MAX_VALUE; // Count the occurrences of each modulo value for (int i = 1; i &lt;= n; i++) { int t = nums[i] % k; hashmap.put(t, hashmap.getOrDefault(t, 0) + 1); anscnt = Math.max(anscnt, hashmap.get(t)); } // Find the minimum number with the maximum occurrence for (int i = 1; i &lt;= n; i++) { if (hashmap.get(nums[i] % k) == anscnt) { ans = Math.min(ans, nums[i]); } } return ans; }} 题目：参加博览会有n场编号从0到n−1的博览会将要举办，编号为i的的博览会举办时间为[starti,endi]，即从第starti天到第endi天，包含第starti天和第endi天。 小明计划参加这些博览会，每天最多可以参加k场博览会。请问小明最多可以参加多少场博览会。需注意，小明不需要全程参加一场博览会，只需要在某一天参加即可。 输入描述第一行输入包含两个整数n和k，n表示博览会的数量，k表示每天最多可以参加的博览会的数量，1≤n≤10^4，1≤k≤10。 以下n行每行包含两个整数start和end，表示第i场博览会的举办时间，1≤starti≤endi≤10^9。 解题思路： 首先可以用一个int[]数组来存每一场的开始时间和结束时间，然后用一个List来存所有的博览会，再对开始时间进行一个升序排序。 然后定义一个time表示当前时间，一个idx表示当前处理的博览会的索引，一个ans来统计参加的博览会的个数，还有一个小顶堆，即优先队列来对博览会的结束时间进行升序排序。小顶堆方便获取最早结束的博览会，即策略是优先参加早结束的博览会。 进入一个循环，条件是有未处理的博览会或者小顶堆不为空， 然后首先判断当前时间点，小顶堆中已经结束的博览会，将这些已过期的出堆， 接着检查当前时间点与下一个博览会的开始时间，若大于则将该博览会加入到堆中 然后尝试参加博览会，进行k次循环去小顶堆中取数，并对ans进行累加，这里需要判断小顶堆中是否为空，若空则直接退出循环。 最后更新当前时间，如果堆为空，直接设置为下一场博览会的开始时间，否则，将时间加1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import java.util.*;public class Main { static int n, k; // Comparator for sorting pairs based on the first element public static class Pair { int start, end; Pair(int start, int end) { this.start = start; this.end = end; } } public static void work(List&lt;Pair&gt; a) { int time = 0; PriorityQueue&lt;Integer&gt; pri = new PriorityQueue&lt;&gt;(); int idx = 0; // Start from 0 for 0-based index int ans = 0; while (idx &lt; n || !pri.isEmpty()) { // Remove elements from the priority queue that have ended before the current time while (!pri.isEmpty() &amp;&amp; pri.peek() &lt; time) { pri.poll(); } // Add new intervals that start at or before the current time while (idx &lt; n &amp;&amp; a.get(idx).start &lt;= time) { pri.add(a.get(idx).end); idx++; } // Process up to k intervals for (int i = 0; i &lt; k; i++) { if (!pri.isEmpty()) { ans++; pri.poll(); } else { break; } } // If the priority queue is empty, move to the start of the next interval if (pri.isEmpty()) { if (idx &lt; n) { time = a.get(idx).start; } } else { time++; } } System.out.println(ans); } public static void main(String[] args) { Scanner scanner = new Scanner(System.in); n = scanner.nextInt(); k = scanner.nextInt(); List&lt;Pair&gt; a = new ArrayList&lt;&gt;(); // Read the intervals for (int i = 0; i &lt; n; i++) { int s = scanner.nextInt(); int e = scanner.nextInt(); a.add(new Pair(s, e)); } // Sort intervals based on start time Collections.sort(a, Comparator.comparingInt(pair -&gt; pair.start)); // Call the work method to process the intervals work(a); scanner.close(); }} 哈希数学题1. 多个数求最小公倍数求两个数的最小公倍数 既然想算最小公倍数，首先要清楚最小公倍数的求法，还有最大公约数的求法: ​ 最小公倍数*最大公约数=两数乘积 只要有最大公约数就可以求出最小公倍数,方法: ​ 辗转相除法 用较大数除以较小数，再用出现的余数（第一余数）去除除数，再用出现的余数（第二余数）去除第一余数，如此反复，直到最后余数是0为止。如果是求两个数的最大公约数，那么最后的除数就是这两个数的最大公约数。 12345678// 最大公因数 greatest common divisor int gcd (int a, int b) { return b == 0 ? a : gcd(b, a % b);}// 最小公倍数 least common multipleint lcm (int a, int b) { return a * b / gcd(a, b);} 求多个数的最小公倍数先求前两个数的最小公倍，再用这个最小公倍数与下一个数求最小公倍数 1234567891011121314151617181920212223242526import java.util.Scanner;public class lc1979 { public static void main(String[] args) { Scanner scanner = new Scanner(System.in); int n = scanner.nextInt(); int[] nums = new int[n]; for (int i = 0; i &lt; n; i++) { nums[i] = scanner.nextInt(); } int ans = nums[0]; for (int i = 1; i &lt; n; i++) { ans = ans * nums[i] / gcd(ans, nums[i]); } System.out.println(ans); } private static int gcd(int a, int b) { while(b&gt;0){ int t = a % b; a = b; b = t; } return a; }} 模拟题1. 验证IP地址s.split(regex s, int limit) 分割”.”需要用”\\\\.“ limit有三种情况 limit&gt;0,分割limit-1次 limit=0,不限分割次数,但会把末尾的空结果删去 limit&lt;0,不限分割次数,不会把末尾的空结果删去 假如没有设置limit,默认是0 s.isEmpty的实现为s.length()==0 Character.isDigit(char c) Character.toLowerCase(char c) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class Solution { public String validIPAddress(String queryIP) { if(queryIP.indexOf('.') &gt; 0){ return this.validIPv4(queryIP) ? \"IPv4\" : \"Neither\"; } if(queryIP.indexOf(':') &gt; 0) { return this.validIPv6(queryIP) ? \"IPv6\" : \"Neither\"; } return \"Neither\"; } private boolean validIPv6(String queryIP) { // 分割查询IP，根据冒号分隔，不限制分隔的数量。 String[] split = queryIP.split(\":\", -1); if(split.length != 8){ return false; } for(String s:split) { if (s.isEmpty() || s.length() &gt; 4) { return false; } for (int i = 0; i &lt; s.length(); i++) { if(!Character.isDigit(s.charAt(i)) &amp;&amp; !(Character.toLowerCase(s.charAt(i)) &gt;= 'a' &amp;&amp; Character.toLowerCase(s.charAt(i)) &lt;= 'f')){ return false; } } } return true; } private boolean validIPv4(String queryIP) { String[] split = queryIP.split(\"\\\\.\", -1); if(split.length != 4){ return false; } for(String s:split){ if(s.isEmpty() || s.length() &gt; 3){ return false; } if(s.length() &gt; 1 &amp;&amp; s.charAt(0) == '0'){ return false; } for (int i = 0; i &lt; s.length(); i++) { if(!Character.isDigit(s.charAt(i))){ return false; } } int num = Integer.parseInt(s); if(num &lt; 0 || num &gt; 255){ return false; } } return true; }} 592. 分数加减运算 给定一个表示分数加减运算的字符串 expression ，你需要返回一个字符串形式的计算结果。 这个结果应该是不可约分的分数，即最简分数。 如果最终结果是一个整数，例如 2，你需要将它转换成分数形式，其分母为 1。所以在上述例子中, 2 应该被转换为 2/1。 示例 1: 12输入: expression = \"-1/2+1/2\"输出: \"0/1\" 示例 2: 12输入: expression = \"-1/2+1/2+1/3\"输出: \"1/3\" 示例 3: 12输入: expression = \"1/3-1/2\"输出: \"-1/6\" 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class Solution { // 1.计算 分母最小公倍数 // 2.逐个变换分子分母 // 3.将 2 中计算的分子相加 // 4.计算相加的分子分母的最大公约数 // 5.用 4 计算的最大公约数 处理分子分母 public String fractionAddition(String expression) { if (expression == null || expression.length() == 0) { return \"0/1\"; } char[] cs = expression.toCharArray(); int pre = 1; int cur = 0; List&lt;Integer&gt; nums1 = new ArrayList&lt;&gt;(); List&lt;Integer&gt; nums2 = new ArrayList&lt;&gt;(); while (cur&lt;cs.length) { // 判断正负 if (cs[cur] == '-') { pre = -1; cur++; } else if (cs[cur] == '+') { pre = 1; cur++; } // 取数 int n1 = 0; int n2 = 0; while (cur &lt; cs.length &amp;&amp; cs[cur] != '/' ) { n1 = n1 * 10 + (cs[cur] - '0'); cur++; } cur++; while (cur &lt; cs.length &amp;&amp; cs[cur] != '+' &amp;&amp; cs[cur] != '-') { n2 = n2 * 10 + (cs[cur] - '0'); cur++; } // 存数 nums1.add(pre * n1); nums2.add(n2); } // 计算 分母 最小公倍数 int nums2LCM = nums1.get(0); for (int n2 : nums2) { nums2LCM = lcm(nums2LCM, n2); } // 逐个变换分子并相加 int nums1Sum = 0; for (int i=0; i&lt;nums1.size(); ++i) { nums1Sum += (nums2LCM / nums2.get(i)) * nums1.get(i); } // 计算相加的分子分母的最大公约数 int tGcd = gcd(Math.abs(nums1Sum), nums2LCM); // 用 4 计算的最大公约数 处理分子分母 return nums1Sum/tGcd + \"/\" + nums2LCM/tGcd; } int gcd(int a, int b) { return b == 0 ? a : gcd(b, a%b); } int lcm(int a, int b) { return a * b / gcd(a, b); }} 详细个人题解： https://leetcode.cn/problems/fraction-addition-and-subtraction/solutions/2870964/mo-ni-by-nanase-6-e9it 数组题48. 旋转图像顺时针旋转90度——主对称轴交换,左右对称轴交换 逆时针旋转90度——主对称轴交换,上下对称轴交换 顺时针旋转180度——顺时针90度x2 …… 12345678910111213141516171819202122232425class Solution { public void rotate(int[][] matrix) { if(matrix == null || matrix.length == 0 || matrix[0].length == 0) return; int row = matrix.length; int col = matrix[0].length; // 主对角线交换 for(int i=0; i&lt;row; ++i){ for(int j=i+1; j&lt; col; ++j){ swap(matrix, i, j, j, i); } } // 左右交换 for(int i=0; i&lt;row; ++i){ for(int j=0; j&lt; col/2; ++j){ swap(matrix, i, j, i, col-j-1); } } } void swap(int[][] matrix, int i1, int j1, int i2, int j2){ int t = matrix[i1][j1]; matrix[i1][j1] = matrix[i2][j2]; matrix[i2][j2] = t; }} 134. 加油站 在一条环路上有 n 个加油站，其中第 i 个加油站有汽油 gas[i] 升。 你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。 给定两个整数数组 gas 和 cost ，如果你可以按顺序绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1 。如果存在解，则 保证 它是 唯一 的。 12345678910输入: gas = [1,2,3,4,5], cost = [3,4,5,1,2]输出: 3解释:从 3 号加油站(索引为 3 处)出发，可获得 4 升汽油。此时油箱有 = 0 + 4 = 4 升汽油开往 4 号加油站，此时油箱有 4 - 1 + 5 = 8 升汽油开往 0 号加油站，此时油箱有 8 - 2 + 1 = 7 升汽油开往 1 号加油站，此时油箱有 7 - 3 + 2 = 6 升汽油开往 2 号加油站，此时油箱有 6 - 4 + 3 = 5 升汽油开往 3 号加油站，你需要消耗 5 升汽油，正好足够你返回到 3 号加油站。因此，3 可为起始索引。 123456789101112131415161718192021class Solution { public int canCompleteCircuit(int[] gas, int[] cost) { // 1 2 3 4 5 // 3 4 5 1 2 // x -2 -2 -2 3 3 // totalSum -2 -4 -6 -3 0 int curSum = 0, totalSum = 0; int ans = 0; for (int i=0; i&lt;gas.length; ++i) { int x = gas[i] - cost[i]; curSum += x; totalSum += x; // 总结：如果x到不了y+1（但能到y），那么从x到y的任一点出发都不可能到达y+1。因为从其中任一点出发的话，相当于从0开始加油，而如果从x出发到该点则不一定是从0开始加油，可能还有剩余的油。既然不从0开始都到不了y+1，那么从0开始就更不可能到达y+1了... if (curSum &lt; 0) { ans = i+1; curSum = 0; } } return totalSum &gt;= 0 ? ans : -1; }} 135.分发糖果 n 个孩子站成一排。给你一个整数数组 ratings 表示每个孩子的评分。 你需要按照以下要求，给这些孩子分发糖果： 每个孩子至少分配到 1 个糖果。 相邻两个孩子评分更高的孩子会获得更多的糖果。 请你给每个孩子分发糖果，计算并返回需要准备的 最少糖果数目 。 123输入：ratings = [1,0,2]输出：5解释：你可以分别给第一个、第二个、第三个孩子分发 2、1、2 颗糖果。 示例 2： 1234输入：ratings = [1,2,2]输出：4解释：你可以分别给第一个、第二个、第三个孩子分发 1、2、1 颗糖果。 第三个孩子只得到 1 颗糖果，这满足题面中的两个条件。 思路: 假设A、B相邻, A在B左侧 定义规则: 左规则: 当RB&gt;RA时, B的糖果要比A多 [定A, 变B] 右规则: 当RA&gt;RB时, A的糖果要比B多 [定B, 变A] 流程: 从左到右遍历,满足左规则; 从右到左遍历,满足右规则; 最后取max left和right的项只能增大或不变。然后取最大值合并后是可以同时满足left和right，这个要分类讨论，不过这里不难。关键在于合并后是否就是最优，这里合并后相对left来说是某些项增大某些不变，要优化肯定涉及到某些项的减小，那只能减小相对left来说增大的项，而这些项相对right来说又是原来的项，不能动，否则破坏right的条件，因此合并后的项不能再优化了，它就是最优解。 12345678910111213141516class Solution { public int candy(int[] ratings) { int[] left = new int[ratings.length]; int[] right = new int[ratings.length]; Arrays.fill(left, 1); Arrays.fill(right, 1); for(int i = 1; i &lt; ratings.length; i++) if(ratings[i] &gt; ratings[i - 1]) left[i] = left[i - 1] + 1; int count = left[ratings.length - 1]; for(int i = ratings.length - 2; i &gt;= 0; i--) { if(ratings[i] &gt; ratings[i + 1]) right[i] = right[i + 1] + 1; count += Math.max(left[i], right[i]); } return count; }} 回溯1.字符串的排列排列组合问题,且不能重复 总结: set-&gt;array x.toArray(new T[0]) 1234567891011121314151617181920212223242526272829class Solution { Set&lt;String&gt; ans = new HashSet&lt;String&gt;(); public String[] goodsOrder(String goods) { char[] cList = goods.toCharArray(); boolean[] isVisit = new boolean[goods.length()]; dfs(cList, isVisit, new StringBuffer()); return ans.toArray(new String[0]); } void dfs(char[] cList, boolean[] isVisit, StringBuffer sb){ if(sb.length() == cList.length){ ans.add(sb.toString()); return; } for(int i=0; i&lt;cList.length; ++i){ if(isVisit[i]){ continue; } isVisit[i] = true; sb.append(cList[i]); dfs(cList, isVisit, sb); sb.deleteCharAt(sb.length()-1); isVisit[i] = false; } }} 46. 全排列 给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 示例 1： 12输入：nums = [1,2,3]输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] 示例 2： 12输入：nums = [0,1]输出：[[0,1],[1,0]] 示例 3： 12输入：nums = [1]输出：[[1]] 1234567891011121314151617181920212223242526class Solution { List&lt;List&lt;Integer&gt;&gt; ans = new ArrayList&lt;&gt;(); List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); public List&lt;List&lt;Integer&gt;&gt; permute(int[] nums) { boolean[] visit = new boolean[nums.length]; dfs(nums, visit); return ans; } void dfs(int[] nums, boolean[] visit) { if (list.size() == nums.length) { ans.add(new ArrayList(list)); return; } for (int i=0; i&lt;nums.length; ++i) { if (visit[i]) { continue; } list.add(nums[i]); visit[i]= true; dfs(nums, visit); visit[i]= false; list.remove(Integer.valueOf(nums[i])); } }} 字符串3. 无重复字符的最长子串给定一个字符串 s ，请你找出其中不含有重复字符的 最长 子串 的长度。 思路: 滑动窗口——有左右指针控制窗口 [left, right] 实际解决: 在右指针自增时候, 移动左指针保持窗口内不含有重复字符 123456789101112131415161718class Solution { public int lengthOfLongestSubstring(String s) { if(s == null || s.length() == 0) return 0; Map&lt;Character,Integer&gt; map = new HashMap&lt;&gt;(); char[] cList = s.toCharArray(); int ans = 0; int left = 0; for(int i=0; i&lt;s.length(); ++i){ // 滑动窗口，存在重复字母的区间需要移动左边界，不存在时不用 if(map.containsKey(cList[i])){ left = Math.max(left, map.get(cList[i])+1); } map.put(cList[i], i); ans = Math.max(ans, i-left+1); } return ans; }} 443. 压缩字符串aabbbccc -&gt; a2b2c3 a -&gt; a abbbbbbbbbbbbbb -&gt; ab14 思路: 最简单的是开辟一个StringBuffer去存, 采用双指针去控制窗口边界,一个存右边界,一个存左边界 要求原地修改, 则需要多一个write指针,控制修改的位置 123456789101112131415161718192021222324252627282930313233class Solution { public int compress(char[] chars) { if(chars == null || chars.length == 0) return 0; int write=0, read=0, left=0; while (read &lt; chars.length) { if(read == chars.length-1 || chars[read] != chars[read+1]){ chars[write++] = chars[read]; int num = read - left + 1; if(num &gt; 1){ int t = write; while(num &gt; 0){ chars[write++] = (char)(num % 10 +'0'); num /= 10; } reverse(chars, t, write-1); } left = read + 1; } read++; } return write; } private void reverse(char[] chars, int left, int right){ while(left &lt; right){ char t = chars[left]; chars[left] = chars[right]; chars[right] = t; left++; right--; } }} 1190. 反转每对括号间的子串给出一个字符串 s（仅含有小写英文字母和括号）。 请你按照从括号内到外的顺序，逐层反转每对匹配括号中的字符串，并返回最终的结果。 思路: 栈+StringBuffer 遇到右括号时才进行字符串处理，这样可以保证我们是按照从括号内到外的顺序处理字符串。 1234567891011121314151617181920212223class Solution { public String reverseParentheses(String s) { // 0. 定义一个 StringBuilder // 1. 遇到'('将当前 String 加到 stack, 重置 sb // 2. 遇到')'将当前 String1 翻转，取出栈内 String2 加在当前 String1 前 // 3. 假如为 char 直接 sb.append() char[] cs = s.toCharArray(); StringBuilder sb = new StringBuilder(); Deque&lt;String&gt; stack = new LinkedList&lt;&gt;(); for (char c : cs) { if (c == '(') { stack.offerLast(sb.toString()); sb.setLength(0); } else if (c == ')') { sb.reverse(); sb.insert(0, stack.pollLast()); } else { sb.append(c); } } return sb.toString(); }} 递归: 需要一个全局idx 12345678910111213141516171819202122232425class Solution { int idx = 0; public String reverseParentheses(String s) { return traversal(s).toString(); } private StringBuffer traversal(String s) { StringBuffer sb = new StringBuffer(); while (idx &lt; s.length()) { if (s.charAt(idx) == '(') { idx++; sb.append(traversal(s).reverse()); } else if(s.charAt(idx) == ')') { idx++; return sb; } else { sb.append(s.charAt(idx)); idx++; } } return sb; }} 2243. 计算字符串的数字和思路: 递归 1234567891011121314151617181920class Solution { public String digitSum(String s, int k) { if (s.length() &lt;= k) { return s; } int curNum = 0; char[] list = s.toCharArray(); StringBuffer sb = new StringBuffer(); for (int i=0; i&lt;list.length; ++i) { curNum += list[i] - '0'; if (i == s.length()-1 || (i+1) % k == 0) { sb.append(Integer.toString(curNum)); curNum = 0; } } return digitSum(sb.toString(), k); }} 394. 字符串解码 给定一个经过编码的字符串，返回它解码后的字符串。 编码规则为: k[encoded_string]，表示其中方括号内部的 encoded_string 正好重复 k 次。注意 k 保证为正整数。 你可以认为输入字符串总是有效的；输入字符串中没有额外的空格，且输入的方括号总是符合格式要求的。 此外，你可以认为原始数据不包含数字，所有的数字只表示重复的次数 k ，例如不会出现像 3a 或 2[4] 的输入。 示例 1： 12输入：s = \"3[a]2[bc]\"输出：\"aaabcbc\" 示例 2： 12输入：s = \"3[a2[c]]\"输出：\"accaccacc\" 示例 3： 12输入：s = \"2[abc]3[cd]ef\"输出：\"abcabccdcdcdef\" 示例 4： 12输入：s = \"abc3[cd]xyz\"输出：\"abccdcdcdxyz\" 栈123456789101112131415161718192021222324252627282930313233class Solution { public String decodeString(String s) { StringBuilder cur = new StringBuilder(); char[] cs = s.toCharArray(); Deque&lt;String&gt; sstack = new LinkedList&lt;&gt;(); Deque&lt;Integer&gt; nstack = new LinkedList&lt;&gt;(); int num = 0; for (int i=0; i&lt;cs.length; ++i) { if (Character.isDigit(cs[i])) { num = num * 10 + (cs[i] - '0'); } else if(cs[i] == '[') { // 开启新的[]拼接 需要将同层的已经确定的字符串 存起来 nstack.offerLast(num); num = 0; sstack.offerLast(cur.toString()); cur.setLength(0); } else if(cs[i] == ']') { // 结束当前[]拼接 计算重复次数,然后连接同一层的已经确定的字符串 int dn = nstack.pollLast(); String oldCur = cur.toString(); while (dn &gt; 1) { cur.append(oldCur); dn--; } cur.insert(0, sstack.pollLast()); } else { cur.append(cs[i]); } } return cur.toString(); }} 算法流程： https://leetcode.cn/problems/decode-string/solutions/19447/decode-string-fu-zhu-zhan-fa-di-gui-fa-by-jyd dfs12345678910111213141516171819202122232425262728293031323334class Solution { StringBuilder ans = new StringBuilder(); int idx = 0; public String decodeString(String s) { char[] cs = s.toCharArray(); return dfs(cs, 0, cs.length-1); } String dfs(char[] cs, int l, int r) { // 确定[]内的 string StringBuilder ans = new StringBuilder(); for (int i=l; i&lt;=r; ++i) { if (Character.isLetter(cs[i])) { ans.append(cs[i]); } else { int num = 0; while (Character.isDigit(cs[i])) { num = num * 10 + (cs[i++] - '0'); } int old = i+1; int fnum = 1; while (fnum &gt; 0) { i++; if (cs[i] == '[') fnum++; else if (cs[i] == ']') fnum--; } String sub = dfs(cs, old, i-1); while (num-- &gt; 0) { ans.append(sub); } } } return ans.toString(); }} 179. 最大数 给定一组非负整数 nums，重新排列每个数的顺序（每个数不可拆分）使之组成一个最大的整数。 注意：输出结果可能非常大，所以你需要返回一个字符串而不是整数。 示例 1： 12输入：nums = [10,2]输出：\"210\" 示例 2： 12输入：nums = [3,30,34,5,9]输出：\"9534330\" 1234567891011121314151617181920212223class Solution { public String largestNumber(int[] nums) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); // s1.compareTo(s2) 理解为 s1-s2 负数则 s1 小 前提是长度一样 // pq 里面的比较关系 s1,s2 后面的值为 - 则 s1 在前 // 要让 s1 在前 需要 s1+s2&gt;s2+s1 记 (s2+s1).compareTo(s1+s2) 为 - PriorityQueue&lt;String&gt; pq = new PriorityQueue&lt;String&gt;((s1,s2)-&gt;(s2+s1).compareTo(s1+s2)); for (int n : nums) { pq.offer(Integer.toString(n)); } // 特殊情况处理 if (pq.peek().equals(\"0\")) { return \"0\"; } StringBuilder sb = new StringBuilder(); for (int n : nums) { String t = pq.poll(); sb.append(t); } return sb.toString(); }} 知识点: 在 PriorityQueue 中使用自定义的 Comparator 来排序元素时，Comparator 的 compare 方法返回的值决定了两个元素的顺序。 返回负数 (&lt; 0)：表示第一个参数（s1）应该排在第二个参数（s2）之前。 返回正数 (&gt; 0)：表示第一个参数（s1）应该排在第二个参数（s2）之后。 返回零 (0)：表示两个参数顺序相同（即不交换位置）。 如果 s1.compareTo(s2) 返回负数（意味着 s1 在字典顺序上小于 s2），则 s1 应该排在 s2 前面。 如果 s1.compareTo(s2) 返回正数（意味着 s1 在字典顺序上大于 s2），则 s1 应该排在 s2 后面。 如果 s1.compareTo(s2) 返回 0，表示 s1 和 s2 相等，保持当前顺序。 动态规划322. 零钱兑换完全背包问题 12345678910111213141516class Solution { public int coinChange(int[] coins, int amount) { int[] dp = new int[amount+1]; int max = amount + 1; Arrays.fill(dp, max); dp[0] = 0; for(int i=1; i&lt;=amount; ++i){ for(int c:coins){ if(i &gt;= c){ dp[i] = Math.min(dp[i], dp[i-c]+1); } } } return dp[amount] &gt; amount ? -1 : dp[amount]; }} 139. 单词拆分 给你一个字符串 s 和一个字符串列表 wordDict 作为字典。如果可以利用字典中出现的一个或多个单词拼接出 s 则返回 true。 注意：不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。 示例 1： 123输入: s = \"leetcode\", wordDict = [\"leet\", \"code\"]输出: true解释: 返回 true 因为 \"leetcode\" 可以由 \"leet\" 和 \"code\" 拼接成。 示例 2： 1234输入: s = \"applepenapple\", wordDict = [\"apple\", \"pen\"]输出: true解释: 返回 true 因为 \"applepenapple\" 可以由 \"apple\" \"pen\" \"apple\" 拼接成。 注意，你可以重复使用字典中的单词。 示例 3： 12输入: s = \"catsandog\", wordDict = [\"cats\", \"dog\", \"sand\", \"and\", \"cat\"]输出: false 思路： dp[i] 前 i 个字母能否由字典拼出 1234567891011121314151617181920class Solution { public boolean wordBreak(String s, List&lt;String&gt; wordDict) { boolean[] dp = new boolean[s.length()+1]; dp[0] = true; for (int i=1; i&lt;=s.length(); ++i) { for (String word : wordDict) { int start = i-word.length(); if (start &lt; 0) { continue; } String sub = s.substring(start,i); if (sub.equals(word) &amp;&amp; dp[start]) { dp[i] = true; } } } return dp[s.length()]; }} 面试题 08.11. 硬币 硬币。给定数量不限的硬币，币值为25分、10分、5分和1分，编写代码计算n分有几种表示法。(结果可能会很大，你需要将结果模上1000000007) 示例1: 12345 输入: n = 5 输出：2 解释: 有两种方式可以凑成总金额:5=55=1+1+1+1+1 示例2: 1234567 输入: n = 10 输出：4 解释: 有四种方式可以凑成总金额:10=1010=5+510=5+1+1+1+1+110=1+1+1+1+1+1+1+1+1+1 不同于爬楼梯 取硬币 1 5 和 5 1是一样的 为了保证使用第二个硬币尝试的时候，dp中 只有 第一个硬币 尝试的结果，需要将coin循环放在外层 123456789101112131415161718class Solution { public int waysToChange(int n) { // dp[i] n分有几种表示法 int[] dp = new int[n+1]; int[] coins = new int[]{1, 5, 10, 25}; dp[0] = 1; for (int coin : coins) { for (int i=1; i&lt;=n; ++i) { //于是我们先遍历硬币，保证在考虑一枚硬币的情况时，没有较大的硬币影响，这样，我们最终每种组合情况，都是以硬币的面额大小非递减组合。保证了同样的情况，调换顺序后重复计算的情况。 if (coin &lt;=i) { dp[i] = (dp[i] + dp[i-coin]) % 1000000007; } } } return dp[n]; }} 滑动窗口1004.最大连续1的个数给定一个二进制数组 nums 和一个整数 k，如果可以翻转最多 k 个 0 ，则返回 数组中连续 1 的最大个数 。 示例 1： 1234输入：nums = [1,1,1,0,0,0,1,1,1,1,0], K = 2输出：6解释：[1,1,1,0,0,1,1,1,1,1,1]粗体数字从 0 翻转到 1，最长的子数组长度为 6。 示例 2： 1234输入：nums = [0,0,1,1,0,0,1,1,1,0,1,1,0,0,0,1,1,1,1], K = 3输出：10解释：[0,0,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1]粗体数字从 0 翻转到 1，最长的子数组长度为 10。 代码思路： 使用 left和 right两个指针，分别指向滑动窗口的左右边界。right主动右移：right 指针每次移动一步。当 A[right]为 0，说明滑动窗口内增加了一个 0；left 被动右移：判断此时窗口内 0 的个数，如果超过了 K，则 left 指针被迫右移，直至窗口内的 0 的个数小于等于 K 为止。滑动窗口长度的最大值就是所求。 12345678910111213141516171819class Solution { public int longestOnes(int[] nums, int k) { int left = 0, right = 0, ans = 0, cur = 0; while (right &lt; nums.length) { if (nums[right] == 0) { cur++; } while (cur &gt; k) { if (nums[left] == 0) { cur--; } left++; } ans = Math.max(ans, right - left + 1); right++; } return ans; }} 滑动窗口模板1234567891011121314def findSubArray(nums): N = len(nums) # 数组/字符串长度 left, right = 0, 0 # 双指针，表示当前遍历的区间[left, right]，闭区间 sums = 0 # 用于统计 子数组/子区间 是否有效，根据题目可能会改成求和/计数 res = 0 # 保存最大的满足题目要求的 子数组/子串 长度 while right &lt; N: # 当右边的指针没有搜索到 数组/字符串 的结尾 sums += nums[right] # 增加当前右边指针的数字/字符的求和/计数 while 区间[left, right]不符合题意: # 此时需要一直移动左指针，直至找到一个符合题意的区间 sums -= nums[left] # 移动左指针前需要从counter中减少left位置字符的求和/计数 left += 1 # 真正的移动左指针，注意不能跟上面一行代码写反 # 到 while 结束时，我们找到了一个符合题意要求的 子数组/子串 res = max(res, right - left + 1) # 需要更新结果 right += 1 # 移动右指针，去探索新的区间 return res 滑动窗口中用到了左右两个指针，它们移动的思路是：以右指针作为驱动，拖着左指针向前走。右指针每次只移动一步，而左指针在内部 while 循环中每次可能移动多步。右指针是主动前移，探索未知的新区域；左指针是被迫移动，负责寻找满足题意的区间。 模板的整体思想是： 定义两个指针 left 和 right 分别指向区间的开头和结尾，注意是闭区间；定义 sums 用来统计该区间内的各个字符出现次数； 第一重 while 循环是为了判断 right 指针的位置是否超出了数组边界；当 right 每次到了新位置，需要增加 right 指针的求和/计数； 第二重 while 循环是让 left 指针向右移动到 [left, right] 区间符合题意的位置；当 left 每次移动到了新位置，需要减少 left 指针的求和/计数； 在第二重 while 循环之后，成功找到了一个符合题意的 [left, right] 区间，题目要求最大的区间长度，因此更新 res 为 max(res, 当前区间的长度) 。 right 指针每次向右移动一步，开始探索新的区间。 二叉树124. 二叉树中的最大路径和路径 被定义为一条从树中任意节点出发，沿父节点-子节点连接，达到任意节点的序列。同一个节点在一条路径序列中 至多出现一次 。该路径 至少包含一个 节点，且不一定经过根节点。 路径和 是路径中各节点值的总和。 给定一个二叉树的根节点 root ，返回其 最大路径和，即所有路径上节点值之和的最大值。 123456789101112131415161718192021222324class Solution { int ans = Integer.MIN_VALUE; public int maxPathSum(TreeNode root) { dfs(root); return ans; } int dfs (TreeNode root) { if (root == null) { return 0; } // 递归计算左右子节点的最大贡献值 // 只有在最大贡献值大于 0 时，才会选取对应子节点 int l = Math.max(dfs(root.left), 0); int r = Math.max(dfs(root.right), 0); // 以当前节点为纽带 int sum = root.val + l + r; ans = Math.max(ans, sum); // 不以当前节点为纽带 return Math.max(l, r) + root.val; }} 双指针283. 移动零 给定一个数组 nums，编写一个函数将所有 0 移动到数组的末尾，同时保持非零元素的相对顺序。 请注意 ，必须在不复制数组的情况下原地对数组进行操作。 475. 供暖器 冬季已经来临。 你的任务是设计一个有固定加热半径的供暖器向所有房屋供暖。 在加热器的加热半径范围内的每个房屋都可以获得供暖。 现在，给出位于一条水平线上的房屋 houses 和供暖器 heaters 的位置，请你找出并返回可以覆盖所有房屋的最小加热半径。 注意：所有供暖器 heaters 都遵循你的半径标准，加热的半径也一样。 示例 1: 123输入: houses = [1,2,3], heaters = [2]输出: 1解释: 仅在位置 2 上有一个供暖器。如果我们将加热半径设为 1，那么所有房屋就都能得到供暖。 示例 2: 123输入: houses = [1,2,3,4], heaters = [1,4]输出: 1解释: 在位置 1, 4 上有两个供暖器。我们需要将加热半径设为 1，这样所有房屋就都能得到供暖。 示例 3： 12输入：houses = [1,5], heaters = [2]输出：3 123456789101112131415161718192021222324class Solution { public int findRadius(int[] houses, int[] heaters) { // 对于一个房子, 要么用前面的暖气, 要么用后面的, 二者取近的 // 可以使用双指针 // 先对两个数组做 sort // 对于每一个房子house, 从 heaters 中找第一个位置大于 house 的, -1 就是第一个位置小于 house // 然后有三种情况, 第一个heater 就满足; 最后一个 heater 也不满足; 中间的 heater 满足,那么就需要比较前后的距离 int ans = 0; Arrays.sort(heaters); Arrays.sort(houses); for (int i=0, j=0; i&lt;houses.length; ++i) { while (j &lt; heaters.length &amp;&amp; houses[i] &gt; heaters[j]) j++; if (j == 0) { ans = Math.max(ans, heaters[j] - houses[i]); } else if (j == heaters.length) { ans = Math.max(ans, houses[i] - heaters[j-1]); } else { ans = Math.max(ans, Math.min(houses[i]-heaters[j-1], heaters[j]-houses[i])); } } return ans; }}","link":"/2024/01/26/%E7%AE%97%E6%B3%95/%E5%8D%8E%E4%B8%BA%E6%89%8B%E6%92%95/"},{"title":"链表","text":"一、双指针技巧23.合并 k 个有序链表 合并 k 个升序的链表并将结果作为一个升序的链表返回其头节点。 数据范围：节点总数 0≤n≤5000，每个节点的val满足 ∣val∣&lt;=1000 要求：时间复杂度 O(nlogn) 解法1: 每次都去寻找当前list中的最小的值，然后去lists中去取到这些节点加到虚节点后。 1234567891011121314151617181920212223242526272829303132class Solution { public ListNode mergeKLists(ListNode[] lists) { ListNode ans = new ListNode(-1); ListNode cur = ans; while(true){ int min_num = Integer.MAX_VALUE; //寻找当前最小的节点值 for(ListNode l : lists){ if(l == null) continue; if(l.val &lt; min_num){ min_num = l.val; } } //根据最小值把所有链表中等于该最小值加入到整合链表 if(min_num == Integer.MAX_VALUE) break; for(int i=0; i&lt;lists.length ; ++i){ if(lists[i] == null) continue; if(lists[i].val == min_num){ cur.next = lists[i]; lists[i] = lists[i].next; cur = cur.next; } } } return ans.next; }} 解法2: 解法1要一直去搜索当前最小值，并没有用到什么数据结构。 解法2引入优先级队列（二叉堆）这种数据结构，把链表节点放入一个最小堆，就可以每次获得k个节点中最小节点 1234567891011121314151617181920class Solution { public ListNode mergeKLists(ListNode[] lists) { ListNode dummy = new ListNode(-1); ListNode cur = dummy; PriorityQueue&lt;ListNode&gt; que = new PriorityQueue&lt;&gt;((o1,o2)-&gt;o1.val-o2.val); for(ListNode node : lists){ if(node!=null) que.add(node); } while(!que.isEmpty()){ cur.next = que.poll(); cur = cur.next; if(cur.next!=null){ que.add(cur.next); } } return dummy.next; }} 时间复杂度分析： 优先队列 pq 中的元素个数最多是 k，所以一次 poll 或者 add 方法的时间复杂度是 O(logk)；所有的链表节点都会被加入和弹出 pq，所以算法整体的时间复杂度是 O(Nlogk)，其中 k 是链表的条数，N 是这些链表的节点总数。 19. 删除链表的倒数第 N 个结点链表在没有给出链表长度的时候我们很难一次遍历就能得到倒数第k个结点。 解法1: 一般是先从头遍历一次得到链表长度，然后在n-k出停止遍历。 123456789101112131415161718class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode cur = new ListNode(0); cur.next = head; head = cur; int size = -1; while(cur!=null){ size += 1; cur = cur.next; } cur = head; for(int i=0; i&lt;size-n; ++i){ cur = cur.next; } cur.next = cur.next.next; return head.next; }} 解法2: 思路： 首先，我们先让一个指针 p1 指向链表的头节点 head，然后走 k 步： 趁这个时候，再用一个指针 p2 指向链表头节点 head： 接下来就很显然了，让 p1 和 p2 同时向前走，p1 走到链表末尾的空指针时前进了 n - k 步，p2 也从 head 开始前进了 n - k 步，停留在第 n - k + 1 个节点上，即恰好停链表的倒数第 k 个节点上： 总结一下：其实这个思路就是固定p1和p2之间的距离为k那么p1走到链表最后p2就是指向倒数第k位 12345678910111213141516171819class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(-1); dummy.next = head; ListNode cur1 = dummy; for(int i=0; i&lt;n+1; ++i){ //找要删的前一个节点 cur1 = cur1.next; } ListNode cur2 = dummy; while(cur1 != null){ cur1 = cur1.next; cur2 = cur2.next; } cur2.next = cur2.next.next; return dummy.next; }} 142. 环形链表 II 给一个长度为n链表，若其中包含环，请找出该链表的环的入口结点，否则，返回null。 数据范围： n≤10000n≤10000，1&lt;=结点值&lt;=10000 要求：空间复杂度 O(1)，时间复杂度 O(n) 思路： 当快慢指针相遇时，让其中任一个指针指向头节点，然后让它俩以相同速度前进，再次相遇时所在的节点位置就是环开始的位置。 同时fast多走的k步是在环里转圈圈，所以k的值就是环长度的整数倍 设： 链表头到环入口长度为–a 环入口到相遇点长度为–b 相遇点到环入口长度为–c 则: (a+b)*2=a+(b+c)k+b 化简可得： a=(k-1)(b+c)+c这个式子的意思是： 链表头到环入口的距离=相遇点到环入口的距离+（k-1）圈环长度。其中k&gt;=1,所以k-1&gt;=0圈。所以两个指针分别从链表头和相遇点出发，最后一定相遇于环入口。 123456789101112131415161718public class Solution { public ListNode detectCycle(ListNode head) { ListNode fast = head; ListNode slow = head; while(fast != null &amp;&amp; fast.next != null){ fast = fast.next.next; slow = slow.next; if(fast == slow) break; } if(fast == null || fast.next == null) return null;//检验是否无环 slow = head; while(slow != fast){ slow = slow.next; fast = fast.next; } return slow; }} 160. 相交链表 输入两个无环的单向链表，找出它们的第一个公共结点，如果没有公共节点则返回空。（注意因为传入数据是链表，所以错误测试数据的提示是用其他方式显示的，保证传入数据是正确的） 数据范围： n≤100要求：空间复杂度 O(1)，时间复杂度 O(n) 例如，输入{1,2,3},{4,5},{6,7}时，两个无环的单向链表的结构如下图所示： 可以看到它们的第一个公共结点的结点值为6，所以返回结点值为6的结点。 解法1: 先算两个链表的长度，然后让长的先走直到两个链表剩余长度相同，最后一起next直到相同 123456789101112131415161718192021222324252627282930313233343536public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { int num1 = 0, num2 = 0; ListNode cur1 = headA, cur2 = headB; while(cur1!=null){ num1++; cur1 = cur1.next; } while(cur2!=null){ num2++; cur2 = cur2.next; } cur1 = headA; cur2 = headB; if(num1&gt;num2){ //cur1先走 for(int i=0; i&lt;num1-num2; ++i){ cur1 = cur1.next; } while(cur1!=cur2){ cur1 = cur1.next; cur2 = cur2.next; } }else{ //cur2先走 for(int i=0; i&lt;num2-num1; ++i){ cur2 = cur2.next; } while(cur1!=cur2){ cur1 = cur1.next; cur2 = cur2.next; } } return cur1; }} 解法2: 将两个链表拼接起来，遍历的时候相同的就是相交节点 这里可以这么理解因为我们要找的是红色部分的第一个。但是绿色部分和橙色部分的数量不一样，那么俩个链表连起来就相同了，相当于将两个部分的数量磨平 123456789101112public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { ListNode cur1 = headA, cur2 = headB; while(cur1 != cur2){ if(cur1 == null) cur1 = headB; else cur1 = cur1.next; if(cur2 == null) cur2 = headA; else cur2 = cur2.next; } return cur1; }} 148. 排序链表 给定一个节点数为n的无序单链表，对其按升序排序。 数据范围：0&lt;n≤100000，保证节点权值在[−109,109][−109,109]之内。 要求：空间复杂度 O(n)，时间复杂度 O(nlogn) 主要通过递归实现链表归并排序，有以下两个环节： 1、分割 cut 环节： 找到当前链表中点，并从中点将链表断开（以便在下次递归 cut 时，链表片段拥有正确边界）； 使用 fast,slow 快慢双指针法，奇数个节点找到中点，偶数个节点找到中心左边的节点。 找到中点 slow 后，执行 slow.next = None 将链表切断。 递归分割时，输入当前链表左端点 head 和中心节点 slow 的下一个节点 tmp(因为链表是从 slow 切断的)。 cut 递归终止条件： 当head.next == None时，说明只有一个节点了，直接返回此节点 2、合并 merge 环节： 将两个排序链表合并，转化为一个排序链表。 双指针法合并，建立辅助ListNode h 作为头部。 设置两指针 left, right 分别指向两链表头部，比较两指针处节点值大小，由小到大加入合并链表头部，指针交替前进，直至添加完两个链表。 返回辅助ListNode h 作为头部的下个节点 h.next。 时间复杂度 O(l + r)，l, r 分别代表两个链表长度。3、特殊情况，当题目输入的 head == None 时，直接返回None。 图解： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import java.util.*;/* * public class ListNode { * int val; * ListNode next = null; * public ListNode(int val) { * this.val = val; * } * } */public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param head ListNode类 the head node * @return ListNode类 */ public ListNode sortInList (ListNode head) { // write code here if(head == null || head.next == null){ return head; } //使用快慢指针寻找链表中点 ListNode slow = head; ListNode fast = head.next; while(fast != null &amp;&amp; fast.next != null){ slow = slow.next; fast = fast.next.next; } ListNode nList = slow.next; slow.next = null; //递归左右两边进行排序 ListNode left = sortInList(head); ListNode right = sortInList(nList); //创建新的链表 ListNode dummy = new ListNode(-1); ListNode cur = dummy; //合并新的链表 while(left != null &amp;&amp; right != null){ if(left.val &lt; right.val){ cur.next = left; left = left.next; }else{ cur.next = right; right = right.next; } cur = cur.next; } //添加未对比的链表部分 cur.next = left == null ? right : left; return dummy.next; }} 328. 奇偶链表 给定一个单链表，请设定一个函数，将链表的奇数位节点和偶数位节点分别放在一起，重排后输出。 注意是节点的编号而非节点的数值。 数据范围：节点数量满足 0≤n≤105，节点中的值都满足 0≤val≤1000 要求：空间复杂度 O(n)，时间复杂度 O(n) 知识点：双指针 双指针指的是在遍历对象的过程中，不是普通的使用单个指针进行访问，而是使用两个指针（特殊情况甚至可以多个），两个指针或是同方向访问两个链表、或是同方向访问一个链表（快慢指针）、或是相反方向扫描（对撞指针），从而达到我们需要的目的。 思路： 如下图所示，第一个节点是奇数位，第二个节点是偶数，第二个节点后又是奇数位，因此可以断掉节点1和节点2之间的连接，指向节点2的后面即节点3，如红色箭头。如果此时我们将第一个节点指向第三个节点，就可以得到那么第三个节点后为偶数节点，因此我们又可以断掉节点2到节点3之间的连接，指向节点3后一个节点即节点4，如蓝色箭头。那么我们再将第二个节点指向第四个节点，又回到刚刚到情况了。 这样我们就可以使用了两个同方向访问指针遍历解决这道题。 具体做法： step 1：判断空链表的情况，如果链表为空，不用重排。 step 2：使用双指针odd和even分别遍历奇数节点和偶数节点，并给偶数节点链表一个头。 step 3：上述过程，每次遍历两个节点，且even在后面，因此每轮循环用even检查后两个元素是否为NULL，如果不为再进入循环进行上述连接过程。 step 4：将偶数节点头接在奇数最后一个节点后，再返回头部。 图示： 1234567891011121314151617181920212223242526public class Solution { /** * 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 * * * @param head ListNode类 * @return ListNode类 */ public ListNode oddEvenList (ListNode head) { // write code here if(head == null || head.next ==null){ return head; } ListNode evenHead = head.next; ListNode cur1 = head, cur2 = evenHead; while(cur2 != null &amp;&amp; cur2.next != null){ cur1.next = cur2.next; cur1 = cur1.next; cur2.next = cur1.next; cur2 = cur2.next; } cur1.next = evenHead; return head; }} 二、递归反转单链表206. 反转链表迭代 一般是用迭代，设定俩个结点去遍历修改链表 12345678910111213class Solution { public ListNode reverseList(ListNode head) { ListNode pre = null; ListNode cur = head; while(cur != null){ ListNode next = cur.next; cur.next = pre; pre = cur; cur = next; } return pre; }} 递归 123456789class Solution { public ListNode reverseList(ListNode head) { if(head == null || head.next == null) return head; ListNode last = reverseList(head.next); head.next.next = head; head.next = null; return last; }} 反转链表前N个节点12345678910111213141516171819ListNode successor = null; // 后驱节点// 反转以 head 为起点的 n 个节点，返回新的头结点ListNode reverseN(ListNode head, int n) { if (n == 1) { // 记录第 n + 1 个节点 successor = head.next; return head; } // 以 head.next 为起点，需要反转前 n - 1 个节点 ListNode last = reverseN(head.next, n - 1); head.next.next = head; // 让反转之后的 head 节点和后面的节点连起来 head.next = successor; return last;} 具体的区别： 1、base case 变为 n == 1，反转一个元素，就是它本身，同时要记录后驱节点。 2、刚才我们直接把 head.next 设置为 null，因为整个链表反转后原来的 head 变成了整个链表的最后一个节点。但现在 head 节点在递归反转之后不一定是最后一个节点了，所以要记录后驱 successor（第 n + 1 个节点），反转之后将 head 连接上。 反转链表的一部分利用反转前n个结点的思想对递归进行判断当left = 1才进行反转 123456789101112131415161718192021class Solution { ListNode next = null; public ListNode reverseBetween(ListNode head, int left, int right) { if(left == 1){ return reverseN(head, right); } // 前进到反转的起点触发 base case head.next = reverseBetween(head.next, left-1, right-1); return head; } ListNode reverseN(ListNode head, int right){ if(right == 1){ next = head.next; return head; } ListNode last = reverseN(head.next, right-1); head.next.next = head; head.next = next; return last; }}","link":"/2023/02/04/%E7%AE%97%E6%B3%95/%E9%93%BE%E8%A1%A8/"},{"title":"docker部署","text":"0.安装DockerDocker 分为 CE 和 EE 两大版本。CE 即社区版（免费，支持周期 7 个月），EE 即企业版，强调安全，付费使用，支持周期 24 个月。 Docker CE 分为 stable test 和 nightly 三个更新频道。 官方网站上有各种环境下的 安装指南，这里主要介绍 Docker CE 在 CentOS上的安装。 1.CentOS安装DockerDocker CE 支持 64 位版本 CentOS 7，并且要求内核版本不低于 3.10， CentOS 7 满足最低内核的要求，所以我们在CentOS 7安装Docker。 1.1.卸载（可选）如果之前安装过旧版本的Docker，可以使用下面命令卸载： 1234567891011yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine \\ docker-ce 1.2.安装docker首先需要大家虚拟机联网，安装yum工具 123yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 --skip-broken 然后更新本地镜像源： 12345678# 设置docker镜像源yum-config-manager \\ --add-repo \\ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo sed -i 's/download.docker.com/mirrors.aliyun.com\\/docker-ce/g' /etc/yum.repos.d/docker-ce.repoyum makecache fast 然后输入命令： 1yum install -y docker-ce docker-ce为社区免费版本。稍等片刻，docker即可安装成功。 1.3.启动dockerDocker应用需要用到各种端口，逐一去修改防火墙设置。非常麻烦，因此建议大家直接关闭防火墙！ 启动docker前，一定要关闭防火墙后！！ 启动docker前，一定要关闭防火墙后！！ 启动docker前，一定要关闭防火墙后！！ 1234# 关闭systemctl stop firewalld# 禁止开机启动防火墙systemctl disable firewalld 通过命令启动docker： 12345systemctl start docker # 启动docker服务systemctl stop docker # 停止docker服务systemctl restart docker # 重启docker服务 然后输入命令，可以查看docker版本： 1docker -v 如图： 1.4.配置镜像加速docker官方镜像仓库网速较差，我们需要设置国内镜像服务： 参考阿里云的镜像加速文档：https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors 2.CentOS7安装DockerCompose2.1.下载Linux下需要通过命令下载： 12# 安装curl -L https://github.com/docker/compose/releases/download/1.23.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose 2.2.修改文件权限修改文件权限： 12# 修改权限chmod +x /usr/local/bin/docker-compose 2.3.Base自动补全命令：12# 补全命令curl -L https://raw.githubusercontent.com/docker/compose/1.29.1/contrib/completion/bash/docker-compose &gt; /etc/bash_completion.d/docker-compose 如果这里出现错误，需要修改自己的hosts文件： 1echo \"199.232.68.133 raw.githubusercontent.com\" &gt;&gt; /etc/hosts 3.Docker镜像仓库搭建镜像仓库可以基于Docker官方提供的DockerRegistry来实现。 官网地址：https://hub.docker.com/_/registry 3.1.简化版镜像仓库Docker官方的Docker Registry是一个基础版本的Docker镜像仓库，具备仓库管理的完整功能，但是没有图形化界面。 搭建方式比较简单，命令如下： 123456docker run -d \\ --restart=always \\ --name registry \\ -p 5000:5000 \\ -v registry-data:/var/lib/registry \\ registry 命令中挂载了一个数据卷registry-data到容器内的/var/lib/registry 目录，这是私有镜像库存放数据的目录。 访问http://YourIp:5000/v2/_catalog 可以查看当前私有镜像服务中包含的镜像 3.2.带有图形化界面版本使用DockerCompose部署带有图象界面的DockerRegistry，命令如下： 123456789101112131415version: '3.0'services: registry: image: registry volumes: - ./registry-data:/var/lib/registry ui: image: joxit/docker-registry-ui:static ports: - 8080:80 environment: - REGISTRY_TITLE=传智教育私有仓库 - REGISTRY_URL=http://registry:5000 depends_on: - registry 3.3.配置Docker信任地址我们的私服采用的是http协议，默认不被Docker信任，所以需要做一个配置： 12345678# 打开要修改的文件vi /etc/docker/daemon.json# 添加内容：\"insecure-registries\":[\"http://192.168.150.101:8080\"]# 重加载systemctl daemon-reload# 重启dockersystemctl restart docker","link":"/2023/11/05/%E8%BF%90%E7%BB%B4/docker%E9%83%A8%E7%BD%B2/"},{"title":"linux下排查占用CPU较高的线程信息","text":"背景当出现线上告警存在服务器CPU负载打满的时候，需要我们去排查定位问题 步骤 通过top命令找到高占用CPU的java进程的PID 1top 根据pid找到CPU使用率较高的线程的TID 1ps -mp ${pid} -o THREAD,tid,time ps: 此命令用于报告当前的进程状态。 -m: 这个选项表示显示线程信息，而不仅仅是进程信息。它允许显示每个进程的线程。 -p ${pid}: 这个选项指定要监视的进程的 PID（进程 ID）。在这里，${pid} 是一个变量，你可以用特定的 PID 值替换它。使用此选项可以只关注给定的进程。 -o THREAD,tid,time: 这个选项指定输出的格式以及要显示的字段： THREAD: 显示线程的状态。 tid: 显示线程 ID (TID)。 time: 显示线程使用的总 CPU 时间。 将占CPU最高的TID转换成十六进制 1print \"%x\\n\" ${tid} 结合进程号和线程号,利用jstack查到异常代码所在行 1jstack -l ${pid} | grep ${thread-hex-id} -A 10 jstack -l ${pid} : jstack 是一个用于打印 Java 虚拟机 (JVM) 中线程堆栈的工具。通过这个工具，你可以看到当前所有线程的状态和栈帧信息。 -l 选项表示输出一些额外的锁信息，包括每个线程的等待锁信息。 ${pid} 代表 Java 进程的进程 ID (PID)，你需要用实际的 PID 来替换它。 | grep ${thread-hex-id} -A 10 : | 是管道符，它将 jstack 命令的输出传递给 grep 命令。 grep ${thread-hex-id} 是用来搜索特定线程的堆栈信息。${thread-hex-id} 应该是某个线程的十六进制 ID，通常是在 Java 应用程序内观察到的线程 ID。 -A 10 选项表示在找到匹配的行后，打印其后 10 行。这对于查看线程的堆栈帧非常有用，因为线程的堆栈信息通常包括多个行。","link":"/2024/07/05/%E8%BF%90%E7%BB%B4/linux%E4%B8%8B%E6%8E%92%E6%9F%A5%E5%8D%A0%E7%94%A8CPU%E8%BE%83%E9%AB%98%E7%9A%84%E7%BA%BF%E7%A8%8B%E4%BF%A1%E6%81%AF/"},{"title":"linux挂载硬盘","text":"1、查看硬盘占用情况1df -l 2、查看硬盘及分区信息1fdisk -l 3、分区123456789101112131415161718192021[root@localhost Desktop]fdisk /dev/sdb Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)WARNING: DOS-compatible mode is deprecated. It's strongly recommended to switch off the mode (command 'c') and change display units to sectors (command 'u').Command (m for help): n # 输入n , 新增分区Command action e extended p primary partition (1-4)p # 输入p , 设定为主分区Partition number (1-4): 1 # 选择1 ，为第1分区First cylinder (1-261, default 1): # enter ,选择默认Using default value 1Last cylinder, +cylinders or +size{K,M,G} (1-261, default 261): # enter ,选择默认Using default value 261Command (m for help): w # 输入w ，写入修改 The partition table has been altered!Calling ioctl() to re-read partition table.Syncing disks. 4、格式化12345678910111213141516171819202122232425[root@localhost Desktop] mkfs -t ext4 /dev/sdb1mke2fs 1.41.12 (17-May-2010)Filesystem label=OS type: LinuxBlock size=4096 (log=2)Fragment size=4096 (log=2)Stride=0 blocks, Stripe width=0 blocks131072 inodes, 524112 blocks26205 blocks (5.00%) reserved for the super userFirst data block=0Maximum filesystem blocks=53687091216 block groups32768 blocks per group, 32768 fragments per group8192 inodes per groupSuperblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912Writing inode tables: done Creating journal (8192 blocks): doneWriting superblocks and filesystem accounting information: doneThis filesystem will be automatically checked every 22 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override. 5、手动挂载命令 ： mount 磁盘 文件目录 # 将磁盘挂载在目录上 1234567891011root@localhost Desktop]# mount /dev/sdb1 /home/newdisk [root@localhost Desktop]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsr0 11:0 1 1024M 0 rom sda 8:0 0 40G 0 disk ├─sda1 8:1 0 300M 0 part /boot├─sda2 8:2 0 3.9G 0 part [SWAP]└─sda3 8:3 0 35.8G 0 part /sdb 8:16 0 2G 0 disk └─sdb1 8:17 0 2G 0 part /home/newdisk # 挂载成功 但这是单次挂载，重启后会失效 6、永久自动挂载永久自动修改的方法是修改/dev/fstab 配置文件， 将磁盘和文件挂载信息写入/etc/fstab，/dev/sdb1 /home/newdisk ext4 defaults 0 0再执行 mount -a 即可 然后执行lsblk查看磁盘挂载情况 123456789101112131415161718192021222324252627[root@localhost Desktop]# vim /etc/fstab ## /etc/fstab# Created by anaconda on Thu Jul 9 00:04:07 2020## Accessible filesystems, by reference, are maintained under '/dev/disk'# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#/dev/sdb1 /home/newdisk ext4 defaults 0 0 UUID=0f7d9402-9f6c-4b87-ab96-8e16dab67957 / ext4 defaults 1 1UUID=2c00004a-e632-48d9-bc8e-d8c83acb2019 /boot ext4 defaults 1 2UUID=50fe9b93-7fe0-4c14-acfc-44c35e3b034a swap swap defaults 0 0tmpfs /dev/shm tmpfs defaults 0 0devpts /dev/pts devpts gid=5,mode=620 0 0sysfs /sys sysfs defaults 0 0proc /proc proc defaults 0 0[root@localhost Desktop]# mount -a [root@localhost Desktop]# lsblkNAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTsr0 11:0 1 1024M 0 rom sda 8:0 0 40G 0 disk ├─sda1 8:1 0 300M 0 part /boot├─sda2 8:2 0 3.9G 0 part [SWAP]└─sda3 8:3 0 35.8G 0 part /sdb 8:16 0 2G 0 disk └─sdb1 8:17 0 2G 0 part /home/newdisk # 挂载成功， 重启之后挂载仍可用。","link":"/2023/05/05/%E8%BF%90%E7%BB%B4/linux%E6%8C%82%E8%BD%BD%E7%A1%AC%E7%9B%98/"}],"tags":[{"name":"面经","slug":"面经","link":"/tags/%E9%9D%A2%E7%BB%8F/"},{"name":"os","slug":"os","link":"/tags/os/"},{"name":"序列化","slug":"序列化","link":"/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"},{"name":"单测","slug":"单测","link":"/tags/%E5%8D%95%E6%B5%8B/"},{"name":"http","slug":"http","link":"/tags/http/"},{"name":"类型转换","slug":"类型转换","link":"/tags/%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"规约","slug":"规约","link":"/tags/%E8%A7%84%E7%BA%A6/"},{"name":"NoSql","slug":"NoSql","link":"/tags/NoSql/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"消息队列","slug":"消息队列","link":"/tags/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"工具","slug":"工具","link":"/tags/%E5%B7%A5%E5%85%B7/"},{"name":"中间件","slug":"中间件","link":"/tags/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"docker","slug":"docker","link":"/tags/docker/"},{"name":"ajax","slug":"ajax","link":"/tags/ajax/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"注册中心","slug":"注册中心","link":"/tags/%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/"},{"name":"分布式搜索引擎","slug":"分布式搜索引擎","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E/"},{"name":"分布式事务","slug":"分布式事务","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/"},{"name":"分布式缓存","slug":"分布式缓存","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"算法","slug":"算法","link":"/tags/%E7%AE%97%E6%B3%95/"},{"name":"linux","slug":"linux","link":"/tags/linux/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"},{"name":"cs","slug":"cs","link":"/categories/cs/"},{"name":"python","slug":"python","link":"/categories/python/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"前端技术","slug":"前端技术","link":"/categories/%E5%89%8D%E7%AB%AF%E6%8A%80%E6%9C%AF/"},{"name":"mysql","slug":"mysql","link":"/categories/mysql/"},{"name":"算法","slug":"算法","link":"/categories/%E7%AE%97%E6%B3%95/"},{"name":"运维","slug":"运维","link":"/categories/%E8%BF%90%E7%BB%B4/"}],"pages":[]}