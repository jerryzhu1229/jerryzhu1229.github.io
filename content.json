{"posts":[{"title":"计算机网络","text":"*OSI七层和作用 **访问网页全过程 首先浏览器对 URL 进行解析，生成HTTP请求报文，包括方法、数据源路径、版本、消息头、消息体。 根据 URL 的域名通过 DNS 协议获取对应 IP地址（DNS 查找过程：浏览器缓存、路由器缓存、DNS 缓存） 根据 IP 地址+端口号，发送 TCP 连接请求建立连接。生成 TCP 报文（若HTTP 报文超出 MSS，需进行分割） 生成IP报文封装源 IP、目标 IP。 根据目标 IP 查找路由表（路由选择协议）得到下一跳 IP，再利用ARP获取对应 MAC 地址，生成 MAC 报文。 网卡驱动程序加上报头和帧校验序列FCS将数字信号转换为电信号传输 路由器对包接收，进行帧校验，检查 MAC 头部中的接收方 MAC 地址是否是自己，若是则查找路由表寻找下一跳路由器IP及其MAC 地址，封装包并转发，重复直至将数据包传输到服务器。 服务器依次拆解报文，得到 HTTP 请求报文，返回封装网页的HTTP 响应报文，重复刚刚的流程发送给浏览器。 浏览器根据 HTTP 响应报文，解析响应体中的 HTML 代码，渲染网页的结构和样式，根据 HTML 中的其他资源的 URL，再次发起 HTTP 请求，获取这些资源 浏览器在不需要和服务器通信时，可以主动关闭 TCP 连接，或者等待服务器的关闭请求 应用层 DNS（Domain Name System）： 解决域名和 IP 地址的映射 域名解析的工作流程 HTTP：根据 DNS获取的目标主机的 IP 发送 HTTP 报文 传输层 HTTP 基于 TCP，数据要经过这俩个协议的封装 网络层 应用层、传输层是端到端协议；网络层是中间件协议，主机与中间系统进行交互。 网络层的核心功能：路由选择和分组转发 路由选择：确定分组从源到目的最优路径的过程 分组转发：将分组从路由器的输入端口转移到合适的输出端口 往哪里传输？或者说，要把数据包发到哪个路由器上？（怎么路由） 根据报文的目标 IP 地址跟路由表每个表项的掩码字段做“与”操作，判断是否匹配该表项的目标 IP 地址。 匹配完所有的表项选择掩码最长的匹配项，根据该表项的出接口和下一跳路由 IP将报文转发；若没有匹配到，则查找是否有缺省路由；若都没有，则丢弃报文 路由表项怎么来的？ direct：链路层发现，优点：自动发现，开销小。缺点：只发现接口所属网段 static：静态路由，需要人为调整 缺省路由 动态路由：RIP（Routing Information Protocol使用跳数作为路径距离）、OSPF（Open Shortest Path First链路开销值判断路径长短）和 BGP（Border Gateway Protocol边界网关协议，在路由选择域之间交换网络层可达性信息） HTTP *HTTP 常见状态码「200 OK」请求已正常处理。 「204 No Content」请求处理成功，但响应头没有 body 数据。 「206 Partial Content」表示响应返回的 body 数据并不是资源的全部，而是其中的一部分 「301 Moved Permanently」 永久性重定向 「302 Found」临时性重定向 「304 Not Modified」不具有跳转的含义，表示资源未修改 「400 Bad Request」表示客户端请求的报文有错误，但只是个笼统的错误 「403 Forbidden」服务器禁止访问资源。权限，未授权IP等 「404 Not Found」服务器上没有请求的资源。路径错误 「500 Internal Server Error」服务器发生错误，是个笼统通用的错误码 「501 Not Implemented」表示客户端请求的功能还不支持 「502 Bad Gateway」网关错误 后端服务器故障（ping、telnet、curl） nginx配置问题（nginx.conf、查看日志） 高负载或者资源耗尽（top） nginx与后端服务器通信问题（查看防火墙） 「503 Service Unavailable」表示服务器当前很忙，暂时无法响应客户端 「504 Gateway timeout」 网关超时 一般指nginx做反向代理服务器时，所连接的服务器tomcat无响应导致的。 为了完成您的 HTTP 请求，该服务器访问一个上游服务器，但没得到及时的响应 nginx超过了自己设置的超时时间 **HTTP与HTTPS区别 HTTP 是超文本传输协议，信息是明文传输，存在安全隐患。HTTPS 则解决 HTTP 不安全的缺陷，在 TCP 和 HTTP 网络层之间加入了 SSL/TLS 安全协议，使得报文能够加密传输。 默认端口号不一样，HTTP：80 HTTPS：443 HTTP 连接只需要 TCP 三次握手；HTTPS 连接还需要 SSL/TLS 四次握手，进入加密报文传输，传输内容对称加密，但对称加密的密钥是用服务器方的证书进行非对称加密 url 前缀：http:// https// SEO（搜索引擎优化）：搜索引擎通常会更青睐使用 HTTPS 协议的网站 SSL/TLS解决了 HTTP 数据透明的问题 工作原理流程 非对称加密 SSL/TLS 的核心要素是非对称加密。非对称加密采用两个密钥——一个公钥，一个私钥。公钥加密的内容，使用私钥可以解开；而私钥加密的内容，公钥可以解开。目前使用最为广泛的非对称密钥为RSA算法。 对称加密 使用 SSL/TLS 进行通信的双方需要使用非对称加密方案来通信，但是非对称加密设计了较为复杂的数学算法，在实际通信过程中，计算的代价较高，效率太低，因此，SSL/TLS 实际对消息的加密使用的是对称加密。 hash算法加密 它是一种不可逆的加密方式，对一组数据使用哈希算法加密，加密后不能解密 保证公钥传输的信赖性（*数字签名&amp;数字证书） CA 发放证书 服务器把证书内容给 CA，利用 hash算法 +私钥生成签名。 数字证书=证书内容+证书签名 服务器发送数字证书给客户端 客户端验证证书 hash算法加密证书内容，CA 公钥解密证书签名，判断是否相同 HTTP 为什么基于 TCP 协议HTTP协议基于TCP协议是出于对数据传输可靠性和完整性的需求，TCP协议提供了数据传输的可靠性和可控性，而HTTP协议定义了数据的格式和意义，两者协同工作来实现Web应用的各种功能。 HTTP 和 RPC 有什么区别 RPC，因为它定制化程度更高，可以采用体积更小的 Protobuf 或其他序列化协议去保存结构体数据，同时也不需要像 HTTP 那样考虑各种浏览器行为，比如 302 重定向跳转啥的。因此性能也会更好一些，这也是在公司内部微服务中抛弃 HTTP，选择使用 RPC 的最主要原因。 HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。 **HTTP/1.1 相比 HTTP/1.0 提高了什么性能？ 连接方式 : HTTP/1.0 为短连接，HTTP/1.1 支持长连接。 状态响应码 : HTTP/1.1 中新加入了大量的状态码，光是错误响应状态码就新增了 24 种。 缓存处理 : 在 HTTP1.0 中主要使用 header 里的 If-Modified-Since,Expires 来做为缓存判断的标准，HTTP1.1 则引入了更多的缓存控制策略，如If-Unmodified-Since 带宽优化及网络连接的使用：HTTP1.0 中，存在一些浪费带宽的现象，HTTP1.1 则在请求头引入了 range 头域，它允许只请求资源的某个部分，即返回码是 206（Partial Content） Host 头处理 : HTTP/1.1 在请求头中加入了Host字段（域名系统（DNS）允许多个主机名绑定到同一个 IP 地址上，但是 HTTP/1.0 并没有考虑这个问题） HTTP/2 做了什么优化？多路复用（Multiplexing）：串行方式（每个请求和响应都需要独立的连接）变为同一连接上可以同时传输多个请求和响应。使 HTTP2更加高效。 二进制帧（Binary Frames）：文本格式的报文变为二进制帧。二进制帧更加紧凑和高效，减少了传输的数据量和带宽消耗。 头部压缩（Header Compression）：HTTP/1.1 支持Body压缩，Header不支持压缩。HTTP/2.0 支持对Header压缩，使用了专门为Header压缩而设计的 HPACK 算法，减少了网络开销。 服务器推送（Server Push）：HTTP/2.0 支持服务器推送，可以在客户端请求一个资源时，将其他相关资源一并推送给客户端，从而减少了客户端的请求次数和延迟。而 HTTP/1.1 需要客户端自己发送请求来获取相关资源。 HTTP/3 做了哪些优化？传输协议：HTTP/2.0 是基于 TCP 协议实现的，HTTP/3.0 新增了 QUIC（Quick UDP Internet Connections） 协议来实现可靠的传输，提供与 TLS/SSL 相当的安全性，具有较低的连接和传输延迟。你可以将 QUIC 看作是 UDP 的升级版本，在其基础上新增了很多功能比如加密、重传等等。 连接建立：HTTP/2.0 需要经过经典的 TCP 三次握手过程（由于安全的 HTTPS 连接建立还需要 TLS 握手，共需要大约 3 个 RTT）。由于 QUIC 协议的特性（TLS 1.3，TLS 1.3 除了支持 1 个 RTT 的握手，还支持 0 个 RTT 的握手）连接建立仅需 0-RTT 或者 1-RTT。这意味着 QUIC 在最佳情况下不需要任何的额外往返时间就可以建立新连接。 队头阻塞：HTTP/2.0 多请求复用一个 TCP 连接，一旦发生丢包，就会阻塞住所有的 HTTP 请求。由于 QUIC 协议的特性，HTTP/3.0 在一定程度上解决了队头阻塞（Head-of-Line blocking, 简写：HOL blocking）问题，一个连接建立多个不同的数据流，这些数据流之间独立互不影响，某个数据流发生丢包了，其数据流不受影响（本质上是多路复用+轮询）。 错误恢复：HTTP/3.0 具有更好的错误恢复机制，当出现丢包、延迟等网络问题时，可以更快地进行恢复和重传。而 HTTP/2.0 则需要依赖于 TCP 的错误恢复机制。 安全性：HTTP/2.0 和 HTTP/3.0 在安全性上都有较高的要求，支持加密通信，但在实现上有所不同。HTTP/2.0 使用 TLS 协议进行加密，而 HTTP/3.0 基于 QUIC 协议，包含了内置的加密和身份验证机制，可以提供更强的安全性。 TCPTCP 基础TCP 头部格式 序列号：在建立连接时由计算机生成的随机数作为其初始值，通过 SYN 包传给接收端主机，每发送一次数据，就「累加」一次该「数据字节数」的大小。用来解决网络包乱序问题。 确认应答号：指下一次「期望」收到的数据的序列号，发送端收到这个确认应答以后可以认为在这个序号以前的数据都已经被正常接收。用来解决丢包的问题。 控制位： ACK：该位为 1 时，「确认应答」的字段变为有效，TCP 规定除了最初建立连接时的 SYN 包之外该位必须设置为 1 。 RST：该位为 1 时，表示 TCP 连接中出现异常必须强制断开连接。 SYN：该位为 1 时，表示希望建立连接，并在其「序列号」的字段进行序列号初始值的设定。 FIN：该位为 1 时，表示今后不会再有数据发送，希望断开连接。当通信结束希望断开连接时，通信双方的主机之间就可以相互交换 FIN 位为 1 的 TCP 段。 为什么需要 TCP 协议？ TCP 工作在哪一层？网络层不可靠，如果需要保障网络数据包的可靠性，那么就需要由上层（传输层）的 TCP 协议来负责。 什么是 TCP 连接？用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接 建立一个 TCP 连接是需要客户端与服务端达成上述三个信息的共识。 Socket：由 IP 地址和端口号组成 序列号：用来解决乱序问题等 窗口大小：用来做流量控制 如何唯一确定一个 TCP 连接呢？TCP 四元组可以唯一的确定一个连接，四元组包括如下： 源地址 源端口 目的地址 目的端口 源地址和目的地址的字段（32 位）是在 IP 头部中，作用是通过 IP 协议发送报文给对方主机。 源端口和目的端口的字段（16 位）是在 TCP 头部中，作用是告诉 TCP 协议应该把报文发给哪个进程。 UDP 包大小 原理上，UDP 包长度 16 位，UDP 包的大小为 2^16-1，即 65535 字节 以太网(Ethernet)数据帧的长度必须在==46-1500==字节之间,这是由以太网的物理特性决定的.这个1500字节被称为链路层的MTU(最大传输单元). 但这并不是指链路层的长度被限制在1500字节,其实这这个MTU指的是链路层的数据区.并不包括链路层的首部和尾部的18个字节.又因为UDP数据报的首部8字节,所以UDP数据报的数据区最大长度为1472字节（1500-20-8）. 但在网络编程中，Internet中的路由器可能有设置成不同的值(小于默认值)，鉴于Internet上的标准MTU值为==576==字节,所以我建议在进行Internet的UDP编程时. 最好将UDP的数据长度控件在548字节(576-20-8)以内. 既然 IP 层会分片，为什么 TCP 层还需要 MSS 呢？ MTU：一个网络包的最大长度，以太网中一般为 1500 字节； MSS：除去 IP 和 TCP 头部之后，一个网络包所能容纳的 TCP 数据的最大长度； 因为 IP 层本身没有超时重传机制，它由传输层的 TCP 来负责超时和重传。那么当如果一个 IP 分片丢失，整个 IP 报文的所有分片都得重传。因此，可以得知由 IP 层进行分片传输，是非常没有效率的。所以，为了达到最佳的传输效能 TCP 协议在建立连接的时候通常要协商双方的 MSS 值，当 TCP 层发现数据超过 MSS 时，则就先会进行分片，当然由它形成的 IP 包的长度也就不会大于 MTU ，自然也就不用 IP 分片了。经过 TCP 层分片后，如果一个 TCP 分片丢失后，进行重发时也是以 MSS 为单位，而不用重传所有的分片，大大增加了重传的效率。 **UDP 和 TCP 有什么区别呢？分别的应用场景是？TCP 和 UDP 区别： 1. 面向连接 2. 服务对象（一对一；一对一，一对多，多对多） 3. 可靠性 可靠交付数据，数据可以无差错、不丢失、不重复、按序到达； 尽最大努力交付，不保证可靠交付数据 4. 拥塞控制、流量控制 5. 首部开销（&gt;=20B，是否使用「选项」字段；8B） 6. 传输方式（流式传输，没有边界；一个包一个包的发送，是有边界的） 7. 分片不同 TCP 的数据大小如果大于 MSS （Maximum Segment Size）大小，则会在传输层进行分片，目标主机收到后，也同样在传输层组装 TCP 数据包，如果中途丢失了一个分片，只需要传输丢失的这个分片。 UDP 的数据大小如果大于 MTU （Maximum Transmit Unit）大小，则会在 IP 层进行分片，目标主机收到后，在 IP 层组装完数据，接着再传给传输层。 TCP 和 UDP 应用场景： 由于 TCP 是面向连接，能保证数据的可靠性交付，因此经常用于： FTP 文件传输； HTTP / HTTPS； 远程登录 由于 UDP 面向无连接，它可以随时发送数据，再加上 UDP 本身的处理既简单又高效，因此经常用于： 包总量较少的通信，如 DNS 等； 语音、电话、视频； TCP 和 UDP 可以使用同一个端口吗？答案：可以的。传输层有两个传输协议分别是 TCP 和 UDP，在内核中是两个完全独立的软件模块。当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。因此，TCP/UDP 各自的端口号也相互独立，如 TCP 有一个 80 号端口，UDP 也可以有一个 80 号端口，二者并不冲突。 TCP 和 UDP 可以同时绑定相同的端口吗？ 可以的。 TCP 和 UDP 传输协议，在内核中是由两个完全独立的软件模块实现的。 当主机收到数据包后，可以在 IP 包头的「协议号」字段知道该数据包是 TCP/UDP，所以可以根据这个信息确定送给哪个模块（TCP/UDP）处理，送给 TCP/UDP 模块的报文根据「端口号」确定送给哪个应用程序处理。 因此， TCP/UDP 各自的端口号也相互独立，互不影响。 多个 TCP 服务进程可以同时绑定同一个端口吗？ 如果两个 TCP 服务进程同时绑定的 IP 地址和端口都相同，那么执行 bind() 时候就会出错，错误是“Address already in use”。 如果两个 TCP 服务进程绑定的端口都相同，而 IP 地址不同，那么执行 bind() 不会出错。 如何解决服务端重启时，报错“Address already in use”的问题？ 当我们重启 TCP 服务进程的时候，意味着通过服务器端发起了关闭连接操作，于是就会经过四次挥手，而对于主动关闭方，会在 TIME_WAIT 这个状态里停留一段时间，这个时间大约为 2MSL。 当 TCP 服务进程重启时，服务端会出现 TIME_WAIT 状态的连接，TIME_WAIT 状态的连接使用的 IP+PORT 仍然被认为是一个有效的 IP+PORT 组合，相同机器上不能够在该 IP+PORT 组合上进行绑定，那么执行 bind() 函数的时候，就会返回了 Address already in use 的错误。 要解决这个问题，我们可以对 socket 设置 ==SO_REUSEADDR== 属性。 这样即使存在一个和绑定 IP+PORT 一样的 TIME_WAIT 状态的连接，依然可以正常绑定成功，因此可以正常重启成功。 客户端的端口可以重复使用吗？ 在客户端执行 connect 函数的时候，只要客户端连接的服务器不是同一个，内核允许端口重复使用。 TCP 连接是由四元组（源IP地址，源端口，目的IP地址，目的端口）唯一确认的，那么只要四元组中其中一个元素发生了变化，那么就表示不同的 TCP 连接的。 所以，如果客户端已使用端口 64992 与服务端 A 建立了连接，那么客户端要与服务端 B 建立连接，还是可以使用端口 64992 的，因为内核是通过四元祖信息来定位一个 TCP 连接的，并不会因为客户端的端口号相同，而导致连接冲突的问题。 客户端 TCP 连接 TIME_WAIT 状态过多，会导致端口资源耗尽而无法建立新的连接吗？ 要看客户端是否都是与同一个服务器（目标地址和目标端口一样）建立连接。 如果客户端都是与同一个服务器（目标地址和目标端口一样）建立连接，那么如果客户端 TIME_WAIT 状态的连接过多，当端口资源被耗尽，就无法与这个服务器再建立连接了。即使在这种状态下，还是可以与其他服务器建立连接的，只要客户端连接的服务器不是同一个，那么端口是重复使用的。 如何解决客户端 TCP 连接 TIME_WAIT 过多，导致无法与同一个服务器建立连接的问题？ 打开 net.ipv4.tcp_tw_reuse 这个内核参数。 因为开启了这个内核参数后，客户端调用 connect 函数时，如果选择到的端口，已经被相同四元组的连接占用的时候，就会判断该连接是否处于 TIME_WAIT 状态。 如果该连接处于 TIME_WAIT 状态并且 TIME_WAIT 状态持续的时间超过了 1 秒，那么就会重用这个连接，然后就可以正常使用该端口了。 **TCP连接/断开TCP 三次握手过程是怎样的？ ==第三次握手是可以携带数据的，前两次握手是不可以携带数据的== 如何在 Linux 系统中查看 TCP 状态？TCP 的连接状态查看，在 Linux 可以通过 netstat -napt(p—pid，t—tcp，u—udp) 命令查看。 为什么要三次握手？不是两次或者四次SYN 同步序列编号(Synchronize Sequence Numbers) 是 TCP/IP 建立连接时使用的握手信号。客户机首先发出一个 SYN 消息，服务器使用 SYN-ACK 应答表示接收到了这个消息，最后客户机再以 ACK(Acknowledgement）消息响应 三次握手最主要的目的就是双方确认自己与对方的发送与接收是正常的。 第一次握手：Client 什么都不能确认；Server 确认了对方发送正常，自己接收正常 第二次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：对方发送正常，自己接收正常 第三次握手：Client 确认了：自己发送、接收正常，对方发送、接收正常；Server 确认了：自己发送、接收正常，对方发送、接收正常 TCP 连接：用于保证可靠性和流量控制维护的某些状态信息，这些信息的组合，包括 Socket、序列号和窗口大小称为连接。 Socket是由IP地址和端口结合的，提供向应用层进程传送数据包的机制。 不使用「两次握手」和「四次握手」的原因： 「两次握手」：无法防止历史连接的建立（主要原因），会造成双方资源的浪费，也无法可靠的同步双方序列号； 「四次握手」：三次握手就已经理论建立可靠连接，所以不需要使用更多的通信次数。 什么是 SYN 攻击？如何避免 SYN 攻击？假设攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会占满服务端的半连接队列，使得服务端不能为正常用户服务。 避免 SYN 攻击方式，可以有以下方法： 增大 TCP 半连接队列 减少 SYN+ACK 重传次数 TCP 四次挥手过程是怎样的？ 为什么挥手需要四次？再来回顾下四次挥手双方发 FIN 包的过程，就能理解为什么需要四次了。 关闭连接时，客户端向服务端发送 FIN 时，仅仅表示客户端不再发送数据了但是还能接收数据。 服务端收到客户端的 FIN 报文时，先回一个 ACK 应答报文，而服务端可能还有数据需要处理和发送，等服务端不再发送数据时，才发送 FIN 报文给客户端来表示同意现在关闭连接。 从上面过程可知，服务端通常需要等待完成数据的发送和处理，所以服务端的 ACK 和 FIN 一般都会分开发送，因此是需要四次挥手。 为什么第四次挥手客户端需要等待 2*MSL（报文段最长寿命） 确保服务端收到客户端的 ack 确保上一次的 tcp 报文信息不会影响下一次的 tcp 连接 TIME_WAIT 过多有什么危害？ 占用系统资源，比如文件描述符、内存资源、CPU 资源、线程资源等； 占用端口资源，端口资源也是有限的 服务器出现大量 TIME_WAIT 状态的原因有哪些？ 第一个场景：HTTP 没有使用长连接 第二个场景：HTTP 长连接超时 第三个场景：HTTP 长连接的请求数量达到上限 服务器出现大量 CLOSE_WAIT 状态的原因有哪些？当服务端出现大量 CLOSE_WAIT 状态的连接的时候，通常都是代码的问题，这时候我们需要针对具体的代码一步一步的进行排查和定位，主要分析的方向就是服务端为什么没有调用 close。 **TCP 如何保证传输的可靠性？ 序列号（对失序数据包重排以及去重） 校验和 重传机制（超时重传；快速重传） 流量控制 拥塞控制 滑动窗口假如发送一个数据包，要等待 ack 才发送下一个，效率太低。 引入窗口，指定窗口大小，指无需等待确认应答，而可以继续发送数据的最大值。 TCP 如何实现流量控制？TCP 利用滑动窗口实现流量控制。流量控制是为了控制发送方发送速率，保证接收方来得及接收。 接收方发送的确认报文中的窗口字段可以用来控制发送方窗口大小，从而影响发送方的发送速率。将窗口字段设置为 0，则发送方不能发送数据。 TCP 为全双工(Full-Duplex, FDX)通信，双方可以进行双向通信，客户端和服务端既可能是发送端又可能是服务端。因此，两端各有一个发送缓冲区与接收缓冲区，两端都各自维护一个发送窗口和一个接收窗口。 TCP 发送窗口可以划分成四个部分： 已经发送并且确认的 TCP 段（已经发送并确认）； 已经发送但是没有确认的 TCP 段（已经发送未确认）； 未发送但是接收方准备接收的 TCP 段（可以发送）； 未发送并且接收方也并未准备接受的 TCP 段（不可发送） TCP 接收窗口可以划分成三个部分： 已经接收并且已经确认的 TCP 段（已经接收并确认）； 等待接收且允许发送方发送 TCP 段（可以接收未确认）； 不可接收且不允许发送方发送 TCP 段（不可接收）。 窗口关闭如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。 只要 TCP 连接一方收到对方的零窗口通知，就启动持续计时器。 如果持续计时器超时，就会发送窗口探测 ( Window probe ) 报文，而对方在确认这个探测报文时，给出自己现在的接收窗口大小。 TCP 的拥塞控制是怎么实现的？拥塞控制，控制的目的就是避免「发送方」的数据填满整个网络。 为了在「发送方」调节所要发送数据的量，定义了一个叫做「拥塞窗口」的概念。 拥塞控制主要是四个算法: 慢启动：当发送方每收到一个 ACK，拥塞窗口 cwnd 的大小就会加 1。指数性的增长 拥塞避免：每当收到一个 ACK 时，cwnd 增加 1/cwnd。线性增长 快速重传：当接收方发现丢了一个中间包的时候，重复3 次发送前一个包的 ACK，于是发送端就会快速地重传，不必等待超时再重传。这种情况下网络阻塞不严重，将ssthresh慢开始门限设为cwnd/2，cwnd 重置为ssthresh，进入快速恢复算法。 快速恢复：由于发送方现在认为网络很可能没有发生拥塞（如果网络发生了严重拥塞，就不会一连有好几个报文段连续到达接收方，也就不会导致接收方连续发送重复确认）。将 cwnd 变为 ssthresh，开始拥塞避免 如何理解是 TCP 面向字节流协议？如何理解字节流？ 先来说说为什么 UDP 是面向报文的协议？ 当用户消息通过 UDP 协议传输时，操作系统不会对消息进行拆分，也就是每个 UDP 报文就是一个用户消息的边界 再来说说为什么 TCP 是面向字节流的协议？ 当用户消息通过 TCP 协议传输时，消息可能会被操作系统分组成多个的 TCP 报文，也就是一个完整的用户消息被拆分成多个 TCP 报文进行传输。我们不能认为一个用户消息对应一个 TCP 报文，正因为这样，所以 TCP 是面向字节流的协议。 粘包/拆包因为TCP是面向流，没有边界，而操作系统在发送TCP数据时，会通过缓冲区来进行优化，例如缓冲区为1024个字节大小。 如果一次请求发送的数据量比较小，没达到缓冲区大小，TCP则会将多个请求合并为同一个请求进行发送，这就形成了粘包问题。 如果一次请求发送的数据量比较大，超过了缓冲区大小，TCP就会将其拆分为多次发送，这就是拆包。 解决： 粘包：固定包的长度，不足用 0 填充 拆包：特殊字符作为边界（FTP 协议） TCP 协议有什么缺陷？ 升级 TCP 的工作很困难； TCP 协议是在内核中实现的，应用程序只能使用不能修改，如果要想升级 TCP 协议，那么只能升级内核。内核升级困难。 TCP 建立连接的延迟； 现在大多数网站都是使用 HTTPS 的，这意味着在 TCP 三次握手之后，还需要经过 TLS 四次握手后，才能进行 HTTP 数据的传输，这在一定程序上增加了数据传输的延迟。 TCP 存在队头阻塞问题； TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据。 网络迁移需要重新建立 TCP 连接； 基于 TCP 传输协议的 HTTP 协议，由于是通过四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接。那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接。 如何基于 UDP 协议实现可靠传输？市面上已经有基于 UDP 协议实现的可靠传输协议的成熟方案了，那就是 QUIC 协议，已经应用在了 HTTP/3。 TCP 四次挥手，可以变成三次吗？当被动关闭方（上图的服务端）在 TCP 挥手过程中，「没有数据要发送」并且「开启了 TCP 延迟确认机制」（默认开启），那么第二和第三次挥手就会合并传输，这样就出现了三次挥手。 什么是 TCP 延迟确认机制？ 当有响应数据要发送时，ACK 会随着响应数据一起立刻发送给对方 当没有响应数据要发送时，ACK 将会延迟一段时间，以等待是否有响应数据可以一起发送 如果在延迟等待发送 ACK 期间，对方的第二个数据报文又到达了，这时就会立刻发送 ACK TCP 序列号和确认号是如何变化的？ 公式一：序列号 = 上一次发送的序列号 + len（数据长度）。特殊情况，如果上一次发送的报文是 SYN 报文或者 FIN 报文，则改为 上一次发送的序列号 + 1。 公式二：确认号 = 上一次收到的报文中的序列号 + len（数据长度）。特殊情况，如果收到的是 SYN 报文或者 FIN 报文，则改为上一次收到的报文中的序列号 + 1。 TCP Keepalive 和 HTTP Keep-Alive 是一个东西吗？HTTP 的 Keep-Alive 也叫 HTTP 长连接，该功能是由「应用程序」实现的，可以使得用同一个 TCP 连接来发送和接收多个 HTTP 请求/应答，减少了 HTTP 短连接带来的多次 TCP 连接建立和释放的开销。 TCP 的 Keepalive 也叫 TCP 保活机制，该功能是由「内核」实现的，当客户端和服务端长达一定时间没有进行数据交互时，内核为了确保该连接是否还有效，就会发送探测报文，来检测对方是否还在线，然后来决定是否要关闭该连接。 ARP 协议详解(网络层)ARP 协议工作时有一个大前提，那就是 ARP 表。 在一个局域网内，每个网络设备都自己维护了一个 ARP 表，ARP 表记录了某些其他网络设备的 IP 地址-MAC 地址映射关系，该映射关系以 &lt;IP, MAC, TTL&gt; 三元组的形式存储。 ARP 的工作原理将分两种场景讨论： 同一局域网内的 MAC 寻址； 从一个局域网到另一个局域网中的网络设备的寻址。 工作原理：ARP 表、广播问询、单播响应","link":"/2024/03/11/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"title":"面经","text":"3/13 芒星面经 策略模式跟状态模式的区别 黑盒测试、白盒测试 一个项目的研发过程 进程有哪几个状态 如何用 ArrayList 写一个阻塞队列 策略模式封装的是行为，而状态模式封装的是变化。策略是外界给的，策略怎么变，是调用者考虑的事情，系统只是根据所给的策略做事情。状态是系统自身的固有的，由系统本身控制，调用者不能直接指定或改变系统的状态转移。 黑盒测试也称：数据驱动测试，包括功能测试和性能测试。白盒测试也称：逻辑驱动测试，包括语句覆盖、判定覆盖、条件覆盖、路径覆盖等。判定某种方法是否为黑盒测试方法，关键还是看是否针对被测对象内部结构还是针对被测对象的整体进行测试。 策略（Strategy）模式的定义：该模式定义了一系列算法，并将每个算法封装起来，使它们可以相互替换，且算法的变化不会影响使用算法的客户。策略模式属于行为模式。 3/20 腾讯 pcg 一面 os 内核级别 shell 脚本 if-else 如何实现 mysql 如何查看死锁 array 跟 arraylist 的区别 innodb 跟 myisam 的读写性能区别 Linux一般会有7个运行级别（可由init N来切换，init0为关机，init 6为重启系统）0 - 停机1 - 单用户模式2 - 多用户，但是没有NFS ，不能使用网络3 - 完全多用户模式4 - 打酱油的，没有用到5 - X11 图形化登录的多用户模式6 - 重新启动 （如果将默认启动模式设置为6，Linux将会不断重启） 是否死锁 1234567891011121314151617181920use ***db;show ENGINE INNODB STATUS;其他方法：1、查询是否锁表show OPEN TABLES where In_use &gt; 0;2、查询进程show processlist (查询到相对应的进程===然后 kill id )3、查看正在锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCKS; 4、查看等待锁的事务SELECT * FROM INFORMATION_SCHEMA.INNODB_LOCK_WAITS; 接口优化方案 项目滑块锁为什么 incr 之后要用滑块锁，incr 就具备原子性了 在 redis 集群模式下【以我们的场景为例】，incr 请求操作也可能在请求时发生网络抖动超时返回。这个时候incr有可能成功，也有可能失败。 如果redis宕机了，扣减的库存和生成的分布式锁的数据还没同步到从节点，那 Redis 恢复的时候会导致库存重复扣减超卖。 这里加分布式锁可以防止：扣减库存成功后→Redis 宕机导致扣减的那个库存丢失→Redis 恢复正常→加分布式锁成功（加锁的是正常的库存数量）的情况。 规则引擎 筛选的标签是什么，根据什么来过滤呢？ 年龄、性别、首单金额、消费总额、忠诚度、点击、搜索、喜好、状态等。这些数据主要根据运营活动策略的配置而进行开发和使用。 那有没有可能你制定的这些标签，数据传进来的时候是丢失的，用户没有某个路径上的数据，是不是就到不了叶子节点了，按你说的就没法领取活动了？ 任何一款决策树在没有标签的数据的前提下，都会影响最终的决策结果。就像风控模型，我本来判断你是黑产，但标签丢失，我就没法准确判断。所以这种情况通常是进行兜底，风险不可评估，走兜底策略决策结果。 既然你的这些规则都是确定的，为什么要用决策树？决策树和布尔检索有什么区别知道吗？ 规则是确定的还有可能新开发的，每个玩法活动应对的场景不同标签也会随之改变。所以决策树规则引擎的目的是尽可能简化通用场景的开发成本，提高研发能效，快速响应业务需求。 量化规则引擎是一个组件，如果有一个新的业务进来，如何复用? 它的复用性体现在哪?能否支持风控可A/Btest需求? 没有把业务逻辑和和具体的功能性服务进行捆绑，只需要在量化规则的库表中加入决策树的配置信息，包括树根、树茎、子叶、果实。在具体的逻辑实现中需要通过子叶判断走哪个树茎最终筛选出一个果实来。这样就可以被新的业务场景使用。因为他是规则引擎结构设计，灵活性高，复用性好。 A/B Test 本身就可以做为决策树中的子叶果实来配置，对AB用户发放不同的策略结果。所以结合风控提供的数据，作为一个逻辑节点使用也是可以的。 规则引擎如何扩容 实际上是对决策树的一个调整，具体到实际的操作就是对规则引擎的库表信息进行增加条件，比如增加新的子叶节点代表加入的新的规则标签，相应要加入树茎代表决策的路径。 规则引擎的设计目的 降低重复编码和提高可维护性的 主要解决抽奖业务场景中对个性化运营诉求，如人群身份标签、活动资格等规则的可配置化的交叉使用。 此规则引擎的设计是一个二叉树判断，具体用到了组合模式，工厂模式。为了便于维护和使用，进行了库表对二叉树的抽象设计，树根、树茎、子叶、果实，映射为二叉树编码的相关属性信息。 ​ 组合模式是一种结构型设计模式，它允许你将对象组合成树形结构以表示“部分-整体”的层次结构。组合模式使得客户端能够统一对待单个对象和对象组合，从而使得系统中的每个对象都可以被一致地处理。 规则引擎中，如果有两个同类型的节点怎么办？比如说规则树中有两个节点代表的同一类型的条件，比如考虑一个年龄大于10，一个岁数大于25，位于不同的规则树节点中，在高并发的情况下，通过rpc调用接口，对于用户的年龄数据查询会查两遍，但是实际上查一次就可以，但是调用端相当于耗费了两次的查询，应该怎么解决？ 对查询的数据加缓存 分库分表 如何扩展：数据迁移 常规方案 如果增加的节点数和扩容操作没有规划，那么绝大部分数据所属的分片都有变化，需要在分片间迁移： 预估迁移耗时，发布停服公告； 停服(用户无法使用服务)，使用事先准备的迁移脚本，进行数据迁移； 修改为新的分片规则； 启动服务器。 免迁移扩容 采用双倍扩容策略，避免数据迁移。扩容前每个节点的数据，有一半要迁移至一个新增节点中，对应关系比较简单。具体操作如下(假设已有 2 个节点 A/B，要双倍扩容至 A/A2/B/B2 这 4 个节点)： 无需停止应用服务器； 新增两个数据库 A2/B2 作为从库，设置主从同步关系为：A=&gt;A2、B=&gt;B2，直至主从数据同步完毕(早期数据可手工同步)； 调整分片规则并使之生效：原 ID%2=0 =&gt; A 改为 ID%4=0 =&gt; A, ID%4=2 =&gt; A2；原 ID%2=1 =&gt; B 改为 ID%4=1 =&gt; B, ID%4=3 =&gt; B2。 解除数据库实例的主从同步关系，并使之生效； 此时，四个节点的数据都已完整，只是有冗余(多存了和自己配对的节点的那部分数据)，择机清除即可(过后随时进行，不影响业务)。 Redis分布式锁 如果redis作为分布式锁的时候，主节点挂掉了，但是数据还没有同步到从节点，这种情况怎么办？ 通常这种情况也就是说主节点挂了，切换到从节点也不能正常工作。那么最要保障的就是不要发生过多的客诉，所以会紧急上挡板，下线活动，提醒用户：“商品售空，运营正在紧急补货中，稍后上架”。待Redis等服务恢复后，再进行上线。 Redis 挡板的实现原理：Redis 挡板的实现原理是通过使用 Redis 的键值对存储来实现的。Redis 挡板的实现步骤如下： 1. 首先，在 Redis 中创建一个键值对，其中键为挡板的名称，值为挡板的值； 2. 然后，在应用程序中，使用 Redis 的 get 方法来获取挡板的值； 3. 如果挡板的值为 true，则表示挡板已经启用，应用程序将不会执行相应的操作； 4. 如果挡板的值为 false，则表示挡板未启用，应用程序将会执行相应的操作。 通过使用 Redis 挡板，可以实现动态控制应用程序的行为，而无需重新部署应用程序。 我理解这个操作相当于一个开关。 Redis的分段锁是如何实现的 首先需要明确为什么要使用分段锁，在抽奖业务里，要保证活动的奖品不超发，就需要对活动库存扣减上锁，一般是使用数据库行级锁或者直接用 redis对活动 id 上锁，这两种都是独占锁，在并发场景下，性能太低。那么基于redis的分段锁是对用户参与的活动库存上锁，具体是先在redis 中对每个活动参与次数进行预热，每一次的活动参与都会 incr redis 中的活动参与次数，假如活动参与次数大于活动库存就会返回活动库存不足，假如可以参与就将活动 id + incr 得到的参数次数 使用setnx上锁，这就是分段锁。 分段锁的核心是去竞态，避免独占影响系统的响应性能。假如一个活动库存只有 100，有 200 个请求参与，那么就只有 100 个请求能申请到分段锁，其余的将会被拒绝，以防止超发。不能保障奖品都发放完全。 为什么滑块锁，incr 后还要加一个 setNx 集群模式下，incr 可能会出现失败，加 setNx 是一个保障 加锁是兜底，你不知道什么时候会出现 incr 不对的情况。如；集群模式下 incr 失败、出现redis问题，需要恢复库存。如果没有锁，可能会超卖。 setNx 是非独占锁，key 可以不释放，那么就解决了独占锁把握不好释放时机，释放早了流程可能还没有结束，释放晚了可能用户都走了。 分布式锁有哪些实现方式？为什么使用Redis实现分布式锁 基于数据库的实现，使用数据库的事务和唯一约束（如唯一索引或唯一约束）来实现分布式锁。 基于ZooKeeper的实现：使用 ZooKeeper 的临时节点（ephemeral node）来实现分布式锁。 基于Redis的实现：通过 SETNX 命令（或 SET 指令的 NX 选项）尝试设置键值对，成功则表示获取锁，失败则表示锁已被其他客户端持有。 性能高、原子操作、支持过期时间、简单易用 Redis 是单机的吗，如果单机的话 Redis 分布式锁有没有什么问题 redis 高可用问题，单机宕机后，服务就崩溃了 Kafka-消息队列 保证活动库存最终一致性 发送 mq 消息异步更新数据库的库存 项目中哪些地方使用了MQ 保存库存最终一致性；抽奖完成后异步调用发奖流程 说说MQ解耦发奖流程，为什么要解耦 在抽奖流程完成后，实际上已经得到了返回给用户的信息，抽奖成功与否，抽中的奖品是什么等等。而发奖流程的时效性并不强，那么通过解耦，可以将业务流程拆分，降低系统的压力提高用户的体验 MQ 解耦发奖后的具体流程 kafka业务方面其实很少用，主要在大数据方面，存在消息丢失的场景，kafka丢消息的话 不会造成客诉吗？ 使用 xxl-job 设置定时任务，按期去扫发奖单，假如发现发奖单中的发奖状态是未发奖，那么就重新发送 mq 消息去执行发奖流程，确保奖品发放，提高系统的可靠性。 为什么选择xxl-job执行定时任务，了解其他任务调度组件吗 功能丰富、易于使用、高可靠性、社区活跃 Quartz 是一个功能强大的开源任务调度框架，支持丰富的调度功能和灵活的配置，但需要用户自行处理分布式部署和集群环境下的任务调度问题。 Elastic Job 是一个分布式任务调度框架，提供了灵活的分片策略和强大的扩展性，适用于大规模分布式任务调度场景。 表的设计先介绍业务 后阐述领域 引入表设计；根据领域驱动中对各个模块的定义，设计数据库表，也就对应了活动表、抽奖策略配置表、准入规则引擎表、用户抽奖单记录表、以及配合这些表数据结构运行的其他表，如：记录用户的参与次数等。 系统的瓶颈在哪里瓶颈都包括哪些，一般有 1.1 资源竞争 - 也就是库存的扣减，它是一种独占的资源。所以优化为更细颗粒度的分段锁了。 1.2 业务流程 - 从参与抽奖、规则判断、扣减库存、抽奖算法、记录结果、发送奖品，全部都在一个事务下，那么这个事务会非常大也很耗时，耗时就代表这会更长时间的占用数据库连接资源。所以要用 MQ 进行解耦，避免做过大的事务处理。 1.3 业务数据 - 对于数据库个人行为的数据，包括个人记录数据时的事务，和查询数据。如果都是一个库的一个表，那么对这个库表的压力就会非常大，所以要分库分表处理这种瞬时业务量很高的场景。之后还就是数据量过多对数据库的存储压力问题。 抽奖项目里边 接口间的幂等性是如何保证的数据库里边设置防重字段+唯一索引实现幂等性，保证一个用户参加一次活动只会一次中奖，而且中奖之后也不会多次发奖。 并做了一层 Redis 控制，避免都打到库上带来压力 分布式事务跨库的事务处理，一种是分布式事务，另外一种就是基于MQ+任务调度补偿的方式，完成最终一致性。 那么鉴于抽奖系统的实时性要求，从用户流程体验上，希望更加流畅，支撑更大的并发量，而不是对整个流程添加过多的事务，降低性能。因为事务来说，是一种集中化的竞态，所以这部分设计上采用最终一致性的方式进行处理，而不是直接添加大块的事务。 抽奖系统mq重发的时候是怎么保证幂等性抽奖系统mq重发的时候是怎么保证幂等性的，因为可能上条消息实际被消费了，但由于某些原因导致了重发，导致实际消费了2遍 最终重要的，一般对于金融、订单、支付等场景，必须使用数据库防重字段做强一致性拦截处理，避免重复消费造成资损和客诉。 部署情况 qps、rt、并发数 两台机器配置为 4核8G，接口的响应时间 rt 为 200ms，每秒可以扛住 300 个请求，qps=300/0.2=1500, pv=10w QPS=100000x0.8/（86400*0.2）=80000/17280=5 Rt = 200ms Kafkakafka 是什么分布式流处理平台，常用于企业级消息引擎 kafka 高性能原理使用 page cache，不依赖 jvm 内存，如果使用 jvm 内存，会增加 gc 负担，增加延迟，创建对象的开销也比较高。 kafka的架构高吞吐量、低延迟、持久性、可扩展性和容错性等特点 Kafka 的架构主要由以下几个核心组件组成： Producer（生产者） : 产生消息的一方。 Consumer（消费者） : 消费消息的一方。 Broker（代理） : 可以看作是一个独立的 Kafka 实例。多个 Kafka Broker 组成一个 Kafka Cluster。 Topic（主题） : Producer 将消息发送到特定的主题，Consumer 通过订阅特定的 Topic(主题) 来消费消息。 Partition（分区） : Partition 属于 Topic 的一部分。一个 Topic 可以有多个 Partition ，并且同一 Topic 下的 Partition 可以分布在不同的 Broker 。 Zookeeper：Kafka 使用 Zookeeper 来管理集群的元数据、进行 Leader 选举和协调 Broker 节点。 AR(Assigned Replicas): AR 是主题被创建后，分区创建时被分配的副本集合，副本个数由副本因子决定。 ISR(In-Sync Replicas): Kafka 中特别重要的概念，指代的是 AR 中那些与 Leader 保持同步的副本集合。在 AR 中的副本可能不在 ISR 中，但 Leader 副本天然就包含在 ISR 中。 在 Kafka 的架构中，Producer 将消息发布到 Topic 中，Broker 存储并处理消息，并且通过 Partition 将 Topic 分割成多个部分，消费者通过 Consumer Group 消费消息。整个架构通过 Zookeeper 进行管理和协调，保证了系统的可靠性和稳定性。 什么是消费组消费者组是 Kafka 提供的可扩展且具有容错性的消费者机制 可扩展： 当生产者向 Topic 写入消息的速度超过了现有消费者的处理速度，此时需要对消费者进行横向伸缩，用多个消费者从同一个主题读取消息，对消息进行分流。同一个分区不能被一个组中的多个 consumer 消费。 容错性 在消费者组中，多个实例共同订阅若干个主题，实现共同消费。当某个实例挂掉的时候，其他实例会自动地承担起它负责消费的分区。 Kafka 的多副本机制了解吗？带来了什么好处？分区（Partition）中的多个副本之间会有一个 leader ，其他副本称为 follower。发送的消息会被发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步。 Kafka 的多分区（Partition）以及多副本（Replica）机制有什么好处呢？ 多分区——一个 topic 可以指定多个 分布在不同 broker 的partition，提高并发能力（负载均衡）。 多副本——提高容灾能力，提高了消息存储的安全性 zookeeper 的作用 注册 broker 注册 topic 负载均衡 位移 offset 的作用在 Kafka 中，每个主题分区下的每条消息都被赋予了一个唯一的 ID 数值，用于标识它在分区中的位置。这个 ID 数值，就被称为位移，或者叫偏移量。一旦消息被写入到分区日志，它的位移值将不能被修改。 保证消费顺序总结一下，对于如何保证 Kafka 中消息消费的顺序，有了下面两种方法： 1 个 Topic 只对应一个 Partition。 （推荐）发送消息的时候指定 key/Partition。 保证消息不丢失生产者丢失消息情况生产者(Producer) 调用send方法发送消息之后，消息可能因为网络问题并没有发送过去。 所以，我们不能默认在调用send方法发送消息之后消息发送成功了。为了确定消息是发送成功，我们要判断消息发送的结果。 回调机制：send()后添加回调函数 123ListenableFuture&lt;SendResult&lt;String, Object&gt;&gt; future = kafkaTemplate.send(topic, o);future.addCallback(result -&gt; logger.info(\"生产者成功发送消息到topic:{} partition:{}的消息\", result.getRecordMetadata().topic(), result.getRecordMetadata().partition()), ex -&gt; logger.error(\"生产者发送消失败，原因：{}\", ex.getMessage())); 重试机制。通过配置重试次数和重试间隔，可以确保消息在发生失败时有机会重新发送 消息确认机制。 acks=0：生产者不会等待任何确认，直接发送下一条消息。 acks=1：生产者会等待 leader 副本确认消息后再发送下一条消息。 acks=all：生产者会等待所有 ISR（In-Sync Replicas，同步副本）确认消息后再发送下一条消息。 消费者丢失消息情况消息在被追加到 Partition(分区)的时候都会分配一个特定的偏移量（offset）。偏移量（offset)表示 Consumer 当前消费到的 Partition(分区)的所在的位置。Kafka 通过偏移量（offset）可以保证消息在分区内的顺序性。 当消费者拉取到了分区的某个消息之后，消费者会自动提交了 offset。自动提交的话会有一个问题，试想一下，当消费者刚拿到这个消息准备进行真正消费的时候，突然挂掉了，消息实际上并没有被消费，但是 offset 却被自动提交了。 解决办法也比较粗暴，我们手动关闭自动提交 offset，每次在真正消费完消息之后再自己手动提交 offset 。 但是，细心的朋友一定会发现，这样会带来消息被重新消费的问题。比如你刚刚消费完消息之后，还没提交 offset，结果自己挂掉了，那么这个消息理论上就会被消费两次。 Kafka Broker 丢失消息情况假如 leader 副本所在的 broker 突然挂掉，那么就要从 follower 副本重新选出一个 leader ，但是 leader 的数据还有一些没有被 follower 副本的同步的话，就会造成消息丢失。 解决办法就是我们设置 acks = all。acks 是 Kafka 生产者(Producer) 很重要的一个参数。当我们配置 acks = all 表示只有所有 ISR 列表的副本全部收到消息时，生产者才会接收到来自服务器的响应。 保证消息不重复消费服务端侧已经消费的数据没有成功提交 offset（根本原因）。 解决方案： 做幂等校验，Redis 的 set，mysql 的主键 拉取到消息即提交：会有消息丢失的风险。允许消息延时的场景，一般会采用这种方式。然后，通过定时任务在业务不繁忙（比如凌晨）的时候做数据兜底。 重试机制Kafka 消费者在默认配置下会进行最多 10 次 的重试，每次重试的时间间隔为 0，即立即进行重试。如果在 10 次重试后仍然无法成功消费消息，则不再进行重试，消息将被视为消费失败。 重试失败后的数据如何再次处理？当达到最大重试次数后，数据会直接被跳过，继续向后进行。当代码修复后，如何重新消费这些重试失败的数据呢？ 死信队列（Dead Letter Queue，简称 DLQ） 是消息中间件中的一种特殊队列。它主要用于处理无法被消费者正确处理的消息，通常是因为消息格式错误、处理失败、消费超时等情况导致的消息被”丢弃”或”死亡”的情况。当消息进入队列后，消费者会尝试处理它。如果处理失败，或者超过一定的重试次数仍无法被成功处理，消息可以发送到死信队列中，而不是被永久性地丢弃。在死信队列中，可以进一步分析、处理这些无法正常消费的消息，以便定位问题、修复错误，并采取适当的措施。 @RetryableTopic 是 Spring Kafka 中的一个注解,它用于配置某个 Topic 支持消息重试，更推荐使用这个注解来完成重试。 1234567891011121314// 重试 5 次，重试间隔 100 毫秒,最大间隔 1 秒@RetryableTopic( attempts = \"5\", backoff = @Backoff(delay = 100, maxDelay = 1000))@KafkaListener(topics = {KafkaConst.TEST_TOPIC}, groupId = \"apple\")private void customer(String message) { log.info(\"kafka customer:{}\", message); Integer n = Integer.parseInt(message); if (n % 5 == 0) { throw new RuntimeException(); } System.out.println(n);} 当达到最大重试次数后，如果仍然无法成功处理消息，消息会被发送到对应的死信队列中。对于死信队列的处理，既可以用 @DltHandler 处理，也可以使用 @KafkaListener 重新消费。 消息积压线上有时因为发送方发送消息速度过快，或者消费方处理消息过慢，可能会导致broker积压大量未消费消息 解决方案：此种情况如果积压了上百万未消费消息需要紧急处理，可以修改消费端程序，让其将收到的消息快速转发到其他topic(可以设置很多分区)，然后再启动多个消费者同时消费新主题的不同分区。如图所示： 由于消息数据格式变动或消费者程序有bug，导致消费者一直消费不成功，也可能导致broker积压大量未消费消息。解决方案：此种情况可以将这些消费不成功的消息转发到其它队列里去(类似死信队列)，后面再慢慢分析死信队列里的消息处理问题。这个死信队列，kafka并没有提供，需要整合第三方插件！ 设置 Kafka 能接收的最大消息的大小？ Broker 端参数: message.max.bytes, Consumer 端参数: fetch.message.max.bytes 监控 Kafka 的框架都有哪些？ JMX 监控: 由于 Kafka 提供的监控指标都是基于 JMX 的 Kafka Manager Kafka Monitor Broker 的 Heap Size 如何设置？任何 Java 进程 JVM 堆大小的设置都需要仔细地进行考量和测试。一个常见的做法是，以默认的初始 JVM 堆大小运行程序，当系统达到稳定状态后，手动触发一次 Full GC，然后通过 JVM 工具查看 GC 后的存活对象大小。之后，将堆大小设置成存活对象总大小的 1.5~2 倍。对于 Kafka 而言，这个方法也是适用的。不过，业界有个最佳实践，那就是将 Broker 的 Heap Size 固定为 6GB。经过很多公司的验证，这个大小是足够且良好的。 Kafka 能手动删除消息吗？Kafka 不需要用户手动删除消息。它本身提供了留存策略，能够自动删除过期消息。当然，它是支持手动删除消息的。 使用 kafka-delete-records 命令，通过将分区 Log Start Offset 值抬高的方式间接删除消息。 __consumer_offsets 是做什么用的？这是一个内部主题，主要用于存储消费者的偏移量，以及消费者的元数据信息 (消费者实例，消费者id 等等) 分区 Leader 选举策略有几种？ OfflinePartition Leader 选举: 每当有分区上线时，就需要执行 Leader 选举。 ReassignPartition Leader 选举: 当你手动运行 kafka-reassign-partitions 命令，或者是调用 Admin 的 alterPartitionReassignments 方法执行分区副本重分配时，可能触发此类选举。 PreferredReplicaPartition Leader 选举: 当你手动运行 kafka-preferred-replica-election 命令，或自动触发了 Preferred Leader 选举时，该类策略被激活 ControlledShutdownPartition Leader 选举: 当 Broker 正常关闭时，该 Broker 上的所有 Leader 副本都会下线，因此，需要为受影响的分区执行相应的 Leader 选举。 从 AR 中挑选首个在 ISR 中的副本，作为新 Leader。 Kafka 的哪些场景中使用了零拷贝（Zero Copy） 基于 mmap 的索引 日志文件读写所用的 TransportLayer Kafka 为什么不支持读写分离？Leader/Follower 模型并没有规定 Follower 副本不可以对外提供读服务。很多框架都是允许这么做的，只是 Kafka 最初为了避免不一致性的问题，而采用了让 Leader 统一提供服务的方式。 不过，自 Kafka 2.4 之后，Kafka 提供了有限度的读写分离，也就是说，Follower 副本能够对外提供读服务。 如何调优 Kafka？ Producer 端:启用压缩，关闭重试 Broker 端：避免 Broker Full GC Consumer: 增加fetch.min.bytes 为什么 redis Pub/Sub 比 kafka 更快一些？二者如何选取Redis是一个内存数据库，其Pub/Sub功能将消息保存在内存中。由于内存访问速度通常远快于磁盘访问速度，因此Redis在处理实时性较高的消息推送时具有优势；Redis的Pub/Sub模型相对简单，使得它在处理发布和订阅操作时的开销较小。 Kafka是一个完整的系统，提供了高吞吐量、分布式的提交日志。它旨在处理大规模数据流，具有强大的持久化能力和容错性。Kafka的分布式架构和分区机制使得它能够在多个消费者之间实现负载均衡，从而提高整体处理能力。 Redis PUB/SUB使用场景： 消息持久性需求不高 吞吐量要求不高 可以忍受数据丢失 数据量不大 Kafka使用场景：(上面以外的其他场景) 高可靠性 高吞吐量 持久性高 多样化的消费处理模型 XXL-JOb核心组件admin：任务管理后台，用于配置和管理任务。 executor：任务执行器，用于执行任务。可以有多个Executor节点，实现任务的分布式执行。 jobcore：任务核心配置，包括任务的执行时间、执行器、调度策略等。 jobhandler：任务处理器，实际执行任务的逻辑。 任务调度原理任务在 admin 配置好后，被分配给 executor 节点，executor 节点根据调度策略和执行时间来执行任务。使用数据库来存储任务信息和日志，确保任务的可靠性 如何进行分片处理开发人员需要实现分片任务的逻辑，并在任务处理器（JobHandler）中指定分片参数。Executor节点会根据分片参数来划分任务，并并行执行。 XXL-Job的定时任务和周期性任务有何区别定时任务是指任务在指定的时间点执行一次，而周期性任务是指任务按照固定的时间间隔反复执行。XXL-Job支持配置定时任务和周期性任务，用户可以根据实际需求选择合适的任务类型。 XXL-Job如何保证任务的高可用性 支持多个Executor节点，实现任务的分布式执行，一台Executor节点故障不会影响整个系统。 支持任务的故障转移，如果某个Executor节点故障，任务可以自动切换到其他可用节点执行。 使用数据库来存储任务信息和执行日志，确保任务的可恢复性。 XXL-Job的任务执行失败了，如何排查和处理？如果XXL-Job的任务执行失败，可以采取以下步骤进行排查和处理： 查看任务执行日志，了解失败原因和异常信息。 检查任务处理器（JobHandler）的实现，确保代码逻辑正确。 检查任务的调度策略和参数配置，是否合理。 查看Executor节点的日志，检查Executor是否正常运行。 如果任务执行超时，可以适当调整任务的超时时间。 如果任务执行异常，可以考虑增加报警规则，及时发现和处理异常任务。 XXL-Job如何处理分片任务的失败和重试？XXL-Job对分片任务的失败和重试提供了支持。如果分片任务的某个分片执行失败，XXL-Job会自动进行重试，直到达到最大重试次数或任务成功执行为止。开发人员可以配置分片任务的最大重试次数和重试策略。 XXL-Job的任务调度器是如何实现的？XXL-Job的任务调度器基于Quartz Scheduler实现。Quartz是一个开源的任务调度框架，提供了丰富的调度功能，XXL-Job在其基础上进行了定制化的扩展。","link":"/2024/03/11/%E9%9D%A2%E7%BB%8F/"},{"title":"Java基础","text":"基础概念JVM vs JDK vs JREJava 虚拟机（JVM）是运行 Java 字节码的虚拟机。 JDK（Java Development Kit），它是功能齐全的 Java SDK，是提供给开发者使用，能够创建和编译 Java 程序的开发套件。 JRE（Java Runtime Environment） 是 Java 运行时环境。它是运行已编译 Java 程序所需的所有内容的集合，主要包括 Java 虚拟机（JVM）、Java 基础类库（Class Library）。 什么是字节码?采用字节码的好处是什么?在 Java 中，JVM 可以理解的代码就叫做字节码（即扩展名为 .class 的文件），它不面向任何特定的处理器，只面向虚拟机。 Java 语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。 为什么说 Java 语言“编译与解释并存”？编译型语言：执行效率高、开发效率低 解释型语言：开发效率高、执行效率低 Java语言需要经过编译(源码经过 javac 编译变成字节码)、解释（字节码经过 解释器和 JIT(即时编译器)变成机器码） AOT 有什么优点？为什么不全部使用 AOT 呢？AOT(ahead of time Compilation)。和 JIT 不同的是，这种编译模式会在程序被执行前就将其编译成机器码，属于静态编译（C、 C++，Rust，Go 等语言就是静态编译） 优点：启动速度、内存占用、打包体积。 缺点：极限处理能力不如 JIT AOT 不能使用反射、动态代理、动态加载，常用框架和库（spring，CGLIB）都需要这些特性质。 基本数据类型基本类型和包装类型的区别？ 基本类型 vs 包装类型 用途： 基本类型：定义常量和局部变量 包装类型：方法参数、对象属性 包装类型可用于泛型，而基本类型不可以。 存储方式： 基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 static 修饰 ）存放在 Java 虚拟机的堆中（类的实例），被static 修饰的存放在 Java 虚拟机的堆中（Class对象）。 包装类型属于对象类型，几乎所有对象实例都存在于堆中。 占用空间：相比于包装类型（对象类型）， 基本数据类型占用的空间往往非常小。 默认值：成员变量包装类型不赋值就是 null ，而基本类型有默认值且不是 null。 比较方式：对于基本数据类型来说，== 比较的是值。对于包装数据类型来说，== 比较的是对象的内存地址。所有整型包装类对象之间值的比较，全部使用 equals() 方法。 为什么说是几乎所有对象实例都存在于堆中呢？这是因为 HotSpot 虚拟机引入了 JIT 优化之后，会对对象进行逃逸分析，如果发现某一个对象并没有逃逸到方法外部，那么就可能通过标量替换来实现栈上分配，而避免堆上分配内存不充部分 基本数据类型是否都存放在栈中？基本数据类型的存储位置取决于它们的作用域和声明方式。如果它们是局部变量，那么它们会存放在栈中；如果它们是成员变量，那么它们会存放在堆中(是否被 static 修饰都是在堆中) 包装类型的缓存机制了解么？作用：提升性能 Byte,Short,Integer,Long 这 4 种包装类默认创建了数值 [-128，127] 的相应类型的缓存数据，Character 创建了数值在 [0,127] 范围的缓存数据，Boolean 直接返回 True or False。 Character 缓存源码: 1234567891011121314public static Character valueOf(char c) { if (c &lt;= 127) { // must cache return CharacterCache.cache[(int)c]; } return new Character(c);}private static class CharacterCache { private CharacterCache(){} static final Character cache[] = new Character[127 + 1]; static { for (int i = 0; i &lt; cache.length; i++) cache[i] = new Character((char)i); }} 如果超出对应范围仍然会去创建新的对象，缓存的范围区间的大小只是在性能和资源之间的权衡。 两种浮点数类型的包装类 Float,Double 并没有实现缓存机制。 自动装箱与拆箱了解吗？原理是什么？什么是自动拆装箱？ 装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 装箱其实就是调用了 包装类的valueOf()方法，拆箱其实就是调用了 xxxValue()方法。 因此， Integer i = 10 等价于 Integer i = Integer.valueOf(10) int n = i 等价于 int n = i.intValue(); 注意：如果频繁拆装箱的话，也会严重影响系统的性能。我们应该尽量避免不必要的拆装箱操作。（常量、局部变量用基本数据类型） 为什么浮点数运算的时候会有精度丢失的风险？与计算机保存浮点数机制有关。表示一个数字时，宽度有限，无线循环的小数存储在计算机时只能被截断，所以导致精度丢失。 浮点型从二进制的视角是怎么存储的？符号位+指数位+尾号位 这32个二进制位的内存编号从高到低 (从31到0), 共包含如下几个部分: sign: 符号位, 即图中蓝色的方块 biased exponent: 偏移后的指数位（偏移量 127 保证指数非负数）, 即图中绿色的方块 fraction: 尾数位, 即图中红色的方块 （IEEE 754）小数位如何计算出来的？如果我们现在想用浮点数表示 0.2，它的结果会是多少呢？ 0.2 转换为二进制数的过程为，不断乘以 2，直到不存在小数为止，在这个计算过程中，得到的整数部分从上到下排列就是二进制的结果。 1234560.2 * 2 = 0.4 -&gt; 00.4 * 2 = 0.8 -&gt; 00.8 * 2 = 1.6 -&gt; 10.6 * 2 = 1.2 -&gt; 10.2 * 2 = 0.4 -&gt; 0（发生循环）... 所以 0.2(D) = 0.00110…(B)。 如何解决浮点数运算的精度丢失问题？BigDecimal 可以实现对浮点数的运算，不会造成精度丢失。通常情况下，大部分需要浮点数精确运算结果的业务场景（比如涉及到钱的场景）都是通过 BigDecimal 来做的。 超过 long 整型的数据应该如何表示？基本数值类型都有一个表达范围，如果超过这个范围就会有数值溢出的风险。 在 Java 中，64 位 long 整型是最大的整数类型。 123long l = Long.MAX_VALUE;System.out.println(l + 1); // -9223372036854775808System.out.println(l + 1 == Long.MIN_VALUE); // true BigInteger 内部使用 int[] 数组来存储任意大小的整形数据。 相对于常规整数类型的运算来说，BigInteger 运算的效率会相对较低。 变量成员变量与局部变量的区别？ 语法形式：成员变量属于类，局部变量属于代码块、方法定义变量和方法的参数；成员变量可以被访问控制修饰符以及static 修饰、局部变量不可以。 存储方式：成员变量存在于堆内存，局部变量则存在于栈内存。 生存时间：从变量在内存中的生存时间上看，成员变量——对象同步，而局部变量——方法同步。 默认值：从变量是否有默认值来看，成员变量如果没有被赋初始值，则会自动以类型的默认值而赋值（一种情况例外:被 final 修饰的成员变量也必须显式地赋值），而局部变量则不会自动赋值。 为什么成员变量有默认值？ 不考虑变量类型。若没有默认值，变量存储的是内存地址对应的任意随机值，即遗留值，可以会出现意外或信息泄露问题 对于编译器 javac 来说，局部变量没赋值很好判断，可以直接报错。而成员变量可能是运行时赋值，无法判断，误报“没默认值”影响用户体验，所以采用自动赋默认值 静态变量有什么作用？所有类的实例共用一份静态变量，节省内存 （若被 private 修饰就不能用类名.变量名访问） 通常情况下，静态变量会被 final 关键字修饰成为常量。 字符型常量和字符串常量的区别?形式 : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。 含义 : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)。 占内存大小：字符常量只占 2 个字节; 字符串常量占若干个字节。 ⚠️ 注意 char 在 Java 中占两个字节。 方法静态方法为什么不能调用非静态成员?在类的非静态成员不存在的时候静态方法就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。 静态方法和实例方法有何不同？ 调用方式： ​ 在外部调用静态方法时，可以使用 类名.方法名 的方式，也可以使用 对象.方法名 的方式，而实例方法只有后面这种方式。也就是说，调用静态方法可以无需创建对象 。 ​ 为了避免混淆建议静态方法使用前一种 访问类成员是否存在限制： ​ 静态方法在访问本类的成员时，只允许访问静态成员（即静态成员变量和静态方法），不允许访问实例成员（即实例成员变量和实例方法），而实例方法不存在这个限制。 重载和重写有什么区别？重载 发生在同一个类中（或者父类和子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同。 重写 重写发生在运行期，是子类对父类的允许访问的方法的实现过程进行重新编写。 方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等，抛出的异常范围小于等于父类（更精细），访问修饰符范围大于等于父类。 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明。 构造方法无法被重写 总结 综上：重写就是子类对父类方法的重新改造，外部样子不能改变，内部逻辑可以改变。 方法的重写要遵循“两同两小一大”（以下内容摘录自《疯狂 Java 讲义》，issue#892open in new window ）： “两同”即方法名相同、形参列表相同； “两小”指的是子类方法返回值类型应比父类方法返回值类型更小或相等，子类方法声明抛出的异常类应比父类方法声明抛出的异常类更小或相等； “一大”指的是子类方法的访问权限应比父类方法的访问权限更大或相等。 ⭐️ 关于 重写的返回值类型 这里需要额外多说明一下，上面的表述不太清晰准确：如果方法的返回类型是 void 和基本数据类型，则返回值重写时不可修改。但是如果方法的返回值是引用类型，重写时是可以返回该引用类型的子类的。 面相对象基础面向对象和面向过程的区别两者的主要区别在于解决问题的方式不同： 面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。 面向对象会先抽象出对象，然后用对象执行方法的方式解决问题。 另外，面向对象开发的程序一般更易维护、易复用、易扩展。 对象的相等和引用相等的区别 对象的相等一般比较的是内存中存放的内容是否相等。 引用相等一般比较的是他们指向的内存地址是否相等。 如果一个类没有声明构造方法，该程序能正确执行吗?构造方法是一种特殊的方法，主要作用是完成对象的初始化工作。 如果一个类没有声明构造方法，也可以执行！因为一个类即使没有声明构造方法也会有默认的不带参数的构造方法。如果我们自己添加了类的构造方法（无论是否有参），Java 就不会添加默认的无参数的构造方法了。 我们一直在不知不觉地使用构造方法，这也是为什么我们在创建对象的时候后面要加一个括号（因为要调用无参的构造方法）。如果我们重载了有参的构造方法，记得都要把无参的构造方法也写出来（无论是否用到），因为这可以帮助我们在创建对象的时候少踩坑。 构造方法有哪些特点？是否可被 override?构造方法特点如下： 名字与类名相同。 没有返回值，但不能用 void 声明构造函数。 生成类的对象时自动执行，无需调用。 构造方法不能被 override（重写）,但是可以 overload（重载）,所以你可以看到一个类中有多个构造函数的情况。 面向对象三大特征封装 封装是指把一个对象的状态信息（也就是属性）隐藏在对象内部，不允许外部对象直接访问对象的内部信息。但是可以提供一些可以被外界访问的方法来操作属性。 继承 继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承，可以快速地创建新的类，可以提高代码的重用，程序的可维护性，节省大量创建新类的时间 ，提高我们的开发效率。 关于继承如下 3 点请记住： 子类拥有父类对象所有的属性和方法（包括私有属性和私有方法），但是父类中的私有属性和方法子类是无法访问，只是拥有。 子类可以拥有自己属性和方法，即子类可以对父类进行扩展。 子类可以用自己的方式实现父类的方法。 多态 多态，顾名思义，表示一个对象具有多种的状态，具体表现为父类的引用指向子类的实例。 多态的特点: 对象类型和引用类型之间具有继承（类）/实现（接口）的关系； 引用类型变量发出的方法调用的到底是哪个类中的方法，必须在程序运行期间才能确定； 多态不能调用“只在子类存在但在父类不存在”的方法； 如果子类重写了父类的方法，真正执行的是子类覆盖的方法，如果子类没有覆盖父类的方法，执行的是父类的方法。 *接口和抽象类有什么共同点和区别？共同点： 都不能被实例化。 都可以包含抽象方法。 都可以有默认实现的方法（Java 8 可以用 default 关键字在接口中定义默认方法）。 区别： 接口主要用于对类的行为进行约束，你实现了某个接口就具有了对应的行为。抽象类主要用于代码复用，强调的是所属关系。 一个类只能继承一个类，但是可以实现多个接口。 接口中的成员变量只能是 public static final 类型的，不能被修改且必须有初始值，而抽象类的成员变量默认 default，可在子类中被重新定义，也可被重新赋值。 深拷贝和浅拷贝区别了解吗？什么是引用拷贝？关于深拷贝和浅拷贝区别，我这里先给结论： 浅拷贝：浅拷贝会在堆上创建一个新的对象（区别于引用拷贝的一点），不过，如果原对象内部的属性是引用类型的话，浅拷贝会直接复制内部对象的引用地址，也就是说拷贝对象和原对象共用同一个内部对象。 深拷贝：深拷贝会完全复制整个对象，包括这个对象所包含的内部对象。 Object创建对象的方式 使用new关键字 1Student s = new Student(); 使用Class类的newInstance方法（反射） 1Student s = Student.class.newInstance(); 使用Constructor类的newInstance方法 12Constructor&lt;Student&gt; constructor = Student.class.getConstructor();Student s = constructor.newInstance(); 这两种newInstance方法就是大家所说的反射。事实上Class的newInstance方法内部调用Constructor的newInstance方法。 使用clone方法 1Student s1 = s.clone(); 使用反序列化 12ObjectInputStream in = new ObjectInputStream(new FileInputStream(\"xxx.obj\"));Student s = in.readObject(); Object 类的常见方法有哪些？1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * native 方法，用于返回当前运行时对象的 Class 对象，使用了 final 关键字修饰，故不允许子类重写。 */public final native Class&lt;?&gt; getClass()/** * native 方法，用于返回对象的哈希码，主要使用在哈希表中，比如 JDK 中的HashMap。 */public native int hashCode()/** * 用于比较 2 个对象的内存地址是否相等，String 类对该方法进行了重写以用于比较字符串的值是否相等。 */public boolean equals(Object obj)/** * native 方法，用于创建并返回当前对象的一份拷贝。 */protected native Object clone() throws CloneNotSupportedException/** * 返回类的名字实例的哈希码的 16 进制的字符串。建议 Object 所有的子类都重写这个方法。 */public String toString()/** * native 方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。 */public final native void notify()/** * native 方法，并且不能重写。跟 notify 一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 */public final native void notifyAll()/** * native方法，并且不能重写。暂停线程的执行。注意：sleep 方法没有释放锁，而 wait 方法释放了锁 ，timeout 是等待时间。 */public final native void wait(long timeout) throws InterruptedException/** * 多了 nanos 参数，这个参数表示额外时间（以纳秒为单位，范围是 0-999999）。 所以超时的时间还需要加上 nanos 纳秒。。 */public final void wait(long timeout, int nanos) throws InterruptedException/** * 跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念 */public final void wait() throws InterruptedException/** * 实例被垃圾回收器回收的时候触发的操作 */protected void finalize() throws Throwable { } Java值传递和引用传递你是怎么理解的？因为 Java 只有值传递，所以，对于 == 来说，不管是比较基本数据类型，还是引用数据类型的变量，其本质比较的都是值，只是引用类型变量存的值是对象的地址。 例子 类A有个方法f，传递参数为Node node，在方法内node = new Node();，这里会影响到外面的node吗？如果在方法内修改node的参数，他会影响到外面的node吗 == 和 equals() 的区别== 对于基本类型和引用类型的作用效果是不同的： 对于基本数据类型来说，== 比较的是值。 对于引用数据类型来说，== 比较的是对象的内存地址。 equals() 不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等。equals()方法存在于Object类中，而Object类是所有类的直接或间接父类，因此所有的类都有equals()方法。 Object 类 equals() 方法： 123public boolean equals(Object obj) { return (this == obj);} equals() 方法存在两种使用情况： 类没有重写 equals()方法：通过equals()比较该类的两个对象时，等价于通过“==”比较这两个对象，使用的默认是 Object类equals()方法。 类重写了 equals()方法：一般我们都重写 equals()方法来比较两个对象中的属性是否相等；若它们的属性相等，则返回 true(即，认为这两个对象相等)。 hashCode() 有什么用？hashCode() 的作用是获取哈希码（int 整数），也称为散列码。这个哈希码的作用是确定该对象在哈希表中的索引位置。 hashCode() 定义在 JDK 的 Object 类中，这就意味着 Java 中的任何类都包含有 hashCode() 函数。另外需要注意的是：Object 的 hashCode() 方法是本地方法，也就是用 C 语言或 C++ 实现的。 ⚠️ 注意：该方法在 Oracle OpenJDK8 中默认是 “使用线程局部状态来实现 Marsaglia’s xor-shift 随机数生成”, 并不是 “地址” 或者 “地址转换而来”, 不同 JDK/VM 可能不同在 Oracle OpenJDK8 中有六种生成方式 (其中第五种是返回地址), 通过添加 VM 参数: -XX:hashCode=4 启用第五种。 为什么要有 hashCode？在 HashMap 和 HashSet 中都需要用到 hashCode，以 HashSet 为例： ​ 当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashCode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashCode 值作比较，如果没有相符的 hashCode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashCode 值的对象，这时会调用 equals() 方法来检查 hashCode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。 其实， hashCode() 和 equals()都是用于比较两个对象是否相等。 那为什么 JDK 还要同时提供这两个方法呢？ 这是因为在一些容器（比如 HashMap、HashSet）中，有了 hashCode() 之后，判断元素是否在对应容器中的效率会更高 那为什么不只提供 hashCode() 方法呢？ 这是因为两个对象的hashCode 值相等并不代表两个对象就相等。 那为什么两个对象有相同的 hashCode 值，它们也不一定是相等的？ 因为 hashCode() 所使用的哈希算法也许刚好会让多个对象传回相同的哈希值。越糟糕的哈希算法越容易碰撞，但这也与数据值域分布的特性有关（所谓哈希碰撞也就是指的是不同的对象得到相同的 hashCode )。 总结下来就是： 如果两个对象的hashCode 值相等，那这两个对象不一定相等（哈希碰撞）。 如果两个对象的hashCode 值相等并且equals()方法也返回 true，我们才认为这两个对象相等。 如果两个对象的hashCode 值不相等，我们就可以直接认为这两个对象不相等。 为什么重写 equals() 时必须重写 hashCode() 方法？因为两个相等的对象的 hashCode 值必须是相等。也就是说如果 equals 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。 如果重写 equals() 时没有重写 hashCode() 方法的话就可能会导致 equals 方法判断是相等的两个对象，hashCode 值却不相等。 思考：重写 equals() 时没有重写 hashCode() 方法的话，使用 HashMap 可能会出现什么问题。 两个相同的对象加到 HashMap 中，对应的 hashCode 不同，但是 HashMap 是先判断 hashCode 是否相同来判断是否有重复 key，最终会导致 HashMap 存在两个相同的对象同时作为 Key，这与 HashMap 的 key 不可以重复相悖。 总结： equals 方法判断两个对象是相等的，那这两个对象的 hashCode 值也要相等。 两个对象有相同的 hashCode 值，他们也不一定是相等的（哈希碰撞）。 概念对象 Object：表示的是某一类事物的抽象的名词和概念，是对一类事物的抽象表示 类 Class：对象在计算机中的表示，如定义一个“人”的类 实例 Instance：根据类创建出来的一个个具体的“对象”，每个对象都拥有相同的方法，但各自的数据可能不同。 Oop：ordinary object point 对象的创建过程 申请空间，给成员变量赋默认值 调用 init 构造函数，给成员变量赋值 建立引用和对象的连接 单例模式饿汉式12345678910/** * 饿汉式单例模式 */public class singletonPattern01 { private static final singletonPattern01 SINGLE = new singletonPattern01(); private singletonPattern01(){}; public static singletonPattern01 getSingle(){ return SINGLE; }} 优点：这种写法比较简单，就是在类加载的时候就完成实例化。避免了线程同步问题。 缺点：在类加载的时候就完成实例化，没有达到Lazy Loading的效果。如果从未使用过这个实例，则会造成内存的浪费。 懒汉式起到了Lazy Loading的效果，但是只能在单线程下使用。 如果在多线程下，一个线程进入了if (singleton == null)判断语句块，还未来得及往下执行，另一个线程也通过了这个判断语句，这时便会产生多个实例。所以 在多线程环境下不可使用这种方式。 结论：在实际开发中，不要使用这种方式. 1234567891011121314 * 懒汉式单例模式 */public class singletonPattern02 { private static singletonPattern02 SINGLE; private singletonPattern02(){ }; public static singletonPattern02 getInstance(){ if(SINGLE == null){ SINGLE = new singletonPattern02(); } return SINGLE; }} 双重检查锁（DCL double check lock）双重检查概念是多线程开发中常使用到的，如代码中所示，我们进行了两次if (singleton == null)检查，这样就可以保证线程安全了。 这样，实例化代码只会执行一次，后面再次访问时，判断if (singleton == null)，直接return实例化对象，也避免的反复进行方法同步. 线程安全；延迟加载；效率较高 结论：在实际开发中，推荐使用这种单例设计模式 12345678910111213141516171819202122public class singletonPattern02 { private static volatile singletonPattern02 SINGLE; private singletonPattern02(){ }; public static singletonPattern02 getInstance(){ if(SINGLE == null){ synchronized (singletonPattern02.class){ if (SINGLE == null){ try{ Thread.sleep(1L); }catch (InterruptedException e) { e.printStackTrace(); } SINGLE = new singletonPattern02(); } } } return SINGLE; }} DCL要不要加 volatile 要。 假如 new singletonPattern02()时候发生指令重排序，先建立了连接，那么 SINGLE！=null ，多线程时候另一个线程就会直接 返回半初始化的对象。 所以说，这段代码要不要加volatile？必须加！加了volatile的这块内存，对于它的读写访问不可以重排序！ https://blog.csdn.net/zhaoyajie1011/article/details/106812327 静态内部类形式这种方式采用了类加载的机制来保证初始化实例时只有一个线程。 静态内部类方式在Singleton类被加载时并不会立即实例化，而是在需要实例化时，调用getSingleTon方法，才会加载Inner类，从而完成Singleton的实例化。 类的静态属性只会在第一次加载类的时候初始化，所以在这里，JVM帮助我们保证了线程的安全性，在类进行初始化时，别的线程是无法进入的。 优点：避免了线程不安全，利用静态内部类特点实现延迟加载，效率高 结论：推荐使用. 1234567891011public class singletonPattern03 { private singletonPattern03(){}; private static class singletonHolder{ private static singletonPattern03 SINGLE = new singletonPattern03(); } public static singletonPattern03 getInstance(){ return singletonHolder.SINGLE; }} java中的引用类型的对象存放在哪里根据上下文来确定。 123456789void func(){ Object obj = new Object();//这个obj在函数的栈里。}class Test{ private Object obj = new Object();//这个obj随对应的Test对象分配在堆里} 对于方法中的局部变量的引用时存放在java运行时数据区的栈中 对于实例变量则是存放在java运行时数据区的堆中。 Class 实例究竟在 method area 还是在 heaphotspot使用了 OOP-KLASS 模型来表示 java 对象 main方法中：Object o = new Object(); jvm在加载class时，创建instanceKlass，表示其元数据，包括常量池、字段、方法等，存放在方法区；instanceKlass是jvm中的数据结构；（vm加载的字节码，也就是.class文件，被加载到方法区里面，叫Klass，是一个C++对象，含有类的信息、虚方法表等。） 在new一个对象时，jvm创建instanceOopDesc，来表示这个对象，存放在堆区，其引用，存放在栈区；它用来表示对象的实例信息，看起来像个指针实际上是藏在指针里的对象；instanceOopDesc对应java中的对象实例； HotSpot并不把instanceKlass暴露给Java，而会另外创建对应的instanceOopDesc来表示java.lang.Class对象，并将后者称为前者的“Java镜像”，klass持有指向oop引用(_java_mirror便是该instanceKlass对Class对象的引用)； 要注意，new操作返回的instanceOopDesc类型指针指向instanceKlass，而instanceKlass指向了对应的类型的Class实例的instanceOopDesc；有点绕，简单说，就是Person实例——&gt;Person的instanceKlass——&gt;Person的Class。 对象在内存中的存储布局instanceOopDesc，只包含数据信息，它包含三部分： Mark Word，主要存储对象运行时记录信息，如hashcode, GC分代年龄，锁状态标志，线程ID，时间戳等; （64 位 os 就是 64 位即 8 字节） 元数据指针，即指向方法区的instanceKlass实例（压缩钱 8 字节，压缩后 4 字节） 实例数据; （成员变量） 另外，如果是数组对象，还多了一个数组长度 对象如何定位直接使用直接指针访问，Java堆中对象的内存布局就必须考虑如何防止访问类型数据的相关信息，reference中存储的直接就是对象地址，如果只是访问对象本身的话，就不需要多一次间接访问的开销。 间接（句柄访问）使用句柄访问，Java堆中将可能会划分出一块内存用来作为句柄池，reference中寸的就是对象的句柄地址，而句柄中包含了对象实例数据与类型数据各自具体的地址信息。 两种访问方式的优势句柄访问： 最大的好处是reference中存储的是稳定句柄地址，在对象被移动（垃圾收集时移动对象是非常普遍的行为）时只会改变句柄中的实例数据指针，而reference本身不需要被修改。指针访问：最大的好处时速度更快，它节省了一次指针定位的时间开销，由于对象的访问在Java中非常频繁，因此这类开销积少成多也是一项极为可观的执行成本。 对象怎么分配 判断分配到栈上。 逃逸分析。没有发生逃逸的对象优先尝试在栈上分配 是否大。 jvm 调优 TLAB（Thread local allocation buffer）。线程本地分配缓存 多线程时，给各线程分配特定的空间 为什么 hotspot 不实用 c++对象来代表 java 对象c++对象有虚函数表，java 对象的虚函数表在 Class 对象中 StringString、StringBuffer、StringBuilder 的区别？可变性 String 是不可变的（后面会详细分析原因）。 StringBuilder 与 StringBuffer 都继承自 AbstractStringBuilder 类，在 AbstractStringBuilder 中也是使用字符数组保存字符串，不过没有使用 final 和 private 关键字修饰，最关键的是这个 AbstractStringBuilder 类还提供了很多修改字符串的方法比如 append 方法。 线程安全性 String 中的对象是不可变的，也就可以理解为常量，线程安全。AbstractStringBuilder 是 StringBuilder 与 StringBuffer 的公共父类，定义了一些字符串的基本操作，如 expandCapacity、append、insert、indexOf 等公共方法。StringBuffer 对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder 并没有对方法进行加同步锁，所以是非线程安全的。 性能 每次对 String 类型进行改变的时候，都会生成一个新的 String 对象，然后将指针指向新的 String 对象。StringBuffer 每次都会对 StringBuffer 对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用 StringBuilder 相比使用 StringBuffer 仅能获得 10%~15% 左右的性能提升，但却要冒多线程不安全的风险。 对于三者使用的总结： 操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer String 为什么是不可变的?被 final 关键字修饰的类不能被继承，修饰的方法不能被重写，修饰的变量是基本数据类型则值不能改变，修饰的变量是引用类型则不能再指向其他对象。因此，final 关键字修饰的数组保存字符串并不是 String不可变的根本原因，因为这个数组保存的字符串是可变的（final 修饰引用类型变量的情况）。 String 真正不可变有下面几点原因： 保存字符串的数组被 final 修饰且为私有的，并且String 类没有提供/暴露修改这个字符串的方法。 String 类被 final 修饰导致其不能被继承，进而避免了子类破坏 String 不可变。 不可变的好处 这个最简单地原因，就是为了安全。 再看下面这个HashSet用StringBuilder做元素的场景，问题就更严重了，而且更隐蔽。 123456789101112131415class Test{ public static void main(String[] args){ HashSet&lt;StringBuilder&gt; hs=new HashSet&lt;StringBuilder&gt;(); StringBuilder sb1=new StringBuilder(\"aaa\"); StringBuilder sb2=new StringBuilder(\"aaabbb\"); hs.add(sb1); hs.add(sb2); //这时候HashSet里是{\"aaa\",\"aaabbb\"} StringBuilder sb3=sb1; sb3.append(\"bbb\"); //这时候HashSet里是{\"aaabbb\",\"aaabbb\"} System.out.println(hs); }}//Output://[aaabbb, aaabbb] StringBuilder型变量sb1和sb2分别指向了堆内的字面量“aaa”和”aaabbb”。把他们都插入一个HashSet。到这一步没问题。但如果后面我把变量sb3也指向sb1的地址，再改变sb3的值，因为StringBuilder没有不可变性的保护，sb3直接在原先”aaa”的地址上改。导致sb1的值也变了。这时候，HashSet上就出现了两个相等的键值”aaabbb”。破坏了HashSet键值的唯一性。所以千万不要用可变类型做HashMap和HashSet键值。 字符串拼接用“+” 还是 StringBuilder?Java 语言本身并不支持运算符重载，“+”和“+=”是专门为 String 类重载过的运算符，也是 Java 中仅有的两个重载过的运算符。 字符串对象通过“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 不过，在循环内使用“+”进行字符串的拼接的话，存在比较明显的缺陷：编译器不会创建单个 StringBuilder 以复用，会导致创建过多的 StringBuilder 对象。 不过，使用 “+” 进行字符串拼接会产生大量的临时对象的问题在 JDK9 中得到了解决。在 JDK9 当中，字符串相加 “+” 改为了用动态方法 makeConcatWithConstants() 来实现，而不是大量的 StringBuilder 了。 String s1 = new String(“abc”);这句话创建了几个字符串对象？会创建 1 或 2 个字符串对象。 1、如果字符串常量池中不存在字符串对象“abc”的引用，那么它会在堆上创建两个字符串对象，其中一个字符串对象的引用会被保存在字符串常量池中。 示例代码（JDK 1.8）： 1String s1 = new String(\"abc\"); 对应的字节码： ldc 命令用于判断字符串常量池中是否保存了对应的字符串对象的引用，如果保存了的话直接返回，如果没有保存的话，会在堆中创建对应的字符串对象并将该字符串对象的引用保存到字符串常量池中。 2、如果字符串常量池中已存在字符串对象“abc”的引用，则只会在堆中创建 1 个字符串对象“abc”。 示例代码（JDK 1.8）： 1234// 字符串常量池中已存在字符串对象“abc”的引用String s1 = \"abc\";// 下面这段代码只会在堆中创建 1 个字符串对象“abc”String s2 = new String(\"abc\"); 对应的字节码： 这里就不对上面的字节码进行详细注释了，7 这个位置的 ldc 命令不会在堆中创建新的字符串对象“abc”，这是因为 0 这个位置已经执行了一次 ldc 命令，已经在堆中创建过一次字符串对象“abc”了。7 这个位置执行 ldc 命令会直接返回字符串常量池中字符串对象“abc”对应的引用。 *String#intern 方法有什么作用?String.intern() 是一个 native（本地）方法，其作用是将指定的字符串对象的引用保存在字符串常量池中，可以简单分为两种情况： 如果字符串常量池中保存了对应的字符串对象的引用，就直接返回该引用。 如果字符串常量池中没有保存了对应的字符串对象的引用，那就在常量池中创建一个指向该字符串对象的引用并返回。 应用场景使用方法 1234567public class Person{String name; public void setName(String paramString) { String str = paramString.intern(); }} 这里是一个能展现出inern()实际作用的场景,首先假设我从数据库里读了一个人的信息出来,然后把这个人的名字赋值给这个Person对象.那么,从数据库读数据,毫无疑问得创建一个字符串对象出来,假定读了10个人的数据,其中三个都叫小明,那么在不使用intern()的情况下,对字符串对象的引用情况如图所示 在使用intern的情况下,对字符串对象的引用情况如图所示 很显然,剩下的那两个小明字符串对象,就都可以回收了,大大节省空间. String 类型的变量和常量做“+”运算时发生了什么？ 先来看字符串不加 final 关键字拼接的情况（JDK1.8）： 12345678String str1 = \"str\";String str2 = \"ing\";String str3 = \"str\" + \"ing\";String str4 = str1 + str2;String str5 = \"string\";System.out.println(str3 == str4);//falseSystem.out.println(str3 == str5);//trueSystem.out.println(str4 == str5);//false 在编译过程中，Javac 编译器（下文中统称为编译器）会进行一个叫做 常量折叠(Constant Folding) 的代码优化。 常量折叠会把常量表达式的值求出来作为常量嵌在最终生成的代码中，这是 Javac 编译器会对源代码做的极少量优化措施之一(代码优化几乎都在即时编译器中进行)。 对于 String str3 = \"str\" + \"ing\"; 编译器会给你优化成 String str3 = \"string\"; 。 并不是所有的常量都会进行折叠，只有编译器在程序编译期就可以确定值的常量才可以： 基本数据类型( byte、boolean、short、char、int、float、long、double)以及字符串常量。 final 修饰的基本数据类型和字符串变量 字符串通过 “+”拼接得到的字符串、基本数据类型之间算数运算（加减乘除）、基本数据类型的位运算（&lt;&lt;、&gt;&gt;、&gt;&gt;&gt; ） 引用的值在程序编译期是无法确定的，编译器无法对其进行优化。 对象引用和“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 我们在平时写代码的时候，尽量避免多个字符串对象拼接，因为这样会重新创建对象。如果需要改变字符串的话，可以使用 StringBuilder 或者 StringBuffer。 不过，字符串使用 final 关键字声明之后，可以让编译器当做常量来处理。 示例代码： 123456final String str1 = \"str\";final String str2 = \"ing\";// 下面两个表达式其实是等价的String c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 常量池中的对象System.out.println(c == d);// true 被 final 关键字修饰之后的 String 会被编译器当做常量来处理，编译器在程序编译期就可以确定它的值，其效果就相当于访问常量。 如果 ，编译器在运行时才能知道其确切值的话，就无法对其优化。 示例代码（str2 在运行时才能确定其值）： 12345678final String str1 = \"str\";final String str2 = getStr();String c = \"str\" + \"ing\";// 常量池中的对象String d = str1 + str2; // 在堆上创建的新的对象System.out.println(c == d);// falsepublic static String getStr() { return \"ing\";} 异常Exception 和 Error 有什么区别？在 Java 中，所有的异常都有一个共同的祖先 java.lang 包中的 Throwable 类。Throwable 类有两个重要的子类: Exception :程序本身可以处理的异常，可以通过 catch 来进行捕获。Exception 又可以分为 Checked Exception (受检查异常，必须处理) 和 Unchecked Exception (不受检查异常，可以不处理)。 Error：Error 属于程序无法处理的错误 ，我们不建议通过catch捕获 。例如 Java 虚拟机运行错误（Virtual MachineError）、虚拟机内存不够错误(OutOfMemoryError)、类定义错误（NoClassDefFoundError）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止 Checked Exception 和 Unchecked Exception 有什么区别？Checked Exception 即 受检查异常 ，Java 代码在编译过程中，如果受检查异常没有被 catch或者throws关键字处理的话，就没办法通过编译。 除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常 。常见的受检查异常有：IO 相关的异常、ClassNotFoundException、SQLException…。 Unchecked Exception 即 不受检查异常 ，Java 代码在编译过程中 ，我们即使不处理不受检查异常也可以正常通过编译。 RuntimeException 及其子类都统称为非受检查异常，常见的有（建议记下来，日常开发中会经常用到）： NullPointerException(空指针错误) IllegalArgumentException(参数错误比如方法入参类型错误) NumberFormatException（字符串转换为数字格式错误，IllegalArgumentException的子类） ArrayIndexOutOfBoundsException（数组越界错误） ClassCastException（类型转换错误） ArithmeticException（算术错误） SecurityException （安全错误比如权限不够） UnsupportedOperationException(不支持的操作错误比如重复创建同一用户) Throwable 类常用方法有哪些？ String getMessage(): 返回异常发生时的简要描述 String toString(): 返回异常发生时的详细信息 String getLocalizedMessage(): 返回异常对象的本地化信息。使用 Throwable 的子类覆盖这个方法，可以生成本地化信息。如果子类没有覆盖该方法，则该方法返回的信息与 getMessage()返回的结果相同 void printStackTrace(): 在控制台上打印 Throwable 对象封装的异常信息 try-catch-finally 如何使用？ try块：用于捕获异常。其后可接零个或多个 catch 块，如果没有 catch 块，则必须跟一个 finally 块。 catch块：用于处理 try 捕获到的异常。 finally 块：无论是否捕获或处理异常，finally 块里的语句都会被执行。当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。 注意：不要在 finally 语句块中使用 return! 当 try 语句和 finally 语句中都有 return 语句时，try 语句块中的 return 语句会被忽略。这是因为 try 语句中的 return 返回值会先被暂存在一个本地变量中，当执行到 finally 语句中的 return 之后，这个本地变量的值就变为了 finally 语句中的 return 返回值。 123456789101112131415public static void main(String[] args) { System.out.println(f(2));}public static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } }}输出0 finally 中的代码一定会执行吗？不一定的！在某些情况下，finally 中的代码不会被执行。 finally 之前虚拟机被终止运行的话，finally 中的代码就不会被执行。 程序所在的线程死亡。 关闭 CPU。 如何使用 try-with-resources 代替try-catch-finally？适用范围（资源的定义）： 任何实现 java.lang.AutoCloseable或者 java.io.Closeable 的对象 关闭资源和 finally 块的执行顺序： 在 try-with-resources 语句中，任何 catch 或 finally 块在声明的资源关闭后运行 Java 中类似于InputStream、OutputStream、Scanner、PrintWriter等的资源都需要我们调用close()方法来手动关闭，一般情况下我们都是通过try-catch-finally语句来实现这个需求，如下： 1234567891011121314//读取文本文件的内容Scanner scanner = null;try { scanner = new Scanner(new File(\"D://read.txt\")); while (scanner.hasNext()) { System.out.println(scanner.nextLine()); }} catch (FileNotFoundException e) { e.printStackTrace();} finally { if (scanner != null) { scanner.close(); }} 使用 Java 7 之后的 try-with-resources 语句改造上面的代码: 1234567try (Scanner scanner = new Scanner(new File(\"test.txt\"))) { while (scanner.hasNext()) { System.out.println(scanner.nextLine()); }} catch (FileNotFoundException fnfe) { fnfe.printStackTrace();}a 当然多个资源需要关闭的时候，使用 try-with-resources 实现起来也非常简单，如果你还是用try-catch-finally可能会带来很多问题。 通过使用分号分隔，可以在try-with-resources块中声明多个资源。 12345678910try (BufferedInputStream bin = new BufferedInputStream(new FileInputStream(new File(\"test.txt\"))); BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(new File(\"out.txt\")))) { int b; while ((b = bin.read()) != -1) { bout.write(b); }}catch (IOException e) { e.printStackTrace();} 异常使用有哪些需要注意的地方？ 不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。 抛出的异常信息一定要有意义。 建议抛出更加具体的异常比如字符串转换为数字格式错误的时候应该抛出NumberFormatException而不是其父类IllegalArgumentException。 使用日志打印异常之后就不要再抛出异常了（两者不要同时存在一段代码逻辑中）。 …… 泛型什么是泛型？泛型：是一种把明确类型的工作推迟到创建对象或者调用方法的时候才去明确的特殊的类型。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，而这种参数类型可以用在类、方法和接口中，分别被称为泛型类、泛型方法、泛型接口。注意:一般在创建对象时，将未知的类型确定具体的类型。当没有指定泛型时，默认类型为Object类型。 有什么作用？ 避免了类型强转的麻烦。 它提供了编译期的类型安全，确保在泛型类型（通常为泛型集合）上只能使用正确类型的对象，避免了在运行时出现ClassCastException。 泛型的使用方式有哪几种？ 泛型类： 泛型类型用于类的定义中，被称为泛型类。最典型的就是各种集合框架容器类，如：List、Set、Map。 123456789101112131415161718192021222324泛型类的定义格式：修饰符 class 类名&lt;代表泛型的变量&gt; { }怕你不清楚怎么使用，这里我还是做了一个简单的泛型类：/** * @param &lt;T&gt; 这里解释下&lt;T&gt;中的T: * 此处的T可以随便写为任意标识，常见的有T、E等形式的参数表示泛型 * 泛型在定义的时候不具体，使用的时候才变得具体。 * 在使用的时候确定泛型的具体数据类型。即在创建对象的时候确定泛型。 */public class Generic&lt;T&gt;{ private T key; public Generic(T key) { this.key = key; } public T getKey(){ return key; }}泛型在定义的时候不具体，使用的时候才变得具体。在使用的时候确定泛型的具体数据类型。即：在创建对象的时候确定泛型。 泛型接口： 12345定义格式修饰符 interface接口名&lt;代表泛型的变量&gt; { }public interface Generator&lt;T&gt; { public T method();} 实现泛型接口，不指定类型： 123456class GeneratorImpl&lt;T&gt; implements Generator&lt;T&gt;{ @Override public T method() { return null; }} 实现泛型接口，指定类型： 123456class GeneratorImpl&lt;T&gt; implements Generator&lt;String&gt;{ @Override public String method() { return \"hello\"; }} 泛型方法： 123456789101112131415161718192021222324定义格式：修饰符 &lt;代表泛型的变量&gt; 返回值类型 方法名(参数){ }例如：/** * * @param t 传入泛型的参数 * @param &lt;T&gt; 泛型的类型 * @return T 返回值为T类型 * 说明： * 1）public 与 返回值中间&lt;T&gt;非常重要，可以理解为声明此方法为泛型方法。 * 2）只有声明了&lt;T&gt;的方法才是泛型方法，泛型类中的使用了泛型的成员方法并不是泛型方法。 * 3）&lt;T&gt;表明该方法将使用泛型类型T，此时才可以在方法中使用泛型类型T。 * 4）与泛型类的定义一样，此处T可以随便写为任意标识，常见的如T、E等形式的参数常用于表示泛型。 */ public &lt;T&gt; T genercMethod(T t){ System.out.println(t.getClass()); System.out.println(t); return t;}fanxing fanxing = new fanxing();String string = fanxing.genercMethod(\"string\");Integer integer = fanxing.genercMethod(123); 泛型通配符 当使用泛型类或者接口时，传递的数据中，泛型类型不确定，可以通过通配符&lt;?&gt;表示。但是一旦使用泛型的通配符后，只能使用Object类中的共性方法，集合中元素自身方法无法使用。 12345678static void tongpeifu(List&lt;?&gt; list){ //只能用 Object 修饰，所以也只能用 Object 自带的方法 Object o = list.get(0);}public static void main(String[] args) { tongpeifu(Arrays.asList(\"1111\"));} 通配符基本使用 泛型的通配符:不知道使用什么类型来接收的时候,此时可以使用?,?表示未知通配符。 此时只能接受数据,不能往该集合中存储数据。 12345678// ？代表可以接收任意类型// 泛型不存在继承、多态关系,泛型左右两边要一样。jdk1.7后右边的泛型可以省略//ArrayList&lt;Object&gt; list = new ArrayList&lt;String&gt;();这种是错误的//泛型通配符?:左边写&lt;?&gt; 右边的泛型可以是任意类型ArrayList&lt;?&gt; list = new ArrayList&lt;String&gt;();//编译错误 不可以存储数据list.add(\"1111\"); 泛型通配符?主要应用在参数传递方面 12345678public static void main(String[] args) { ArrayList&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); test(list1); ArrayList&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); test(list2);}public static void test(ArrayList&lt;?&gt; coll){} 通配符高级使用 之前设置泛型的时候，实际上是可以任意设置的，只要是类就可以设置。但是在JAVA的泛型中可以指定一个泛型的上限和下限。 泛型的上限： 格式： 类型名称 &lt;? extends 类 &gt; 对象名称 意义： 只能接收该类型及其子类 泛型的下限： 格式： 类型名称 &lt;? super 类 &gt; 对象名称 意义： 只能接收该类型及其父类型 123456789101112131415161718192021public static void main(String[] args) { Collection&lt;Integer&gt; list1 = new ArrayList&lt;Integer&gt;(); Collection&lt;String&gt; list2 = new ArrayList&lt;String&gt;(); Collection&lt;Number&gt; list3 = new ArrayList&lt;Number&gt;(); Collection&lt;Object&gt; list4 = new ArrayList&lt;Object&gt;(); getElement1(list1); getElement1(list2);//报错 getElement1(list3); getElement1(list4);//报错 getElement2(list1);//报错 getElement2(list2);//报错 getElement2(list3); getElement2(list4); }// 泛型的上限：此时的泛型?，必须是Number类型或者Number类型的子类public static void getElement1(Collection&lt;? extends Number&gt; coll){}// 泛型的下限：此时的泛型?，必须是Number类型或者Number类型的父类public static void getElement2(Collection&lt;? super Number&gt; coll){} 反射什么是反射反射在程序运行期间动态获取类和操纵类的一种技术。通过反射机制，可以在运行时动态地创建对象、调用方法、访问和修改属性，以及获取类的信息。 反射有什么作用让 java 代码更灵活 运行时获得类的属于、方法 运行时构造任意类对象 运行时调用任意对象方法 反射的应用场景了解么？ jdk 动态代理 注解 jdbc 链接 反射机制的优缺点优点：可以让咱们的代码更加灵活、为各种框架提供开箱即用的功能提供了便利 缺点：让我们在运行时有了分析操作类的能力，这同样也增加了安全问题。比如可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时）。另外，反射的性能也要稍差点（编译器无法优化，无法使用 JIT），不过，对于框架来说实际是影响不大的。 反射底层原理java 在编译后会生成一个 class 文件，执行反射的时候会将class 文件载入到方法区并在堆中构建一个 class 对象作为方法区Klass 的访问入口，反射通过Class 对象找到其类中的方法和属性等 流程： 1、当我们编写完一个Java项目之后，每个java文件都会被编译成一个.class文件。 2、这些class文件在程序运行时会被ClassLoader加载到JVM中，当一个类被加载以后，JVM就会在内存中自动产生一个Class对象。 3、通过Class对象获取Field/Method/Construcor 要想通过反射获取一个类的信息，首先要获取该类对应的Class类实例（Class 对象）。Class类没有公共的构造方法，Class类对象是在二进制字节流被JVM加载时，通过调用类加载器的defineClass()方法来构建的。 获取 Class 对象的四种方式12345678910111213//1. 知道具体类的情况下可以使用：Class alunbarClass = TargetObject.class;//2. 通过 Class.forName()传入类的全路径获取：Class alunbarClass1 = Class.forName(\"cn.javaguide.TargetObject\");//3. 通过对象实例instance.getClass()获取：TargetObject o = new TargetObject();Class alunbarClass2 = o.getClass();//4. 通过类加载器xxxClassLoader.loadClass()传入类路径获取:ClassLoader.getSystemClassLoader().loadClass(\"cn.javaguide.TargetObject\");//通过类加载器获取 Class 对象不会进行初始化，意味着不进行包括初始化等一系列步骤，静态代码块和静态对象不会得到执行 反射的一些基本操作 创建一个我们要使用反射操作的类 TargetObject。 123456789101112131415public class TargetObject { private String value; public TargetObject() { value = \"JavaGuide\"; } public void publicMethod(String s) { System.out.println(\"I love \" + s); } private void privateMethod() { System.out.println(\"value is \" + value); }} 使用反射操作这个类的方法以及参数 1234567891011121314151617181920212223242526272829import java.lang.reflect.Field;import java.lang.reflect.InvocationTargetException;import java.lang.reflect.Method;public class test_targetObject { public static void main(String[] args) throws ClassNotFoundException, InstantiationException, IllegalAccessException, NoSuchMethodException, InvocationTargetException, NoSuchFieldException { Class&lt;?&gt; targetObject = Class.forName(\"TargetObject\"); Object o = targetObject.newInstance(); for (Method declaredMethod : targetObject.getDeclaredMethods()) { System.out.println(declaredMethod); } Method publicMethod = targetObject.getDeclaredMethod(\"publicMethod\", String.class); publicMethod.invoke(o, \"fml\"); Field value = targetObject.getDeclaredField(\"value\"); //为了调用private方法我们取消安全检查 value.setAccessible(true); value.set(o, \"fmlzuibang\"); /** * 调用 private 方法 */ Method privateMethod = targetObject.getDeclaredMethod(\"privateMethod\"); privateMethod.setAccessible(true); privateMethod.invoke(o); }} 注解何谓注解Annotation （注解） 是 Java5 开始引入的新特性，可以看作是一种特殊的注释，主要用于修饰类、方法或者变量，提供某些信息供程序在编译或者运行时使用。 注解本质是一个继承了Annotation 的特殊接口： 123456789@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override {}public interface Override extends Annotation{} JDK 提供了很多内置的注解（比如 @Override、@Deprecated），同时，我们还可以自定义注解。 注解的解析方法有哪几种？注解只有被解析之后才会生效，常见的解析方法有两种： 编译期直接扫描：编译器在编译 Java 代码的时候扫描对应的注解并处理，比如某个方法使用@Override 注解，编译器在编译的时候就会检测当前的方法是否重写了父类对应的方法。 运行期通过反射处理：像框架中自带的注解(比如 Spring 框架的 @Value、@Component)都是通过反射来进行处理的。 SPI何谓 SPI?SPI 即 Service Provider Interface ，字面意思就是：“服务提供者的接口”。一种服务发现机制，允许在运行时动态地加载实现特定接口的类，而不需要在代码中显式地指定该类，从而实现解耦和灵活性。 SPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。 SPI 和 API 有什么区别？说到 SPI 就不得不说一下 API 了，从广义上来说它们都属于接口，而且很容易混淆。下面先用一张图说明一下： 一般模块之间都是通过接口进行通讯，那我们在服务调用方和服务实现方（也称服务提供者）之间引入一个“接口”。 当实现方提供了接口和实现，我们可以通过调用实现方的接口从而拥有实现方给我们提供的能力，这就是 API ，这种接口和实现都是放在实现方的。 当接口存在于调用方这边时，就是 SPI ，由接口调用方确定接口规则，然后由不同的厂商去根据这个规则对这个接口进行实现，从而提供服务。 Java SPI 的优缺点？优点： 松耦合度：在运行时动态加载实现类，而无需在编译时将实现类硬编码到代码中 扩展性：可以为同一个接口定义多个实现类。这使得应用程序更容易扩展和适应变化。 缺点： 安全性不足：SPI提供者必须将其实现类名称写入到配置文件中，因此如果未正确配置，则可能存在安全风险。 性能损失：每次查找服务提供者都需要重新读取配置文件，这可能会增加启动时间和内存开销。 上面对Java SPI的缺点说了一下，我们来说一下：Spring的SPI机制相对于Java原生的SPI机制进行了改造和扩展，主要体现在以下几个方面： 支持多个实现类：Spring的SPI机制允许为同一个接口定义多个实现类，而Java原生的SPI机制只支持单个实现类。这使得在应用程序中使用Spring的SPI机制更加灵活和可扩展。 支持自动装配：Spring的SPI机制支持自动装配，可以通过将实现类标记为Spring组件（例如@Component），从而实现自动装配和依赖注入。这在一定程度上简化了应用程序中服务提供者的配置和管理。 支持动态替换：Spring的SPI机制支持动态替换服务提供者，可以通过修改配置文件或者其他方式来切换服务提供者。而Java原生的SPI机制只能在启动时加载一次服务提供者，并且无法在运行时动态替换。 提供了更多扩展点：Spring的SPI机制提供了很多扩展点，例如BeanPostProcessor、BeanFactoryPostProcessor等，可以在服务提供者初始化和创建过程中进行自定义操作。 应用场景Java SPI机制是一种服务提供者发现的机制，适用于需要在多个实现中选择一个进行使用的场景。 常见的应用场景包括： 应用名称 具体应用场景 数据库驱动程序加载 JDBC为了实现可插拔的数据库驱动，在Java.sql.Driver接口中定义了一组标准的API规范，而具体的数据库厂商则需要实现这个接口，以提供自己的数据库驱动程序。在Java中，JDBC驱动程序的加载就是通过SPI机制实现的。 日志框架的实现 流行的开源日志框架，如Log4j、SLF4J和Logback等，都采用了SPI机制。用户可以根据自己的需求选择合适的日志实现，而不需要修改代码。 Spring框架 Spring框架中的Bean加载机制就使用了SPI思想，通过读取classpath下的META-INF/spring.factories文件来加载各种自定义的Bean。 Dubbo框架 Dubbo框架也使用了SPI思想，通过接口注解@SPI声明扩展点接口，并在classpath下的META-INF/dubbo目录中提供实现类的配置文件，来实现扩展点的动态加载。 参考：https://blog.csdn.net/qq_52423918/article/details/130968307 序列化和反序列化什么是序列化?什么是反序列化? 序列化：将数据结构或对象转换成二进制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过 序列化的主要目的是通过网络传输对象或者说是将对象存储到文件系统、数据库、内存中。 序列化协议对应于 TCP/IP 4 层模型的哪一层？ 应用层（表示层） 如果有些字段不想进行序列化怎么办？使用 transient 关键字修饰 关于 transient 还有几点注意： transient 只能修饰变量，不能修饰类和方法。 transient 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 int 类型，那么反序列后结果就是 0。 static 变量因为不属于任何对象(Object)，所以无论有没有 transient 关键字修饰，均不会被序列化。 JDK 自带的序列化方式JDK 自带的序列化，只需实现 java.io.Serializable接口即可。 serialVersionUID 有什么作用？序列化号 serialVersionUID 属于版本控制的作用。反序列化时，会检查 serialVersionUID 是否和当前类的 serialVersionUID 一致。如果 serialVersionUID 不一致则会抛出 InvalidClassException 异常。强烈推荐每个序列化类都手动指定其 serialVersionUID，如果不手动指定，那么编译器会动态生成默认的 serialVersionUID serialVersionUID 不是被 static 变量修饰了吗？为什么还会被“序列化”？static 修饰的变量是静态变量，位于方法区，本身是不会被序列化的。但是，serialVersionUID 的序列化做了特殊处理，在序列化时，会将 serialVersionUID 序列化到二进制字节流中；在反序列化时，也会解析它并做一致性判断。 为什么不推荐使用 JDK 自带的序列化？ 性能差 存在安全问题 不支持跨语言调用 I/OJava IO 流了解吗？IO 即 Input/Output，输入和输出。数据输入到计算机内存的过程即输入，反之输出到外部存储（比如数据库，文件，远程主机）的过程即输出。数据传输过程类似于水流，因此称为 IO 流。IO 流在 Java 中分为输入流和输出流，而根据数据的处理方式又分为字节流和字符流。 Java IO 流的 40 多个类都是从如下 4 个抽象类基类中派生出来的。 InputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 I/O 流为什么要分为字节流和字符流呢?问题本质想问：不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么 I/O 流操作要分为字节流操作和字符流操作呢？ 字符流是由 Java 虚拟机将字节转换得到的，这个过程还算是比较耗时（字节流优势）； 如果我们不知道编码类型的话，使用字节流的过程中很容易出现乱码问题（字符流优势）。 Java 中 3 种常见 IO 模型BIO (Blocking I/O)同步阻塞 IO 模型中，应用程序发起 read 调用后，会一直阻塞，直到内核把数据拷贝到用户空间。 NIO (Non-blocking/New I/O) 存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。 I/O 多路复用模型 IO 多路复用模型中，线程首先发起 select 调用，询问内核数据是否准备就绪，等内核把数据准备好了，用户线程再发起 read 调用。read 调用的过程（数据从内核空间 -&gt; 用户空间）还是阻塞的。 目前支持 IO 多路复用的系统调用，有 select，epoll 等等。select 系统调用，目前几乎在所有的操作系统上都有支持。 select 调用：内核提供的系统调用，它支持一次查询多个系统调用的可用状态。几乎所有的操作系统都支持。 epoll 调用：linux 2.6 内核，属于 select 调用的增强版本，优化了 IO 的执行效率。 IO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。 Java 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。 AIO (Asynchronous I/O) 目前来说 AIO 的应用还不是很广泛。Netty 之前也尝试使用过 AIO，不过又放弃了。这是因为，Netty 使用了 AIO 之后，在 Linux 系统上的性能并没有多少提升。 零拷贝零拷贝主主要解决操作系统在处理 I/O 操作时频繁复制数据的问题。零拷贝的常见实现技术有： mmap+write、sendfile 下图展示了各种零拷贝技术的对比图： CPU 拷贝 DMA 拷贝 系统调用 上下文切换 传统方法 2 2 read+write 4 mmap+write 1 2 mmap+write 4 sendfile 1 2 sendfile 2 mmapmmap是Linux提供的一种内存映射文件的机制，它实现了将内核中读缓冲区地址与用户空间缓冲区地址进行映射，从而实现内核缓冲区与用户缓冲区的共享。 这样就减少了一次用户态和内核态的CPU拷贝，但是在内核空间内仍然有一次CPU拷贝。 mmap对大文件传输有一定优势，但是小文件可能出现碎片 sendfile建立了两个文件之间的传输通道 sendfile方式只使用一个函数就可以完成之前的read+write 和 mmap+write的功能，少了2次状态切换，由于数据不经过用户缓冲区，因此该数据无法被修改。 kafka和netty都是基于sendfile() 代理模式一种设计模式：使用代理对象来代替对真实对象(real object)的访问，这样就可以在不修改原目标对象的前提下，扩展目标对象的功能。（比如说在目标对象的某个方法执行前后你可以增加一些自定义的操作。） 静态代理 实现和应用角度： ​ 对目标对象的每个方法的增强都是手动完成的，非常不灵活（比如接口一旦新增加方法，目标对象和代理对象都要进行修改）且麻烦(需要对每个目标类都单独写一个代理类） JVM 层面： ​ 静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。 静态代理实现步骤: 定义一个接口及其实现类； 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。 动态代理JDK 动态代理机制在 Java 动态代理机制中 InvocationHandler 接口和 Proxy 类是核心。 使用步骤： 定义一个接口及其实现类； 定义一个实现 InvocationHandler 并重写invoke方法的类，在 invoke 方法中会调用原生方法（被代理类的方法）并自定义一些处理逻辑； 12345678public interface InvocationHandler { /** * 当你使用代理对象调用方法的时候实际会调用到这个方法 */ public Object invoke(Object proxy, Method method, Object[] args) throws Throwable;} 通过 Proxy.newProxyInstance(ClassLoader loader,Class&lt;?&gt;[] interfaces,InvocationHandler h) 方法创建代理对象； 12345678910/**loader :类加载器，用于加载代理对象。* interfaces : 被代理类实现的一些接口；* h : 实现了 InvocationHandler 接口的对象；**/public static Object newProxyInstance(ClassLoader loader, Class&lt;?&gt;[] interfaces, InvocationHandler h) throws IllegalArgumentException{ ......} CGLIB 动态代理机制在 CGLIB 动态代理机制中 MethodInterceptor 接口和 Enhancer类是核心。 使用步骤 定义一个类； 自定义 MethodInterceptor 并重写 intercept 方法，intercept用于拦截增强被代理类的方法，和 JDK 动态代理中的 invoke 方法类似； 123456789101112131415161718192021222324import net.sf.cglib.proxy.MethodInterceptor;import net.sf.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * 自定义MethodInterceptor */public class DebugMethodInterceptor implements MethodInterceptor { /** * @param o 被代理的对象（需要增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { //调用方法之前，我们可以添加自己的操作 System.out.println(\"before method \" + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\"after method \" + method.getName()); return object; }} 通过 Enhancer 类的 create()创建代理类； 1234567891011121314151617import net.sf.cglib.proxy.Enhancer;public class CglibProxyFactory { public static Object getProxy(Class&lt;?&gt; clazz) { // 创建动态代理增强类 Enhancer enhancer = new Enhancer(); // 设置类加载器 enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类 enhancer.setSuperclass(clazz); // 设置方法拦截器 enhancer.setCallback(new DebugMethodInterceptor()); // 创建代理类 return enhancer.create(); }} JDK 动态代理和 CGLIB 动态代理对比 JDK 动态代理只能代理实现了接口的类或者直接代理接口，而 CGLIB 可以代理未实现任何接口的类。 CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为 final 类型的类和方法。 就二者的效率来说，大部分情况都是 JDK 动态代理更优秀 静态代理和动态代理的对比 灵活性：动态代理更加灵活，不需要必须实现接口（静态代理、jdk 动态代理），可以直接代理实现类(CGLIB)，并且可以不需要针对每个目标类都创建一个代理类。另外，静态代理中，接口一旦新增加方法，目标对象和代理对象都要进行修改，这是非常麻烦的！ JVM 层面：静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。而动态代理是在运行时动态生成类字节码，并加载到 JVM 中的。","link":"/2024/03/12/Java%E5%9F%BA%E7%A1%80/"},{"title":"集合","text":"集合框架底层数据结构总结List ArrayList：Object[] 数组。详细可以查看：ArrayList 源码分析。 Vector：Object[] 数组。 LinkedList：双向链表(JDK1.6 之前为循环链表，JDK1.7 取消了循环)。详细可以查看：LinkedList 源码分析。 Set HashSet(无序，唯一): 基于 HashMap 实现的，底层采用 HashMap 来保存元素。 LinkedHashSet: LinkedHashSet 是 HashSet 的子类，并且其内部是通过 LinkedHashMap 来实现的。 TreeSet(有序，唯一): 红黑树(自平衡的排序二叉树)。 Queue PriorityQueue: Object[] 数组来实现小顶堆。详细可以查看：PriorityQueue 源码分析。 DelayQueue:PriorityQueue。详细可以查看：DelayQueue 源码分析。 ArrayDeque: 可扩容动态双向数组。 再来看看 Map 接口下面的集合。 Map HashMap：JDK1.8 之前 HashMap 由数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。详细可以查看：HashMap 源码分析。 LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。详细可以查看：LinkedHashMap 源码分析 Hashtable：数组+链表组成的，数组是 Hashtable 的主体，链表则是主要为了解决哈希冲突而存在的。 TreeMap：红黑树（自平衡的排序二叉树）。 List**ArrayList 和 Array（数组）的区别？ArrayList 内部基于动态数组实现，比 Array（静态数组） 使用起来更加灵活： 动态地扩容或缩容 使用泛型来确保类型安全 只能存储对象（Array 都可以） 不需要指定大小 ArrayList 插入和删除元素的时间复杂度？尾部插入：当 ArrayList 的容量未达到极限时，往列表末尾插入元素的时间复杂度是 O(1)，因为它只需要在数组末尾添加一个元素即可；当容量已达到极限并且需要扩容时，则需要执行一次 O(n) 的操作将原数组复制到新的更大的数组中，然后再执行 O(1) 的操作添加元素。 LinkedList 插入和删除元素的时间复杂度？ 指定位置插入/删除：需要先移动到指定位置，再修改指定节点的指针完成插入/删除，因此需要移动平均 n/2 个元素，时间复杂度为 O(n)。 LinkedList 为什么不能实现 RandomAccess 接口？RandomAccess 是一个标记接口，用来表明实现该接口的类支持随机访问（即可以通过索引快速访问元素）。由于 LinkedList 底层数据结构是链表，内存地址不连续，只能通过指针来定位，不支持随机快速访问，所以不能实现 RandomAccess 接口。 RandomAccess 接口只是标识，并不是说 ArrayList 实现 RandomAccess 接口才具有快速随机访问功能的！是为了能够更好地判断集合是否ArrayList或者LinkedList，从而能够更好选择更优的遍历方式，提高性能！（Collections类中的binarySearch（）） ArrayList 与 LinkedList 区别?底层数据结构 插入和删除是否受元素位置的影响 是否支持快速随机访问 内存空间占用 **ArrayList 的扩容机制 先从 ArrayList 的构造函数说起 有三种构造方法 1234567891011121314/** * 带初始容量参数的构造函数。（用户自己指定容量） */public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) {//初始容量大于0 //创建initialCapacity（10）大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) {//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else {//初始容量小于0，抛出异常 throw new IllegalArgumentException(\"Illegal Capacity: \" + initialCapacity); }} 以无参数构造方法创建 ArrayList 时，实际上初始化赋值的是一个空数组。当真正对数组进行添加元素操作时，才真正分配容量。即向数组中添加第一个元素时，数组容量扩为 10。 add 方法 这里以无参构造函数创建的 ArrayList 为例分析，即一开始为空的数组。 当我们要 add 进第 1 个元素到 ArrayList 时，elementData.length 为 0 （因为还是一个空的 list），因为执行了 ensureCapacityInternal() 方法 ，所以 minCapacity 此时为 10。此时，minCapacity - elementData.length &gt; 0成立，所以会进入 grow(minCapacity) 方法。 当 add 第 2 个元素时，minCapacity 为 2，此时 elementData.length(容量)在添加第一个元素后扩容成 10了。此时，minCapacity - elementData.length &gt; 0 不成立，所以不会进入 （执行）grow(minCapacity)方法。 添加第 3、4···到第 10 个元素时，依然不会执行 grow 方法，数组容量都为 10。 直到添加第 11 个元素，minCapacity(为 11)比 elementData.length（为 10）要大。进入 grow 方法进行扩容。 grow 方法 1private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数. 需要保证newCapacity&gt;=minCapacity 假如newCapacity &gt; MAX_ARRAY_SIZE需要进一步进入hugeCapacity() 方法判断minCapacity &gt; MAX_ARRAY_SIZE 无序性和不可重复性的含义是什么 无序性不等于随机性 ，无序性是指存储的数据在底层数组中并非按照数组索引的顺序添加 ，而是根据数据的哈希值决定的。 不可重复性是指添加的元素按照 equals() 判断时 ，返回 false，需要同时重写 equals() 方法和 hashCode() 方法。 比较 HashSet、LinkedHashSet 和 TreeSet 三者的异同同： HashSet、LinkedHashSet 和 TreeSet 都是 Set 接口的实现类，都能保证元素唯一，并且都不是线程安全的。 异： HashSet、LinkedHashSet 和 TreeSet 的主要区别在于底层数据结构不同。HashSet 的底层数据结构是哈希表（基于 HashMap 实现）。LinkedHashSet 的底层数据结构是链表和哈希表，元素的插入和取出顺序满足 FIFO。TreeSet 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序。 底层数据结构不同又导致这三者的应用场景不同。HashSet 用于不需要保证元素插入和取出顺序的场景，LinkedHashSet 用于保证元素的插入和取出顺序满足 FIFO 的场景，TreeSet 用于支持对元素自定义排序规则的场景。 QueueQueue 与 Deque 的区别Queue 是单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循 先进先出（FIFO） 规则。 Deque 是双端队列，在队列的两端均可以插入或删除元素。 Deque 还提供有 push() 和 pop() 等其他方法，可用于模拟栈。 1Deque&lt;Integer&gt; a = new LinkedList&lt;&gt;(); ArrayDeque 与 LinkedList 的区别ArrayDeque 和 LinkedList 都实现了 Deque 接口，两者都具有队列的功能，但两者有什么区别呢？ ArrayDeque 是基于可变长的数组和双指针来实现，而 LinkedList 则通过链表来实现。 ArrayDeque 不支持存储 NULL 数据，但 LinkedList 支持。 ArrayDeque 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 LinkedList 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。 从性能的角度上，选用 ArrayDeque 来实现队列要比 LinkedList 更好。此外，ArrayDeque 也可以用于实现栈。 说一说 PriorityQueue PriorityQueue 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据 PriorityQueue 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。 PriorityQueue 是非线程安全的，且不支持存储 NULL 和 non-comparable 的对象。 PriorityQueue 默认是小顶堆，但可以接收一个 Comparator 作为构造参数，从而来自定义元素优先级的先后。 PriorityQueue 在面试中可能更多的会出现在手撕算法的时候，典型例题包括堆排序、求第 K 大的数、带权图的遍历等，所以需要会熟练使用才行。 Map（重要）HashMap 的 put过程1.首先，根据键的哈希值计算出键的哈希码。HashMap使用键的哈希码来确定键值对在哈希表中的存储位置。 2.接下来，通过哈希码计算出键值对在哈希表中的索引位置。HashMap使用一个称为“哈希函数”的算法来计算索引位置。常用的哈希函数是将哈希码与哈希表的容量进行取模运算，得到索引位置。 3.如果该索引位置上没有其他键值对，则直接将键值对存储在该位置上。如果该索引位置上已经存在其他键值对，则发生了“哈希冲突”。 4.当发生哈希冲突时，HashMap使用链表或红黑树来解决冲突。在JDK1.8之前，HashMap使用链表来解决冲突，即在冲突的索引位置上，将新的键值对插入到链表的尾部。而在JDK1.8及以后的版本中，当链表长度超过一定阈值时，HashMap会将链表转换为红黑树，以提高查找效率。 5.最后，将键值对成功存储在哈希表中，并返回先前关联的值（如果存在）。如果该索引位置上已经存在其他键值对，则需要遍历链表或红黑树，找到对应的键值对，并更新其值。 HashMap 和 Hashtable 的区别 线程是否安全 效率 对 Null key 和 Null value 的支持 初始容量大小和每次扩充容量大小的不同: ① 创建时如果不指定容量初始值，Hashtable 默认的初始大小为 11，之后每次扩充，容量变为原来的 2n+1。HashMap 默认的初始化大小为 16。之后每次扩充，容量变为原来的 2 倍。② 创建时如果给定了容量初始值，那么 Hashtable 会直接使用你给定的大小，而 HashMap 会将其扩充为 2 的幂次方大小 底层数据结构:JDK1.8 以后的 HashMap 在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树），以减少搜索时间 HashMap 和 HashSet 区别如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法。 HashMap 和 TreeMap 区别相比于HashMap来说 TreeMap 主要多了对集合中的元素根据键排序的能力以及对集合内元素的搜索的能力。 HashSet 如何检查重复?当你把对象加入HashSet时，HashSet 会先计算对象的hashcode值来判断对象加入的位置，同时也会与其他加入的对象的 hashcode 值作比较，如果没有相符的 hashcode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashcode 值的对象，这时会调用equals()方法来检查 hashcode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让加入操作成功。 HashMap 的底层实现JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashcode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。 所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法影响性能 换句话说使用扰动函数之后可以减少碰撞。 JDK 1.8 HashMap 的 hash 方法源码: JDK 1.8 的 hash 方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。 1234567 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);} 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 相比于之前的版本， JDK1.8 之后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间。 TreeMap、TreeSet 以及 JDK1.8 之后的 HashMap 底层都用到了红黑树。红黑树就是为了解决二叉查找树的缺陷，因为二叉查找树在某些情况下会退化成一个线性结构。 HashMap 的长度为什么是 2 的幂次方Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。散列值是不能直接拿来用的。 用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) &amp; hash”。（n 代表数组长度）。一开始想到的是 % 取余 但是效率比 按位与 低。这也就解释了 HashMap 的长度为什么是 2 的幂次方。 解释：（只有 n 为 2 的幂次方，n-1 用二进制表示时低位才可能都是 1，与 hash 做按位&amp; 才能得到全部低位数据） HashMap 多线程操作导致死循环问题JDK1.7 及之前版本的 HashMap 在多线程环境下扩容操作可能存在死循环问题，这是由于当一个桶位中有多个元素需要进行扩容时，多个线程同时对链表进行操作，头插法可能会导致链表中的节点指向错误的位置，从而形成一个环形链表，进而使得查询元素的操作陷入死循环无法结束。 为了解决这个问题，JDK1.8 版本的 HashMap 采用了尾插法而不是头插法来避免链表倒置，使得插入的节点永远都是放在链表的末尾，避免了链表中的环形结构。但是还是不建议在多线程下使用 HashMap，因为多线程下使用 HashMap 还是会存在数据覆盖的问题。并发环境下，推荐使用 ConcurrentHashMap 。 *HashMap 为什么线程不安全？JDK1.7 及之前版本，在多线程环境下，HashMap 扩容时会造成死循环和数据丢失的问题。 数据丢失这个在 JDK1.7 和 JDK 1.8 中都存在 情况 1：哈希冲突，导致两个线程同时进行 hash 判断都可以插入，a 线程判断可以插入后挂起，b 线程执行插入，完成后另a 线程继续执行插入导致数据被覆盖 情况 2：两个线程同时 put 操作导致 size 的值只增加了 1，实际上只有一个元素被添加到了 HashMap 中，进而导致数据覆盖的问题 ConcurrentHashMap 和 Hashtable 的区别底层数据结构 JDK1.7 的 ConcurrentHashMap 底层采用 分段的数组+链表 实现，JDK1.8 采用的数据结构跟 HashMap1.8 的结构一样，数组+链表/红黑二叉树。 Hashtable 和 JDK1.8 之前的 HashMap 的底层数据结构类似都是采用 数组+链表 的形式，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的； 实现线程安全的方式（重要） 在 JDK1.7 的时候，ConcurrentHashMap 对整个桶数组进行了分割分段(Segment，分段锁)，每一把锁只锁容器其中一部分数据（下面有示意图） 到了 JDK1.8 的时候，ConcurrentHashMap 已经摒弃了 Segment 的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并使用 synchronized 、CAS 、node来保证并发安全。 （未发生 hash 冲突时，采用 CAS加入新 node 到数组中；若发生 hash冲突时，采用 synchronized 来添加该节点到链表 or 红黑树） Hashtable(同一把锁) :使用 synchronized 来保证线程安全，锁住整个数组，效率底下。 Hashtable : JDK1.7 的 ConcurrentHashMap： Segment 数组中的每个元素包含一个 HashEntry 数组，每个 HashEntry 数组属于链表结构。 JDK1.8 的 ConcurrentHashMap： JDK1.8 的 ConcurrentHashMap 不再是 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。不过，Node 只能用于链表的情况，红黑树的情况需要使用 **TreeNode**。当冲突链表达到一定长度时，链表会转换成红黑树。 CAS&amp;synchronizedCAS：在判断数组中当前位置为null的时候，使用CAS来把这个新的Node写入数组中对应的位置 synchronized ：当数组中的指定位置不为空时，通过加锁来添加这个节点进入数组(链表&lt;8)或者是红黑树（链表&gt;=8） 所以这部分是进行了cas： else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } ConcurrentHashMap 为什么 key 和 value 不能为 null？避免二义性： 值没有在集合中 ； 值本身就是 null。 单线程下可以容忍歧义，而多线程下无法容忍","link":"/2024/03/12/%E9%9B%86%E5%90%88/"},{"title":"JVM","text":"** java 类加载加载过程分为三部分:加载、连接、初始化 加载 将二进制字节流读入内存(1.7 使用 jvm 内存,1.8使用本地内存),然后在堆中创建Class 对象,作为.class 进入内存后的数据访问入口. 这里只读入二进制字节流,后续的验证阶段要拿二进制字节流来验证.class 文件,验证通过,才会将.class 文件转为运行时数据结构 将永久代 -&gt;元空间原因 现实使用中存在问题：方法区存储类的元数据信息，我们不清楚一个程序到底有多少类需要被加载，且方法区位于JVM内存，我们不清楚需要给方法区分配多大内存，太小容易PermGen OOM，太大，在触发Full GC时又极其影响性能 连接 包括验证、准备、解析 验证: 保证加载的字节流符合 JVM 规范,不会有安全隐患, 包括元数据验证(是否继承了不能继承的类),符号引用验证(引用的其他类是否存在) 准备:为类的类变量开辟空间并赋默认值 解析:将 Class 在常量池的符号引用转变成直接引用(也就是得到类或者字段、方法在内存中的指针或者偏移量) 初始化 为类变量初始化值,有两种方式: 声明时直接赋值 在静态代码块中赋值 类加载时机 创建类的实例 调用类的静态方法 访问类的静态变量 反射 初始化子类 运行 main 方法,其所在类会被加载 类加载顺序 先加载、连接当前类 若父类没有加载, 则去加载、连接父类直到 Object 被加载 然后从父类开始初始化(静态变量赋值、静态初始化块) 类加载器JVM 中内置了三个重要的 ClassLoader： **BootstrapClassLoader(启动类加载器)**：最顶层的加载类，由 C++实现，通常表示为 null，并且没有父级，主要用来加载 JDK 内部的核心类库（ %JAVA_HOME%/lib目录下的jar 包和类）以及被 -Xbootclasspath参数指定的路径下的所有类。 **ExtensionClassLoader(扩展类加载器)**：主要负责加载 %JRE_HOME%/lib/ext 目录下的 jar 包和类以及被 java.ext.dirs 系统变量所指定的路径下的所有类。 **AppClassLoader(应用程序类加载器)**：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。 双亲委派模型 双亲委派，又叫做父类委托，即在加载一个类时，==自底向上找判断父类加载器是否已经加载该类，如果启动类加载器都没有加载当前类，则自顶向下尝试加载该类==，此处的父子关系并非继承，而是一种组合关系，是类加载器的复用。 双亲委派模型的好处 避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类）， 保证 java 核心 api 不被篡改 例子：即使自己写一个 java.lang.Object类，也会使用启动类加载器加载官方 Object 打破双亲委派模型方法自定义类加载器继承ClassLoader，重写 **loadClass()**方法。若不想打破则重写 findClass()方法 例子：Tomcat。为了实现web应用程序之间的类加载器相互隔离独立的是WebAppClassLoader类加载器。它为什么可以隔离每个web应用程序呢？原因就是它打破了”双亲委派”的机制，如果收到类加载的请求，它会先尝试自己去加载，如果找不到在交给父加载器去加载，这么做的目的就是为了优先加载Web应用程序自己定义的类来实现web应用程序相互隔离独立的。 **JVM 内存区域JDK 1.8： 线程私有的： 程序计数器：记录程序执行到的位置 虚拟机栈 局部变量表：存放了编译期可知的各种数据类型 操作数栈：存放方法执行过程中产生的中间计算结果 动态链接：将符号引用转换为调用方法的直接引用。多态 方法返回地址 本地方法栈：执行本地方法 线程共享的： 堆：存放类实例和数组 JDK 8 版本之后 PermGen(永久代) 已被 Metaspace(元空间) 取代，元空间使用的是本地内存 方法区 ==（方法区逻辑上在堆中）存储已被虚拟机加载的 类型信息、域（Field）信息、方法信息、运行时常量池、字符串常量池、静态变量、JIT 代码缓存等数据== 类型信息 对每个加载的类型（类 class、接口 interface、枚举 enum、注解 annotation），JVM 必须在方法区中存储以下类型信息 这个类型的完整有效名称（全名=包名.类名） 这个类型直接父类的完整有效名（对于 interface或是 java.lang.Object，都没有父类） 这个类型的修饰符（public，abstract，final 的某个子集） 这个类型直接接口的一个有序列表 域（Field）信息 JVM 必须在方法区中保存类型的所有域的相关信息以及域的声明顺序 域的相关信息包括：域名称、域类型、域修饰符（public、private、protected、static、final、volatile、transient 的某个子集） 方法（Method）信息 方法名称 方法的返回类型 方法参数的数量和类型 方法的修饰符（public，private，protected，static，final，synchronized，native，abstract 的一个子集） 方法的字符码（bytecodes）、操作数栈、局部变量表及大小 运行时常量池：字面量（包括整数、浮点数和字符串字面量）和符号引用（包括对类型、字段、方法的符号引用） 字符串常量池。主要目的是为了避免字符串的重复创建。 运行时常量池（Runtime Constant Pool）是虚拟机规范中是方法区的一部分，在加载类和结构到虚拟机后，就会创建对应的运行时常量池；而字符串常量池是这个过程中常量字符串的存放位置。所以从这个角度，字符串常量池属于虚拟机规范中的方法区，它是一个逻辑上的概念；而堆区，永久代以及元空间是实际的存放位置。 方法区（method area）只是 JVM 规范中定义的一个概念，用于存储类信息、常量池、静态变量、JIT编译后的代码等数据，并没有规定如何去实现它，不同的厂商有不同的实现。而永久代（PermGen）是 Hotspot虚拟机特有的概念， Java8 的时候又被元空间取代了，永久代和元空间都可以理解为方法区的落地实现。 ==永久代物理是堆的一部分，和新生代，老年代地址是连续的（受垃圾回收器管理），而元空间存在于本地内存（我们常说的堆外内存，不受垃圾回收器管理），这样就不受 JVM 限制了，也比较难发生OOM（都会有溢出异常）== 是否有永久代，字符串常量池放在哪里？ 方法区逻辑上规范，由哪些实际的部分实现的？ jdk1.6及之前 有永久代，运行时常量池（包括字符串常量池），静态变量存放在永久代上 这个时期方法区在HotSpot中是由永久代来实现的，以至于这个时期说方法区就是指永久代 jdk1.7 有永久代，但已经逐步“去永久代”，字符串常量池、静态变量移除，保存在堆中； 这个时期方法区在HotSpot中由永久代（类型信息、字段、方法、常量）和堆（字符串常量池、静态变量）共同实现 jdk1.8及之后 取消永久代，类型信息、字段、方法、常量保存在本地内存的元空间，但字符串常量池、静态变量仍在堆中 这个时期方法区在HotSpot中由本地内存的元空间（类型信息、字段、方法、常量）和堆（字符串常量池、静态变量）共同实现 java 内存模型（JMM）JMM是什么？有什么存在作用？java 内存模型跟 cpu 缓存模型类型，是基于 cpu 缓存模型来建立的，Java内存模型定义了共享内存系统中多线程程序读写操作行为的规范，是为了解决并发编程问题而存在的。 JMM对内存的划分？划分为主内存和工作内存两种 所有的变量都存储在主内存中。 每个线程都有一个私有的工作内存，本地内存中存储了该线程以读/写共享变量的拷贝副本。 线程对变量的所有操作都必须在本地内存中进行，而不能直接读写主内存。 不同的线程之间无法直接访问对方本地内存中的变量。 主内存和工作内存的交互操作 lock：锁定。作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock：解锁。作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read：读取。作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load：载入。作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use：使用。作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign：赋值。作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store：存储。作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write：写入。作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 内存交互基本操作的三个特性的理解？原子性（synchroinzed）、可见性（volatile）、以及有序性（happen-before）。 volatile和synchronized的区别 volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，其他线程被阻塞住。 volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的 volatile仅能实现变量的修改可见性和有序性，不能保证原子性；而synchronized则可以保证变量的修改可见性（内存屏障）、有序性（内存屏障）和原子性 sychronized底层是通过monitorenter的指令来进行加锁的、通过monitorexit指令来释放锁的。 monitorenter指令其实还具有Load屏障的作用。 也就是通过monitorenter指令之后，synchronized内部的共享变量，每次读取数据的时候被强制从主内存读取最新的数据。 同样的道理monitorexit指令也具有Store屏障的作用，也就是让synchronized代码块内的共享变量，如果数据有变更的，强制刷新回主内存。 这样通过这种方式，数据修改之后立即刷新回主内存，其他线程进入synchronized代码块后，使用共享变量的时候强制读取主内存的数据，上一个线程对共享变量的变更操作，它就能立即看到了。 4条禁止重排序的内存屏障分别为： StoreStore屏障：禁止StoreStore屏障的前后Store写操作重排 LoadLoad屏障：禁止LoadLoad屏障的前后Load读操作进行重排 LoadStore屏障：禁止LoadStore屏障的前面Load读操作跟LoadStore屏障后面的Store写操作重排 StoreLoad屏障：禁止LoadStore屏障前面的Store写操作跟后面的Load/Store 读写操作重排 同样的道理啊，也是通过monitorenter、monitorexit指令嵌入上面的内存屏障；monitorenter、monitorexit这两条指令其实就相当于复合指令，既具有加锁、释放锁的功能，同时也具有内存屏障的功能。 volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。 volatile标记的变量不会被编译器优化；synchronized标记的变量可以被编译器优化 happen-beforehappen-before原则是Java内存模型中定义的两项操作之间的偏序关系。 Happens-Before关系只是描述结果的可见性，并不表示指令执行的先后顺序，也就是说只要不对结果产生影响，仍然允许指令重排序。 为了具体说明，请看前面提到过的计算圆面积的示例代码： 123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面计算圆的面积的示例代码存在三个 happens- before 关系： A happens- before B； B happens- before C； A happens- before C； 由于 A happens- before B，happens- before 的定义会要求：A 操作执行的结果要对 B 可见，且 A 操作的执行顺序排在 B 操作之前。 但是从程序语义的角度来说，对 A 和 B 做重排序即不会改变程序的执行结果，也还能提高程序的执行性能（允许这种重排序减少了对编译器和处理器优化的束缚）。也就是说，上面这 3 个 happens- before 关系中，虽然 2 和 3 是必需要的，但 1 是不必要的。因此，JMM 把 happens- before 要求禁止的重排序分为了下面两类： 会改变程序执行结果的重排序。 不会改变程序执行结果的重排序。 JMM 对这两种不同性质的重排序，采取了不同的策略： 对于会改变程序执行结果的重排序，JMM 要求编译器和处理器必须禁止这种重排序。 对于不会改变程序执行结果的重排序，JMM 对编译器和处理器不作要求（JMM 允许这种重排序）。 下面是 JMM 的设计示意图： Java垃圾回收内存分配和回收原则大对象直接进入老年代大对象就是需要大量连续内存空间的对象（比如：字符串、数组）。 大对象直接进入老年代的行为是由虚拟机动态决定的，它与具体使用的垃圾回收器和相关参数有关。大对象直接进入老年代是一种优化策略，旨在避免将大对象放入新生代，从而减少新生代的垃圾回收频率和成本。 主要进行 gc 的区域针对 HotSpot VM 的实现，它里面的 GC 其实准确分类只有两大种： 部分收集 (Partial GC)： 新生代收集（Minor GC / Young GC）：只对新生代进行垃圾收集； 老年代收集（Major GC / Old GC）：只对老年代进行垃圾收集。需要注意的是 Major GC 在有的语境中也用于指代整堆收集； 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集。 整堆收集 (Full GC)：收集整个 Java 堆和方法区。 空间分配担保空间分配担保是为了确保在 Minor GC 之前老年代本身还有容纳新生代所有对象的剩余空间。 JVM是如何判断一个对象是可回收的？可达性分析算法 基本思想就是通过一系列的称为 “GC Roots” 的对象作为起点，从这些节点开始向下搜索，节点所走过的路径称为引用链，当一个对象到 GC Roots 没有任何引用链相连的话，则证明此对象是不可用的，需要被回收。 哪些对象可以作为 GC Roots 呢？ 虚拟机栈(栈帧中的局部变量表)中引用的对象 本地方法栈(Native 方法)中引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 垃圾收集算法标记-清除算法标记-清除（Mark-and-Sweep）算法分为“标记（Mark）”和“清除（Sweep）”阶段：首先标记出所有不需要回收的对象，在标记完成后统一回收掉所有没有被标记的对象。 这种垃圾收集算法会带来两个明显的问题： 效率问题：标记和清除两个过程效率都不高。 空间问题：标记清除后会产生大量不连续的内存碎片。 复制算法为了解决标记-清除算法的效率和内存碎片问题，复制（Copying）收集算法出现了。它可以将内存分为大小相同的两块，每次使用其中的一块。当这一块的内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉。这样就使每次的内存回收都是对内存区间的一半进行回收。 虽然改进了标记-清除算法，但依然存在下面这些问题： 可用内存变小：可用内存缩小为原来的一半。 不适合老年代：如果存活对象数量比较大，复制性能会变得很差。 标记-整理算法标记-整理（Mark-and-Compact）算法是根据老年代的特点提出的一种标记算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象回收，而是让所有存活的对象向一端移动，然后直接清理掉端边界以外的内存。 由于多了整理这一步，因此效率也不高，适合老年代这种垃圾回收频率不是很高的场景。 分代收集算法比如在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集。而老年代的对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以我们必须选择“标记-清除”或“标记-整理”算法进行垃圾收集。 **垃圾收集器Serial 收集器新生代采用标记-复制算法，老年代采用标记-整理算法。 “单线程” 的意义不仅仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作的时候必须暂停其他所有的工作线程（ “Stop The World” ），直到它收集结束。 不良用户体验但简单而高效（与其他收集器的单线程相比） ParNew 收集器ParNew 收集器其实就是 Serial 收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和 Serial 收集器完全一样。 它是许多运行在 Server 模式下的虚拟机的首要选择，除了 Serial 收集器外，只有它能与 CMS 收集器（真正意义上的并发收集器，后面会介绍到）配合工作。 Parallel Scavenge 收集器Parallel Scavenge 收集器关注点是吞吐量（高效率的利用 CPU）。CMS 等垃圾收集器的关注点更多的是用户线程的停顿时间（提高用户体验）。所谓吞吐量就是 CPU 中用于运行用户代码的时间与 CPU 总消耗时间的比值。 新生代采用标记-复制算法，老年代采用标记-整理算法。 **CMS 收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。它非常符合在注重用户体验的应用上使用。并发收集器。 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。 CMS收集器是基于标记-清除算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为四个步骤，包括：1）初始标记（CMS initial mark） 2）并发标记（CMS concurrent mark） 3）重新标记（CMS remark） 4）并发清除（CMS concurrent sweep） 初始标记：标记直接与GCRoots 相连的对象，速度很快 并发标记： 从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行。这里使用三色标记法。每个对象分为三种类型，黑色代表自身和成员都检查，灰色代表自身检查完成员没有检查，白色代表自身跟成员都还未检查。GC 开始前所有对象都是白色，GC 一开始所有根能够直达的对象标记为灰色，层序遍历其子对象。然后待其子对象都标记变灰色后，该对象变成黑色。当 GC 结束之后灰色对象将全部没了，剩下黑色的为存活对象，白色的为垃圾。但是由于是并发标记，用户线程会跟 gc 线程交替执行，会出现标记变动的情况。 问题：存在错标问题，标记线程已经确定 b、d 是黑色，然后用户线程将 b、d 断开同时 d 没有其他对象被引用了，但是他已经是黑色不会在去检查，所以就不会被 gc 回收，也就是浮动垃圾。 重新标记：修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录 并发清除：开启用户线程，同时 GC 线程开始对未标记的区域做清扫，这个阶段也是可以与用户线程同时并发的。由于在整个过程中耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一起工作，所以从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。 主要优点：并发收集、低停顿。但是它有下面三个明显的缺点： 吞吐量低: 低停顿时间是以牺牲吞吐量为代价的，导致 CPU 利用率不够高。 无法处理浮动垃圾，只能到下一次 GC 时才能进行回收，因此需要预留出一部分内存。如果预留的内存不够存放浮动垃圾，就会出现 Concurrent Mode Failure，这时虚拟机将临时启用 Serial Old 来替代 CMS。 它使用的回收算法-“标记-清除”算法会导致收集结束时会有大量空间碎片产生。往往出现老年代空间剩余，但无法找到足够大连续空间来分配当前对象，不得不提前触发一次 Full GC。 **G1 收集器它是一款面向服务端应用的垃圾收集器，在多 CPU 和大内存的场景下有很好的性能。以极高概率满足 GC 停顿时间要求的同时,还具备高吞吐量性能特征. 其中初始标记、最终标记、筛选回收这三个步骤仍然需要“Stop The World”。 特点： 分代收集：G1(Garbage First)物理内存不再分代，而是由一块一块的Region组成,但是逻辑分代仍然存在。G1 把堆划分成多个大小相等的独立区域(Region)，每个 region 根据需要扮演新生代或老年代，新生代和老年代不再物理隔离， G1 可以直接对新生代和老年代一起回收。 可预测的停顿： 每个Region 可以单独进行垃圾回收，通过对每个 Region 维护一个优先列表，每次根据允许的收集时间，优先回收价值最大的 Region，控制停顿时间。 避免全堆扫描：每个 Region 都有一个 Remembered Set，用来记录该 Region 对象的引用对象所在的 Region。通过使用 Remembered Set，在做可达性分析的时候就可以避免全堆扫描。 空间整合：与 CMS 的“标记-清除”算法不同，G1 从整体来看是基于“标记-整理”算法实现的收集器；从局部上（两个 region 来看）来看是基于“标记-复制”算法实现的。 G1GC 的主要功能是并发标记和转移。其中并发标记由并发标记线程来执行。 并发标记的作用是在尽量不暂停用户线程的情况下标记出存活对象。而且，还需要在并发标记结束之后记录下每个区域内存活对象的数量。这个信息在转移时会用到。 转移的作用是将待回收区域内的存活对象复制到其他的空闲区域，然后将待回收区域重置为空闲状态。这很像复制 GC 算法，只不过是以区域为单位进行的。 几个步骤： 初始标记：暂停所有用户线程，标记 gc roots 直接相连的对象，速度很快。 并发标记：采用三色标记法，以 gc roots直接相连的对象为起点，进行广度优先遍历，配合原始快照记录用户线程更改引用的关系的原始引用到Remembered Set Logs，并将更改的引用加入到 gc 堆栈中。 最终标记：最终标记阶段需要把 并发标记生成的Remembered Set Logs 的数据合并到 Remembered Set 中，然后扫描 gc 堆栈，配合 Rset 执行标记。 筛选回收：首先对各个 Region 中的回收价值和成本进行排序，根据用户所期望的 GC 停顿时间来制定回收计划。此阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分 Region，时间是用户可控制的，而且停顿用户线程将大幅度提高收集效率。 三色标记算法 白色：没有检查（或者检查过了，确实没有引用指向它了） 灰色：自身被检查了，成员没被检查完（可以认为访问到了，但是正在被检查，就是图的遍历里那些在队列中的节点） 黑色：自身和成员都被检查完了 具体流程: 首先创建三个集合：白、灰、黑。 将所有对象放入白色集合中。 然后从根节点开始遍历所有对象（注意这里并不「递归遍历」），把遍历到的对象从白色集合放入灰色集合。 之后遍历灰色集合，将灰色对象引用的对象从白色集合放入灰色集合，之后将此灰色对象放入黑色集合 重复 上一步骤 直到灰色中无任何对象 通过write-barrier检测对象有变化，CMS 采用增量更新，在黑色连接白色后，将黑色变为灰色；G1 采用初始快照，将删除灰色到白色的连接记录到 remembered set log 中，这样在后续的重新标记/最终标记的时候进行处理。 收集所有白色对象（垃圾） 第一种问题： 错标标记过不是垃圾的，变成了垃圾（也叫浮动垃圾） 会在 CMS 出现，因为 cms 采用的是增量更新，不会对引用的删除进行处理。 第二种问题：漏标，或者叫错杀 产生漏标问题的条件有两个： 黑色对象指向了白色对象 灰色对象指向白色对象的引用消失 G1：写屏障+ SATB （Snapshot At The Beginning） 在开始标记的时候生成一个快照图标记存活对象 在一个引用断开后，要将此引用推到 GC 的堆栈里，保证白色对象(垃圾)还能被 GC 线程扫描到 配合 Rset，去扫描哪些 Region 引用到当前的白色对象，若没有引用到当前对象，则回收 cms：写屏障+ 增量更新 新增如果新增黑色到白色的引用，那么jvm会通过写屏障，来把黑色置为灰色 删除如果删除引用，jvm什么都不会做，这个导致了浮动垃圾 为什么 ThreadLocalMap 的 key 是弱引用，而 value 是强引用？ 问题一：为什么 ThreadLocalMap 的 key 是弱引用？【假设 Entry 的 key 是对 ThreadLocal 对象的强引用】。如果在其他地方都没有对这个 ThreadLocla 对象的引用了，然后在使用 ThreadLocalMap 的过程中又没有正确地在用完后就调用 remove 方法，所以这个 ThreadLocal 对象和所关联的 value 对象就会跟随着线程一直存在，这样就会可能会造成内存泄漏问题。 特别是在使用线程池的时候，核心线程是会一直存在直到程序结束，如果这些线程中的 ThreadLocalMap 中的数据没有被及时清理，就会一直占用内存，而且在线程复用时可能会导致数据错乱的危险。 【Entry 的 key 是对 ThreadLocal 对象的弱引用】：弱引用就意味着，如果没有其他引用对象的强引用关系，那么这个仅被弱引用引用着的对象在下次 GC 时就会被回收掉，这样在一定程度上降低内存泄漏的风险。但同时也引入了新的问题，key 虽然被回收了，但是 value 对象还在，我们无法获取，也无法删除，这样也会存在内存泄漏的风险。虽然 ThreadLocalMap 中在进行 set 和 get 操作时会进行启发式清理和探测式清理，清理一部分 key 为 null 的 Entry 对象，但是这也只是一种后备选择方案，最重要的还是开发人员在编写代码时记得在使用完数据后及时调用 remove() 方法手动清理。 【内存泄漏就是，有些对象已经不再使用了，但是由于没有正确处理对象的引用关系，使得这个无用的对象还一直被 GC Root 直接或间接引用着，垃圾回收时就无法清理掉这些对象，如果这类对象存在很多，就会导致内存泄漏。简单地说就是有些无用对象占用着宝贵的内存空间，但又没办法清理掉它们】 问题二：为什么 ThreadLocalMap 的 value 是强引用？【假设Entry 的 value 是弱引用】：假设 key 所引用的 ThreadLocal 对象还被其他的引用对象强引用着，那么这个 ThreadLocal 对象就不会被 GC 回收，但如果 value 是弱引用且不被其他引用对象引用着，那 GC 的时候就被回收掉了，那线程通过 ThreadLocal 来获取 value 的时候就会获得 null，显然这不是我们希望的结果。因为对我们来说，value 才是我们想要保存的数据，ThreadLcoal 只是用来关联 value 的，如果 value 都没了，还要 ThreadLocal 干嘛呢？所以 value 不能是弱引用。 请问会不会出现：将登录用户信息放入ThreadLocal中，业务代码中还未使用，此时GC把弱引用的key删除了，导致后续业务中获取用户信息失败？？？ 每个线程都会有一个ThreadLocal的强引用在指向着堆中的ThreadLocal对象，知道线程终止key才会失效，一般来说是不会被清理掉的，弱引用是框架层面的思考，加了一层保险 调试排错 - Linux命令文本操作文本查询-grepgrep常用命令： 12345678910111213141516# 基本使用grep yoursearchkeyword f.txt #文件查找grep 'KeyWord otherKeyWord' f.txt cpf.txt #多文件查找, 含空格加引号grep 'KeyWord' /home/admin -r -n #目录下查找所有符合关键字的文件grep 'keyword' /home/admin -r -n -i # -i 忽略大小写grep 'KeyWord' /home/admin -r -n --include *.{vm,java} #指定文件后缀grep 'KeyWord' /home/admin -r -n --exclude *.{vm,java} #反匹配# cat + grepcat f.txt | grep -i keyword # 查找所有keyword且不分大小写 cat f.txt | grep -c 'KeyWord' # 统计Keyword次数# seq + grepseq 10 | grep 5 -A 3 #上匹配seq 10 | grep 5 -B 3 #下匹配seq 10 | grep 5 -C 3 #上下匹配，平时用这个就妥了 Grep的参数： 1234567-i, --ignore-case：忽略字符大小写;-n, --line-number：显示行号;-c, --count：统计匹配到的行数; print a count of matching lines;-B, --before-context=NUM：print NUM lines of leading context 后#行 -A, --after-context=NUM：print NUM lines of trailing context 前#行 -C, --context=NUM：print NUM lines of output context 前后各#行 文本分析 - awkawk基本命令： 12345678910111213# 基本使用awk '{print $4,$6}' f.txtawk '{print NR,$0}' f.txt cpf.txt awk '{print FNR,$0}' f.txt cpf.txtawk '{print FNR,FILENAME,$0}' f.txt cpf.txtawk '{print FILENAME,\"NR=\"NR,\"FNR=\"FNR,\"$\"NF\"=\"$NF}' f.txt cpf.txtecho 1:2:3:4 | awk -F: '{print $1,$2,$3,$4}'# 匹配awk '/ldb/ {print}' f.txt #匹配ldbawk '!/ldb/ {print}' f.txt #不匹配ldbawk '/ldb/ &amp;&amp; /LISTEN/ {print}' f.txt #匹配ldb和LISTENawk '$5 ~ /ldb/ {print}' f.txt #第五列匹配ldb 内建变量 12345`NR`: 已经读出的记录数，就是行号，从1开始，NR可以理解为Number of Record的缩写。`FNR`: 各文件分别计数的行号，FNR可以理解为File Number of Record。`NF`: 一条记录的字段的数目，NF可以理解为Number of Field。 文本处理 - sedsed常用： 1234567891011121314151617181920212223242526# 文本打印sed -n '3p' xxx.log #只打印第三行sed -n '$p' xxx.log #只打印最后一行sed -n '3,9p' xxx.log #只查看文件的第3行到第9行sed -n -e '3,9p' -e '=' xxx.log #打印3-9行，并显示行号sed -n '/root/p' xxx.log #显示包含root的行sed -n '/hhh/,/omc/p' xxx.log # 显示包含\"hhh\"的行到包含\"omc\"的行之间的行# 文本替换sed -i 's/root/world/g' xxx.log # 用world 替换xxx.log文件中的root; s==search 查找并替换, g==global 全部替换, -i: implace# 文本插入sed '1,4i hahaha' xxx.log # 在文件第一行和第四行的每行下面添加hahahased -e '1i happy' -e '$a new year' xxx.log #【界面显示】在文件第一行添加happy,文件结尾添加new yearsed -i -e '1i happy' -e '$a new year' xxx.log #【真实写入文件】在文件第一行添加happy,文件结尾添加new year# 文本删除sed '3,9d' xxx.log # 删除第3到第9行,只是不显示而已sed '/hhh/,/omc/d' xxx.log # 删除包含\"hhh\"的行到包含\"omc\"的行之间的行sed '/omc/,10d' xxx.log # 删除包含\"omc\"的行到第十行的内容# 与find结合find . -name \"*.txt\" |xargs sed -i 's/hhhh/\\hHHh/g'find . -name \"*.txt\" |xargs sed -i 's#hhhh#hHHh#g'find . -name \"*.txt\" -exec sed -i 's/hhhh/\\hHHh/g' {} \\;find . -name \"*.txt\" |xargs cat 文件操作文件监听 - tail最常用的tail -f filename 1234567# 基本使用tail -f xxx.log # 循环监听文件tail -300f xxx.log #倒数300行并追踪文件tail +20 xxx.log #从第 20 行至文件末尾显示文件内容# tailf使用tailf xxx.log #等同于tail -f -n 10 打印最后10行，然后追踪文件 tail的参数 1234-f 循环读取-c&lt;数目&gt; 显示的字节数-n&lt;行数&gt; 显示文件的尾部 n 行内容-s, --sleep-interval=S 与-f合用,表示在每次反复的间隔休眠S秒 文件查找 - find12345678910111213sudo -u admin find /home/admin /tmp /usr -name \\*.log(多个目录去找)find . -iname \\*.txt(大小写都匹配)find . -type d(当前目录下的所有子目录)find /usr -type l(当前目录下所有的符号链接)find /usr -type l -name \"z*\" -ls(符号链接的详细信息 eg:inode,目录)find /home/admin -size +250000k(超过250000k的文件，当然+改成-就是小于了)find /home/admin f -perm 777 -exec ls -l {} \\; (按照权限查询文件)find /home/admin -atime -1 1天内访问过的文件find /home/admin -ctime -1 1天内状态改变过的文件 find /home/admin -mtime -1 1天内修改过的文件find /home/admin -amin -1 1分钟内访问过的文件find /home/admin -cmin -1 1分钟内状态改变过的文件 find /home/admin -mmin -1 1分钟内修改过的文件 查看网络和进程查看所有网络接口的属性1ifconfig 查看防火墙设置1iptables -L 查看路由表1route -n netstat查看所有监听端口 12netstat -lntp-l, --listening display listening server sockets 查看所有已经建立的连接 12netstat -antp -a, --all, --listening display all sockets (default: connected) 查看网络统计信息进程 123netstat -s -s, --statistics display networking statistics (like SNMP) 查看所有进程123ps -ef | grep java-e：显示所有进程。-f：全格式显示。 toptop除了看一些基本信息之外，剩下的就是配合来查询vm的各种问题了 1top -H -p pid 查看磁盘和内存相关查看内存使用12free -h-h或--human-readable 以K，M，G为单位，提高信息的可读性。 查看各分区使用情况12df -h-h或--human-readable 以K，M，G为单位，提高信息的可读性。 查看指定目录的大小123du -sh-s或--summarize 仅显示指定目录或文件的总大小，而不显示其子目录的大小。-h或--human-readable 以K，M，G为单位，提高信息的可读性。 查看内存总量1grep MemTotal /proc/meminfo 查看空闲内存量1grep MemFree /proc/meminfo 查看所有分区1fdisk -l JVM 参数堆内存 1234567891011-Xms2G -Xmx5G-XX:NewSize=256m-XX:MaxNewSize=1024m-XX:PermSize=N #方法区 (永久代) 初始大小-XX:MaxPermSize=N #方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen-XX:MetaspaceSize=N #设置 Metaspace 的初始大小（是一个常见的误区，后面会解释）-XX:MaxMetaspaceSize=N #设置 Metaspace 的最大大小 垃圾回收器1234-XX:+UseSerialGC-XX:+UseParallelGC-XX:+UseParNewGC-XX:+UseG1GC GC日志可以通过在java命令种加入参数来指定对应的gc类型，打印gc日志信息并输出至文件等策略。 123456-XX:+PrintGC 输出GC日志-XX:+PrintGCDetails 输出GC的详细日志-XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式）-XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800）-XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息-Xloggc:../logs/gc.log 日志文件的输出路径 ​ 处理 OOM1234-XX:+HeapDumpOnOutOfMemoryError-XX:HeapDumpPath=./java_pid&lt;pid&gt;.hprof-XX:OnOutOfMemoryError=\"&lt; cmd args &gt;;&lt; cmd args &gt;\"-XX:+UseGCOverheadLimit 讲一下JVM调优过程？https://zhuanlan.zhihu.com/p/488615913 分析和定位当前系统的瓶颈 1）CPU指标 123456// 显示系统各个进程的资源使用情况top// 查看某个进程中的线程占用情况top -Hp pid// 查看当前 Java 进程的线程堆栈信息jstack pid 2）JVM 内存指标 123456// 查看 Java 进程的配置信息，包括系统属性和JVM命令行标志jinfo pid// 输出 Java 进程当前的 gc 情况jstat -gc pid// 输出 Java 堆详细信息jmap -heap pid 制订优化方案 代码bug：升级修复bug。典型的有：死循环、使用无界队列。 不合理的JVM参数配置：优化 JVM 参数配置。典型的有：年轻代内存配置过小、堆内存配置过小、元空间配置过小。 对比优化前后的指标，统计优化效果 OOM分析、排查堆内存不足1java.lang.OutOfMemoryError: Java heap space 原因 存在大对象分配 存在内存泄漏 解决方法 检查是否有大对象分配，最有可能的是大数组分配 通过jmap 命令，把堆内存 dump 下来，用 mat 工具分析，检查是否存在内存泄露问题 如果没有找到明显的内存泄露，使用 -Xmx 加大最大堆内存 永久代/元空间溢出12java.lang.OutOfMemoryError: PermGen spacejava.lang.OutOfMemoryError: Metaspace 原因 在 java7 之前（字符串常量池还在永久代），频繁使用 String.intern方法 反射类加载、动态代理生成的类加载 解决方法 检查是否空间设置太小，使用 -XX:MetaspaceSize和-XX:MaxMetaspaceSize 加大堆内存 检查代码里面是否有大量反射操作 利用 jmap 命令，dump内存信息，利用 mat 检查是否存在大量代理类 CPU 飙升分析、排查定位问题 top命令查看最耗CPU的进程（输入top命令后键入P，进程按照CPU从高到底排序) top -Hp 进程 id 查看该进程中最耗CPU的线程 将线程号转为16进制printf '%x\\n' 线程号 查看线程在干什么jstack 进程号 | grep 线程号 jmap dump 下堆内存信息 问题分析 内存消耗大，导致 full gc 次数太多 通过：jstack命令可以看到这些线程主要是垃圾回收线程 解决：是否生成大量对象 是否代码有问题 通过：jstack 命令，可直接定位到代码行。是否存在无限循环递归问题 死锁 通过：jstack 命令，会打印出业务死锁的位置","link":"/2024/03/12/JVM/"},{"title":"操作系统","text":"进程和线程进程有哪几种状态创建、就绪、执行、阻塞、结束 java 线程有哪几种状态new、runnable、blocked、waiting、time_waiting、terminated 区别定义： 进程：进程是程序在计算机中的一次执行活动，是操作系统进行资源分配和调度的基本单位。 线程：线程是进程的执行单元，是操作系统能够进行运算调度的最小单位。 资源开销： 进程：每个进程都有独立的地址空间和系统资源，因此创建和销毁进程的开销较大。 线程：线程共享相同的地址空间和系统资源，因此创建和销毁线程的开销较小。线程切换的开销也比进程切换的开销小，因为线程之间共享相同的地址空间。 通信方式： 进程：进程间通信需要借助于操作系统提供的进程间通信（IPC）机制，例如管道、消息队列、信号量、共享内存、套接字等。 线程：线程间通信可以直接通过共享内存进行，因为它们共享相同的地址空间，也可以通过同步机制（如互斥锁、条件变量）进行通信。 并发性： 进程：进程是独立的执行单位，不同进程之间的执行互不干扰，具有较强的隔离性。 线程：线程共享相同的地址空间，可以更方便地进行数据共享和通信，但也增加了线程间竞争和同步的复杂性。 进程的通信方式 管道 匿名管道，通信范围是存在父子关系的进程 命名管道，可以在不相关的进程间也能相互通信 不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，存在通信方式是效率低的问题 消息队列。消息队列是保存在内核中的消息链表，但存在用户态与内核态之间的数据拷贝开销的问题 共享内存。拿出一块虚拟地址空间来，映射到相同的物理内存中，但存在多进程竞争共享资源问题。 信号量。其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据。 互斥信号量： 同步信号量： 信号，对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。信号是进程间通信机制中唯一的异步通信机制，因为可以在任何时候发送信号给某一进程。 过程：用kill函数发送信号，在接收进程里，通过signal函数调用sighandler，来启动对应的函数处理信号消息 Socket，不仅可以跨网络与不同主机的进程间通信，还可以在同主机上进程间通信 本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是绑定一个本地文件，这也就是它们之间的最大区别。 线程的通信方式锁机制：包括互斥锁、条件变量、读写锁 互斥锁提供了以排他方式防止数据结构被并发修改的方法。 读写锁允许多个线程同时读共享数据，而对写操作是互斥的。 条件变量可以以原子的方式阻塞进程，直到某个特定条件为真为止。对条件的测试是在互斥锁的保护下进行的。条件变量始终与互斥锁一起使用。 信号量机制(Semaphore)：包括无名线程信号量和命名线程信号量 信号机制(Signal)：类似进程间的信号处理 僵尸进程和孤儿进程的区别？僵尸进程：一个子进程退出时会处于 ZOMBIE 状态，此时它占用的进程描述符没有被释放，只有等它的父进程调用 wait() 或 waitpid() 获取到子进程的信息后，最后由父进程决定是否将子进程资源释放。如果子进程的资源由于某种原因一直得不到释放，那么就一直处于僵死状态，变成了僵尸进程。 孤儿进程：当父进程退出了，但是它的子进程还没有退出，这些子进程就变成了孤儿进程。孤儿进程只是暂时的，系统会在父进程退出时启动寻父机制，为子进程找到一个新的父亲：首先在当前进程组中寻找，如果找不到就会返回 init (PID=1) 进程作为父进程。 系统中如果驻留大量的僵死进程是危险的，因为会一直占用系统资源，解决的直接办法就是杀死父进程，让他们变成孤儿进程，最后会被新的进程领养，新的父进程会例行调用 wait() 来检查子进程状态，清除相关的僵死进程。 进程是怎么调度的？批处理系统： 先来先服务(FCFS) 短作业优先 交互式系统： 时间片轮转调度 优先级调度 实时系统： 软实时和硬实时。前者可以容忍一定时间的延迟，而后者需要满足绝对的截止时间 进程有哪几种状态，他们是如何转换的？进程主要有三种状态：运行态、就绪态、阻塞态。 进程和线程的创建方式？Linux 中进程的创建主要是通过 fork 系统调用，线程被当做一种特殊的进程，也是用 fork 创建，不过通过传递不同的参数，指明共享父进程的地址空间，打开的文件等资源。 Windows 创建进程执行的是 CreateProcess，而线程创建是 Pthread_create 子进程创建时会拷贝父进程哪些资源？Linux系统中，子进程的创建不会马上拷贝父进程的所有资源，而是以只读的方式共享大部分父进程的资源，当需要修改地址空间资源时，触发只读保护，这时才会拷贝一份地址空间。这种机制叫做 **写时拷贝(copy-on-write)**。这种优化可以避免拷贝大量根本不会使用到的数据。 fork 系统调用实际上只是为子进程创建一个唯一的进程描述符，分配了一个有效的 PID，有的 Linux 系统 fork 调用也会复制一份父进程的页表。 进程上下文切换和线程上下文切换进程上下文切换不仅需要保存虚拟内存、全局变量、文件描述符等用户空间资源，还需要保存内核堆栈、寄存器、程序计数器等内核资源。 线程上下文切换只需要保存自己的线程栈和寄存器内容，比进程切换开销小很多。 什么是系统调用？为什么要有系统调用？系统调用是在一个进程中，由用户态切换到内核态，在内核中执行任务，或者申请操作系统的资源。系统调用是一种保护操作系统的机制，它提供一系列定义良好的 API 接口来和操作系统交互，避免用户程序直接对内核进行操作，保证了系统的稳定、安全、可靠。 内核态和用户态是什么这两种状态其实对应着应用程序访问资源的权限：在用户态只能访问受限的资源，如虚拟内存，全局变量等，而要访问内核等资源需要通过系统调用等方式陷入到内核中；内核态可以访问操作系统的所有资源，包括内存、I/O 等资源。 如何实现进程同步？ 信号量：是一个整型变量，用来实现计数器功能，主要提供 down 和 up 操作（即 P 和 V 操作），这两个操作都是原子性的。当执行 down 操作使信号量值变为 0 时，会导致当前进程睡眠，而执行 up 操作 +1 时，会同时唤醒一个进程。 管程：管程是由一个过程、变量和数据结构组成的一个集合，把需要控制的那部分代码独立出来执行，它有一个重要的特性，同一时刻在管程中只能有一个活跃的进程。为了避免一个进程一直占用管程，引入了条件变量和 wait 和 signal 操作。当发生当前进程无法运行时，执行 wait 操作，将当前进程阻塞，同时调入在管程外等待的另一进程执行，而另一个进程满足条件变量时，会执行 signal 操作将正在睡眠的进程唤醒，然后马上退出管程。 硬中断&amp;&amp;软中断中断处理程序的上部分和下半部可以理解为： 上半部直接处理硬件请求，也就是硬中断，主要是负责耗时短的工作，特点是快速执行； 下半部是由内核触发，也就说软中断，主要是负责上半部未完成的工作，通常都是耗时比较长的事情，特点是延迟执行； 软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU 锁（内核里常用的一种锁）等。 虚拟内存物理内存和虚拟内存的区别 内存空间：物理内存是实际存在的计算机内存，又计算机硬件管理。虚拟内存是一个抽象概念，使用硬盘空间来模拟物理内存，以扩展可用内存空间。 访问速度：物理内存的访问速度非常快，通常只需要几纳秒。虚拟内存的访问速度相对较慢，通常需要几毫秒。 大小限制：物理内存的大小通常是固定的，取决于计算机硬件的配置。虚拟内存的大小通常是可变的，取决于操作系统的配置和需要。 如何转变，机制是什么操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存 内存分段和内存分页：段表、页表 为什么需要虚拟内存 让进程的运行内存超过物理内存大小，由于局部性原理，对于没有进程使用的内存可以换出物理内存。 每个进程都有自己的页表，即各自的虚拟内存空间是独立的，解决了多进程之间地址冲突问题。 分段和分页区别 段则是信息的逻辑单位，分成了栈段、堆段、数据段、代码段等不同属性的段。分段的目的是为了能更好地满足用户的需要。页是信息的物理单位，分页是为实现离散分配方式，提高内存的利用率。 页的大小固定，且由系统决定；而段的长度却不固定，决定于用户所编写的程序 分页的地址空间是一维的，程序员只需利用一个记忆符，即可表示一个地址；而分段的作业地址空间是二维的，程序员在标识一个地址时，既需给出段名，又需给出段内地址 为什么要分段分页 分段是将程序分为具有逻辑意义的不同大小的段，方便实现信息的共享与保护。 分页是为了解决分段粒度大，因为段需要整段的加载进内存以及整段换出，造成内存碎片大，不易于管理（不会产生外部碎片） 分段优点： 将程序分为具有逻辑意义的不同大小的段，方便实现信息的共享与保护 分段缺点： 外部内存碎片 内存交换的效率低（段太大，导致内存交换代价高） 分页优点： 利用率高，解决外部碎片，但是存在内部碎片 避免整段换出 分页缺点： 不好实现信息的共享与保护 分页：分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小 页表：虚拟地址与物理地址之间通过页表来映射 多级页表 页表一定要覆盖全部虚拟地址空间 如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表 I/O 多路复用：select/poll/epollselect、poll 和 epoll 是用于在 Unix/Linux 系统中进行 I/O 复用的系统调用。它们的目的都是为了在一个进程中同时监视多个文件描述符的可读、可写和异常事件，以便实现高效的 I/O 操作。它们的主要区别如下： select： 使用一个位图（bitmap）来表示文件描述符集合，文件描述符数量有限。 效率较低，每次调用 select 函数都需要将文件描述符集合从用户态拷贝到内核态，通过轮询的方式去判断该 socket 是否有事件发生，若有则标记为可读或可写，然后内核再将就绪的文件描述符集合从内核态拷贝到用户态，再去通过轮询去找到哪些 socket 可读或者可写，开销较大。 poll： 使用动态数组来表示文件描述符集合，支持的文件描述符数量也有限。 相对于 select，poll 的效率略微提高，但也是轮询查看是否就绪 epoll： 用法：先用epoll_create 创建一个 epoll对象 epfd可以支持非常大的文件描述符集合（上限就为系统定义的进程打开的最大文件描述符个数），再通过 epoll_ctl 将需要监视的 socket 添加到epfd中，最后调用 epoll_wait 等待数据。 epoll 通过两个方面，很好解决了 select/poll 的问题。 第一点不用来回复制，epoll 在内核里使用红黑树来跟踪进程所有待检测的文件描述符，把需要监控的 socket 通过 epoll_ctl() 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)。 select/poll 每次操作时都传入整个 socket 集合给内核，而 epoll 因为在内核维护了红黑树，可以保存所有待检测的 socket ，所以只需要传入一个待检测的 socket 第二点不用轮询判断， epoll 使用事件驱动的机制，内核里通过rdllist双向链表来记录就绪事件，当某个 socket 有事件发生时，通过回调函数内核会将其加入到rdlist中，当用户调用 epoll_wait() 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。 epoll 支持边缘触发和水平触发的方式，而 select/poll 只支持水平触发，一般而言，边缘触发的方式会比水平触发的效率高。 Socket 编程过程 服务端首先调用 socket() 函数，创建网络协议为 IPv4，以及传输协议为 TCP 的 Socket 接着调用 bind() 函数，给这个 Socket 绑定一个 IP 地址和端口，目的是 绑定端口的目的：通过 tcp端口号，来找到对应的应用程序 绑定 IP 地址的目的：一台机器是可以有多个网卡的，每个网卡都有对应的 IP 地址 绑定完 IP 地址和端口后，就可以调用 listen() 函数进行监听，此时对应 TCP 状态图中的 listen 服务端进入了监听状态后，通过调用 accept() 函数，来从内核获取客户端的连接，如果没有客户端连接，则会阻塞等待客户端连接的到来。 客户端在创建好 Socket 后，调用 connect() 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后万众期待的 TCP 三次握手就开始了。 当 TCP 全连接队列不为空后，服务端的 accept() 函数，就会从内核中的 TCP 全连接队列里拿出一个已经完成连接的 Socket 返回应用程序，后续数据传输都用这个 Socket。 连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 read() 和 write() 函数来读写数据。 select，poll，epoll都是IO多路复用机制，即可以监视多个描述符，一旦某个描述符就绪（读或写就绪），能够通知程序进行相应读写操作。 但select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间。 select，poll实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select和poll在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。这就是回调机制带来的性能提升。 select，poll每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。 死锁死锁是指在多线程或多进程并发执行的过程中，由于互相持有对方所需的资源，并且同时等待对方释放资源，导致所有涉及的线程或进程都无法继续执行下去，陷入无限等待的状态。 互斥、不可剥夺、请求保持、循环依赖 解决方案： 主要有一下三种方法： 死锁防止 死锁避免 死锁检测和恢复 死锁防止 破坏互斥：使资源同时访问而非互斥使用 破坏请求保持：采用静态分配的方式，静态分配的方式是指进程必须在执行之前就申请需要的全部资源，且直至所要的资源全部得到满足后才开始执行。 破坏循环等待条件：按序申请、按序释放 死锁避免 银行家算法（Banker’s algorithm）用于避免系统资源分配中的死锁问题，它属于破坏死锁的第四个条件，即破坏循环等待条件。 当一个进程申请使用资源的时候，银行家算法通过先 试探 分配给该进程资源，然后通过安全性算法判断分配后的系统是否处于安全状态，若不安全则试探分配作废，让该进程继续等待。","link":"/2024/03/12/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"title":"MySQL","text":"综合题慢查询的原因。怎么定位慢查询，怎么优化慢查询，思路是什么。原因 索引失效 连接查询没有用到索引，执行了笛卡尔查询 大数据量排序与分组 复杂的子查询 表结构设计问题 事务锁竞争 定位 慢查询日志。MySQL的慢查询日志会记录执行时间超过long_query_time 的SQL语句 show processlist。实时展示当前MySQL正在执行的线程,可以通过processlist来发现一些状态显示为Lock等的慢查询。 优化 加索引 SQL 语句优化。去除无效查询条件,优化Join查询,避免全表扫描等。 优化表结构。去除冗余字段,拆分过大的表,适当垂直拆分或者水平拆分表。 使用视图。将复杂查询封装为视图,以简化查询过程。 加缓存优化数据统计类的慢查询 部署读写分离架构,主库负责写,从库负责读,分散数据库压力。 使用 explain 优化你的 mysql 性能Explain 特点 explain 返回的结果是以表为粒度的，每个表输出一行，这里的表示广义上的表，可以是一个子查询，也可以是一个 UNION 后的结果。 explain 只能解析 Select 查询，对于 update，insert 等都不支持，我们可以使用 select 来模拟 update 操作近似获取 update 的执行过程 Explain 中的列idSELECT 查询的标识符. 每个 SELECT 都会自动分配一个唯一的标识符. select_typeselect_type 表示了查询的类型, 它的常用取值有: SIMPLE：表示此查询不包含 UNION 查询或子查询 SUBQUERY：包含在 Select 列表中的子查询，也就是不在 FROM 子句中的子查询 DERIVED：表示包含在 From 子句中的 Select 查询 UNION：表示此查询是 UNION 的第二和随后的查询 UNION RESULT： 从 UNION 匿名临时表检索结果的 SELECT PRIMARY, 表示此查询是最外层的查询 DEPENDENT UNION： UNION 中的第二个或后面的查询语句, DEPENDENT 意味着 Select 依赖于外层查询中发现的数据 DEPENDENT SUBQUERY: 包含在 Select 列表中的子查询， 但子查询依赖于外层查询的结果. table查询的是哪个表，mysql 查询优化器执行的关联顺序并不和我们写 SQL 时关联的顺序一致，下面我们讲一下 Mysql 是如何对关联查询作优化的： Mysql 查询执行计划总是按照左侧深度优先树的规则去执行，也就是从一个表开始一直嵌套循环，并不会类似平衡二叉树一样两个分支同时执行 在多表关联时，可以通过多种不同的关联顺序获取相同的执行结果，查询优化器会评估不同的顺序选择一个代价最小的关联查询 如果你不想要优化器改变你的关联顺序，可以使用 STRAIGHT JOIN 关键字强制使用你的关联顺序去执行 如果关联表特别多时，超过 optimizer_search_depth 的限制时，优化器评估每一种关联顺序的执行成本太高，这时会选择“贪婪”的搜索模式 typetype 字段比较重要, 它提供了判断查询是否高效的重要依据依据. 通过 type 字段, 我们判断此次查询是全表扫描还是索引扫描等，type 类型的性能比较，通常来说, 不同的 type 类型的性能关系如下: ALL &lt; index &lt; range &lt; ref &lt; eq_ref &lt; const &lt; system &lt; NULL NULL：这种访问意味着 Mysql 能在优化阶段分解查询语句，在执行阶段不需要访问表或者索引 system: 预先知道整个表中只有一条数据. 这个类型是特殊的 const 类型 12# 因为表中backend_user是主键，所以子查询里最多可以选出一条数据，所以最外层查询的type是system，里层查询的type是constexplain select * from (select * from backend_user where id = 1) a; const：针对主键或唯一索引的等值查询扫描, 最多只返回一行数据. const 查询速度非常快, 因为它仅仅读取一次即可 eq_ref: 此类型通常出现在多表的 join 查询, 表示对于前表的每一个结果, 都只能匹配到后表的一行结果. 并且查询的比较操作通常是 =, 查询效率较高. ref：此类型通常出现在多表的 join 查询, 针对于非唯一或非主键索引, 或者是使用了最左前缀规则索引的查询，可能会查询出多个值 range: 表示使用索引范围查询, 通过索引字段范围获取表中部分数据记录. 这个类型通常出现在 =, &lt;&gt;, &gt;, &gt;=, &lt;, &lt;=, IS NULL, &lt;=&gt;, BETWEEN, IN() 操作中。但是对于同样的 type = range 的查询，性能上还是有区别的： 1234# 虽然是都是范围查询，其实第二个查询时多个等值条件查询# 对于第一个查询，mysql 无法再使用该列后面的其它查询索引了，而第二个则可以继续使用索引select id from actor where id &gt; 45 and class_id = 3;select id from actor where id in (44, 47, 48) and class_id = 3; index: 表示通过索引进行全表扫描和 ALL 类型类似, 有点是避免了排序，确定是需要承担按照索引次序读取表的开销。如果 Extra 列中出现了 “Using index” 表明是用了覆盖索引，此时开销非常小。 ALL: 表示全表扫描, 这个类型的查询是性能最差的查询之一。一般情况下都会从头到尾扫描所有行，除非使用了 Limit 或者 Extra 列中显示 “Using distinct/not exists”。 possible_keys此次查询中可能选用的索引，这些索引列是根据查询的列以及比较操作符来判断的，可能在后续的真实查询中没有用到也有可能 key此次查询中确切使用到的索引，如果在 possible_keys 中没有出现而在 key 中出现，说明优化器可能出于另外原因比如选择覆盖索引，所以 possiable_keys 揭示了哪一个索引有助于高效进行查找，而 key 显示了采用哪一个索引可以最小化查询成本。 key_len表示查询优化器使用了索引的字节数. 这个字段可以评估组合索引是否完全被使用, 或只有最左部分字段被使用到，比如我们建了一个组合索引(col1, col2)，那么如下两条查询虽然用到的都是这个组合索引，但是对应的key_len的只是不一样的。key_len 显示了在索引字段中可能的最大长度，而不是数据使用的实际字节数 12select * from table1 where col1 = 1;select * from table1 where col1 = 1 and col2 = 2; ref这一列显示了之前的表在 key 列记录的索引中查找值所用的列或者常量 rowsrows 也是一个重要的字段. MySQL 查询优化器根据统计信息, 估算 SQL 要查找到结果集需要扫描读取的数据行数。这个值非常直观显示 SQL 的效率好坏, 原则上 rows 越少越好。 通过把每个表的 rows 值相乘可以粗略的估算出整个查询要检查的行数 这个值只是一个估算的值，不是实际查出来的值 filteredfiltered 是在 MYSQL 5.1 中加进来的，在使用 EXPLAIN EXTENDED 时出现，表示此查询条件所过滤的数据的百分比，将 rows 除以 filtered 可以估算出整个表数据行数。 ExtraEXplain 中的很多额外的信息会在 Extra 字段显示, 常见的有以下几种内容: Using filesort 当 Extra 中有 Using filesort 时, 表示 MySQL 需额外的排序操作, 不能通过索引顺序达到排序效果. 一般有 Using filesort, 都建议优化去掉, 因为这样的查询 CPU 资源消耗大。 但是 Explain 不会告诉你 Mysql 将使用文件排序还是内存排序： 1234-- 比如我们建立索引为：KEY `user_product_detail_index` (`user_id`, `product_name`, `productor`)，那么如下两个查询EXPLAIN SELECT * FROM order_info ORDER BY product_name； -- Using filesort，不能通过索引进行排序，需要优化EXPLAIN SELECT * FROM order_info ORDER BY user_id, product_name；-- 无 Using filesort，通过索引进行排序，优化成功 Using index “覆盖索引扫描”, 表示查询在索引树中就可查找所需数据, 不用扫描表数据文件, 往往说明性能不错 Using where 这意味着 Mysql 服务器在存储引擎检索行后再进行过滤，一般出现 “Using where” 会受益于不同的索引 Using temporary 查询有使用临时表, 一般出现于排序, 分组和多表 join 的情况, 临时表可能是内存临时表或者文件临时表 基础执行一条 SQL 查询语句，期间发生了什么？ 连接器：建立连接、管理链接、校验个人身份 查询缓存：key-value形式——查询语句是否命中 解析器：词法解析、语法解析，建立语法树 执行 SQL： 预处理阶段：判断是否存在表面、字段名；将*替换成全部列 优化阶段：基于查询成本，选择最佳的执行计划 执行阶段：根据执行计划执行 SQL查询语句，从存储引擎读取记录，返回给客户端 MySQL 一行记录是怎么存储的？ MySQL 的 NULL 值是怎么存放的？ MySQL 的 Compact 行格式中会用「NULL值列表」来标记值为 NULL 的列，NULL 值并不会存储在行格式中的真实数据部分。 NULL值列表会占用 1 字节空间，当表中所有字段都定义成 NOT NULL，行格式中就不会有 NULL值列表，这样可节省 1 字节的空间。 MySQL 怎么知道 varchar(n) 实际占用数据的大小？ MySQL 的 Compact 行格式中会用「变长字段长度列表」存储变长字段实际占用的数据大小。 varchar(n) 中 n 最大取值为多少？ 一行记录最大能存储 65535 字节的数据，但是这个是包含「变长字段字节数列表所占用的字节数」和「NULL值列表所占用的字节数」。所以， 我们在算 varchar(n) 中 n 最大值时，需要减去这两个列表所占用的字节数。 如果一张表只有一个 varchar(n) 字段，且允许为 NULL，字符集为 ascii。varchar(n) 中 n 最大取值为 65532。 计算公式：65535 - 变长字段字节数列表所占用的字节数 - NULL值列表所占用的字节数 = 65535 - 2 - 1 = 65532。 如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 &lt;= 65535。 行溢出后，MySQL 是怎么处理的？ 如果一个数据页存不了一条记录，InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。 Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。 Compressed 和 Dynamic 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。 为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？1、B+Tree vs B Tree B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。 B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树无法做到这一点。 B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），这些冗余索引让 B+ 树在插入、删除的效率都更高，比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化； 2、B+Tree vs 二叉树 对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。 在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 34 层左右，也就是说一次数据查询操作只需要做 34 次的磁盘 I/O 操作就能查询到目标数据。根节点可以包含的关键字数量范围是 [2, m-1]。 非根节点至少包含m/2个关键字，至多包含m-1个关键字。 而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。 3、B+Tree vs Hash Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。 但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。 多表查询（各种join连接详解）A）内连接：join=inner join B）外连接：left join=left outer join，right join=right outer join，union C）交叉连接：cross join 2.1 内连接（只有一种场景） 123select a.*, b.* from tablea ainner join tableb bon a.id = b.id 这种场景下得到的是满足某一条件的A，B内部的数据；正因为得到的是内部共有数据，所以连接方式称为内连接。 2.2 外连接（六种场景） 2.2.1 left join 或者left outer join(等同于left join) 123select a.*, b.* from tablea aleft outer join tableb bon a.id = b.id 这种场景下得到的是A的所有数据，和满足某一条件的B的数据; 2.2.2 [ left join 或者left outer join(等同于left join) ] + [ where B.column is null ] 1234select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idWhere b.id is null left join表a的数据全部显示，匹配表b的数据也显示，而b.id再次过滤掉 表b的id为空的。 2.2.3 right join 或者right outer join(等同于right join) 123select a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.id 2.2.5 full join （mysql不支持，但是可以用 left join union right join代替） 1234567select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idunionselect a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.id 这种场景下得到的是满足某一条件的公共记录，和独有的记录 2.2.6 full join + is null（mysql不支持，但是可以用 （left join + is null） union （right join+is null代替） 123456789select a.id aid,a.age,b.id bid,b.name from tablea aleft join tableb bon a.id = b.idwhere b.id is nullunionselect a.id aid,a.age,b.id bid,b.name from tablea aright join tableb bon a.id = b.idwhere a.id is null 2.3 交叉连接 （cross join） 2.3.1 实际应用中还有这样一种情形，想得到A，B记录的排列组合，即笛卡儿积，这个就不好用集合和元素来表示了。需要用到cross join： 12select a.id aid,a.age,b.id bid,b.name from tablea across join tableb b 2.3.2 还可以为cross join指定条件 （where）： 123select a.id aid,a.age,b.id bid,b.name from tablea across join tableb bwhere a.id = b.id 这种情况下实际上实现了内连接的效果 MySql 数据的存储结构聚簇索引（Innodb）和非聚簇索引（myISAM） 对于InnoDB引擎来说，是按照聚簇索引的形式存储数据 对于MyISAM引擎来说，是按照非聚簇索引的形式存储数据： 存储引擎是InnoDB, 在data目录下会看到2类文件：.frm、.ibd（1）.frm–表结构的文件。（2）.ibd–表数据文件 存储引擎是MyISAM, 在data目录下会看到3类文件：.frm、.myi、.myd（1）.frm–表定义，是描述表结构的文件。（2）.MYD–”D”数据信息文件，是表的数据文件。（3）.MYI–”I”索引信息文件，是表数据文件中任何索引的数据树 聚簇索引和非聚簇索引的存储方式区别： 在MyISAM引擎索引和数据是分开存储的，而InnoDB是索引和数据是一起以idb文件的形式进行存储的。 在访问速度上，聚簇索引比非聚簇索引快。非聚簇索引需要先查询一遍索引文件，得到索引，跟据索引获取数据。而聚簇索引的索引树的叶子节点的直接指向要查找的数据行。 MyISAM 和 InnoDB 有什么区别？ 事务支持：MyISAM 不提供事务支持。InnoDB 提供事务支持，实现了 SQL 标准定义了四个隔离级别。 MyISAM 不支持数据库异常崩溃后的安全恢复，而 InnoDB 支持。 表锁差异：InnoDB 支持行级别的锁粒度，MyISAM 不支持，只支持表级别的锁粒度 读写过程：MyISAM在读写过程中相互阻塞；InnoDB读写阻塞与事务隔离级别相关 读写性能：MyISAM读取性能优越，但是写入性能差（如果执行大量的select，MyISAM是更好的选择）；InnoDB写入性能较强（如果执行大量的insert或者update，InnoDB是更好的选择） 外键支持：MyISAM 不支持外键，而 InnoDB 支持。 存储结构：MyISAM 不支持聚簇索引（数据保存在连续内存中，主键索引：主键列值+行号，二级索引：索引列值+行号）；InnoDB 的数据是存储在主键索引==聚簇索引中 innoDB 比 MyISAM 好在哪？事务支持 表锁差异 外键支持 数据库异常崩溃后的安全恢复 MyISAM 适合什么场景读写性能：MyISAM读取性能优越，但是写入性能差（如果执行大量的select，MyISAM是更好的选择）；InnoDB写入性能较强（如果执行大量的insert或者update，InnoDB是更好的选择） InnoDB的底层结构InnoDB的底层结构主要由2部分组成：内存结构和磁盘结构。 2、内存结构由缓冲池（Buffer Pool），写缓冲（Change Buffer），日志缓冲（ Log Buffer），自适应hash索引（Adaptive Hash Index）组成。3、缓冲池（Buffer Pool）主要是缓存表数据与索引数据，加快访问速度。内部采用基于LRU算法的变体算法来管理缓存对象。4、写缓冲（Change Buffer）主要是缓存辅助索引的更新操作，加快辅助索引的更新速度。5、日志缓冲（ Log Buffer）使大型事务可以运行，而无需在事务提交之前将redo日志数据写入磁盘，节省了磁盘I/O。注意事务提交时刷redo log有三种策略。 三大范式第一范式（1NF)：每列的原子性【不可再分】 第二范式（2NF)：确保表中每列与主键相关，而不能只与主键的某部分相关【消除部分函数依赖】 第三范式（3NF): 属性不依赖于其他非主属性【消除传递依赖】 索引联合索引范围查询联合索引的最左匹配原则，在遇到范围查询（如 &gt;、&lt;）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。注意，对于 &gt;=、&lt;=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配 索引下推 在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个回表，到「主键索引」上找出数据行，再对比 b 字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 什么时候需要 / 不需要创建索引？需要： 字段唯一；经常用于where查询、 groupby条件和 orderby条件 的字段 不需要：区分度小的；数据少的；经常更新的 设置索引的时候会考虑哪些方面设计原则 针对你的SQL语句里的where条件、order by条件以及group by条件去设计索引 where条件里是要根据哪些字段来筛选数据？order by要根据哪些字段来排序？group by要根据哪些字段来分组聚合 考虑方向： （1）字段基数，尽量选择值多的，发挥 B+树快速二分查找的优势。判断方式——select count(distinct(column_name))/count(*) from table_name; 超过了0.5就适合做索引，越接近1越适合 （2）选字段类型小的列，占用磁盘空间小，查询性能好 （3）频繁更新的字段不适合做主键。主键一定是自增的而且业务无关，防止聚簇索引频繁页分裂 （4）查询语句不加函数，不然回走全表扫描 根据这些字段设计一个或者两个联合索引 哪些字段要放到联合索引中去？在联合索引里，字段的顺序要怎么排列呢？ （1）尽量让每一个联合索引都尽量去包含上你的where、order by、group by里的字段。 （2）字段顺序要符合最左匹配原则：仔细审查每个SQL语句，是不是每个where、order by、group by后面跟的字段顺序，都是某个联合索引的最左侧字段开始的部分字段。 （3）先让联合索引最左侧开始的多个字段使用等值匹配，最最后一个字段使用范围匹配 有什么优化索引的方法？ 前缀索引优化； 提高索引的查询速度；减小索引项的大小 覆盖索引优化； 不需要查询出包含整行记录的所有信息，也就减少了大量的 I/O 操作。 主键索引最好是自增的； 避免页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。 防止索引失效； 使用左或者左右模糊匹配的时候，也就是 like %xx 或者 like %xx%这两种方式 在查询条件中对索引列做了计算（+ - * /）、函数、类型转换操作 联合索引要能正确使用需要遵循最左匹配原则，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。 在 WHERE 子句中，or 的前后字段都要上索引 列和列对比，这种情况会被认为还不如走全表扫描。 存在NULL值条件(is null/ is not null)，索引中无法存储NULL值，所以where条件判断如果对字段进行了NULL值判断（is NULL/ is not null），则数据库放弃索引而进行全表查询 优化案例SELECT * FROM t_order ORDER BY text LIMIT 1000000, 101、使用覆盖索引 SELECT id, ‘text’ FROM t_order ORDER BY text LIMIT 1000000, 10 因为实际开发中，用SELECT查询一两列操作是非常少的，因此上述的覆盖索引的适用范围就比较有限。 2、子查询优化 可以通过把分页的SQL语句改写成子查询的方法获得性能上的提升。 SELECT * FROM t_order where id&gt;=(select id from t_order order by text limit 1000000, 1) LIMIT 10 但是这种优化方法也有局限性：这种写法，要求主键ID必须是连续的 3、延迟关联 和上述的子查询做法类似，我们可以使用JOIN，先在索引列上完成分页操作，然后再回表获取所需要的列。 SELECT a.* FROM t_order as a inner join ( select id from t_order order by text limit 1000000, 10) as b on a.id = b.id 4、记录上次查询结束的位置 和上面使用的方法都不同，记录上次结束位置优化思路是使用某种变量记录上一次数据的位置，下次分页时直接从这个变量的位置开始扫描，从而避免MySQL扫描大量的数据再抛弃的操作。 SELECT * FROM t_order where id&gt;=1000000 LIMIT 10 聚簇索引&amp;二级索引聚簇索引是一种特殊类型的索引，它定义了表中数据的物理排序方式。在聚簇索引中，表中的行按照索引的顺序存储在磁盘上。 如果叶子节点存储的是实际数据的就是聚簇索引，一个表只能有一个聚簇索引；如果叶子节点存储的不是实际数据，而是主键值则就是二级索引，一个表中可以有多个二级索引。 在使用二级索引进行查找数据时，如果查询的数据能在二级索引找到，那么就是「索引覆盖」操作，如果查询的数据不在二级索引里，就需要先在二级索引找到主键值，需要去聚簇索引中获得数据行，这个过程就叫作「回表」。 MySQL 使用 like “%x“，索引一定会失效吗？从这个思考题我们知道了，使用左模糊匹配（like “%xx”）并不一定会走全表扫描，关键还是看数据表中的字段。 如果数据库表中的字段只有主键+二级索引，那么即使使用了左模糊匹配，也不会走全表扫描（type=all），而是走全扫描二级索引树(type=index)。 count(*) 和 count(1) 有什么区别？哪个性能最好？count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。 所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。 再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。 事务事务有哪些特性？原子性（Atomicity） 一个事务中的所有操作，要么全部完成，要么全部不完成 一致性（Consistency） 数据满足完整性约束，数据库保持一致性状态 隔离性（Isolation） 隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致 持久性（Durability） 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？原子性是通过 undo log（回滚日志）; 一致性则是通过持久性+原子性+隔离性来保证； 隔离性是通过 MVCC（多版本并发控制） 或锁机制来保证的； 持久性是通过 redo log （重做日志）来保证的； 并行事务会引发什么问题？脏读如果一个事务「读到」了另一个「未提交事务修改过的数据」，就意味着发生了「脏读」现象。 不可重复读在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。 幻读在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。 事务的隔离级别有哪些？各自的应用场景 读未提交(Read Uncommitted) 特点：事务中的修改操作(INSERT、UPDATE、DELETE)立即生效，无需等待事务提交；事务读取数据时可以读取其他事务未提交的数据。 应用场景：对于一些对数据一致性要求不高的场景，比如读取系统的实时监控数据。 读已提交(Read Committed) 特点：事务中的修改操作需要等待事务提交后才生效；事务读取数据时只能读取其他事务已提交的数据。 应用场景：适用于大部分常规业务场景，能够保证读取的数据具有较高的一致性。 可重复读(Repeatable Read)： 特点：事务中的修改操作需要等待事务提交后才生效；事务读取数据时只能读取事务开始时的快照数据，其他事务对数据的修改不可见。 应用场景：适用于需要保证读取数据一致性的应用，例如订单交易等。 串行化(Serializable)： 特点：事务中的修改操作需要等待事务提交后才生效；事务读取数据时只能读取事务开始时的快照数据，并且其他事务对数据进行了读取和修改的过程中，该数据将被锁定，其他事务无法访问。 应用场景：适用于对数据一致性要求极高的场景，例如金融领域的转账操作。 Read View Read View 中四个字段作用； 聚簇索引记录中两个跟事务有关的隐藏列； trx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里； roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。 MVCC核心包括：undo log、版本链、read view 通过「事务的 Read View 里的事务 id」和「版本链中记录的事务 id」的比对,来控制并发事务访问一个记录的行为叫 MVCC（多版本并发控制）。 在创建 Read View 后，我们可以将记录中的 trx_id 划分这三种情况： 一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况： 如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，可见 如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，不可见 如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：在——不可见；不在——可见 对于「读提交」和「可重复读」隔离级别的事务来说，它们是通过 Read View 来实现的，它们的区别在于创建 Read View 的时机不同： 「读提交」隔离级别是在每个 select 都会生成一个新的 Read View 「可重复读」隔离级别是启动事务时生成一个 Read View 对于幻读现象，不建议将隔离级别升级为串行化。MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象 针对快照读（普通 select 语句），是通过 MVCC 方式解决了幻读 针对当前读（select … for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读 MySQL 可重复读隔离级别，完全解决幻读了吗？场景一： 对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。 场景二： T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id &gt; 100 得到了 3 条记录。 T2 时刻：事务 B 往插入一个 id= 200 的记录并提交； T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id &gt; 100 for update 就会得到 4 条记录，此时也发生了幻读现象。 解决幻读的场景： 事务在处理跟查询数量有关的操作时候。事务在查询某个范围内的订单数量，然后另一个事务插入了一个新的订单，导致第一个事务再次查询时，订单数量增加了。 总结： 所以，MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select … for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。 锁全局锁要使用全局锁，则要执行这条命令： 1flush tables with read lock 执行后，整个数据库就处于只读状态了，这时其他线程执行以下操作，都会被阻塞： 对数据的增删改操作，比如 insert、delete、update等语句； 对表结构的更改操作，比如 alter table、drop table 等语句。 如果要释放全局锁，则要执行这条命令： 1unlock tables 当然，当会话断开了，全局锁会被自动释放。 全局锁应用场景是什么？ 全局锁主要应用于做全库逻辑备份，这样在备份数据库期间，不会因为数据或表结构的更新，而出现备份文件的数据与预期的不一样。 加全局锁又会带来什么缺点呢？ 加上全局锁，意味着整个数据库都是只读状态。 那么如果数据库里有很多数据，备份就会花费很多的时间，关键是备份期间，业务只能读数据，而不能更新数据，这样会造成业务停滞。 既然备份数据库数据的时候，使用全局锁会影响业务，那有什么其他方式可以避免？ 有的，如果数据库的引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作，不会影响备份的数据。 备份数据库的工具是 mysqldump，在使用 mysqldump 时加上 –single-transaction 参数的时候，就会在备份数据库之前先开启事务。这种方法只适用于支持「可重复读隔离级别的事务」的存储引擎。 InnoDB 存储引擎默认的事务隔离级别正是可重复读，因此可以采用这种方式来备份数据库。 但是，对于 MyISAM 这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法。 表级锁 MySQL 表级锁有哪些？具体怎么用的。 MySQL 里面表级别的锁有这几种： 表锁； 表锁除了会限制别的线程的读写外，也会限制本线程接下来的读写操作。 也就是说如果本线程对学生表加了「共享表锁」，那么本线程接下来如果要对学生表执行写操作的语句，是会被阻塞的，当然其他线程对学生表进行写操作时也会被阻塞，直到锁被释放。 元数据锁（MDL）; 我们不需要显示的使用 MDL，因为当我们对数据库表进行操作时，会自动给这个表加上 MDL： 对一张表进行 CRUD 操作时，加的是 MDL 读锁； 对一张表做结构变更操作的时候，加的是 MDL 写锁； MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。 意向锁； 意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables … read）和独占表锁（lock tables … write）发生冲突。表锁和行锁是满足读读共享、读写互斥、写写互斥的。意向锁的目的是为了快速判断表里是否有记录被加锁，从而判断是否能加表锁。 AUTO-INC 锁； 在插入数据时，会加一个表级别的 AUTO-INC 锁，然后为被 AUTO_INCREMENT 修饰的字段赋值递增的值，等插入语句执行完成后，才会把 AUTO-INC 锁释放掉。 什么 SQL 语句会加行级锁？InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁，所以后面的内容都是基于 InnoDB 引擎 的。 普通的 select 语句是不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过 MVCC（多版本并发控制）实现的。 如果要在查询时对记录加行级锁，可以使用下面这两个方式，这两种查询会加锁的语句称为锁定读。 12345//对读取的记录加共享锁(S型锁)select ... lock in share mode;//对读取的记录加独占锁(X型锁)select ... for update; 上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin 或者 start transaction 开启事务的语句。 **除了上面这两条锁定读语句会加行级锁之外，update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)**。 12345//对操作的记录加独占锁(X型锁)update table .... where id = 1;//对操作的记录加独占锁(X型锁)delete from table where id = 1; 共享锁（S锁）满足读读共享，读写互斥。独占锁（X锁）满足写写互斥、读写互斥。 行级锁有哪些种类？不同隔离级别下，行级锁的种类是不同的。 在读已提交隔离级别下，行级锁的种类只有记录锁（解决脏读），也就是仅仅把一条记录锁上。 在可重复读隔离级别下，行级锁的种类除了有记录锁，还有间隙锁（避免幻读），所以行级锁的种类主要有三类： Record Lock，记录锁，也就是仅仅把一条记录锁上； Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；间隙锁之间是兼容的，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的。 Next-Key Lock：Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身。 MySQL 行级锁的加锁规则唯一索引等值查询： 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「记录锁」。 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成「间隙锁」。 非唯一索引等值查询： 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。 当查询的记录「不存在」时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。 唯一索引的范围查询 对每一个扫描到的索引加 next-key 锁 唯一索引在满足一些条件的时候，索引的 next-key lock 退化为间隙锁或者记录锁。 「大于等于」找到满足的记录加上 next-key lock，找到等值的记录会退化成记录锁 「小于等于、小于」出现第一个不匹配的会退化成间隙锁 非唯一索引的范围查询 对每一个扫描到的非唯一索引加 next-key 锁（不会退化），并对匹配的主键索引加记录锁 其实理解 MySQL 为什么要这样加锁，主要要以避免幻读角度去分析，这样就很容易理解这些加锁的规则了。 update 没加索引会锁全表？还有一件很重要的事情，在线上在执行 update、delete、select … for update 等具有加锁性质的语句，一定要检查语句是否走了索引，**==如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了==**，这是挺严重的问题。 解决方案： 打开 MySQL sql_safe_updates 参数 使用 force index([index_name]) 可以告诉优化器使用哪个索引 MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读吗？在 MySQL 的可重复读隔离级别下，针对当前读的语句会对索引加记录锁+间隙锁，这样可以避免其他事务执行增、删、改时导致幻读的问题。 MySQL 死锁了，怎么办？ 避免死锁： 设置事务等待锁的超时时间 开启主动死锁检测 两个事务即使生成的间隙锁的范围是一样的，也不会发生冲突，因为间隙锁目的是为了防止其他事务插入数据，因此间隙锁与间隙锁之间是相互兼容的。 在执行插入语句时，如果插入的记录在其他事务持有间隙锁范围内，插入语句就会被阻塞，因为插入语句在碰到间隙锁时，会生成一个插入意向锁，然后插入意向锁和间隙锁之间是互斥的关系。 如果两个事务分别向对方持有的间隙锁范围内插入一条记录，而插入操作为了获取到插入意向锁，都在等待对方事务的间隙锁释放，于是就造成了循环等待，满足了死锁的四个条件：互斥、占有且等待、不可强占用、循环等待，因此发生了死锁。 日志三种日志： undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的原子性，主要用于事务回滚和 MVCC。 redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的持久性，主要用于掉电等故障恢复； binlog （归档日志）：是 Server 层生成的日志，主要用于数据备份和主从复制； 为什么需要 undo log？执行执行一条“增删改”语句的时候，虽然没有输入 begin 开启事务和 commit 提交事务，但是 MySQL 会隐式开启事务来执行“增删改”语句的，执行完就自动提交事务的，这样就保证了执行完“增删改”语句后，我们可以及时在数据库表看到“增删改”的结果了。 每当 InnoDB 引擎对一条记录进行操作（修改、删除、新增）时，要把回滚时需要的信息都记录到 undo log 里，比如： 在插入一条记录时，要把这条记录的主键值记下来，这样之后回滚时只需要把这个主键值对应的记录删掉就好了； 在删除一条记录时，要把这条记录中的内容都记下来，这样之后回滚时再把由这些内容组成的记录插入到表中就好了； 在更新一条记录时，要把被更新的列的旧值记下来，这样之后回滚时再把这些列更新为旧值就好了。 在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。 undo log 两大作用： 实现事务回滚，保障事务的原子性。事务处理过程中，如果出现了错误或者用户执 行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。 实现 MVCC（多版本并发控制）关键因素之一。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。 刷盘策略 undo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。 buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。 为什么需要 Buffer Pool？Innodb 存储引擎设计了一个缓冲池（Buffer Pool），来提高数据库的读写性能。 有了 Buffer Poo 后： 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。 **为什么需要 redo log ？为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，这个时候更新就算完成了。 后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。 WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。 什么是 redo log？ redo log 是物理日志，记录了某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新，每当执行一个事务就会产生这样的一条或者多条物理日志。 在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。 当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。 被修改 Undo 页面，需要记录对应 redo log 吗？ 需要的。 开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。 不过，在内存修改该 Undo 页面后，需要记录对应的 redo log。 redo log 和 undo log 区别在哪？ 这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于： redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值； undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值； 事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务。 所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 crash-safe（崩溃恢复）。可以看出来， redo log 保证了事务四大特性中的持久性。 redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？ 可以说这是 WAL 技术的另外一个优点：MySQL 的写操作从磁盘的「随机写」变成了「顺序写」，提升语句的执行性能。 针对为什么需要 redo log 这个问题我们有两个答案： 实现事务的持久性，让 MySQL 有 crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失； 将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。 产生的 redo log 是直接写入磁盘的吗？ 不是的。 实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。 所以，redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘如下图： redo log 什么时候刷盘？主要有下面几个时机： MySQL 正常关闭时； 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘； InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（由innodb_flush_log_at_trx_commit控制） 当设置该参数为 0 时，表示每次事务提交时 ，还是将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。 当设置该参数为 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失。 当设置该参数为 2 时，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到操作系统的文件缓存（page cache）。 InnoDB 的后台线程每隔 1 秒： 针对参数 0 ：会把缓存在 redo log buffer 中的 redo log ，通过调用 write() 写到操作系统的 Page Cache，然后调用 fsync() 持久化到磁盘。所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失; 针对参数 2 ：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 这三个参数的数据安全性和写入性能的比较如下： 数据安全性：参数 1 &gt; 参数 2 &gt; 参数 0 写入性能：参数 0 &gt; 参数 2&gt; 参数 1 redo log 文件写满了怎么办？默认情况下， InnoDB 存储引擎有 1 个重做日志文件组( redo log Group），「重做日志文件组」由有 2 个 redo log 文件组成 重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。 图中的： write pos 和 checkpoint 的移动都是顺时针方向； write pos ～ checkpoint 之间的部分（图中的红色部分），用来记录新的更新操作； check point ～ write pos 之间的部分（图中蓝色部分）：待落盘的脏数据页记录； 如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），然后 MySQL 恢复正常运行，继续执行新的更新操作。 为什么需要 binlog ？binlog 文件是记录了所有数据库表结构变更和表数据修改的日志，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。 redo log 和 binlog 有什么区别？为什么有了 binlog， 还要有 redo log？ 1、适用对象不同： binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用； redo log 是 Innodb 存储引擎实现的日志； 2、文件格式不同： binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下： STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致； ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已； MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式； redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新； 3、写入方式不同： binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。 redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。 4、用途不同： binlog 用于备份恢复、主从复制； redo log 用于掉电等故障恢复。 如果不小心整个数据库的数据被删除了，能使用 redo log 文件恢复数据吗？ 不可以使用 redo log 文件恢复，只能使用 binlog 文件恢复。 因为 redo log 文件是循环写，是会边写边擦除日志的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里擦除。 binlog 文件保存的是全量的日志，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。 什么时候 binlog cache 会写到 binlog 文件？ MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog =N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 主从复制是怎么实现？MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。 这个过程一般是异步的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。 MySQL 集群的主从复制过程梳理成 3 个阶段： 写入 Binlog：主库写 binlog 日志，提交事务，并更新本地存储数据。 同步 Binlog：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。 回放 Binlog：回放 binlog，并更新存储引擎中的数据。 在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。 从库是不是越多越好？ 不是的。 因为从库数量增加，从库连接上来的 I/O 线程也比较多，主库也要创建同样多的 log dump 线程来处理复制的请求，对主库资源消耗比较高，同时还受限于主库的网络带宽。 所以在实际使用中，一个主库一般跟 2～3 个从库（1 套数据库，1 主 2 从 1 备主），这就是一主多从的 MySQL 集群结构。 MySQL 主从复制还有哪些模型？ 主要有三种： 同步复制：MySQL 主库提交事务的线程要等待所有从库的复制成功响应，才返回客户端结果。这种方式在实际项目中，基本上没法用，原因有两个：一是性能很差，因为要复制到所有节点才返回响应；二是可用性也很差，主库和所有从库任何一个数据库出问题，都会影响业务。 异步复制（默认模型）：MySQL 主库提交事务的线程并不会等待 binlog 同步到各从库，就返回客户端结果。这种模式一旦主库宕机，数据就会发生丢失。 半同步复制：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险。 binlog 什么时候刷盘？ 什么时候 binlog cache 会写到 binlog 文件？ 在事务提交的时候，执行器把 binlog cache 里的完整事务写入到 binlog 文件中，并清空 binlog cache。如下图： MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率： sync_binlog = 0 的时候，表示每次提交事务都只 write，不 fsync，后续交由操作系统决定何时将数据持久化到磁盘； sync_binlog = 1 的时候，表示每次提交事务都会 write，然后马上执行 fsync； sync_binlog =N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 在MySQL中系统默认的设置是 sync_binlog = 0，也就是不做任何强制性的磁盘刷新指令，这时候的性能是最好的，但是风险也是最大的。因为一旦主机发生异常重启，还没持久化到磁盘的数据就会丢失。 而当 sync_binlog 设置为 1 的时候，是最安全但是性能损耗最大的设置。因为当设置为 1 的时候，即使主机发生异常重启，最多丢失一个事务的 binlog，而已经持久化到磁盘的数据就不会有影响，不过就是对写入性能影响太大。 如果能容少量事务的 binlog 日志丢失的风险，为了提高写入的性能，一般会 sync_binlog 设置为 100~1000 中的某个数值。 为什么需要两阶段提交？事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。 举个例子，假设 id = 1 这行数据的字段 name 的值原本是 ‘jay’，然后执行 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况： 如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入。 如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入。 在持久化 redo log 和 binlog 这两份日志的时候，如果出现半成功的状态，就会造成主从环境的数据不一致性。这是因为 redo log 影响主库的数据，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致。 两阶段提交的过程是怎样的？ 从图中可看出，事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下： prepare 阶段：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）； commit 阶段：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功； 异常重启会出现什么现象？ 在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID： 如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。 如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。对应时刻 B 崩溃恢复的情况。 可以看到，对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了。 所以说，两阶段提交是以 binlog 写成功为事务提交成功的标识，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。 处于 prepare 阶段的 redo log 加上完整 binlog，重启就提交事务，MySQL 为什么要这么设计? binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。 所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。 事务没提交的时候，redo log 会被持久化到磁盘吗？ 会的。 事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。 也就是说，事务没提交的时候，redo log 也是可能被持久化到磁盘的。 但不会造成数据不一致，mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。 redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。 两阶段提交有什么问题？ 磁盘 I/O 次数高：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。 锁竞争激烈：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。 MySQL日志具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下: 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录： 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新； 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样： 如果一样的话就不进行后续更新流程； 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作； 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。 InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。 至此，一条记录更新完了。 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）： prepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘； commit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）； 至此，一条更新语句执行完成。 内存Buffer PoolInnodb 存储引擎设计了一个缓冲池（*Buffer Pool*），来提高数据库的读写性能。 Buffer Pool 以页为单位缓冲数据，可以通过 innodb_buffer_pool_size 参数调整缓冲池的大小，默认是 128 M。 Innodb 通过三种链表来管理缓页： Free List （空闲页链表），管理空闲页； Flush List （脏页链表），管理脏页； LRU List，管理脏页+干净页，将最近且经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。； InnoDB 对 LRU 做了一些优化，我们熟悉的 LRU 算法通常是将最近查询的数据放到 LRU 链表的头部，而 InnoDB 做 2 点优化： 将 LRU 链表 分为young 和 old 两个区域，加入缓冲池的页，优先插入 old 区域；页被访问时，才进入 young 区域，目的是为了解决预读失效的问题。 当「页被访问」且「 old 区域停留时间超过 innodb_old_blocks_time 阈值（默认为1秒）」时，才会将页插入到 young 区域，否则还是插入到 old 区域，目的是为了解决批量数据访问，大量热数据淘汰的问题。 可以通过调整 innodb_old_blocks_pct 参数，设置 young 区域和 old 区域比例。 在开启了慢 SQL 监控后，如果你发现「偶尔」会出现一些用时稍长的 SQL，这可因为脏页在刷新到磁盘时导致数据库性能抖动。如果在很短的时间出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。 MyBatis mybatis 的执行流程 首先读取配置文件，然后加载映射文件，由SqlSessionFactory工厂对象去创建核心对象SqlSession，SqlSession对象会通过Executor执行器对象执行sql。 然后Executor执行器对象会调用StatementHandler对象去真正的访问数据库执行sql语句。在执行sql语句前MapperStatement会先对映射信息进行封装，存储要映射的SQL语句的id、参数等信息 然后调用ParameterHandler去设置编译参数【#{}，${}】 然后TypeHandler进行数据库类型和JavaBean类型映射处理。 然后调用JBDC原生API进行处理，获取执行结果，这个执行结果交给ResultSetHandler 来进行结果集封装，然后将结果返回给StatementHandler。","link":"/2024/03/12/MySQL/"},{"title":"并发","text":"线程基础线程有哪几种状态? 分别说明从一种状态到另一种状态转变有哪些方式? 线程有哪几种创建方式?方式一：继承于Thread类 步骤：1.创建一个继承于Thread类的子类2.重写Thread类的run() –&gt; 将此线程执行的操作声明在run()中3.创建Thread类的子类的对象4.通过此对象调用start()执行线程 方式二：实现Runnable接口 步骤：1.创建一个实现了Runnable接口的类2.实现类去实现Runnable中的抽象方法：run()3.创建实现类的对象4.将此对象作为参数传递到Thread类的构造器中，创建Thread类的对象5.通过Thread类的对象调用start()① 启动线程②调用当前线程的run()–&gt;调用了Runnable类型的target的run() 方式一和方式二的比较： 开发中优先选择实现Runnable接口的方式 原因：（1）实现的方式没有类的单继承性的局限性（2）实现的方式更适合来处理多个线程有共享数据的情况 方式三：实现Callable接口 步骤：1.创建一个实现Callable的实现类2.实现call方法，将此线程需要执行的操作声明在call()中3.创建Callable接口实现类的对象4.将此Callable接口实现类的对象作为传递到FutureTask构造器中，创建FutureTask的对象5.将FutureTask的对象作为参数传递到Thread类的构造器中，创建Thread对象，并调用start()6.获取Callable中call方法的返回值 实现Callable接口的方式创建线程的强大之处 call()可以有返回值的 call()可以抛出异常，被外面的操作捕获，获取异常的信息 Callable是支持泛型的 方式四：使用线程池 步骤：1.以方式二或方式三创建好实现了Runnable接口的类或实现Callable的实现类2.实现run或call方法3.创建线程池4.调用线程池的execute方法执行某个线程，参数是之前实现Runnable或Callable接口的对象 线程池好处：1.提高响应速度（减少了创建新线程的时间）2.降低资源消耗（重复利用线程池中线程，不需要每次都创建）3.便于线程管理 基础线程机制有哪些?线程的中断方式有哪些?线程的互斥同步方式有哪些? 如何比较和选择?线程之间有哪些协作方式?乐观锁&amp;&amp;悲观锁乐观锁认为在访问数据的时候，其他线程不怎么会对该数据进行修改，所以不加锁。只是在修改的时候通过一种验证数据是否被修改的机制来解决并发问题。 有两种方式： CAS 版本号 悲观锁认为数据经常会被其他线程修改，所以在某一个线程抢占到该数据时，对其上锁，其他线程想要访问会被阻塞。 CAS 的问题CAS引发的ABA问题： ABA问题是指在CAS操作时，其他线程将变量值A改为了B，但是又被改回了A，等到本线程使用期望值A与当前变量进行比较时，发现变量A没有变，于是CAS就将A值进行了交换操作，但是实际上该值已经被其他线程改变过，这与乐观锁的设计思想不符合。ABA问题的解决思路是，每次变量更新的时候把变量的版本号加1，那么A-B-A就会变成A1-B2-A3，只要变量被某一线程修改过，改变量对应的版本号就会发生递增变化，从而解决了ABA问题。 CAS导致自旋消耗： 多个线程争夺同一个资源时，如果自旋一直不成功，将会一直占用CPU。 解决方法：破坏掉for死循环，当超过一定时间或者一定次数时，return退出。 CAS只能单变量： CAS的原子操作只能针对一个共享变量，假如需要针对多个变量进行原子操作也是可以解决的。 方法：CAS操作是针对一个变量的，如果对多个变量操作，1. 可以加锁来解决。2 .封装成对象类解决。 **SynchronizedSynchronized 可以作用在哪分为对象锁、类锁 对象锁： 方法锁：synchronized 修饰普通方法，锁对象默认 this 代码块锁：手动指定锁对象 类锁： 方法锁：synchronized 修饰静态方法 代码块锁：手动指定锁对象为 Class 对象 Synchronized本质上是通过什么保证线程安全的加锁、释放锁原理无锁-》偏向锁-》轻量级锁-》重量级锁 可重入原理可重入：同一线程的外层函数获得锁之后，内层函数可以直接再次获取该锁 保证可见性原理Synchronized的happens-before规则，即监视器锁规则：对同一个监视器的解锁，happens-before于对该监视器的加锁。且解锁前的结果对于加锁后的操作是可见的 **锁的升级 Java中锁主要存在四种状态：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，随着竞争的激烈而逐渐升级。==锁只能升级而不能降级== 应用场景首先简单说下先偏向锁、轻量级锁、重量级锁三者各自的应用场景： 偏向锁：只有一个线程进入临界区；轻量级锁：多个线程交替进入临界区；重量级锁：多个线程同时进入临界区。 偏向锁流程： ​ ==JVM使用CAS操作把线程ID记录到对象的Mark Word当中，并修改标识位==。此时进入偏向模式，若接下来没有其他线程进入临界区，该线程可以不用执行任何同步操作进入临界区。 ​ 线程不会自己释放偏向锁，当其他线程想要抢占锁时，会进行判断之前的占有线程是否完成，（1）如果在运行中，则持有偏向锁的线程会被挂起，JVM会消除它身上的偏向锁，将锁升级到轻量级锁（可能多个线程交替进入临界区）；（2）如果不在运行中，则利用 CAS 更改对象头的线程 ID 轻量级锁​ 轻量级锁是由偏向级锁升级来的，一段时间内有两个线程进入临界区，偏向锁就会升级为轻量级锁。 流程： 首先，JVM会将锁对象的Mark Word恢复成为无锁状态，在当前两线程的栈桢中各自分配一个空间，叫做Lock Record，把锁对象的Mark Word在两线程的栈桢中各自复制了一份，官方称为：Displaced Mark Word 然后一个线程尝试使用CAS将对象头中的Mak Word替换为指向锁记录的指针，如果替换成功，则当前线程获得锁，如果失败，则当前线程自旋重新尝试获取锁。当自旋获取锁失败超过 10 次时，表示竞争比较激烈，则轻量级锁会膨胀成重量级锁 轻量级解锁时，会使用原子的CAS操作将Displaced Mark Word替换回到对象头中，如果成功，则表示没有发生竞争关系。如果失败，表示当前锁存在竞争关系。锁就会膨胀成重量级锁。 重量级锁当多个线程竞争同一个锁时，会导致除锁的拥有者外，其余线程都会自旋，这将导致自旋次数过多，cpu效率下降，所以会将锁升级为重量级锁。 重量级锁需要操作系统的介入，依赖操作系统底层的Mutex Lock。JVM会创建一个monitor对象，把这个对象的地址更新到Mark Word中。 当一个线程获取了该锁后，其余线程想要获取锁，必须等到这个线程释放锁后才可能获取到，没有获取到锁的线程，就进入了阻塞状态。 具体流程： 加锁、释放锁通过「monitor 对象」。在并发抢占时，一个对象对应一个 monitor，同一时间只有一个线程能获得 monitor。 主要指令有「monitorenter」尝试获取monitor 所有权 和 「monitorexit」释放monitor 所有权 调用「monitorenter」时有三种情况： 当 monitor 计数器为 0，说明没有其他线程抢占，enter 成功，线程获取该 monitor，计数器+1 已经获取该monitor的线程，可以重复执行 enter，执行依次计数器+1 若 monitor 计数器不为 0，说明锁已经被其他线程获取，需要等待锁的释放。 调用「monitorexit」时： 释放对于monitor的所有权，将monitor的计数器减1，如果减完以后，计数器不是0，则代表刚才是重入进来的，当前线程还继续持有这把锁的所有权，如果计数器变成0，则代表当前线程不再拥有该monitor的所有权，即释放锁。 该图可以看出，任意线程获取Object锁的时候，首先要获得Object的监视器，如果获取失败，该线程就进入同步状态，线程状态变为BLOCKED，当Object的监视器占有者释放后，在同步队列中得线程就会有机会重新获取该监视器。 Synchronized有什么样的缺陷？Lock解决相应问题？如何选择？ 灵活性低：加锁释放锁的时机单一 效率低：锁的释放情况少，只有代码完成或者异常退出的时候才释放；不能中断一个正在使用锁的进程，不能对试图获取锁的进程设置超时。Lock 可以中断和超时 无法知道是否成功获得锁：Lock 可以拿到是否获取到锁的状态 Lock类这里不做过多解释，主要看里面的4个方法: lock(): 加锁 unlock(): 解锁 tryLock(): 尝试获取锁，返回一个boolean值 tryLock(long,TimeUtil): 尝试获取锁，可以设置超时 建议： 1.如果可以的话，尽量就不使用lock也不要使用Synchronized关键字而是使用java.Util.concurrent中的各种包中的类2.如果Synchronized在程序中适用，那么我们就优先选择Synchronized，因为这样可以减少我们所需要编写的代码，可以减少出错率3.如果特别需要用的lock的时候，我们再使用lock Synchronized在使用时有何注意事项?锁对象不能为空，作用域不宜过大，避免死锁 锁对象不能为空：就是我们如果指定了一个对象为我们的锁对象，那么他就必须是被实例化过的，就是被new过的，不是一个空对象，因为我们的Synchronized是放在我们的对象头中修饰的，如果这个对象为空，更没有对象头，那么这个锁是没有办法工作的作用域不宜过大：作用域是指被Synchronized代码块所包括的范围，如果我们尽量多的代码都被Synchronized所修饰，那么我们代码就会达到安全的目的，但是对于效率就会降低，我们多线程编程，目的是为了提高效率，再不需要安全的情况下，我们并行去执行，是可以提高运行的效率，所以如果我们把Synchronized作用域设置的过大，就会影响我们程序的执行效率的。 多个线程等待同一个Synchronized锁的时候，JVM如何选择下一个获取锁的线程? 队列： Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中； Entry List：候选者队列，Contention List中那些有资格成为候选资源的线程被移动到Entry List中； WaitSet：阻塞队列，哪些调用wait方法被阻塞的线程被放置在这里，直到某个时刻通过notify或者notifyAll唤醒，会重新进去EntryList中； 线程： OnDeck：任意时刻，最多只有一个线程正在竞争锁资源，该线程被成为OnDeck，从 EntryList 中选出。 Owner：从刚刚申请的线程和 OnDeck 选一个最终获得锁，当前已经获取到所资源的线程被称为Owner； 根据 JVM 随机选择，要么是候选者队列中的线程，要么是刚刚申请获取锁的线程 所以——synchronized实际上是非公平的，新来的线程先尝试自旋获取锁，如果获取不到就进入竞争队列，而在等待区中等候已久的线程可能再次等待，这样有利于提高性能，但是也可能会导致饥饿现象。 AQSAQS核心思想？如何实现？底层数据结构？核心思想：如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制。这个机制主要用的是 CLH 队列的变体实现的，将暂时获取不到锁的线程加入到队列中。 实现： state：volatile int state变量，表示同步状态 通过内置的 FIFO 队列来完成资源获取的排队工作 通过 CAS 完成对 State 值的修改。 state： 独占模式 state=0，说明可以抢资源，&gt;0 说明不能 共享模式 state&gt;0，说明可以抢资源，=0 说明不能 设计模式： 模版方法 底层数据结构： 同步队列： CLH 锁是对自旋锁的一种改进，是单向链表，AQS又对 CLH 锁进行改进, 变成一个==虚拟的双向队列，即同步队列==（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系），AQS 是通过将每条请求共享资源的线程封装成一个Node节点来实现锁的分配；当获取同步状态成功的线程释放同步状态的时候，唤醒队列中下一个节点去抢同步状态。 共享模式的区别在于，唤醒的时候，可能会唤醒队列里一批的节点 在 CLH 队列锁中，一个节点表示一个线程，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）。 AQS 每个节点的状态如下所示，在源码中如下所示： 1volatile int waitStatus; AQS 同样提供了该状态变量的原子读写操作，但和同步器状态不同的是，节点状态在 AQS 中被清晰的定义，如下表所示： 状态名 描述 SIGNAL 表示该节点正常等待 PROPAGATE 应将 releaseShared 传播到其他节点 CONDITION 该节点位于条件队列，不能用于同步队列节点 CANCELLED 由于超时、中断或其他原因，该节点被取消 condition等待队列： AQS通过内部类ConditionObject构建等待队列（可有多个），当Condition调用wait()方法后，线程将会加入等待队列中，而当Condition调 用signal()方法后 ，线程将从等待队列转移动同步队列中进行锁竞争 。 AQS 的核心原理图 AQS 使用 int 成员变量 state 表示同步状态，通过内置的 CLH 队列来完成获取资源线程的排队工作。 state 变量由 volatile 修饰，用于展示当前临界资源的获锁情况。 同步状态 状态信息 state 可以通过 protected 类型的getState()、setState()和compareAndSetState() 进行操作。并且，这几个方法都是 final 修饰的，在子类中无法被重写。 123456789101112//返回同步状态的当前值protected final int getState() { return state;} // 设置同步状态的值protected final void setState(int newState) { state = newState;}//原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值）protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update);} AQS定义什么样的资源获取方式?==可以通过修改 State 字段表示的同步状态来实现多线程的独占模式和共享模式（加锁过程）。== AQS有哪些核心的方法?acquire该方法以独占模式获取(资源)，忽略中断，即线程在aquire过程中，中断此线程是无效的。 1234public final void acquire(int arg) { if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();} release以独占模式释放对象 12345678910public final boolean release(int arg) { if (tryRelease(arg)) { // 释放成功 // 保存头节点 Node h = head; if (h != null &amp;&amp; h.waitStatus != 0) // 头节点不为空并且头节点状态不为0 unparkSuccessor(h); //释放头节点的后继结点 return true; } return false;} **ReentrantLock修改 State 字段为 0 来实现独占方式 再实现两个 AQS 提供的模板方法 tryAcquire和tryRelease。 流程 如何实现可重入比较拥有同步状态的线程是否和当前请求同步状态的线程相同，若相同则cas 让 state+1， 有公平锁和非公平锁两种 公平锁：在请求共享资源时发现资源被占用，该线程就会添加到sync queue中的尾部，而不会先尝试获取资源。 非公平锁：每一次都会尝试去获取资源，如果此时该资源恰好被释放，则会被当前线程获取，这就造成了不公平的现象，当获取不成功，再加入队列尾部。 默认用非公平锁 三个内部类： 说明: ReentrantLock类内部总共存在Sync、NonfairSync、FairSync三个类，NonfairSync与FairSync类继承自Sync类，Sync类继承自AbstractQueuedSynchronizer抽象类。 ReentrantLock和 Synchornized 关系相同 独占锁 可重入 不同 syn 是关键字，reen 是类 syn 基于 monitor，reen 基于AQS syn 只有非公平方式，reen 有公平方式和非公平方式 syn不可以响应中断，reen 可以同时获取锁时可以限时等待 syn 利用 object 的 wait() 和 notify() 实现线程间的等待通知机制；reen 使用Condition的 await() 和 signal() 实现 syn 自动释放 monitor，reen 需要显式调用unlock（）释放锁 syn 只能关联一个条件队列（waitset），reen可以关联多个条件队列（lock.newCondition()生成多个 condition 对象） 应用场景syn 由于使用方便简单，建议在竞争不激烈的时候使用。在竞争激烈的时候 reen 性能会好点 reen 由于具有 syn 所不具有的灵活性、可中断、有限等待等特性，可以用于时间锁等待、可中断锁等待 CountdownLatch和CyclicBarrier的区别使用场景与具体实现CountdownLatch和CyclicBarrier都属于线程同步的工具 CountdownLatch使用场景顾名思义CountdownLatch可以当做一个计数器来使用,比如某线程需要等待其他几个线程都执行过某个时间节点后才能继续执行 我们来模拟一个场景,某公司一共有十个人,门卫要等十个人都来上班以后,才可以休息,代码实现如下 1234567891011121314151617181920212223242526272829public static void main(String[] args) { final CountDownLatch latch = new CountDownLatch(10); for (int i = 0; i &lt; 10; i++) { //lambda中只能只用final的变量 final int times = i; new Thread(() -&gt; { try { System.out.println(\"子线程\" + Thread.currentThread().getName() + \"正在赶路\"); Thread.sleep(1000 * times); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"到公司了\"); //调用latch的countDown方法使计数器-1 latch.countDown(); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"开始工作\"); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); } try { System.out.println(\"门卫等待员工上班中...\"); //主线程阻塞等待计数器归零 latch.await(); System.out.println(\"员工都来了,门卫去休息了\"); } catch (InterruptedException e) { e.printStackTrace(); } } 可以看到子线程并没有因为调用latch.countDown而阻塞,会继续进行该做的工作,只是通知计数器-1,即完成了我们如上说的场景,只需要在所有进程都进行到某一节点后才会执行被阻塞的进程.如果我们想要多个线程在同一时间进行就要用到CyclicBarrier了 原理CountDownLatch是共享锁的一种实现，通过 AQS 实现，内部使用 syn类。将 state 设置为 count。主要有两个重要方法countDown()和 await()，countDown()调用tryReleaseShared()以 CAS 的方式减少 state，直至 state 为 0 。调用 await()则是会一直阻塞线程，直至 state 为 0，线程才会被唤醒，执行后续语句。 我们先来看看CountdownLatch的构造方法 1234public CountDownLatch(int count) { if (count &lt; 0) throw new IllegalArgumentException(\"count &lt; 0\"); this.sync = new Sync(count); } Sync是CountdownLatch的静态内部类,继承了AbstractQueuedSynchronizer(即AQS,提供了一种实现阻塞锁和一系列依赖FIFO等待队列的同步器的工具,回头单讲)抽象类, 在Sync的构造方法中,调用了setState方法,可以视作初始化了一个标记来记录当前计数器的数量 CountdownLatch的两个核心方法,await和countdown,先来看await 1234public void await() throws InterruptedException { //可以视作将线程阻塞 sync.acquireSharedInterruptibly(1); } await调用的是AQS的方法,可以视作阻塞线程 countdown方法 123public void countDown() { sync.releaseShared(1); } 调用了sync的一个方法,再来看看这个方法的实现 1234567public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } 再来看这个tryReleaseShared方法 123456789101112protected boolean tryReleaseShared(int releases) { for (;;) { //获取标记位 int c = getState(); if (c == 0) return false; int nextc = c-1; //用cas的方式更新标记位 if (compareAndSetState(c, nextc)) return nextc == 0; } } 可以看到在调用tryReleaseShared实际上是将标记位-1并且返回标记位是否为0,如果标记位为0 那么调用的doReleaseShared可以视作将阻塞的线程放行,这样整个的流程就通了 CyclicBarrier使用场景我们重新模拟一个新的场景,就用已经被说烂的跑步场景吧,十名运动员各自准备比赛,需要等待所有运动员都准备好以后,裁判才能说开始然后所有运动员一起跑,代码实现如下 123456789101112131415161718192021222324public static void main(String[] args) { final CyclicBarrier cyclicBarrier = new CyclicBarrier(10,()-&gt;{ System.out.println(\"所有人都准备好了裁判开始了\"); }); for (int i = 0; i &lt; 10; i++) { //lambda中只能只用final的变量 final int times = i; new Thread(() -&gt; { try { System.out.println(\"子线程\" + Thread.currentThread().getName() + \"正在准备\"); Thread.sleep(1000 * times); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"准备好了\"); cyclicBarrier.await(); System.out.println(\"子线程\" + Thread.currentThread().getName() + \"开始跑了\"); } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } }).start(); } } 可以看到所有线程在其他线程没有准备好之前都在被阻塞中,等到所有线程都准备好了才继续执行 我们在创建CyclicBarrier对象时传入了一个方法,当调用CyclicBarrier的await方法后,当前线程会被阻塞等到所有线程都调用了await方法后 调用传入CyclicBarrier的方法,然后让所有的被阻塞的线程一起运行 原理CyclicBarrier是利用ReentrantLock的condition的 await()和 signalAll()来进行线程的阻塞和唤醒【类似Object.wait()和notifyAll()】在count不为0时阻塞,在count=0时唤醒所有线程 二者区别 CountDownLatch减计数，CyclicBarrier加计数。 CountDownLatch是一次性的，CyclicBarrier可以重用。 CountDownLatch和CyclicBarrier都有让多个线程等待同步然后再开始下一步动作的意思，但是CountDownLatch的下一步的动作实施者是主线程，具有不可重复性；而CyclicBarrier的下一步动作实施者还是“其他线程”本身，具有往复多次实施动作的特点。 线程池什么是线程池、为什么要用、底层实现线程池是一种池化技术，主要思想是资源复用。 优点：减少创建销毁的开销；任务响应快；限制线程数量，防止资源利用率过高 底层逻辑是使用线程+阻塞队列实现，队列中有任务线程就会消费，假如没有线程就会被阻塞，等待队列中新的任务的到来，假如队列满了，则会出发设定的饱和策略（包括抛出异常、原线程执行、丢弃任务、丢弃最早未处理任务） 如何创建1234567public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, RejectedExecutionHandler handler) 一般使用 TreadPoolExecutor 建立线程池，有三个重要参数（核心线程数、最大线程数、阻塞队列）。不使用 Executors 去创建的原因：fixedThreadPool 阻塞队列的长度是 Integer.MAX_VALUE，cachedThreadPool 的线程数量最大是Integer.MAX_VALUE。都会导致 OOM 阻塞队列workQueue: 用来保存等待被执行的任务的阻塞队列 LinkedBlockingQueue无界队列: 基于链表结构的阻塞队列，按FIFO排序任务； SynchronousQueue同步队列: 没有容量，不存储元素，目的是保证对于提交的任务，如果有空闲线程，则使用空闲线程来处理；否则新建一个线程来处理任务。CachedThreadPool 的最大线程数是 Integer.MAX_VALUE ，可以理解为线程数是可以无限扩展的，可能会创建大量线程，从而导致 OOM DelayQueue（延迟阻塞队列）：DelayQueue 的内部元素并不是按照放入的时间排序，而是会按照延迟的时间长短对任务进行排序，内部采用的是“堆”的数据结构，可以保证每次出队的任务都是当前队列中执行时间最靠前的。 饱和策略handler 线程池的饱和策略，当阻塞队列满了，且没有空闲的工作线程，如果继续提交任务，必须采取一种策略处理该任务，线程池提供了4种策略: AbortPolicy: 直接抛出异常，默认策略； CallerRunsPolicy: 用调用者所在的线程来执行任务； DiscardOldestPolicy: 丢弃阻塞队列中靠最前的任务，并执行当前任务； DiscardPolicy: 直接丢弃任务； 线程池处理任务的流程 submit()or execute()一个任务的流程 当前线程数量&lt;核心线程数，新建线程处理任务 阻塞队列未满，加到阻塞队列等待执行 阻塞队列满&amp;当前线程数&lt;最大线程数，新建线程处理 达到最大线程数，使用饱和策略 提交任务的方法submit()submit()方法可以接收Callable、Runnable两种类型的参数 submit()方法也用于启动任务的执行，但是启动之后会返回Future对象，代表一个异步执行实例，可以通过该异步执行实例去获取结果。 submit()方法返回的Future对象（异步执行实例），可以进行异步执行过程中的异常捕获。 execute()Execute()方法只能接收Runnable类型的参数 execute()方法主要用于启动任务的执行，而任务的执行结果和可能的异常调用者并不关心。 execute()方法在启动任务执行后，任务执行过程中可能发生的异常调用者并不关心 联系在ThreadPoolExecutor类的实现中，内部核心的任务提交方法是execute()方法，虽然用户程序通过submit()也可以提交任务，但是实际上submit()方法中最终调用的还是execute()方法。 Callable类型的任务是可以返回执行结果的，而Runnable类型的任务不可以返回执行结果。 Runnable和Callable的主要区别为：Callable允许有返回值，Runnable不允许有返回值；Runnable不允许抛出异常，Callable允许抛出异常。 内置线程池类型FixedThreadPool12345return new ThreadPoolExecutor( nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); 核心线程数和最大线程数一样，且线程没有过期时间，线程池的线程数量达corePoolSize后，即使线程池没有可执行任务时，也不会释放线程。 使用了LinkedBlockingQueue无界队列, 所以FixedThreadPool永远不会拒绝, 即饱和策略失效 SingleThreadExecutor12345return new ThreadPoolExecutor( 1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;())); 初始化的线程池中只有一个线程 使用了LinkedBlockingQueue无界队列, 所以SingleThreadPool永远不会拒绝, 即饱和策略失效 CachedThreadPool12345return new ThreadPoolExecutor( 0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;()); 最大线程数为Integer.MAX_VALUE 使用SynchronousQueue同步队列； 线程有过期时间，空闲会释放线程 ScheduledThreadPool12345super( corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue()); 最大线程数为Integer.MAX_VALUE 使用无界的DelayedWorkQueue延迟阻塞队列 关闭线程池遍历线程池中的所有线程，然后逐个调用线程的interrupt方法来中断线程. 关闭方式 - shutdown将线程池里的线程状态设置成SHUTDOWN状态, 然后中断所有没有正在执行任务的线程. 关闭方式 - shutdownNow将线程池里的线程状态设置成STOP状态, 然后停止所有正在执行或暂停任务的线程. 只要调用这两个关闭方法中的任意一个, isShutDown() 返回true. 当所有任务都成功关闭了, isTerminated()返回true. 任务的执行==execute –&gt; addWorker –&gt;runworker (getTask)== addWorker方法：主要负责创建新的线程并执行任务 线程池创建新线程执行任务时，需要获取全局锁: 线程池的工作线程通过Woker类实现，启动线程本质是执行了Worker的runWorker方法。 线程执行完任务怎么通知阻塞队列？ firstTask执行完成之后，通过==getTask==方法从阻塞队列中获取等待的任务，如果队列中没有任务，getTask方法会被阻塞并挂起，不会占用cpu资源； CompletableFuture是 JDK1.8 中引入的一个基于事件驱动的异步回调类。 当使用异步线程去执行一个任务时，在任务完成后触发一个后续动作。可以实现任务的编排，主要方法有： thenCombine。两个任务都执行结束后触发事件回调 thenCompose。第一个任务执行玩后自动触发执行第二个任务","link":"/2024/03/12/%E5%B9%B6%E5%8F%91/"},{"title":"Redis","text":"Redis 基础什么是 Redis基于 c 开发 NoSQL 数据库 内存数据库，支持持久化 KY键值对数据 为什么要用 Redis/为什么要用缓存？主要是因为 Redis 具备「高性能」和「高并发」两种特性。 1、高性能：直接操作内存 2、高并发：单台设备的 Redis 的 QPS（Query Per Second，每秒钟处理完请求的次数） 是 MySQL 的 10 倍 Redis 单线程模式是怎样的？Redis 单线程指的是「接收客户端请求-&gt;解析请求 -&gt;进行数据读写等操作-&gt;发送数据给客户端」这个过程是由一个线程（主线程）来完成的，这也是我们常说 Redis 是单线程的原因。 但是，Redis 程序并不是单线程的，Redis 在启动的时候，是会启动后台线程（BIO）的： Redis 在 2.6 版本，会启动 2 个后台线程，分别处理关闭文件、AOF 刷盘这两个任务； Redis 在 4.0 版本之后，新增了一个新的后台线程，用来异步释放 Redis 内存，也就是 lazyfree 线程。 Redis 采用单线程为什么还这么快？ Redis 的大部分操作都在内存中完成，并且采用了高效的数据结构，因此 Redis 瓶颈可能是机器的内存或者网络带宽，而并非 CPU。 Redis 采用单线程模型可以避免了多线程之间的竞争，省去了多线程切换带来的时间和性能上的开销，而且也不会导致死锁问题。 Redis 采用了 I/O 多路复用机制处理大量的客户端 Socket 请求，IO 多路复用机制是指一个线程处理多个 IO 流，就是我们经常听到的 select/epoll 机制。简单来说，在 Redis 只运行单线程的情况下，该机制允许内核中，同时存在多个监听 Socket 和已连接 Socket。内核会一直监听这些 Socket 上的连接请求或数据请求。一旦有请求到达，就会交给 Redis 线程处理，这就实现了一个 Redis 线程处理多个 IO 流的效果。 Redis 数据结构Redis 数据类型以及使用场景分别是什么？常见的有五种数据类型：String（字符串），Hash（哈希），List（列表），Set（集合）、Zset（有序集合）。 Redis 五种数据类型的应用场景： String 类型的应用场景 需要存储常规数据的场景 举例：缓存 Session、Token、图片地址、序列化后的对象(相比较于 Hash 存储更节省内存)。 相关命令：SET、GET。 需要计数的场景 举例：用户单位时间的请求数（简单限流可以用到）、页面单位时间的访问数。 相关命令：SET、GET、 INCR、DECR 。 分布式锁 利用 SETNX key value 命令可以实现一个最简易的分布式锁（存在一些缺陷，通常不建议这样实现分布式锁）。 List 类型的应用场景： 信息流展示 举例：最新文章、最新动态。 相关命令：LPUSH、LRANGE。 消息队列（但是有两个问题：1. 生产者需要自行实现全局唯一 ID；2. 不能以消费组形式消费数据）等。 Hash 类型： 对象数据存储场景 举例：用户信息、商品信息、文章信息、购物车信息。 相关命令：HSET （设置单个字段的值）、HMSET（设置多个字段的值）、HGET（获取单个字段的值）、HMGET（获取多个字段的值）。 Set 类型： 需要存放的数据不能重复的场景 举例：网站 UV 统计（数据量巨大的场景还是 HyperLogLog更适合一些）、文章点赞、动态点赞等场景。 相关命令：SCARD（获取集合数量） 。 需要获取多个数据源交集、并集和差集的场景 举例：共同好友(交集)、共同粉丝(交集)、共同关注(交集)、好友推荐（差集）、音乐推荐（差集）、订阅号推荐（差集+交集） 等场景。 相关命令：SINTER（交集）、SINTERSTORE （交集）、SUNION （并集）、SUNIONSTORE（并集）、SDIFF（差集）、SDIFFSTORE （差集）。 需要随机获取数据源中的元素的场景 举例：抽奖系统、随机点名等场景。 相关命令：SPOP（随机获取集合中的元素并移除，适合不允许重复中奖的场景）、SRANDMEMBER（随机获取集合中的元素，适合允许重复中奖的场景）。 Zset 类型： 需要随机获取数据源中的元素根据某个权重进行排序的场景 举例：各种排行榜比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等。 相关命令：ZRANGE (从小到大排序)、 ZREVRANGE （从大到小排序）、ZREVRANK (指定元素排名)。 特殊数据类型 BitMap（2.2 版新增）：二值状态统计的场景，比如签到、判断用户登陆状态、连续签到用户总数等； HyperLogLog（2.8 版新增）： 数量量巨大（百万、千万级别以上）的计数场景 举例：热门网站每日/每周/每月访问 ip 数统计、热门帖子 uv 统计、 相关命令：PFADD、PFCOUNT 。 GEO（3.2 版新增）： 需要管理使用地理空间数据的场景 举例：附近的人，滴滴叫车。 相关命令: GEOADD、GEORADIUS、GEORADIUSBYMEMBER 。 Stream（5.0 版新增）：消息队列，相比于基于 List 类型实现的消息队列，有这两个特有的特性：自动生成全局唯一消息ID，支持以消费组形式消费数据。 五种常见的 Redis 数据类型是怎么实现？ String 类型内部实现 String 类型的底层的数据结构实现主要是 SDS（简单动态字符串）。 SDS 不仅可以保存文本数据，还可以保存二进制数据。因为 SDS 使用 len 属性的值而不是空字符来判断字符串是否结束，并且 SDS 的所有 API 都会以处理二进制的方式来处理 SDS 存放在 buf[] 数组里的数据。所以 SDS 不光能存放文本数据，而且能保存图片、音频、视频、压缩文件这样的二进制数据。 **SDS 获取字符串长度的时间复杂度是 O(1)**。因为 C 语言的字符串并不记录自身长度，所以获取长度的复杂度为 O(n)而 SDS 结构里用 len 属性记录了字符串长度，所以复杂度为 O(1)。 Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出。因为 SDS 在拼接字符串之前会检查 SDS 空间是否满足要求，如果空间不够会自动扩容，所以不会导致缓冲区溢出的问题。 List 类型内部实现 List 类型的底层数据结构是由双向链表或压缩列表实现的： 如果列表的元素个数小于 512 个（默认值，可由 list-max-ziplist-entries 配置），列表每个元素的值都小于 64 字节（默认值，可由 list-max-ziplist-value 配置），Redis 会使用压缩列表作为 List 类型的底层数据结构； 如果列表的元素不满足上面的条件，Redis 会使用双向链表作为 List 类型的底层数据结构； 但是在 Redis 3.2 版本之后，List 数据类型底层数据结构就只由 quicklist 实现了，替代了双向链表和压缩列表。 Hash 类型内部实现 Hash 类型的底层数据结构是由压缩列表或哈希表实现的： 如果哈希类型元素个数小于 512 个（默认值，可由 hash-max-ziplist-entries 配置），所有值小于 64 字节（默认值，可由 hash-max-ziplist-value 配置）的话，Redis 会使用压缩列表作为 Hash 类型的底层数据结构； 如果哈希类型元素不满足上面条件，Redis 会使用哈希表作为 Hash 类型的底层数据结构。 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 实际redis 定义的dict结构下有两个哈希表dictht，一个是用来存数据的，另一个是用来 rehash 的。每个哈希表是一个数组，每一个元素是指向「哈希表节点」的指针。 rehash 的触发条件 若负载因子&gt;=1，若没有执行bgsave 或者 bgrewriteaof，才会立即进行 rehash 若负载因子&gt;=5，此时说明哈希冲突非常严重了，不管有没有有在执行 RDB 快照或 AOF 重写，都会强制进行 rehash 操作。 rehash dict 下的两个 dictht 哈希表，正常情况下只会往「哈希表 1」中插入数据，「哈希表 2」是不会分配空间。 当触发了 rehash 条件时： 给「哈希表 2」分配比「哈希表 1」 大一倍的空间 将「哈希表 1 」的数据迁移到「哈希表 2」 中； 迁移完成后，「哈希表 1 」的空间会被释放，并把「哈希表 2」 设置为「哈希表 1」，然后在「哈希表 2」 新创建一个空白的哈希表，为下次 rehash 做准备。 缺点：当数据量特别大时，会造成redis 长时间的阻塞 渐进 rehash 数据从一次性迁移完成，变成分批迁移 当触发了 rehash 条件时： 给「哈希表 2」分配比「哈希表 1」 大一倍的空间 在 rehash 期间，每次对于哈希表元素的新增、修改和查找操作时，redis除了执行对应操作之外，还会将「哈希表 1 」中索引位置上的所有 key-value 迁移到「哈希表 2」 上； 随着对哈希表的请求操作的增加，最后完成数据的迁移 在 rehash 时需要满足： 查找一个 key 的值的话，先会在「哈希表 1」 里面进行查找，如果没找到，就会继续到哈希表 2 里面进行找到。 新增一个 key-value 时，会被保存到「哈希表 2 」里面，而「哈希表 1」 则不再进行任何添加操作，这样保证了「哈希表 1 」的 key-value 数量只会减少，随着 rehash 操作的完成，最终「哈希表 1 」就会变成空表。 Set 类型内部实现 Set 类型的底层数据结构是由哈希表或整数集合实现的： 如果集合中的元素都是整数且元素个数小于 512 （默认值，set-maxintset-entries配置）个，Redis 会使用整数集合作为 Set 类型的底层数据结构； 如果集合中的元素不满足上面条件，则 Redis 使用哈希表作为 Set 类型的底层数据结构。 整数集合 本质是一块连续的内存区域 12345678typedef struct intset { //编码方式 uint32_t encoding; //集合包含的元素数量 uint32_t length; //保存元素的数组 int8_t contents[];} intset; 根据 encoding 来决定 contents[]的真正类型 encoding 有三种 int16_t、 int32_t、 int64_t 升级操作 假如一开始都是存 int16，然后加入一个只有 int32 才能存的数，那么就会触发升级操作。 流程包括： 数组扩充 将元素从后向前转换类型放在对应位置 好处： 节省内存资源 缺点： 但不支持降级操作 ZSet 类型内部实现 Zset 类型的底层数据结构是由压缩列表或跳表实现的： 当有序集合对象同时满足以下两个条件时，使用 ziplist： ZSet 保存的键值对数量少于 128 个； 每个元素的长度小于 64 字节。 如果不满足上述两个条件，那么使用 skiplist 。 在 Redis 7.0 中，压缩列表数据结构已经废弃了，交由 listpack 数据结构来实现了。 ziplist压缩列表是 Redis 为了节约内存而开发的，它是由连续内存块组成的顺序型数据结构，有点类似于数组。 压缩列表在表头有三个字段： zlbytes，记录整个压缩列表占用对内存字节数； zltail，记录压缩列表「尾部」节点距离起始地址由多少字节，也就是列表尾的偏移量； zllen，记录压缩列表包含的节点数量； zlend，标记压缩列表的结束点，固定值 0xFF（十进制255）。 压缩列表节点包含三部分内容： prevlen，记录了「前一个节点」的长度，目的是为了实现从后向前遍历； encoding，记录了当前节点实际数据的「类型和长度」，类型主要有两种：字符串和整数。 data，记录了当前节点的实际数据，类型和长度都由 encoding 决定； 当我们往压缩列表中插入数据时，压缩列表就会根据数据类型是字符串还是整数，以及数据的大小，会使用不同空间大小的 prevlen 和 encoding 这两个元素里保存的信息，这种根据数据大小和类型进行不同的空间大小分配的设计思想，正是 Redis 为了节省内存而采用的。 缺陷：存在连锁更新问题。多个连续的长度为 250～253 的元素，当插入一个长度大于等于 254 的元素时，会让后面的元素的 prevlen 从之前的 1 字节变成 5 字节，而不断发生内存重新分配的问题。 quicklistquicklist 的结构体跟链表的结构体类似，都包含了表头和表尾，区别在于 quicklist 的节点是 quicklistNode。quicklistNode 结构体里包含了前一个节点和下一个节点指针，这样每个 quicklistNode 形成了一个双向链表。但是链表节点的元素不再是单纯保存元素值，而是保存了一个压缩列表。 quicklist 会控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来规避潜在的连锁更新的风险，但是这并没有完全解决连锁更新的问题。 listpackquicklist 虽然通过控制 quicklistNode 结构里的压缩列表的大小或者元素个数，来减少连锁更新带来的性能影响，但是并没有完全解决连锁更新的问题。 因为 quicklistNode 还是用了压缩列表来保存元素，压缩列表连锁更新的问题，来源于它的结构设计，所以要想彻底解决这个问题，需要设计一个新的数据结构。 于是，Redis 在 5.0 新设计一个数据结构叫 listpack，目的是替代压缩列表，它最大特点是 listpack 中每个节点不再包含前一个节点的长度了，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患。 listpack 头包含两个属性，分别记录了 listpack 总字节数和元素数量，然后 listpack 末尾也有个结尾标识。 Listpack entry主要包含三个方面内容： encoding，定义该元素的编码类型，会对不同长度的整数和字符串进行编码； data，实际存放的数据； len，encoding+data的总长度； 可以看到，listpack 没有压缩列表中记录前一个节点长度的字段了，listpack 只记录当前节点的长度，当我们向 listpack 加入一个新元素的时候，不会影响其他节点的长度字段的变化，从而避免了压缩列表的连锁更新问题。 为什么用跳表实现有序集合zset结构体中有两个数据结构：跳表、哈希表。好处是既能进行高效的范围查询，又能进行高效的单点查询 或者是在数据量少的时候用压缩列表 跳表的建立跳表可以理解为在原始链表基础上，建立「多层」有序链表，将增删改查的时间复杂度变为O(log n)。 12345678typedef struct zskiplist { //跳表的头尾节点，便于在O(1)时间复杂度内访问跳表的头节点和尾节点； struct zskiplistNode *header, *tail; //跳表的长度，便于在O(1)时间复杂度获取跳表节点的数量； unsigned long length; //跳表的最大层数，便于在O(1)时间复杂度获取跳表中层高最大的那个节点的层数量； int level;} zskiplist; 1234567891011121314typedef struct zskiplistNode { //Zset 对象的元素值 sds ele; //元素权重值 double score; //后向指针 struct zskiplistNode *backward; //节点的level数组，保存每层上的前向指针和跨度 struct zskiplistLevel { struct zskiplistNode *forward; unsigned long span; } level[];} zskiplistNode; 跳表节点查询查找一个跳表节点的过程时，跳表会从头节点的最高层开始，逐一遍历每一层。在遍历某一层的跳表节点时，会用跳表节点中的 SDS 类型的元素和元素的权重来进行判断，判断是走这一层下一个节点还是走当前节点的下一层的下一个节点，共有两个判断条件： 如果下一个节点的权重「小于」要查找的权重时，跳表就会访问该层上的下一个节点。 如果下一个节点的权重「等于」要查找的权重时，并且下一个节点的 SDS 类型数据「小于」要查找的数据时，跳表就会访问该层上的下一个节点。 如果上面两个条件都不满足，或者下一个节点为空时，跳表就会使用目前遍历到的节点的 level 数组里的下一层指针，然后沿着下一层指针继续查找，这就相当于跳到了下一层接着查找。 跳表节点层数设置理想情况是每一层索引是下一层元素个数的二分之一。 Redis 则采用一种巧妙的方法是，跳表在创建节点的时候，随机生成每个节点的层数，并没有严格维持相邻两层的节点数量比例为 2 : 1 的情况。 具体的做法是，跳表在创建节点时候，会生成范围为[0-1]的一个随机数，如果这个随机数小于 0.25（相当于概率 25%），那么层数就增加 1 层，然后继续生成下一个随机数，直到随机数的结果大于 0.25 结束，最终确定该节点的层数。 这样的做法，相当于每增加一层的概率不超过 25%，层数越高，概率越低，层高最大限制是 64。 虽然我前面讲解跳表的时候，图中的跳表的「头节点」都是 3 层高，但是其实如果层高最大限制是 64，那么在创建跳表「头节点」的时候，就会直接创建 64 层高的头节点。 和其余三种数据结构的比较平衡树 vs 跳表 平衡条件必须满足（所有节点的左右子树高度差不超过 1，即平衡因子为范围为 [-1,1]）。平衡树的插入、删除和查询的时间复杂度和跳表一样都是 **O(log n)**。 对于范围查询来说，它也可以通过中序遍历的方式达到和跳表一样的效果。但是它的每一次插入或者删除操作都需要保证整颗树左右节点的绝对平衡，只要不平衡就要通过旋转操作来保持平衡，这个过程是比较耗时的。 红黑树 vs 跳表 红黑树查询性能略微逊色于 AVL 树，但插入和删除效率更高。红黑树的插入、删除和查询的时间复杂度和跳表一样都是 **O(log n)**。 红黑树是一个黑平衡树，即从任意节点到另外一个叶子节点，它所经过的黑节点是一样的。当对它进行插入操作时，需要通过旋转和染色（红黑变换）来保证黑平衡。不过，相较于 AVL 树为了维持平衡的开销要小一些。 相比较于红黑树来说，跳表的实现也更简单一些。并且，按照区间来查找数据这个操作，红黑树的效率没有跳表高。 红黑树（Red-Black Tree）是一种自平衡的二叉查找树，它在每个节点上增加了一个额外的属性，即节点的颜色，可以是红色或黑色。红黑树具有以下特点： 节点颜色：每个节点要么是红色，要么是黑色。 根节点和叶子节点：根节点是黑色的，叶子节点（NIL节点）是黑色的。 颜色约束：红色节点的子节点必须是黑色的。换句话说，不能有两个相邻的红色节点，即红色节点不能连续存在。 黑色高度约束：从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点，这个数目称为黑色高度。 平衡性：保证了树的黑色高度相对平衡，确保了最长路径不超过最短路径的两倍。 通过这些约束，红黑树能够在插入和删除节点时自动调整结构，以保持这些约束，从而保证了树的平衡性，使得查找、插入和删除等操作的最坏情况时间复杂度都是 O(log n)。 B+树 vs 跳表 内存数据库它不可能存储大量的数据，所以对于索引不需要通过 B+树这种方式进行维护。 使用跳表实现 zset 时相较前者来说更简单一些，在进行插入时只需通过索引将数据插入到链表中合适的位置再随机维护一定高度的索引即可，也不需要像 B+树那样插入时发现失衡时还需要对节点分裂与合并。 总结： 从内存占用上来比较，跳表比平衡树更灵活一些。平衡树每个节点包含 2 个指针（分别指向左右子树），而跳表每个节点包含的指针数目平均为 1/(1-p)，具体取决于参数 p 的大小。如果像 Redis里的实现一样，取 p=1/4，那么平均每个节点包含 1.33 个指针，比平衡树更有优势。 在做范围查找的时候，跳表比平衡树操作要简单。在平衡树上，我们找到指定范围的小值之后，还需要以中序遍历的顺序继续寻找其它不超过大值的节点。如果不对平衡树进行一定的改造，这里的中序遍历并不容易实现。而在跳表上进行范围查找就非常简单，只需要在找到小值之后，对第 1 层链表进行若干步的遍历就可以实现。 从算法实现难度上来比较，跳表比平衡树要简单得多。平衡树的插入和删除操作可能引发子树的调整，逻辑复杂，而跳表的插入和删除只需要修改相邻节点的指针，操作简单又快速 Redis 持久化Redis 共有三种数据持久化的方式： AOF （append only file）日志：每执行一条写操作命令，就把该命令以追加的方式写入到一个文件里； RDB 快照：将某一时刻的内存数据，以二进制的方式写入磁盘； 混合持久化方式：Redis 4.0 新增的方式，集成了 AOF 和 RBD 的优点； AOF 日志是如何实现的？Redis 在执行完一条写操作命令后，就会把该命令以追加的方式写入到一个文件里，然后 Redis 重启时，会读取该文件记录的命令，然后逐一执行命令的方式来进行数据恢复。 「$3 set」表示这部分有 3 个字节，也就是「set」命令这个字符串的长度。 为什么先执行命令，再把数据写入日志呢？ 好处： 避免额外的检查开销（语法检查） 不会阻塞当前写操作命令的执行 风险： 数据可能会丢失 可能阻塞后续操作 AOF 写回策略有几种？ Redis 写入 AOF 日志的过程 redis 执行完写操作后，将命令追加到 server.aof_buf 缓冲区 然后通过 write()系统调用将缓冲区的内容写到内核缓冲区page cache，等待内核将数据写入硬盘 具体内核缓冲区什么时候写入硬盘有三种写回策略 这三种策略只是在控制 fsync() 函数的调用时机。 AOF 日志过大，会触发什么机制？ Redis 为了避免 AOF 文件越写越大，提供了 AOF 重写机制，当 AOF 文件的大小超过所设定的阈值后，Redis 就会启用 AOF 重写机制，来压缩 AOF 文件。 AOF 重写机制是在重写时，读取当前数据库中的所有键值对，然后将每一个键值对用一条命令记录到「新的 AOF 文件」，等到全部记录完后，就将新的 AOF 文件替换掉现有的 AOF 文件。 为什么重写 AOF 的时候，不直接复用现有的 AOF 文件，而是先写到新的 AOF 文件再覆盖过去？如果 AOF 重写过程中失败了，现有的 AOF 文件就会造成污染，可能无法用于恢复使用。 重写 AOF 日志的过程是怎样的？ redis 的重写 AOF 过程是由后台==子进程== bgrewriteaof 来完成的 好处： 子进程进行 AOF 重写期间，主进程可以继续处理命令请求，从而避免阻塞主进程； 子进程带有主进程的数据副本，这里使用子进程而不是线程，因为如果是使用线程，多线程之间会共享内存，那么在修改共享内存数据的时候，需要通过加锁来保证数据的安全，而这样就会降低性能。而使用子进程，创建子进程时，父子进程是共享内存数据的，不过这个共享的内存只能以只读的方式，而当父子进程任意一方修改了该共享内存，就会发生==「写时复制（Copy On Write）」==，于是父子进程就有了独立的数据副本，就不用加锁来保证数据安全。 写时复制： 主进程在通过 fork 系统调用生成子进程时，操作系统会把主进程的「页表」复制一份给子进程，这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个。 这样一来，子进程就共享了父进程的物理内存数据了，这样能够节约物理内存资源，页表对应的页表项的属性会标记该物理内存的权限为只读。 不过，当父进程或者子进程在向这个内存发起写操作时，CPU 就会触发写保护中断，这个写保护中断是由于违反权限导致的，然后操作系统会在「写保护中断处理函数」里进行物理内存的复制，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写，最后才会对内存进行写操作，这个过程被称为「**写时复制(*Copy On Write*)**」。 写时复制顾名思义，在发生写操作的时候，操作系统才会去复制物理内存，这样是为了防止 fork 创建子进程时，由于物理内存数据的复制时间过长而导致父进程长时间阻塞的问题。 但是重写过程中，主进程依然可以正常处理命令，那问题来了，重写 AOF 日志过程中，如果主进程修改了已经存在 key-value，那么会发生写时复制，此时这个 key-value 数据在子进程的内存数据就跟主进程的内存数据不一致了，这时要怎么办呢？ 为了解决这种数据不一致问题，Redis 设置了一个 AOF 重写缓冲区，这个缓冲区在创建 bgrewriteaof 子进程之后开始使用。在重写 AOF 期间，当 Redis 执行完一个写命令之后，它会同时将这个写命令写入到 「AOF 缓冲区」和 「AOF 重写缓冲区」。 当子进程完成 AOF 重写工作（扫描数据库中所有数据，逐一把内存数据的键值对转换成一条命令，再将命令记录到重写日志）后，会向主进程发送一条信号，主进程收到该信号后，会调用一个信号处理函数，该函数主要做以下工作： 将 AOF 重写缓冲区中的所有内容追加到新的 AOF 的文件中，使得新旧两个 AOF 文件所保存的数据库状态一致； 新的 AOF 的文件进行改名，覆盖现有的 AOF 文件。 RDB 快照是如何实现的呢？AOF 缺点：因为 AOF 日志记录的是操作命令，不是实际的数据，所以用 AOF 方法做故障恢复时，需要全量把日志都执行一遍，一旦 AOF 日志非常多，势必会造成 Redis 的恢复操作缓慢。 RDB 快照就是记录某一个瞬间的内存数据，记录的是实际数据，而 AOF 文件记录的是命令操作的日志，而不是实际的数据。 因此在 Redis 恢复数据时， RDB 恢复数据的效率会比 AOF 高些，因为直接将 RDB 文件读入内存就可以，不需要像 AOF 那样还需要额外执行操作命令的步骤才能恢复数据。 RDB 做快照时会阻塞线程吗？ Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave，他们的区别就在于是否在「主线程」里执行： 执行了 save 命令，就会在主线程生成 RDB 文件，由于和执行操作命令在同一个线程，所以如果写入 RDB 文件的时间太长，会阻塞主线程； 执行了 bgsave 命令，会创建一个子进程来生成 RDB 文件，这样可以避免主线程的阻塞； RDB 快照缺点 Redis 的快照是全量快照，也就是说每次执行快照，都是把内存中的「所有数据」都记录到磁盘中。所以执行快照是一个比较重的操作，如果频率太频繁，可能会对 Redis 性能产生影响。如果频率太低，服务器故障时，丢失的数据会更多。 RDB 在执行快照的时候，数据能修改吗？ 可以的，执行 bgsave 过程中，Redis 依然可以继续处理操作命令的，也就是数据是能被修改的，关键的技术就在于写时复制技术（Copy-On-Write, COW）。 执行 bgsave 命令的时候，会通过 fork() 创建子进程，此时子进程和父进程是共享同一片内存数据的，因为创建子进程的时候，会复制父进程的页表，但是页表指向的物理内存还是一个，此时如果主线程执行读操作，则主线程和 bgsave 子进程互相不影响。 如果主线程执行写操作，则被修改的数据会复制一份副本，然后 bgsave 子进程会把该副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据。 存在问题 ： 在 RDB 执行快照的时候，主线程对内存的修改只能到下一次 RDB 的时候才能持久化。假如在完成本次 RDB 快照后 redis 宕机，就会丢失在快照期间修改的数据 另外，写时复制的时候会出现这么个极端的情况。 在 Redis 执行 RDB 持久化期间，刚 fork 时，主进程和子进程共享同一物理内存，但是途中主进程处理了写操作，修改了共享内存，于是当前被修改的数据的物理内存就会被复制一份。 那么极端情况下，如果所有的共享内存都被修改，则此时的内存占用是原先的 2 倍。 所以，针对写操作多的场景，我们要留意下快照过程中内存的变化，防止内存被占满了。 为什么会有混合持久化？RDB 优点是数据恢复速度快，但是快照的频率不好把握。 AOF 优点是丢失数据少，但是数据恢复不快。 AOF 文件的前半部分是 RDB 格式的全量数据，后半部分是 AOF 格式的增量数据。 这样的好处在于，重启 Redis 加载数据的时候，由于前半部分是 RDB 内容，这样加载的时候速度会很快。 加载完 RDB 的内容后，才会加载后半部分的 AOF 内容，这里的内容是 Redis 后台子进程重写 AOF 期间，主线程处理的操作命令，可以使得数据更少的丢失。 混合持久化优点： 混合持久化工作在 AOF 日志重写过程，==既保证了 Redis 重启速度，又降低数据丢失风险。== 混合持久化缺点： AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性差； 兼容性差，如果开启混合持久化，那么此混合持久化 AOF 文件，就不能用在 Redis 4.0 之前版本了。 Redis 集群/高可用主从复制主从复制是 Redis 高可用服务的最基础的保证，实现方案就是将从前的一台 Redis 服务器，同步数据到多台从 Redis 服务器上，即一主多从的模式，且主从服务器之间采用的是「读写分离」的方式。 主服务器可以进行读写操作，当发生写操作时自动将写操作同步给从服务器，而从服务器一般是只读，并接受主服务器同步过来写操作命令，然后执行这条命令。 也就是说，所有的数据修改只在主服务器上进行，然后将最新的数据同步给从服务器，这样就使得主从服务器的数据是一致的。 主从复制共有三种模式：全量复制、基于长连接的命令传播、增量复制。 主从服务器第一次同步的时候，就是采用全量复制，此时主服务器会两个耗时的地方，分别是生成 RDB 文件和传输 RDB 文件。 第一次同步完成后，主从服务器都会维护着一个长连接，主服务器在接收到写操作命令后，就会通过这个连接将写命令传播给从服务器，来保证主从服务器的数据一致性。 如果遇到网络断开，增量复制就可以上场了，不过这个还跟 repl_backlog_size 这个大小有关系。 如果它配置的过小，主从服务器网络恢复时，可能发生「从服务器」想读的数据已经被覆盖了，那么这时就会导致主服务器采用全量复制的方式。所以为了避免这种情况的频繁发生，要调大这个参数的值，以降低主从服务器断开后全量同步的概率。 全量复制 第一阶段是建立链接、协商同步【发送主服务器的 runID 和复制进度 offset】； 第二阶段是主服务器同步数据给从服务器【bgsave 生成 RDB】； 第三阶段是主服务器发送新写操作命令给从服务器【将第二阶段期间新增的写命令存放在 replication buffer，然后发送给从服务器执行】。 基于长连接的命令传播 增量复制主从服务器在完成第一次同步后，就会基于长连接进行命令传播。 若网络中断则会使用增量复制重新同步。 主要有三个步骤： 从服务器在恢复网络后，会发送 psync 命令给主服务器，此时的 psync 命令里的 offset 参数不是 -1； 主服务器收到该命令后，然后用 CONTINUE 响应命令告诉从服务器接下来采用增量复制的方式同步数据； 然后主服务将主从服务器断线期间，所执行的写命令发送给从服务器，然后从服务器执行这些命令 主服务器怎么知道要将哪些增量数据发送给从服务器呢？ 答案藏在这两个东西里： repl_backlog_buffer，是一个「环形」缓冲区，用于主从服务器断连后，从中找到差异的数据； replication offset，标记上面那个缓冲区的同步进度，主从服务器都有各自的偏移量，主服务器使用 master_repl_offset 来记录自己「写」到的位置，从服务器使用 slave_repl_offset 来记录自己「读」到的位置。 怎么判断 Redis 某个节点是否正常工作？基本都是通过互相的 ping-pong 心态检测机制，如果有一半以上的节点去 ping 一个节点的时候没有 pong 回应，集群就会认为这个节点挂掉了，会断开与这个节点的连接。 Redis 主从节点发送的心态间隔是不一样的，而且作用也有一点区别： Redis 主节点默认每隔 10 秒对从节点发送 ping 命令，判断从节点的存活性和连接状态，可通过参数repl-ping-slave-period控制发送频率。 Redis 从节点每隔 1 秒发送 replconf ack{offset} 命令，给主节点上报自身当前的复制偏移量，目的是为了： 实时监测主从节点网络状态； 上报自身复制偏移量， 检查复制数据是否丢失， 如果从节点数据丢失， 再从主节点的复制缓冲区中拉取丢失数据。 主从复制架构中，过期key如何处理？主节点处理了一个key或者通过淘汰算法淘汰了一个key，这个时间主节点模拟一条del命令发送给从节点，从节点收到该命令后，就进行删除key的操作。 Redis 是同步复制还是异步复制？Redis 主节点每次收到写命令之后，先写到内部的缓冲区，然后异步发送给从节点。 主从复制中两个 Buffer(replication buffer 、repl backlog buffer)有什么区别？ 出现的阶段不一样： repl backlog buffer 是在增量复制阶段出现，一个主节点只分配一个 repl backlog buffer； replication buffer 是在全量复制阶段和增量复制阶段都会出现，主节点会给每个新连接的从节点，分配一个 replication buffer； 这两个 Buffer 都有大小限制的，当缓冲区满了之后，发生的事情不一样： 当 repl backlog buffer 满了，因为是环形结构，会直接覆盖起始位置数据; 当 replication buffer 满了，会导致连接断开，删除缓存，从节点重新连接，重新开始全量复制。 哨兵模式 在使用 Redis 主从服务的时候，会有一个问题，就是当 Redis 的主从服务器出现故障宕机时，需要手动进行恢复。 为了解决这个问题，Redis 增加了哨兵模式（Redis Sentinel），因为哨兵模式做到了可以监控主从服务器，并且提供主从节点故障转移的功能。 Redis Cluster切片集群模式当 Redis 缓存数据量大到一台服务器无法缓存时，就需要使用 Redis 切片集群（Redis Cluster ）方案，它将数据分布在不同的服务器上，以此来降低系统对单主节点的依赖，从而提高 Redis 服务的读写性能。 Redis Cluster 是如何分片的?Redis Cluster 方案采用哈希槽（Hash Slot），来处理数据和节点之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 【2^14】个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中，具体执行过程分为两大步： 根据键值对的 key，按照 CRC16 算法 (opens new window)计算一个 16 bit 的值。 再用 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。 接下来的问题就是，这些哈希槽怎么被映射到具体的 Redis 节点上的呢？有两种方案： 平均分配： 在使用 cluster create 命令创建 Redis 集群时，Redis 会自动把所有哈希槽平均分布到集群节点上。比如集群中有 9 个节点，则每个节点上槽的个数为 16384/9 个。 手动分配： 可以使用 cluster meet 命令手动建立节点间的连接，组成集群，再使用 cluster addslots 命令，指定每个节点上的哈希槽个数。 为了方便你的理解，我通过一张图来解释数据、哈希槽，以及节点三者的映射分布关系。 上图中的切片集群一共有 2 个节点，假设有 4 个哈希槽（Slot 0～Slot 3）时，我们就可以通过命令手动分配哈希槽，比如节点 1 保存哈希槽 0 和 1，节点 2 保存哈希槽 2 和 3。 12redis-cli -h 192.168.1.10 –p 6379 cluster addslots 0,1redis-cli -h 192.168.1.11 –p 6379 cluster addslots 2,3 然后在集群运行的过程中，key1 和 key2 计算完 CRC16 值后，对哈希槽总个数 4 进行取模，再根据各自的模数结果，就可以被映射到哈希槽 1（对应节点1） 和 哈希槽 2（对应节点2）。 需要注意的是，在手动分配哈希槽时，需要把 16384 个槽都分配完，否则 Redis 集群无法正常工作。 一个最基本的 Redis Cluster 架构是怎样的?为了保证高可用，Redis Cluster 至少需要 3 个 master 以及 3 个 slave，也就是说每个 master 必须有 1个 slave。master 和 slave 之间做主从复制，slave 会实时同步 master 上的数据。 不同于普通的 Redis 主从架构，这里的 slave 不对外提供读服务，主要用来保障 master 的高可用，当master 出现故障的时候替代它。 如果 master 有多个 slave 的话，Redis Cluster 中的其他节点会从这个 master 的所有 slave 中选出一 个替代 master 继续提供服务。Redis Cluster 总是希望数据最完整的 slave 被提升为新的 master。 **Redis Cluster 是去中心化的(各个节点基于 Gossip 进行通信)**，任何一个 master 出现故障，其它的 master 节点不受影响，因为 key 找的是哈希槽而不是 Redis 节点。不过，Redis Cluster 至少要保证宕 机的 master 有一个 slave 可用。 Redis Cluster 扩容和缩容本质是进行重新分片，动态迁移哈希槽。为了保证 Redis Cluster 在扩容和缩容期间依然能够对外正常提供服务，Redis Cluster 提供了重定向机制，两种不同的类型: ASK 重定向 ASK 重定向并不会同步更新客户端缓存的哈希槽分配信息 MOVED 重定向。 如果客户端请求的 key 对应的哈希槽应该迁移完成的话，就会返回 -MOVED 重定向错误，告知客户端当 前哈希槽是由哪个节点负责，客户端向目标节点发送请求并更新缓存的哈希槽分配信息。 Redis Cluster中的节点是怎么进行通信的?Redis Cluster 中的各个节点基于 Gossip 协议 来进行通信共享信息，每个 Redis 节点都维护了一份集群的状态信息。 Redis Cluster 相当于是内置了 Sentinel 机制，Redis Cluster 内部的各个 Redis 节点通过 Gossip 协议互相探测健康状态，在故障时可 以自动切换。 集群脑裂导致数据丢失怎么办？ 什么是脑裂？ 总结：由于网络问题，集群节点之间失去联系。主从数据不同步；重新平衡选举，产生两个主服务。等网络恢复，旧主节点会降级为从节点，再与新主节点进行同步复制的时候，由于会从节点会清空自己的缓冲区，所以导致之前客户端写入的数据丢失了。 解决方案 当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。 原主库就会被限制接收客户端写请求，客户端也就不能在原主库中写入新数据了。 等到新主库上线时，就只有新主库能接收和处理客户端请求，此时，新写的数据会被直接写到新主库中。而原主库会被哨兵降为从库，即使它的数据被清空了，也不会有新数据丢失。 Redis 过期删除与内存淘汰Redis 使用的过期删除策略是什么？Redis 是可以对 key 设置过期时间的，因此需要有相应的机制将已过期的键值对删除，而做这个工作的就是过期键值删除策略。 每当我们对一个 key 设置了过期时间时，Redis 会把该 key 带上过期时间存储到一个过期字典（expires dict）中，也就是说「过期字典」保存了数据库中所有 key 的过期时间。 当我们查询一个 key 时，Redis 首先检查该 key 是否存在于过期字典中： 如果不在，则正常读取键值； 如果存在，则会获取该 key 的过期时间，然后与当前系统时间进行比对，如果比系统时间大，那就没有过期，否则判定该 key 已过期。 Redis 使用的过期删除策略是「惰性删除+定期删除」这两种策略配和使用。 什么是惰性删除策略？ 惰性删除策略的做法是，不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key。 惰性删除策略的优点： 因为每次访问时，才会检查 key 是否过期，所以此策略只会使用很少的系统资源，因此，惰性删除策略对 CPU 时间最友好。 惰性删除策略的缺点： 如果一个 key 已经过期，而这个 key 又仍然保留在数据库中，那么只要这个过期 key 一直没有被访问，它所占用的内存就不会释放，造成了一定的内存空间浪费。所以，惰性删除策略对内存不友好。 什么是定期删除策略？ 定期删除策略的做法是，每隔一段时间「随机」从数据库中取出一定数量的 key 进行检查，并删除其中的过期key。 Redis 的定期删除的流程： 从过期字典中随机抽取 20 个 key； 检查这 20 个 key 是否过期，并删除已过期的 key； 如果本轮检查的已过期 key 的数量，超过 5 个（20/4），也就是「已过期 key 的数量」占比「随机抽取 key 的数量」大于 25%，则继续重复步骤 1；如果已过期的 key 比例小于 25%，则停止继续删除过期 key，然后等待下一轮再检查。 可以看到，定期删除是一个循环的流程。那 Redis 为了保证定期删除不会出现循环过度，导致线程卡死现象，为此增加了定期删除循环流程的时间上限，默认不会超过 25ms。 定期删除策略的优点： 通过限制删除操作执行的时长和频率，来减少删除操作对 CPU 的影响，同时也能删除一部分过期的数据减少了过期键对空间的无效占用。 定期删除策略的缺点： 难以确定删除操作执行的时长和频率。如果执行的太频繁，就会对 CPU 不友好；如果执行的太少，那又和惰性删除一样了，过期 key 占用的内存不会及时得到释放。 可以看到，惰性删除策略和定期删除策略都有各自的优点，所以 Redis 选择「惰性删除+定期删除」这两种策略配和使用，以求在合理使用 CPU 时间和避免内存浪费之间取得平衡。 Redis 持久化时，对过期键会如何处理的？Redis 持久化文件有两种格式：RDB（Redis Database）和 AOF（Append Only File），下面我们分别来看过期键在这两种格式中的呈现状态。 RDB 文件分为两个阶段，RDB 文件生成阶段和加载阶段。 RDB 文件生成阶段：从内存状态持久化成 RDB（文件）的时候，会对 key 进行过期检查，过期的键「不会」被保存到新的 RDB 文件中，因此 Redis 中的过期键不会对生成新 RDB 文件产生任何影响。 RDB 加载阶段：RDB 加载阶段时，要看服务器是主服务器还是从服务器，分别对应以下两种情况： 如果 Redis 是「主服务器」运行模式的话，在载入 RDB 文件时，程序会对文件中保存的键进行检查，过期键「不会」被载入到数据库中。所以过期键不会对载入 RDB 文件的主服务器造成影响； 如果 Redis 是「从服务器」运行模式的话，在载入 RDB 文件时，不论键是否过期都会被载入到数据库中。但由于主从服务器在进行数据同步时，从服务器的数据会被清空。所以一般来说，过期键对载入 RDB 文件的从服务器也不会造成影响。 AOF 文件分为两个阶段，AOF 文件写入阶段和 AOF 重写阶段。 AOF 文件写入阶段：当 Redis 以 AOF 模式持久化时，如果数据库某个过期键还没被删除，那么 AOF 文件会保留此过期键，当此过期键被删除后，Redis 会向 AOF 文件追加一条 DEL 命令来显式地删除该键值。 AOF 重写阶段：执行 AOF 重写时，会对 Redis 中的键值对进行检查，已过期的键不会被保存到重写后的 AOF 文件中，因此不会对 AOF 重写造成任何影响。 Redis 主从模式中，对过期键会如何处理？当 Redis 运行在主从模式下时，从库不会进行过期扫描，从库对过期的处理是被动的。也就是即使从库中的 key 过期了，如果有客户端访问从库时，依然可以得到 key 对应的值，像未过期的键值对一样返回。 从库的过期键处理依靠主服务器控制，主库在 key 到期时，会在 AOF 文件里增加一条 del 指令，同步到所有的从库，从库通过执行这条 del 指令来删除过期的 key。 Redis 内存淘汰策略有哪些？虽然redis的确是不断的删除一些过期数据，但是很多没有设置过期时间的数据也会越来越多，那么redis内存不够用的时候是怎么处理的呢？这里我们就会谈到淘汰策略。 ==当redis的内存超过最大允许的内存之后，Redis会触发内存淘汰策略，删除一些不常用的数据，以保证redis的正常运行== Redis 内存淘汰策略共有八种，这八种策略大体分为「不进行数据淘汰」和「进行数据淘汰」两类策略。 1、不进行数据淘汰的策略 noeviction（Redis3.0之后，默认的内存淘汰策略） ：它表示当运行内存超过最大设置内存时，不淘汰任何数据，而是不再提供服务，直接返回错误。 2、进行数据淘汰的策略 针对「进行数据淘汰」这一类策略，又可以细分为「在设置了过期时间的数据中进行淘汰」和「在所有数据范围内进行淘汰」这两类策略。 在设置了过期时间的数据中进行淘汰： volatile-random：随机淘汰设置了过期时间的任意键值； volatile-ttl：优先淘汰更早过期的键值。 volatile-lru（Redis3.0 之前，默认的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最久未使用的键值； volatile-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰所有设置了过期时间的键值中，最少使用的键值； 在所有数据范围内进行淘汰： allkeys-random：随机淘汰任意键值; allkeys-lru：淘汰整个键值中最久未使用的键值； allkeys-lfu（Redis 4.0 后新增的内存淘汰策略）：淘汰整个键值中最少使用的键值。 内存淘汰策略可以通过配置文件来修改，redis.conf对应的配置项是maxmemory-policy 修改对应的值就行，默认是noeviction **缓存*缓存读写策略有哪几种？**Cache Aside Pattern（旁路缓存模式）写： 先更新 db 然后直接删除 cache 。 读： 从 cache 中读取数据，读取到就直接返回 cache 中读取不到的话，就从 db 中读取数据返回 再把数据放到 cache 中。 问题： 在写数据的过程中，可以先删除 cache ，后更新 db 么？ redis 速度要比 db 快，删 cache 和更新 db 中间可能会穿插读操作 假如使用了该方案可以用双删解决 在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？ 发生情况概率很小，redis 速度要比 db 快。在缓存没有数据情况下，从 db 读数据和更新 cache 中穿插更新 db 操作 Cache Aside 策略适合读多写少的场景，不适合写多的场景 Cache Aside Pattern 的缺陷 缺陷 1：首次请求数据一定不在 cache 的问题 解决办法：可以将热点数据可以提前放入 cache 中。 缺陷 2：写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率 。 解决办法：数据库和缓存数据强一致（加锁同步更新） Read/Write Through Pattern（读写穿透）写（Write Through）： 先查 cache，cache 中不存在，直接更新 db。 cache 中存在，则先更新 cache，然后 cache 服务自己更新 db（同步更新 cache 和 db）。 读(Read Through)： 从 cache 中读取数据，读取到就直接返回 。 读取不到的话，先从 db 加载，写入到 cache 后返回响应。 问题： 和 Cache Aside Pattern 一样， Read-Through Pattern 也有首次请求数据一定不再 cache 的问题，对于热点数据可以提前放入缓存中。 Write Behind Pattern（异步缓存写入）Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。 Write Back 策略特别适合写多的场景 问题：还没同步，缓存宕掉 使用场景：消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制 优势：db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。 缓存穿透根本不存在于缓存中，也不存在于数据库中 解决： 1）缓存空值或者默认值 *2）布隆过滤器 布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成 布隆过滤器会通过 3 个操作完成标记： 第一步，使用 N 个哈希函数分别对数据做哈希计算，得到 N 个哈希值； 第二步，将第一步得到的 N 个哈希值对位图数组的长度取模，得到每个哈希值在位图数组的对应位置。 第三步，将每个哈希值在位图数组的对应位置的值设置为 1； 缺点：存在哈希冲突的可能性，判断可能存在，但是不一定存在；但判断不存在，一定不存在 扩容：因为布隆过滤器的不可逆，我们没法重新建一个更大的布隆过滤器然后去把数据重新导入。这边采取的扩容的方法是，保留原有的布隆过滤器，建立一个更大的，新增数据都放在新的布隆过滤器中，去重的时候检查所有的布隆过滤器。 3）非法请求限制 在 API 入口处我们要判断求请求参数是否合理（请求参数是否含有非法值、请求字段是否存在） 缓存雪崩 缓存在同一时间大面积的失效或者redis宕机，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力。 针对大量缓存失效的情况： 设置不同的失效时间比如随机设置缓存的失效时间。 互斥锁，保证只有一个请求来构建缓存 不给热点数据设置过期时间，由后台异步更新缓存 针对 Redis 故障宕机情况： 采用 Redis 集群。主从节点的方式，如果 Redis 缓存的主节点故障宕机，从节点可以切换成为主节点，继续提供缓存服务 服务熔断。暂停业务应用对缓存服务的访问，直接返回错误 请求限流机制。只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务 缓存击穿请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 可以发现缓存击穿跟缓存雪崩很相似，你可以认为缓存击穿是缓存雪崩的一个子集。 解决： 不给热点数据设置过期时间，由后台异步更新缓存 请求数据库写数据到缓存之前，先获取互斥锁，保证只有一个请求会落到数据库上，减少数据库的压力。 数据库和缓存如何保证一致性？1）先更新数据库，再更新缓存；先更新缓存，再更新数据库 当两个请求并发更新同一条数据的时候，可能会出现缓存和数据库中的数据不一致的现象 2）先更新数据库，还是先删除缓存？ Cache Aside 策略：先更新数据库，再删除缓存 当出现读-更新-删，会出现不一致。但出现不一致的概率低，因为缓存的写入通常要远远快于数据库的写入， 所以，「先更新数据库 + 再删除缓存」的方案，是可以保证数据一致性的。同时可以还给缓存数据加上了「过期时间」来兜底，达到最终一致。 但是更新数据库和删除缓存不能保证两个操作都执行成功，也就会需要用到过期时间来兜底，这个中间会存在不一致性。 解决方法： 重试机制。 引入消息队列，将第二个操作（删除缓存）要操作的数据加入到消息队列，由消费者来操作数据。 订阅 MySQL binlog，再操作缓存 两种方法都是采用异步操作缓存 为什么是删除缓存，而不是更新缓存呢？删除一个数据，相比更新一个数据更加轻量级，出问题的概率更小。在实际业务中，缓存的数据可能不是直接来自数据库表，也许来自多张底层数据表的聚合。 系统设计中有一个思想叫 Lazy Loading，适用于那些加载代价大的操作，删除缓存而不是更新缓存，就是懒加载思想的一个应用。 Redis 应用基于 Redis 实现分布式锁实现 Redis 的 SET 命令有个 NX 参数可以实现「key不存在才插入」，所以可以用它来实现分布式锁： 如果 key 不存在，则显示插入成功，可以用来表示加锁成功； 如果 key 存在，则会显示插入失败，可以用来表示加锁失败。 用 EX 参数设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作 1SET lock_key unique_value NX PX 10000 lock_key 就是 key 键； unique_value 是客户端生成的唯一的标识，区分来自不同客户端的锁操作； NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作； PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁 基于 Redis 实现分布式锁的优点： 性能高效（这是选择缓存实现分布式锁最核心的出发点）。 实现方便。很多研发工程师选择使用 Redis 来实现分布式锁，很大成分上是因为 Redis 提供了 setnx 方法，实现分布式锁很方便。 避免单点故障（因为 Redis 是跨集群部署的，自然就避免了单点故障）。 基于 Redis 实现分布式锁的缺点： 超时时间不好设置 。如果锁的超时时间设置过长，会影响性能，如果设置的超时时间过短会保护不到共享资源。比如在有些场景中，一个线程 A 获取到了锁之后，由于业务代码执行时间可能比较长，导致超过了锁的超时时间，自动失效，注意 A 线程没执行完，后续线程 B 又意外的持有了锁，意味着可以操作共享资源，那么两个线程之间的共享资源就没办法进行保护了。 那么如何合理设置超时时间呢？ 我们可以基于续约的方式设置超时时间：先给锁设置一个超时时间，然后启动一个守护线程，让守护线程在一段时间后，重新设置这个锁的超时时间。实现方式就是：写一个守护线程，然后去判断锁的情况，当锁快失效的时候，再次进行续约加锁，当主线程执行完成后，销毁续约锁即可，不过这种方式实现起来相对复杂。 Redis 主从复制模式中的数据是异步复制的，这样导致分布式锁的不可靠性。如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。 单机redis挂了怎么办，主从集群如何保证分布式锁RedLock 是对集群的每个节点进行加锁，如果大多数节点（N/2+1）加锁成功，则才会认为加锁成功。 这样即使集群中有某个节点挂掉了，因为大部分集群节点都加锁成功了，所以分布式锁还是可以继续使用的。 存在问题 RedLock 主要存在以下两个问题： 性能问题：RedLock 要等待大多数节点返回之后，才能加锁成功，而这个过程中可能会因为网络问题，或节点超时的问题，影响加锁的性能。 并发安全性问题：当客户端加锁时，如果遇到 GC 可能会导致加锁失效，但 GC 后误认为加锁成功的安全事故，例如以下流程： 客户端 A 请求 3 个节点进行加锁。 在节点回复处理之前，客户端 A 进入 GC 阶段（存在 STW，全局停顿）。 之后因为加锁时间的原因，锁已经失效了。 客户端 B 请求加锁（和客户端 A 是同一把锁），加锁成功。 客户端 A GC 完成，继续处理前面节点的消息，误以为加锁成功。 此时客户端 B 和客户端 A 同时加锁成功，出现并发安全性问题。 其余的分布式锁的实现基于数据库实现分布式锁：主要是利用数据库的唯一索引来实现，唯一索引天然具有排他性，这刚好符合我们对锁的要求：同一时刻只能允许一个竞争者获取锁。加锁时我们在数据库中插入一条锁记录，利用业务id进行防重。当第一个竞争者加锁成功后，第二个竞争者再来加锁就会抛出唯一索引冲突，如果抛出这个异常，我们就判定当前竞争者加锁失败。防重业务id需要我们自己来定义，例如我们的锁对象是一个方法，则我们的业务防重id就是这个方法的名字，如果锁定的对象是一个类，则业务防重id就是这个类名。 基于Zookeeper：Zookeeper一般用作配置中心，其实现分布式锁的原理和Redis类似，我们在Zookeeper中创建瞬时节点，利用节点不能重复创建的特性来保证排他性。 基于Redis实现延时任务两种方案： Redis 过期事件监听 Redisson 内置的延时队列 Redis 过期事件监听实现延时任务功能的原理？发布订阅 (pub/sub) 功能。在 pub/sub 中，引入了一个叫做 channel（频道） 的概念，有点类似于消息队列中的 topic（主题）。 在 pub/sub 模式下，生产者需要指定消息发送到哪个 channel 中，而消费者则订阅对应的 channel 以获取消息。 Redis 中有很多默认的 channel，这些 channel 是由 Redis 本身向它们发送消息的，而不是我们自己编写的代码。其中，__keyevent@0__:expired 就是一个默认的 channel，负责监听 key 的过期事件。也就是说，当一个 key 过期之后，Redis 会发布一个 key 过期的事件到__keyevent@&lt;db&gt;__:expired这个 channel 中。 我们只需要监听这个 channel，就可以拿到过期的 key 的消息，进而实现了延时任务功能。 Redis 过期事件监听实现延时任务功能有什么缺陷？1、时效性差：过期事件消息是在 Redis 服务器删除 key 时发布的，而不是一个 key 过期之后就会就会直接发布。因此，就会存在我设置了 key 的过期时间，但到了指定时间 key 还未被删除，进而没有发布过期事件的情况。 2、丢消息：Redis 的 pub/sub 模式中的消息并不支持持久化，这与消息队列不同 3、多服务实例下消息重复消费：Redis 的 pub/sub 模式目前只有广播模式，这意味着当生产者向特定频道发布一条消息时，所有订阅相关频道的消费者都能够收到该消息。 Redisson 延迟队列原理是什么？有什么优势？Redisson 的延迟队列 RDelayedQueue 是基于 Redis 的 SortedSet 来实现的。SortedSet 是一个有序集合，其中的每个元素都可以设置一个分数，代表该元素的权重。Redisson 利用这一特性，将需要延迟执行的任务插入到 SortedSet 中，并给它们设置相应的过期时间作为分数。 优势： 减少了丢消息的可能：DelayedQueue 中的消息会被持久化，即使 Redis 宕机了，根据持久化机制，也只可能丢失一点消息，影响不大。当然了，你也可以使用扫描数据库的方法作为补偿机制。 消息不存在重复消费问题：每个客户端都是从同一个目标队列中获取任务的，不存在重复消费的问题。 大 key 如何处理什么是 Redis 大 key大 key 并不是指 key 的值很大，而是 key 对应的 value 很大。 一般而言，下面这两种情况被称为大 key： String 类型的值大于 10 KB； Hash、List、Set、ZSet 类型的元素的个数超过 5000个； 大 key 会带来以下四种影响： 客户端超时阻塞。由于 Redis 执行命令是单线程处理，然后在操作大 key 时会比较耗时，那么就会阻塞 Redis，从客户端这一视角看，就是很久很久都没有响应。 引发网络阻塞。每次获取大 key 产生的网络流量较大，如果一个 key 的大小是 1 MB，每秒访问量为 1000，那么每秒会产生 1000MB 的流量，这对于普通千兆网卡的服务器来说是灾难性的。 阻塞工作线程。如果使用 del 删除大 key 时，会阻塞工作线程，这样就没办法处理后续的命令。 内存分布不均。集群模型在 slot 分片均匀情况下，会出现数据和查询倾斜情况，部分有大 key 的 Redis 节点占用内存多，QPS 也会比较大。 大 key 如何产生 一直往 value 塞数据，没有删除机制 没有合理做分片，将大 key 变小 key 如何找到大 key ？1、redis-cli –bigkeys 查找大key 2、使用 SCAN 命令查找大 key 3、使用 RdbTools 工具查找大 key 解决大 key 问题 删除大 key unlink 异步惰性非阻塞删除 scan 游标式迭代扫描删除 压缩和拆分 key 对于 string采用序列化、压缩算法 对于 string 压缩后还是大 key，则进行拆分，使用 multiget 实现事务读取 对于 list/set 等集合，进行分片 Redis 管道有什么用？管道技术（Pipeline）是客户端提供的一种批处理技术，用于一次处理多个 Redis 命令，从而提高整个交互的性能。 使用管道技术可以解决多个命令执行时的网络等待，它是把多个命令整合到一起发送给服务器端处理之后统一返回给客户端，这样就免去了每条命令执行后都要等待的情况，从而有效地提高了程序的执行效率。 但使用管道技术也要注意避免发送的命令过大，或管道内的数据太多而导致的网络阻塞。 要注意的是，管道技术本质上是客户端提供的功能，而非 Redis 服务器端的功能。 Redis 事务Redis事务的概念事务提供了一种将多个命令打包，然后一次性、有序地执行的机制。 多个命令会被人队到事务队列中， 然后按先进先出(FIFO)的顺序执行。 事务在执行过程中不会被中断，当事务队列中的所有命令都被执行完毕之后，事务才会结束。 Redis事务不支持回滚机制。 Redis 事务没有原子性 有持久性和一致性 至于隔离性, 都不存在多个事务的情况 毕竟单线程 Redis事务没有隔离级别批量操作在发送 EXEC 命令前被放入队列缓存，并不会被实际执行，也就不存在事务内的查询要看到事务里的更新，事务外查询不能看到。 Redis事务不保证原子性Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 Redis事务的三个阶段（1）开始事务 （2）命令入队 （3）执行事务 Redis事务命令 Redis事务案例正常执行 放弃事务 全体连坐 在事务队列中存在命令性错误（类似于java编译性错误），则执行EXEC命令时，所有命令都不会执行。 比如 命令写错 冤头债主 在事务队列中存在语法性错误（类似于java的1/0的运行时异常），则执行EXEC命令时，其他正确命令会被执行，错误命令抛出异常。 比如 对 string incr 使用watch 案例一：使用watch检测balance，事务期间balance数据未变动，事务执行成功。 案例二：使用watch检测balance，在开启事务后（标注1处），在新窗口执行标注2中的操作，更改balance的值，模拟其他客户端在事务执行期间更改watch监控的数据，然后再执行标注1后命令，执行EXEC后，事务未成功执行。 一但执行 EXEC 开启事务的执行后，无论事务使用执行成功， WARCH 对变量的监控都将被取消。故当事务执行失败后，需重新执行WATCH命令对变量进行监控，并开启新的事务进行操作。","link":"/2024/03/14/Redis/"},{"title":"Spring","text":"Spring 是什么？有什么用？是什么： Spring是一个轻量级的控制反转(IoC)和面向切面(AOP)的容器框架。 有什么用： 通过控制反转和依赖注入实现松耦合。 支持面向切面的编程，将应用业务逻辑和系统服务拆分。 支持声明式事务。通过声明式方式灵活地进行事务的管理，提高开发效率和质量。 方便集成各种优秀框架。（quartz、mybatis） SpringBoot 是什么？有什么用？是什么： 快速开发的 spring 框架 有什么用： 快速开发 快速整合 配置简化 内嵌服务容器 SpringBoot 核心注解@SpringBootApplication 包括三个重要注解： @EnableAutoConfiguration。提供自动配置 @Configuration。代替 xml配置文件 @ComponentScan。开启组件扫描，即自动扫描包路径下的 @Component 注解进行注册 bean 实例到 context 中 **Spring 核心之控制反转（IOC）控制反转 - Inversion of Control (IoC)： IoC 不是一种技术，只是一种思想，它能指导我们设计出松耦合、更优良的程序。把创建和查找注入依赖对象的控制权交给了IoC容器，所以对象与对象之间是松散耦合，利于功能复用，更重要的是使得程序的整个体系结构变得非常灵活。 也就是从原来的由用户管理Bean转变为IoC 容器管理Bean。 谁控制谁，控制什么？ 谁控制谁？当然是IoC 容器控制了对象；控制什么？那就是主要控制了外部资源获取 为何是反转，哪些方面反转了? 为何是反转？因为由容器帮我们查找及注入依赖对象，对象只是被动的接受依赖对象，所以是反转；哪些方面反转了？依赖对象的获取被反转了。 IoC和DI是什么关系控制反转是通过依赖注入实现的，其实它们是同一个概念的不同角度描述。“依赖注入”明确描述了“被注入对象依赖IoC容器配置依赖对象”。通俗来说就是IoC是设计思想，DI是实现方式。 IoC配置的三种方式xml 配置顾名思义，就是将bean的信息配置.xml文件里，通过Spring加载文件为我们创建bean。这种方式出现很多早前的SSM项目中，将第三方类库或者一些配置工具类都以这种方式进行配置，主要原因是由于第三方类不支持Spring注解。 优点： 可以使用于任何场景，结构清晰，通俗易懂 缺点： 配置繁琐，不易维护，枯燥无味，扩展性差 举例： 配置xx.xml文件 声明命名空间和配置bean Java 配置将类的创建交给我们配置的JavcConfig类来完成，Spring只负责维护和管理，采用纯Java创建方式。其本质上就是把在XML上的配置声明转移到Java配置类中 优点：适用于任何场景，配置方便，因为是纯Java代码，扩展性高，十分灵活 缺点：由于是采用Java类的方式，声明不明显，如果大量配置，可读性比较差 举例： 创建一个配置类， 添加@Configuration注解声明为配置类 创建方法，方法上加上@bean，该方法用于创建实例并返回，该实例创建后会交给spring管理，方法名建议与实例名相同（首字母小写）。注：实例类不需要加任何注解 注解配置通过在类上加注解的方式，来声明一个类交给Spring管理，Spring会自动扫描带有@Component，@Controller，@Service，@Repository这四个注解的类，然后帮我们创建并管理，前提是需要先配置Spring的注解扫描器。 优点：开发便捷，通俗易懂，方便维护。 缺点：具有局限性，对于一些第三方资源，无法添加注解。只能采用XML或JavaConfig的方式配置 举例： 对类添加@Component相关的注解，比如@Controller，@Service，@Repository 设置ComponentScan的basePackage, 比如&lt;context:component-scan base-package='tech.pdai.springframework'&gt;, 或者@ComponentScan(\"tech.pdai.springframework\")注解，或者 new AnnotationConfigApplicationContext(\"tech.pdai.springframework\")指定扫描的basePackage. 依赖注入的三种方式setter方式在XML配置方式中，property都是setter方式注入 本质上包含两步： 第一步，需要new UserServiceImpl()创建对象, 所以需要默认构造函数 第二步，调用setUserDao()函数注入userDao的值, 所以需要setUserDao()函数 构造函数在XML配置方式中，&lt;constructor-arg&gt;是通过构造函数参数注入 注解注入以@Autowired（自动注入）注解注入为例，修饰符有三个属性：Constructor，byType，byName。默认按照byType注入。 constructor：通过构造方法进行自动注入，spring会匹配与构造方法参数类型一致的bean进行注入，如果有一个多参数的构造方法，一个只有一个参数的构造方法，在容器中查找到多个匹配多参数构造方法的bean，那么spring会优先将bean注入到多参数的构造方法中。 1234567public class FooService { private FooFormatter fooFormatter; @Autowired public FooService(FooFormatter fooFormatter) { this.fooFormatter = fooFormatter; }} byName：被注入bean的id名必须与set方法后半截匹配，并且id名称的第一个单词首字母必须小写，这一点与手动set注入有点不同。 byType：查找所有的set方法，将符合符合参数类型的bean注入。 @Autowired和@Resource以及@Inject等注解注入有何区别？@Autowired Autowired注解源码 在Spring 2.5 引入了 @Autowired 注解 123456@Target({ElementType.CONSTRUCTOR, ElementType.METHOD, ElementType.PARAMETER, ElementType.FIELD, ElementType.ANNOTATION_TYPE})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Autowired { boolean required() default true;} 从Autowired注解源码上看，可以使用在下面这些地方： 12345@Target(ElementType.CONSTRUCTOR) #构造函数@Target(ElementType.METHOD) #方法@Target(ElementType.PARAMETER) #方法参数@Target(ElementType.FIELD) #字段、枚举的常量@Target(ElementType.ANNOTATION_TYPE) #注解 还有一个value属性，默认是true。 简单总结： 1、@Autowired是Spring自带的注解，通过AutowiredAnnotationBeanPostProcessor 类实现的依赖注入 2、@Autowired可以作用在CONSTRUCTOR、METHOD、PARAMETER、FIELD、ANNOTATION_TYPE 3、@Autowired默认是根据类型（byType ）进行自动装配的 4、如果有多个类型一样的Bean候选者，需要指定按照名称（byName ）进行装配，则需要配合@Qualifier。 简单使用代码： 在字段属性上。 12@Autowiredprivate HelloDao helloDao; 或者 12345678private HelloDao helloDao;public HelloDao getHelloDao() { return helloDao;}@Autowiredpublic void setHelloDao(HelloDao helloDao) { this.helloDao = helloDao;} 或者 123456private HelloDao helloDao;//@Autowiredpublic HelloServiceImpl(@Autowired HelloDao helloDao) { this.helloDao = helloDao;}// 构造器注入也可不写@Autowired，也可以注入成功。 将@Autowired写在被注入的成员变量上，setter或者构造器上，就不用再xml文件中配置了。 如果有多个类型一样的Bean候选者，则默认根据设定的属性名称进行获取。如 HelloDao 在Spring中有 helloWorldDao 和 helloDao 两个Bean候选者。 12@Autowiredprivate HelloDao helloDao; 首先根据类型获取，发现多个HelloDao，然后根据helloDao进行获取，如果要获取限定的其中一个候选者，结合@Qualifier进行注入。 123@Autowired@Qualifier(\"helloWorldDao\")private HelloDao helloDao; 注入名称为helloWorldDao 的Bean组件。@Qualifier(“XXX”) 中的 XX是 Bean 的名称，所以 @Autowired 和 @Qualifier 结合使用时，自动注入的策略就从 byType 转变成 byName 了。 多个类型一样的Bean候选者，也可以@Primary进行使用，设置首选的组件，也就是默认优先使用哪一个。 注意：使用@Qualifier 时候，如何设置的指定名称的Bean不存在，则会抛出异常，如果防止抛出异常，可以使用： 123@Qualifier(\"xxxxyyyy\")@Autowired(required = false)private HelloDao helloDao; 在SpringBoot中也可以使用@Bean+@Autowired进行组件注入，将@Autowired加到参数上，其实也可以省略。 12345@Beanpublic Person getPerson(@Autowired Car car){ return new Person();}// @Autowired 其实也可以省略 @Resource Resource注解源码 123456@Target({TYPE, FIELD, METHOD})@Retention(RUNTIME)public @interface Resource { String name() default \"\"; // 其他省略} 从Resource注解源码上看，可以使用在下面这些地方： 123@Target(ElementType.TYPE) #接口、类、枚举、注解@Target(ElementType.FIELD) #字段、枚举的常量@Target(ElementType.METHOD) #方法 name 指定注入指定名称的组件。 简单总结： 1、@Resource是JSR250规范的实现，在javax.annotation包下 2、@Resource可以作用TYPE、FIELD、METHOD上 3、@Resource是默认根据属性名称进行自动装配的，如果有多个类型一样的Bean候选者，则可以通过name进行指定进行注入 简单使用代码： 12345@Componentpublic class SuperMan { @Resource private Car car;} 按照属性名称 car 注入容器中的组件。如果容器中BMW还有BYD两种类型组件。指定加入BMW。如下代码： 12345@Componentpublic class SuperMan { @Resource(name = \"BMW\") private Car car;} name 的作用类似 @Qualifier @Inject Inject注解源码 1234@Target({ METHOD, CONSTRUCTOR, FIELD })@Retention(RUNTIME)@Documentedpublic @interface Inject {} 从Inject注解源码上看，可以使用在下面这些地方： 123@Target(ElementType.CONSTRUCTOR) #构造函数@Target(ElementType.METHOD) #方法@Target(ElementType.FIELD) #字段、枚举的常量 简单总结： 1、@Inject是JSR330 (Dependency Injection for Java)中的规范，需要导入javax.inject.Inject jar包 ，才能实现注入 2、@Inject可以作用CONSTRUCTOR、METHOD、FIELD上 3、@Inject是根据类型进行自动装配的，如果需要按名称进行装配，则需要配合@Named； 简单使用代码： 12@Injectprivate Car car; 指定加入BMW组件。 123@Inject@Named(\"BMW\")private Car car; @Named 的作用类似 @Qualifier！ 总结1、@Autowired是Spring自带的，@Resource是JSR250规范实现的，@Inject是JSR330规范实现的 2、@Autowired、@Inject用法基本一样，不同的是@Inject没有required属性 3、@Autowired、@Inject是默认按照类型匹配的，@Resource是按照名称匹配的 4、@Autowired如果需要按照名称匹配需要和@Qualifier一起使用，@Inject和@Named一起使用，@Resource则通过name进行指定 bean 创建的方式 xml 1&lt;bean id=\"student\" class=\"cn.com.demo.Student\"/&gt; xml&lt;context:component-scan&gt; +注解@Component（@Controller 、@Service 、@Repository）定义 bean 通过@ComponentScan扫描，去除XML文件 通过@Import创建bean 如果@Import写的是配置类，则配置类会被加载为bean，配置类中有@bean注解的类，同样会被加载为bean， 容器初始化时，通过register或者registerBean进行创建bean 实现ImportSelector接口，并实现selectImports方法 实现ImportBeanDefinitionRegistrar接口，并实现registerBeanDefinitions方法 实现BeanDefinitionRegistryPostProcessor接口 *Spring核心之面向切面编程(AOP)描述一下Spring AOP？AOP为Aspect Oriented Programming的缩写，意为：面向切面编程，是一种开发理念，是 OOP面向对象编程的补充。在OOP 中最小的单元是 Class 对象，但在 AOP 中最小的单元是“切面”。一个切面可以包含很多类型和对象，对他们进行模块化管理。简单来说就是将代码中重复的部分抽取出来，在需要执行的时候使用动态代理，在不修改源码的基础上对方法进行增强。 Spring 根据类是否实现接口来判断动态代理方式，若实现接口会使用 JDK 的动态代理，核心是 InvocationHandler 接口和 Proxy 类，如果没有实现接口会使用 CGLib 动态代理，CGLiB 是在运行时动态生成某个类的子类。CGLiB 生成代理类的效率低，但是调用方法的效率高。 常用场景包括日志、统计（方法调用次数）、安防（监控，熔断、限流）、性能（缓存）等 简述 AOP 中几个重要概念 Join point：连接点，指程序执行过程中的一个点，例如方法调用、异常处理等。在 Spring AOP 中，仅支持方法级别的连接点。 Pointcut：切点，用于匹配连接点，一个 AspectJ 中包含哪些 Join point 需要由 Pointcut 进行筛选。 Advice：通知，切面对某个连接点所产生的动作 AspectJ：切面，只是一个概念，没有具体的接口或类与之对应，是 Join point，Advice 和 Pointcut 的一个统称。 Weaving：织入，在切点的引导下，将通知逻辑插入到目标方法上，使得我们的通知逻辑在方法调用时得以执行。 AOP proxy：AOP 代理，指在 AOP 实现框架中实现切面协议的对象。在 Spring AOP 中有两种代理，分别是 JDK 动态代理和 CGLIB 动态代理。 Target object：目标对象，就是被代理的对象。 在Spring AOP中关注点和横切关注点有什么不同？关注点是我们想在应用的模块中实现的行为。关注点可以被定义为：我们想实现以解决特定业务问题的方法。比如，在所有电子商务应用中，不同的关注点（或者模块）可能是库存管理、航运管理、用户管理等。 横切关注点是贯穿整个应用程序的关注点。像日志、安全和数据转换，它们在应用的每一个模块都是必须的，所以他们是一种横切关注点。 AOP有哪些可用的实现框架？基于Java的主要AOP实现有： AspectJ：完整的 AOP 实现框架 Spring AOP：非完整的 AOP 实现框架 Spring中有哪些不同的通知类型 前置通知(Before Advice): 在连接点之前执行的Advice，不过除非它抛出异常，否则没有能力中断执行流。使用 @Before 注解使用这个Advice。 返回之后通知(After Retuning Advice): 在连接点正常结束之后执行的Advice。例如，如果一个方法没有抛出异常正常返回。通过 @AfterReturning 关注使用它。 抛出（异常）后执行通知(After Throwing Advice): 如果一个方法通过抛出异常来退出的话，这个Advice就会被执行。通用 @AfterThrowing 注解来使用。 后置通知(After Advice): 无论连接点是通过什么方式退出的(正常返回或者抛出异常)都会执行在结束后执行这些Advice。通过 @After 注解使用。 围绕通知(Around Advice): 围绕连接点执行的Advice，就你一个方法调用。这是最强大的Advice。通过 @Around 注解使用。 什么是织入(weaving)？Spring AOP 框架仅支持有限的几个 AspectJ 切入点的类型，它允许将切面运用到在 IoC 容器中声明的 bean 上。如果你想使用额外的切入点类型或者将切面应用到在 Spring IoC 容器外部创建的类，那么你必须在你的 Spring 程序中使用 AspectJ 框架，并且使用它的织入特性。织入是将切面与外部的应用类型或者类连接起来以创建通知对象(adviced object)的过程。这可以在编译时(比如使用 AspectJ 编译器)、加载时或者运行时完成。Spring AOP 跟其他纯 Java AOP 框架一样，只在运行时执行织入。在协议上，AspectJ 框架支持编译时和加载时织入。 CGLIB&amp;&amp;JDKJDK动态代理的实现方案有两种，JDK动态代理和CGLIB动态代理，区别在于JDK自带的动态代理，必须要有接口，而CGLIB动态代理有没有接口都可以。 JDK动态代理：JDK原生的实现方式，需要被代理的目标类必须实现接口。通过反射。核心是 proxy 类和 invocationHandler 接口的 invoke 方法。 主要流程是： 定义一个接口，该接口是被代理对象和代理对象共同实现的。 创建一个实现 InvocationHandler 接口的代理类，并在其中实现 invoke 方法。在 invoke 方法中，可以根据需要执行额外的逻辑，然后调用目标对象的方法。 使用 Proxy 类的静态方法 newProxyInstance 创建代理对象。该方法接收三个参数：ClassLoader、要实现的接口数组以及 InvocationHandler 对象。 通过代理对象调用方法。 缺点： 只能代理接口，无法代理具体类。 被代理的目标对象必须实现至少一个接口。 动态代理的性能相对较低，因为涉及到反射操作。 1234567891011121314151617181920212223public class JDKMatrimonialAgency implements InvocationHandler { //被代理的对象，把引用给保存下来 private Object target; public Object getInstance(Object target) throws Exception{ this.target = target; Class&lt;?&gt; clazz = target.getClass(); return Proxy.newProxyInstance(clazz.getClassLoader(),clazz.getInterfaces(),this); } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { before(); Object obj = method.invoke(this.target,args); after(); return obj; } private void before(){ System.out.println(\"这里是婚介所,请提供你的需求\"); } private void after(){ System.out.println(\"已经找到合适的,尽快安排你相亲\"); }} CGLIBcglib动态代理：通过继承被代理的目标类（认干爹模式）实现代理，所以不需要目标类实现接口。(CGLIB 通过动态生成一个需要被代理类的子类（即被代理类作为父类），该子类重写被代理类的所有不是 final 修饰的方法，并在子类中采用MethodInterceptor接口的 intercept方法拦截父类所有的方法调用，进而织入横切逻辑。) 12345678910111213141516171819202122232425public class CglibMatrimonialAgency implements MethodInterceptor { public Object getInstance(Class&lt;?&gt; clazz) throws Exception{ Enhancer enhancer = new Enhancer(); //要把哪个设置为即将生成的新类的父类 enhancer.setSuperclass(clazz); enhancer.setCallback(this); return enhancer.create(); } @Override public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable { //业务的增强 before(); Object obj = methodProxy.invokeSuper(o,objects); after(); return obj; } private void before(){ System.out.println(\"这里是婚介所,请提供你的需求\"); } private void after(){ System.out.println(\"已经找到合适的,尽快安排你相亲\"); }} 对比 jdk 实现了被代理类的接口；cglib 是继承了被代理类 jdk 和 cglib 都是在运行期生成字节码，jkd 直接写 class字节码；cglib 使用 asm 框架写 class 字节码，cglib 代理实现更复杂，生成代理类比 jdk 效率低 jdk 调用代理方法，是通过反射机制调用；cglib 是通过 fastclass 机制直接调用方法，cglib 执行效率更高 过滤器&amp;&amp;拦截器&amp;&amp;Spring AOP过滤器过滤器拦截的是 URL，能过滤所有 web 请求 应用场景： 自动登录 同一设置编码格式 访问权限控制 敏感字符过滤 拦截器拦截器拦截的是部分 URL。Java中的拦截器是基于Java反射机制实现的，更准确的划分，是基于JDK实现的动态代理。 应用场景： 日志记录 权限检查：如登录检查 性能检查：检测方法的执行时间 AOP拦截的是类的元数据（类、方法） 相对于拦截器更加细致，而且非常灵活，拦截器只能针对URL做拦截，而AOP针对具体的代码，能够实现更加复杂的业务逻辑 应用场景 事务控制 异常处理 打印日志 对比三者功能类似，但各有优势，从过滤器–》拦截器–》AOP，拦截规则越来越细致，执行顺序依次是过滤器、拦截器、切面。一般情况下数据被过滤的时机越早对服务的性能影响越小，因此我们在编写相对比较公用的代码时，优先考虑过滤器，然后是拦截器，最后是aop。 拦截器和过滤器的区别 1、拦截器是SpringMVC自带的，过滤器依赖于Servlet容器。 2、拦截器是基于java的反射机制的，而过滤器是基于函数回调。 3、拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。 4、拦截器可以访问controller上下文、值栈里的对象，而过滤器不能访问。 AOP 失效场景==首先，Spring的AOP其实是通过动态代理实现的，所以，想要让AOP生效，前提必须是动态代理生效，并且可以调用到代理对象的方法== 非Spring管理的对象 Spring的AOP只能拦截由Spring容器管理的Bean对象。如果您使用了非受Spring管理的对象，则AOP将无法对其进行拦截。 同一个Bean内部方法调用 如果一个Bean内部的方法直接调用同一个Bean内部的另一个方法，AOP将无法拦截这个内部方法调用。因为AOP是基于代理的，只有通过代理对象才能触发AOP拦截。 静态方法 final 方法 AOP无法拦截final方法。final方法是不可重写的，因此AOP无法生成代理对象来拦截这些方法。 异步方法 对于使用Spring的异步特性（如@Async注解）的方法，AOP拦截器可能无法正常工作。这是因为异步方法在运行时会创建新的线程或使用线程池，AOP拦截器无法跟踪到这些新线程中的方法调用。 经典：针对第二个方法存在事务失效的问题 解决方案： 拆分两个方法，把加了事务的方法单独用一个类包装【@Servcie】 类中注入自己，用注入的类来调用加了事务的方法【@Autowired】 获取当前类的代理类来调用方法【AopContext.currentProxy()】 **Spring中 Bean 的生命周期Spring 只帮我们管理单例模式 Bean 的完整生命周期，对于 prototype 的 bean ，Spring 在创建好交给使用者之后则不会再管理后续的生命周期。 如果 BeanFactoryPostProcessor 和 Bean 关联, 首先尝试从Bean工厂中获取Bean) 如果 InstantiationAwareBeanPostProcessor 和 Bean 关联，则会调用实例化前的方法 根据配置情况调用 Bean 构造方法实例化 Bean。 利用依赖注入完成 Bean 中所有属性值的配置注入。 如果 InstantiationAwareBeanPostProcessor 和 Bean 关联，则会调用实例化后的方法 调用Bean 实现的Aware接口 ，如 BeanNameAware、BeanClassLoaderAware、BeanFactoryAware、ApplicationContextAware，设置当前 Bean 的一些属性 如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的预初始化方法对 Bean 进行加工操作，此处非常重要，Spring 的 AOP 就是利用它实现的。 如果 Bean 实现了 InitializingBean 接口，则 Spring 将调用 afterPropertiesSet() 方法。(或者有执行@PostConstruct注解的方法) 如果在配置文件中通过 init-method 属性指定了初始化方法，则调用该初始化方法。 如果 BeanPostProcessor 和 Bean 关联，则 Spring 将调用该接口的初始化方法 。此时，Bean 已经可以被应用系统使用了。 如果在 &lt;bean&gt; 中指定了该 Bean 的作用范围为 scope=”singleton”，则将该 Bean 放入 Spring IoC 的缓存池中，将触发 Spring 对该 Bean 的生命周期管理；如果在 &lt;bean&gt; 中指定了该 Bean 的作用范围为 scope=”prototype”，则将该 Bean 交给调用者，调用者管理该 Bean 的生命周期，Spring 不再管理该 Bean。 如果 Bean 实现了 DisposableBean 接口，则 Spring 会调用 destory() 方法将 Spring 中的 Bean 销毁；(或者有执行@PreDestroy注解的方法) 如果在配置文件中通过 destory-method 属性指定了 Bean 的销毁方法，则 Spring 将调用该方法对 Bean 进行销毁。 **Spring如何解决循环依赖问题Spring单例模式下的属性依赖三级缓存： 第一层缓存（singletonObjects）：单例对象缓存池，已经实例化并且属性赋值，这里的对象是成熟对象； 第二层缓存（earlySingletonObjects）：单例对象缓存池，已经实例化但尚未属性赋值，这里的对象是半成品对象； 第三层缓存（singletonFactories）: 单例工厂的缓存 解决循环依赖的关键是通过将实例化但是还没有初始化的对象提高曝光在第三级缓存 singtonfactory 好比“A对象setter依赖B对象，B对象setter依赖A对象”，A首先实例化并将本身提早曝光到singletonFactories中，此时要进行属性赋值，发现本身依赖对象B，此时就尝试去get(B)，发现B尚未被create，因此走create流程，B在属性赋值的时候发现本身依赖了对象A，因而尝试get(A)，尝试一级缓存singletonObjects(确定没有，由于A还没初始化彻底)，尝试二级缓存earlySingletonObjects（也没有），尝试三级缓存singletonFactories，因为A经过ObjectFactory将本身提早曝光了，因此B可以经过ObjectFactory.getObject拿到A对象(半成品)，B拿到A对象后顺利完成了初始化，彻底初始化以后将本身放入到一级缓存singletonObjects中。此时返回A中，A此时能拿到B的对象顺利完成本身的初始化阶段，最终A也完成了初始化，进去了一级缓存singletonObjects中，并且更加幸运的是，因为B拿到了A的对象引用，因此B如今hold住的A对象完成了初始化。 Spring为何不能解决非单例属性之外的循环依赖？Spring为什么不能解决构造器的循环依赖？构造器注入形成的循环依赖： 也就是beanB需要在beanA的构造函数中完成初始化，beanA也需要在beanB的构造函数中完成初始化，这种情况的结果就是两个bean都不能完成初始化，循环依赖难以解决。 Spring解决循环依赖主要是依赖三级缓存，但是的在调用构造方法之前还未将其放入三级缓存之中，因此后续的依赖调用构造方法的时候并不能从三级缓存中获取到依赖的Bean，因此不能解决。 Spring为什么不能解决prototype作用域循环依赖？这种循环依赖同样无法解决，因为spring不会缓存‘prototype’作用域的bean，而spring中循环依赖的解决正是通过缓存来实现的。 Spring为什么不能解决多例的循环依赖？多实例Bean是每次调用一次getBean都会执行一次构造方法并且给属性赋值，根本没有三级缓存，因此不能解决循环依赖。 那么其它循环依赖如何解决？ 生成代理对象产生的循环依赖 这类循环依赖问题解决方法很多，主要有： 使用@Lazy注解，延迟加载 使用@DependsOn注解，指定加载先后关系 修改文件名称，改变循环依赖类的加载顺序 使用@DependsOn产生的循环依赖 这类循环依赖问题要找到@DependsOn注解循环依赖的地方，迫使它不循环依赖就可以解决问题。 多例循环依赖 这类循环依赖问题可以通过把bean改成单例的解决。 构造器循环依赖 这类循环依赖问题可以通过使用@Lazy注解解决。 *SpringMVC 执行流程 用户请求-&gt;前端控制器（dispatcherServlet）。前端控制器收到请求后自己不进行处理，而是委托给其他的解析器进行处理。 dispatcherServlet-&gt;handlerMapping。HandlerMapping 将会把请求映射为 HandlerExecutionChain 对象（包含一 个Handler 处理器（页面控制器）对象、多个HandlerInterceptor 拦截器）对象，通过这种策略模式，很容易添加新的映射策略； dispatcherServlet-&gt;handlerAdapter，HandlerAdapter 将会把处理器包装为适配器，从而支持多种类型的处理器 handlerAdaptor-&gt;handler 功能方法调用，HandlerAdapter 将会根据适配的结果调用真正的处理器的功能处 理方法，完成功能处理；并返回一个ModelAndView 对象（包含模型数据、逻辑视图名）； ModelAndView-&gt;viewResolver，ViewResolver 将把逻辑视图名解析为具体的View，通过这种策 略模式，很容易更换其他视图技术； View-&gt;渲染，返回响应给用户。 Spring 事务声明式事务底层原理Spring容器在初始化每个单例bean的时候，会遍历容器中的所有BeanPostProcessor实现类，并执行其postProcessAfterInitialization方法，在执行AbstractAutoProxyCreator类的postProcessAfterInitialization方法时会创建代理对象。在创建代理的过程中会获取当前目标方法对应的拦截器，此时会得到TransactionInterceptor实例，在它的invoke方法中实现事务的开启和回滚。 例子： 如果在类A上标注Transactional注解，Spring容器会在启动的时候，为类A创建一个代理类B，类A的所有public方法都会在代理类B中有一个对应的代理方法，调用类A的某个public方法会进入对应的代理方法中进行处理；如果只在类A的b方法(使用public修饰)上标注Transactional注解，Spring容器会在启动的时候，为类A创建一个代理类B，但只会为类A的b方法创建一个代理方法，调用类A的b方法会进入对应的代理方法中进行处理，调用类A的其它public方法，则还是进入类A的方法中处理。在进入代理类的某个方法之前，会先执行TransactionInterceptor类中的invoke方法，完成整个事务处理的逻辑，如是否开启新事务、在目标方法执行期间监测是否需要回滚事务、目标方法执行完成后提交事务等。 事务失效的场景 方法不是 public 方法被 final 修饰 方法被 static 修饰 方法内部调用：有时候我们需要在某个 Service 类的某个方法中，调用另外一个事务方法 未被 spring 管理","link":"/2024/03/12/Spring/"}],"tags":[{"name":"面经","slug":"面经","link":"/tags/%E9%9D%A2%E7%BB%8F/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"},{"name":"cs","slug":"cs","link":"/categories/cs/"},{"name":"中间件","slug":"中间件","link":"/categories/%E4%B8%AD%E9%97%B4%E4%BB%B6/"}],"pages":[]}